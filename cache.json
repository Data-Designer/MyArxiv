{"2026-02-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2602.20135v1","updated":"2026-02-23T18:46:27Z","published":"2026-02-23T18:46:27Z","title":"KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration","summary":"With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.","authors":["Mohammad Amanlou","Erfan Shafiee Moghaddam","Yasaman Amou Jafari","Mahdi Noori","Farhan Farsi","Behnam Bahrak"],"pdf_url":"","comment":"Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)"},{"id":"http://arxiv.org/abs/2602.20133v1","updated":"2026-02-23T18:45:31Z","published":"2026-02-23T18:45:31Z","title":"AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization","summary":"The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.","authors":["Mert Cemri","Shubham Agrawal","Akshat Gupta","Shu Liu","Audrey Cheng","Qiuyang Mang","Ashwin Naren","Lutfi Eren Erdogan","Koushik Sen","Matei Zaharia","Alex Dimakis","Ion Stoica"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20130v1","updated":"2026-02-23T18:42:50Z","published":"2026-02-23T18:42:50Z","title":"To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering","summary":"Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.","authors":["Zaifu Zhan","Min Zeng","Shuang Zhou","Yiran Song","Xiaoyi Chen","Yu Hou","Yifan Wu","Yang Ruan","Rui Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20122v1","updated":"2026-02-23T18:37:49Z","published":"2026-02-23T18:37:49Z","title":"NanoKnow: How to Know What Your Language Model Knows","summary":"How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.","authors":["Lingwei Gu","Nour Jedidi","Jimmy Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.07751v4","updated":"2026-02-23T18:25:13Z","published":"2025-06-09T13:34:50Z","title":"AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking","summary":"Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.","authors":["Silin Gao","Antoine Bosselut","Samy Bengio","Emmanuel Abbe"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.05165v3","updated":"2026-02-23T18:23:57Z","published":"2026-02-05T00:33:02Z","title":"EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.","authors":["Kevin Han","Yuhang Zhou","Mingze Gao","Gedi Zhou","Serena Li","Abhishek Kumar","Xiangjun Fan","Weiwei Li","Lizhu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17842v2","updated":"2026-02-23T18:12:05Z","published":"2025-07-23T18:10:43Z","title":"Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning","summary":"Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html.","authors":["Yimeng Zhang","Tian Wang","Jiri Gesi","Ziyi Wang","Yuxuan Lu","Jiacheng Lin","Sinong Zhan","Vianne Gao","Ruochen Jiao","Junze Liu","Kun Qian","Yuxin Tang","Ran Xue","Houyu Zhang","Qingjun Cui","Yufan Guo","Dakuo Wang"],"pdf_url":"","comment":"Accepted by ICLR 2026. The project page is available at https://damon-demon.github.io/shop-r1.html"},{"id":"http://arxiv.org/abs/2510.13632v2","updated":"2026-02-23T18:05:51Z","published":"2025-10-15T14:57:16Z","title":"Closing the Gap Between Text and Speech Understanding in LLMs","summary":"Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.","authors":["Santiago Cuervo","Skyler Seto","Maureen de Seyssel","Richard He Bai","Zijin Gu","Tatiana Likhomanenko","Navdeep Jaitly","Zakaria Aldeneh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20092v1","updated":"2026-02-23T18:02:23Z","published":"2026-02-23T18:02:23Z","title":"BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop","summary":"BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.\n  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.","authors":["Leshem Choshen","Ryan Cotterell","Mustafa Omer Gul","Jaap Jumelet","Tal Linzen","Aaron Mueller","Suchir Salhan","Raj Sanjay Shah","Alex Warstadt","Ethan Gotlieb Wilcox"],"pdf_url":"","comment":"8 pages, 1 table. arXiv admin note: substantial text overlap with arXiv:2502.10645"},{"id":"http://arxiv.org/abs/2602.20091v1","updated":"2026-02-23T18:02:04Z","published":"2026-02-23T18:02:04Z","title":"How Retrieved Context Shapes Internal Representations in RAG","summary":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.","authors":["Samuel Yeh","Sharon Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.14242v3","updated":"2026-02-23T17:49:38Z","published":"2026-01-20T18:53:44Z","title":"APEX-Agents","summary":"We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open source Archipelago, our infrastructure for agent execution and evaluation.","authors":["Bertie Vidgen","Austin Mann","Abby Fennelly","John Wright Stanly","Lucas Rothman","Marco Burstein","Julien Benchek","David Ostrofsky","Anirudh Ravichandran","Debnil Sur","Neel Venugopal","Alannah Hsia","Isaac Robinson","Calix Huang","Olivia Varones","Daniyal Khan","Michael Haines","Austin Bridges","Jesse Boyle","Koby Twist","Zach Richards","Chirag Mahapatra","Brendan Foody","Osvald Nitski"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22842v3","updated":"2026-02-23T17:45:58Z","published":"2025-05-28T20:22:23Z","title":"Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation","summary":"Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.","authors":["Arthur S. Bianchessi","Yasmin C. Aguirre","Rodrigo C. Barros","Lucas S. Kupssinskü"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.13139v2","updated":"2026-02-23T17:38:41Z","published":"2026-02-13T17:47:08Z","title":"OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report","summary":"Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.","authors":["Mariia Fedorova","Nikolay Arefyev","Maja Buljan","Jindřich Helcl","Stephan Oepen","Egil Rønningstad","Yves Scherrer"],"pdf_url":"","comment":"VarDial'26 workshop at the EACL 2026 conference"},{"id":"http://arxiv.org/abs/2602.20065v1","updated":"2026-02-23T17:22:46Z","published":"2026-02-23T17:22:46Z","title":"Multilingual Large Language Models do not comprehend all natural languages to equal degrees","summary":"Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.","authors":["Natalia Moskvina","Raquel Montero","Masaya Yoshida","Ferdy Hubers","Paolo Morosi","Walid Irhaymi","Jin Yan","Tamara Serrano","Elena Pagliarini","Fritz Günther","Evelina Leivada"],"pdf_url":"","comment":"36 pages, 3 figures, 2 tables, 4 supplementary tables"},{"id":"http://arxiv.org/abs/2602.20052v1","updated":"2026-02-23T17:02:45Z","published":"2026-02-23T17:02:45Z","title":"Entropy in Large Language Models","summary":"In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.","authors":["Marco Scharringhausen"],"pdf_url":"","comment":"7 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.09109v4","updated":"2026-02-23T16:56:22Z","published":"2024-11-14T00:52:45Z","title":"Personalized Help for Optimizing Low-Skilled Users' Strategy","summary":"AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment CICERO, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it.","authors":["Feng Gu","Wichayaporn Wongkamjan","Jonathan K. Kummerfeld","Denis Peskoff","Jonathan May","Jordan Boyd-Graber"],"pdf_url":"","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2602.20042v1","updated":"2026-02-23T16:51:43Z","published":"2026-02-23T16:51:43Z","title":"Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously","summary":"Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.","authors":["Han Bao","Yue Huang","Xiaoda Wang","Zheyuan Zhang","Yujun Zhou","Carl Yang","Xiangliang Zhang","Yanfang Ye"],"pdf_url":"","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.20040v1","updated":"2026-02-23T16:49:37Z","published":"2026-02-23T16:49:37Z","title":"AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization","summary":"Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.","authors":["Fahmida Liza Piya","Rahmatollah Beheshti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.06932v3","updated":"2026-02-23T16:39:36Z","published":"2026-01-11T14:36:36Z","title":"Symphonym: Universal Phonetic Embeddings for Cross-Script Name Matching","summary":"Linking names across historical sources, languages, and writing systems remains a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches require language-specific phonetic algorithms or fail to capture phonetic relationships across different scripts. This paper presents Symphonym, a neural embedding system that maps names from any script into a unified 128-dimensional phonetic space, enabling direct similarity comparison without runtime phonetic conversion. Symphonym uses a Teacher-Student architecture where a Teacher network trained on articulatory phonetic features produces target embeddings, while a Student network learns to approximate these embeddings directly from characters. The Teacher combines Epitran (extended with 100 new language-script mappings), Phonikud for Hebrew, and CharsiuG2P for Chinese, Japanese, and Korean. Training used 32.7 million triplet samples of toponyms spanning 20 writing systems from GeoNames, Wikidata, and Getty Thesaurus of Geographic Names. On the MEHDIE Hebrew-Arabic historical toponym benchmark, Symphonym achieves Recall@10 of 97.6% and MRR of 90.3%, outperforming Levenshtein and Jaro-Winkler baselines (Recall@1: 86.7% vs 81.5% and 78.5%). Evaluation on 12,947 real cross-script training pairs shows 82.6% achieve greater than 0.75 cosine similarity, with best performance on Arabic-Cyrillic (94--100%) and Cyrillic-Latin (94.3%) combinations. The fixed-length embeddings enable efficient retrieval in digital humanities workflows, with a case study on medieval personal names demonstrating effective transfer from modern place names to historical orthographic variation.","authors":["Stephen Gadd"],"pdf_url":"","comment":"29 pages, 3 tables"},{"id":"http://arxiv.org/abs/2602.20020v1","updated":"2026-02-23T16:28:46Z","published":"2026-02-23T16:28:46Z","title":"gencat: Generative computerized adaptive testing","summary":"Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\\% in the key early testing stages.","authors":["Wanyong Feng","Andrew Lan"],"pdf_url":"","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2602.20017v1","updated":"2026-02-23T16:23:49Z","published":"2026-02-23T16:23:49Z","title":"QUIETT: Query-Independent Table Transformation for Robust Reasoning","summary":"Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.","authors":["Gaurav Najpande","Tampu Ravi Kumar","Manan Roy Choudhury","Neha Valeti","Yanjie Fu","Vivek Gupta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10604v2","updated":"2026-02-23T16:07:40Z","published":"2026-02-11T07:53:51Z","title":"Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters","summary":"We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.","authors":["Ailin Huang","Ang Li","Aobo Kong","Bin Wang","Binxing Jiao","Bo Dong","Bojun Wang","Boyu Chen","Brian Li","Buyun Ma","Chang Su","Changxin Miao","Changyi Wan","Chao Lou","Chen Hu","Chen Xu","Chenfeng Yu","Chengting Feng","Chengyuan Yao","Chunrui Han","Dan Ma","Dapeng Shi","Daxin Jiang","Dehua Ma","Deshan Sun","Di Qi","Enle Liu","Fajie Zhang","Fanqi Wan","Guanzhe Huang","Gulin Yan","Guoliang Cao","Guopeng Li","Han Cheng","Hangyu Guo","Hanshan Zhang","Hao Nie","Haonan Jia","Haoran Lv","Hebin Zhou","Hekun Lv","Heng Wang","Heung-Yeung Shum","Hongbo Huang","Hongbo Peng","Hongyu Zhou","Hongyuan Wang","Houyong Chen","Huangxi Zhu","Huimin Wu","Huiyong Guo","Jia Wang","Jian Zhou","Jianjian Sun","Jiaoren Wu","Jiaran Zhang","Jiashu Lv","Jiashuo Liu","Jiayi Fu","Jiayu Liu","Jie Cheng","Jie Luo","Jie Yang","Jie Zhou","Jieyi Hou","Jing Bai","Jingcheng Hu","Jingjing Xie","Jingwei Wu","Jingyang Zhang","Jishi Zhou","Junfeng Liu","Junzhe Lin","Ka Man Lo","Kai Liang","Kaibo Liu","Kaijun Tan","Kaiwen Yan","Kaixiang Li","Kang An","Kangheng Lin","Lei Yang","Liang Lv","Liang Zhao","Liangyu Chen","Lieyu Shi","Liguo Tan","Lin Lin","Lina Chen","Luck Ma","Mengqiang Ren","Michael Li","Ming Li","Mingliang Li","Mingming Zhang","Mingrui Chen","Mitt Huang","Na Wang","Peng Liu","Qi Han","Qian Zhao","Qinglin He","Qinxin Du","Qiuping Wu","Quan Sun","Rongqiu Yang","Ruihang Miao","Ruixin Han","Ruosi Wan","Ruyan Guo","Shan Wang","Shaoliang Pang","Shaowen Yang","Shengjie Fan","Shijie Shang","Shiliang Yang","Shiwei Li","Shuangshuang Tian","Siqi Liu","Siye Wu","Siyu Chen","Song Yuan","Tiancheng Cao","Tianchi Yue","Tianhao Cheng","Tianning Li","Tingdan Luo","Wang You","Wei Ji","Wei Yuan","Wei Zhang","Weibo Wu","Weihao Xie","Wen Sun","Wenjin Deng","Wenzhen Zheng","Wuxun Xie","Xiangfeng Wang","Xiangwen Kong","Xiangyu Liu","Xiangyu Zhang","Xiaobo Yang","Xiaojia Liu","Xiaolan Yuan","Xiaoran Jiao","Xiaoxiao Ren","Xiaoyun Zhang","Xin Li","Xin Liu","Xin Wu","Xing Chen","Xingping Yang","Xinran Wang","Xu Zhao","Xuan He","Xuanti Feng","Xuedan Cai","Xuqiang Zhou","Yanbo Yu","Yang Li","Yang Xu","Yanlin Lai","Yanming Xu","Yaoyu Wang","Yeqing Shen","Yibo Zhu","Yichen Lv","Yicheng Cao","Yifeng Gong","Yijing Yang","Yikun Yang","Yin Zhao","Yingxiu Zhao","Yinmin Zhang","Yitong Zhang","Yixuan Zhang","Yiyang Chen","Yongchi Zhao","Yongshen Long","Yongyao Wang","Yousong Guan","Yu Zhou","Yuang Peng","Yuanhao Ding","Yuantao Fan","Yuanwei Lu","Yuanzhen Yang","Yuchu Luo","Yudi Zhao","Yue Peng","Yueqiang Lin","Yufan Lu","Yuling Zhao","Yunzhou Ju","Yurong Zhang","Yusheng Li","Yuxiang Yang","Yuyang Chen","Yuzhu Cai","Zejia Weng","Zetao Hong","Zexi Li","Zhe Xie","Zheng Ge","Zheng Gong","Zheng Zeng","Zhenyi Lu","Zhewei Huang","Zhichao Chang","Zhiguo Huang","Zhiheng Hu","Zidong Yang","Zili Wang","Ziqi Ren","Zixin Zhang","Zixuan Wang"],"pdf_url":"","comment":"Technical report for Step 3.5 Flash"},{"id":"http://arxiv.org/abs/2602.19991v1","updated":"2026-02-23T15:57:16Z","published":"2026-02-23T15:57:16Z","title":"Cross-lingual Matryoshka Representation Learning across Speech and Text","summary":"Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.","authors":["Yaya Sy","Dioula Doucouré","Christophe Cerisara","Irina Illina"],"pdf_url":"","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2512.07805v4","updated":"2026-02-23T15:57:05Z","published":"2025-12-08T18:39:13Z","title":"Group Representational Position Encoding","summary":"We present GRAPE (Group Representational Position Encoding), a unified framework for positional encoding based on group actions. GRAPE unifies two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\operatorname{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n) = \\exp(n \\, ω\\, \\mathbf{L})$ with a rank-2 skew-symmetric generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes correspond to canonical coordinate pairs with a log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise from rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Overall, GRAPE provides a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project page: https://github.com/model-architectures/GRAPE.","authors":["Yifan Zhang","Zixiang Chen","Yifeng Liu","Zhen Qin","Huizhuo Yuan","Kangping Xu","Yang Yuan","Quanquan Gu","Andrew Chi-Chih Yao"],"pdf_url":"","comment":"Published in ICLR 2026; Project Page: https://github.com/model-architectures/GRAPE"},{"id":"http://arxiv.org/abs/2408.07543v6","updated":"2026-02-23T15:56:57Z","published":"2024-08-14T13:23:43Z","title":"MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts","summary":"With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.","authors":["Hao Liang","Linzhuang Sun","Minxuan Zhou","Zirong Chen","Meiyi Qiang","Mingan Lin","Tianpeng Li","Fan Yang","Zenan Zhou","Wentao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19969v1","updated":"2026-02-23T15:30:52Z","published":"2026-02-23T15:30:52Z","title":"ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting","summary":"The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.","authors":["Yuxing Tian","Fengran Mo","Weixu Zhang","Yiyan Qi","Jian-Yun Nie"],"pdf_url":"","comment":"Accepted by EACL2026"},{"id":"http://arxiv.org/abs/2602.19961v1","updated":"2026-02-23T15:27:41Z","published":"2026-02-23T15:27:41Z","title":"Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval","summary":"With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.","authors":["Yibo Yan","Jiahao Huo","Guanbo Feng","Mingdong Ou","Yi Cao","Xin Zou","Shuliang Liu","Yuanhuiyi Lyu","Yu Huang","Jungang Li","Kening Zheng","Xu Zheng","Philip S. Yu","James Kwok","Xuming Hu"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.19948v1","updated":"2026-02-23T15:17:18Z","published":"2026-02-23T15:17:18Z","title":"Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming","summary":"Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.","authors":["Ian Steenstra","Paola Pedrelli","Weiyan Shi","Stacy Marsella","Timothy W. Bickmore"],"pdf_url":"","comment":"This paper is a condensed version of the first author's Ph.D. dissertation submitted to Northeastern University"},{"id":"http://arxiv.org/abs/2602.19919v1","updated":"2026-02-23T14:58:51Z","published":"2026-02-23T14:58:51Z","title":"Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling","summary":"Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.","authors":["Xiang Li","Zikai Wei","Yiyan Qi","Wanyun Zhou","Xiang Liu","Penglei Sun","Yongqi Zhang","Xiaowen Chu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19895v1","updated":"2026-02-23T14:37:01Z","published":"2026-02-23T14:37:01Z","title":"DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning","summary":"Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.","authors":["Zhongwei Wan","Yun Shen","Zhihao Dou","Donghao Zhou","Yu Zhang","Xin Wang","Hui Shen","Jing Xiong","Chaofan Tao","Zixuan Zhong","Peizhou Huang","Mi Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02840v2","updated":"2026-02-23T14:32:06Z","published":"2025-12-02T14:53:23Z","title":"promptolution: A Unified, Modular Framework for Prompt Optimization","summary":"Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation.","authors":["Tom Zehle","Timo Heiß","Moritz Schlager","Matthias Aßenmacher","Matthias Feurer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19883v1","updated":"2026-02-23T14:28:13Z","published":"2026-02-23T14:28:13Z","title":"Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection","summary":"ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics that maps each ODRL constraint to the set of knowledge-base concepts satisfying it. Conflict detection reduces to denotation intersection under a three-valued verdict -- Conflict, Compatible, or Unknown -- that is sound under incomplete knowledge. The framework covers all three ODRL composition modes (and, or, xone) and all three semantic domains arising in practice: taxonomic (class subsumption), mereological (part-whole containment), and nominal (identity). For cross-dataspace interoperability, we define order-preserving alignments between knowledge bases and prove two guarantees: conflicts are preserved across different KB standards, and unmapped concepts degrade gracefully to Unknown -- never to false conflicts. A runtime soundness theorem ensures that design-time verdicts hold for all execution contexts. The encoding stays within the decidable EPR fragment of first-order logic. We validate it with 154 benchmarks across six knowledge base families (GeoNames, ISO 3166, W3C DPV, a GDPR-derived taxonomy, BCP 47, and ISO 639-3) and four structural KBs targeting adversarial edge cases. Both the Vampire theorem prover and the Z3 SMT solver agree on all 154 verdicts. A key finding is that exclusive composition (xone) requires strictly stronger KB axioms than conjunction or disjunction: open-world semantics blocks exclusivity even when positive evidence appears to satisfy exactly one branch.","authors":["Daham Mustafa","Diego Collarana","Yixin Peng","Rafiqul Haque","Christoph Lange-Bever","Christoph Quix","Stephan Decker"],"pdf_url":"","comment":"17 pages, 6 tables. Working draft. Supplementary material (154 TPTP/SMT-LIB benchmarks, Isabelle/HOL theory file) will be made available at https://github.com/Daham-Mustaf/odrl-benchmark upon publication"},{"id":"http://arxiv.org/abs/2602.19878v1","updated":"2026-02-23T14:24:46Z","published":"2026-02-23T14:24:46Z","title":"Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics","summary":"Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic.\n  We classify ODRL's left operands by value-domain structure (scalar, dimensional, concept-valued), grounded in the ODRL 2.2 specification text, and show that dimensional ambiguity is intrinsic to the constraint syntax.\n  We present an axis-decomposition framework that refines each dimensional operand into axis-specific scalar operands and prove four properties: deterministic interpretation, AABB completeness, sound over-approximation under projection, and conservative extension.\n  Conflict detection operates in two layers: per-axis verdicts are always decidable; box-level verdicts compose through Strong Kleene conjunction into a three-valued logic (Conflict, Compatible, Unknown). For ODRL's disjunctive (odrl:or) and exclusive-or (odrl:xone) logical constraints, where per-axis decomposition does not apply, the framework encodes coupled multi-axis conjectures directly.\n  We instantiate the framework as the ODRL Spatial Axis Profile--15 axis-specific left operands for the five affected base terms--and evaluate it on 117 benchmark problems spanning nine categories across both TPTP FOF (Vampire) and SMT-LIB (Z3) encodings, achieving full concordance between provers. Benchmark scenarios are inspired by constraints arising in cultural heritage dataspaces such as Datenraum Kultur. All meta-theorems are mechanically verified in Isabelle/HOL.","authors":["Daham Mustafa","Diego Collarana","Yixin Peng","Rafiqul Haque","Christoph Lange-Bever","Christoph Quix","Stephan Decker"],"pdf_url":"","comment":"16 pages, 5 tables. Preprint"},{"id":"http://arxiv.org/abs/2509.11826v2","updated":"2026-02-23T14:07:06Z","published":"2025-09-15T12:11:59Z","title":"Collaborative Document Editing with Multiple Users and AI Agents","summary":"Current AI writing support tools are largely designed for individuals, complicating collaboration when co-writers must leave the shared workspace to use AI and then communicate and reintegrate results. We propose integrating AI agents directly into collaborative writing environments. Our prototype makes AI use visible to all users through two new shared objects: user-defined agent profiles and tasks. Agent responses appear in the familiar comment feature. In a user study (N=30), 14 teams worked on writing projects during one week. Interaction logs and interviews show that teams incorporated agents into existing norms of authorship, control, and coordination, rather than treating them as team members. Agent profiles were viewed as personal territory, while created agents and outputs became shared resources. We discuss implications for team-based AI interaction, highlighting opportunities and boundaries for treating AI as a shared resource in collaborative work.","authors":["Florian Lehmann","Krystsina Shauchenka","Daniel Buschek"],"pdf_url":"","comment":"27 pages, 10 figures, 6 tables, ACM CHI 2026"},{"id":"http://arxiv.org/abs/2601.19657v4","updated":"2026-02-23T14:06:12Z","published":"2026-01-27T14:32:36Z","title":"One Token Is Enough: Improving Diffusion Language Models with a Sink Token","summary":"Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.","authors":["Zihou Zhang","Zheyong Xie","Li Zhong","Haifeng Liu","Yao Hu","Shaosheng Cao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19855v1","updated":"2026-02-23T13:55:36Z","published":"2026-02-23T13:55:36Z","title":"SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals","summary":"We present SHIELD, a novel methodology for automated and integrated safety signal detection in clinical trials. SHIELD combines disproportionality analysis with semantic clustering of adverse event (AE) terms applied to MedDRA term embeddings. For each AE, the pipeline computes an information-theoretic disproportionality measure (Information Component) with effect size derived via empirical Bayesian shrinkage. A utility matrix is constructed by weighting semantic term-term similarities by signal magnitude, followed by spectral embedding and clustering to identify groups of related AEs. Resulting clusters are annotated with syndrome-level summary labels using large language models, yielding a coherent, data-driven representation of treatment-associated safety profiles in the form of a network graph and hierarchical tree. We implement the SHIELD framework in the context of a single-arm incidence summary, to compare two treatment arms or for the detection of any treatment effect in a multi-arm trial. We illustrate its ability to recover known safety signals and generate interpretable, cluster-based summaries in a real clinical trial example. This work bridges statistical signal detection with modern natural language processing to enhance safety assessment and causal interpretation in clinical trials.","authors":["Francois Vandenhende","Anna Georgiou","Theodoros Psaras","Ellie Karekla"],"pdf_url":"","comment":"3 figures, 1 table"},{"id":"http://arxiv.org/abs/2509.11851v2","updated":"2026-02-23T13:51:33Z","published":"2025-09-15T12:31:00Z","title":"The AI Memory Gap: Users Misremember What They Created With AI or Without","summary":"As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.","authors":["Tim Zindulka","Sven Goller","Daniela Fernandes","Robin Welsch","Daniel Buschek"],"pdf_url":"","comment":"22 pages, 10 figures, 10 tables, ACM CHI 2026"},{"id":"http://arxiv.org/abs/2602.18324v2","updated":"2026-02-23T13:49:57Z","published":"2026-02-20T16:24:23Z","title":"PsihoRo: Depression and Anxiety Romanian Text Corpus","summary":"Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.","authors":["Alexandra Ciobotaru","Ana-Maria Bucur","Liviu P. Dinu"],"pdf_url":"","comment":"This article was accepted at LREC 2026"},{"id":"http://arxiv.org/abs/2512.24943v2","updated":"2026-02-23T13:41:40Z","published":"2025-12-31T16:09:08Z","title":"RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment","summary":"Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.","authors":["Chenji Lu","Zhuo Chen","Hui Zhao","Zhenyi Wang","Pengjie Wang","Chuan Yu","Jian Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19840v1","updated":"2026-02-23T13:40:44Z","published":"2026-02-23T13:40:44Z","title":"SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation","summary":"Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.","authors":["Jingzhuo Wu","Jiajun Zhang","Keyan Jin","Dehua Ma","Junbo Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19815v1","updated":"2026-02-23T13:13:40Z","published":"2026-02-23T13:13:40Z","title":"Keyboards for the Endangered Idu Mishmi Language","summary":"We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities.","authors":["Akhilesh Kakolu Ramarao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.17596v4","updated":"2026-02-23T13:11:55Z","published":"2024-12-23T14:13:44Z","title":"Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context","summary":"While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.","authors":["Kai Ruan","Xuan Wang","Jixiang Hong","Peng Wang","Yang Liu","Hao Sun"],"pdf_url":"","comment":"Updated manuscript and title"},{"id":"http://arxiv.org/abs/2507.11230v3","updated":"2026-02-23T12:16:53Z","published":"2025-07-15T12:00:30Z","title":"Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages","summary":"Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features","authors":["Lyzander Marciano Andrylie","Inaya Rahmanisa","Mahardika Krisna Ihsani","Alfan Farizki Wicaksono","Haryo Akbarianto Wibowo","Alham Fikri Aji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19743v1","updated":"2026-02-23T11:42:56Z","published":"2026-02-23T11:42:56Z","title":"NILE: Formalizing Natural-Language Descriptions of Formal Languages","summary":"This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.","authors":["Tristan Kneisel","Marko Schmellenkamp","Fabian Vehlken","Thomas Zeume"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.13614v3","updated":"2026-02-23T11:42:27Z","published":"2025-10-15T14:43:31Z","title":"MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning","summary":"Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Liming Zhu","Wenjie Zhang"],"pdf_url":"","comment":"Accepted by The Web Conference 2026 (WWW, 2026)"},{"id":"http://arxiv.org/abs/2506.03867v3","updated":"2026-02-23T11:19:00Z","published":"2025-06-04T11:58:18Z","title":"EuroGEST: Investigating gender stereotypes in multilingual language models","summary":"Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are 'beautiful', 'empathetic' and 'neat' and men are 'leaders', 'strong, tough' and 'professional'. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.","authors":["Jacqueline Rowe","Mateusz Klimaszewski","Liane Guillou","Shannon Vallor","Alexandra Birch"],"pdf_url":"","comment":"In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 32074-32096, Suzhou, China. Association for Computational Linguistics. 9 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.22774v2","updated":"2026-02-23T10:36:46Z","published":"2025-05-28T18:43:26Z","title":"Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages","summary":"This paper presents a novel treebank-driven approach to comparing syntactic structures in speech and writing using dependency-parsed corpora. Adopting a fully inductive, bottom-up method, we define syntactic structures as delexicalized dependency (sub)trees and extract them from spoken and written Universal Dependencies (UD) treebanks in two syntactically distinct languages, English and Slovenian. For each corpus, we analyze the size, diversity, and distribution of syntactic inventories, their overlap across modalities, and the structures most characteristic of speech. Results show that, across both languages, spoken corpora contain fewer and less diverse syntactic structures than their written counterparts, with consistent cross-linguistic preferences for certain structural types across modalities. Strikingly, the overlap between spoken and written syntactic inventories is very limited: most structures attested in speech do not occur in writing, pointing to modality-specific preferences in syntactic organization that reflect the distinct demands of real-time interaction and elaborated writing. This contrast is further supported by a keyness analysis of the most frequent speech-specific structures, which highlights patterns associated with interactivity, context-grounding, and economy of expression. We argue that this scalable, language-independent framework offers a useful general method for systematically studying syntactic variation across corpora, laying the groundwork for more comprehensive data-driven theories of grammar in use.","authors":["Kaja Dobrovoljc"],"pdf_url":"","comment":"Accepted manuscript. Published in Corpus Linguistics and Linguistic Theory (2026)"},{"id":"http://arxiv.org/abs/2512.16183v2","updated":"2026-02-23T10:29:46Z","published":"2025-12-18T05:08:26Z","title":"A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media","summary":"Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.","authors":["Mengfan Shen","Kangqi Song","Xindi Wang","Wei Jia","Tao Wang","Ziqiang Han"],"pdf_url":"","comment":"41 pages,3figures and 9 tables"},{"id":"http://arxiv.org/abs/2404.10652v5","updated":"2026-02-23T10:22:38Z","published":"2024-04-16T15:28:30Z","title":"ViTextVQA: A Large-Scale Visual Question Answering Dataset and a Novel Multimodal Feature Fusion Method for Vietnamese Text Comprehension in Images","summary":"Visual Question Answering (VQA) is a challenging task that requires the joint understanding of natural language and visual content. While early research primarily focused on recognizing objects and scene context, it often overlooked scene text-an essential source of explicit semantic information. This paper introduces \\textbf{ViTextVQA} (\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual \\textbf{Q}uestion \\textbf{A}nswering), the first large-scale Vietnamese dataset specializing in text-based VQA. The dataset contains \\textbf{over 16,000} images and \\textbf{over 50,000} question-answer pairs. To tackle this task efficiently, \\textbf{ViTextBLIP-2} (Vietnamese Text-based Bootstrapped Language-Image Model via Fine-tuning) is proposed, a novel multimodal feature fusion method designed to optimize Vietnamese text-based VQA. Experiments with state-of-the-art models highlight the importance of token ordering in OCR text for answer generation, leading to significant performance improvements. The ViTextVQA dataset is publicly available for research purposes.","authors":["Quan Van Nguyen","Dan Quang Tran","Huy Quang Pham","Thang Kien-Bao Nguyen","Nghia Hieu Nguyen","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"","comment":"International Journal of Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2507.23465v3","updated":"2026-02-23T10:22:06Z","published":"2025-07-31T11:41:04Z","title":"Role-Aware Language Models for Secure and Contextualized Access Control in Organizations","summary":"As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.","authors":["Saeed Almheiri","Yerulan Kongrat","Adrian Santosh","Ruslan Tasmukhanov","Josemaria Loza Vera","Muhammad Dehan Al Kautsar","Fajri Koto"],"pdf_url":"","comment":"AACL 2025 - Main"},{"id":"http://arxiv.org/abs/2602.19643v1","updated":"2026-02-23T09:41:46Z","published":"2026-02-23T09:41:46Z","title":"KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge","summary":"Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.","authors":["Alex Robertson","Huizhi Liang","Mahbub Gani","Rohit Kumar","Srijith Rajamohan"],"pdf_url":"","comment":"EACL 2026 Findings"},{"id":"http://arxiv.org/abs/2602.19626v1","updated":"2026-02-23T09:14:05Z","published":"2026-02-23T09:14:05Z","title":"Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding","summary":"We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.","authors":["Roberto Tacconelli"],"pdf_url":"","comment":"10 pages"},{"id":"http://arxiv.org/abs/2602.19612v1","updated":"2026-02-23T08:58:48Z","published":"2026-02-23T08:58:48Z","title":"Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning","summary":"Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.","authors":["Borisiuk Anna","Andrey Savchenko","Alexander Panchecko","Elena Tutubalina"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.13033v2","updated":"2026-02-23T08:47:46Z","published":"2026-02-13T15:39:31Z","title":"Buy versus Build an LLM: A Decision Framework for Governments","summary":"Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.\n  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.","authors":["Jiahao Lu","Ziwei Xu","William Tjhi","Junnan Li","Antoine Bosselut","Pang Wei Koh","Mohan Kankanhalli"],"pdf_url":"","comment":"The short version of this document is published as an ACM TechBrief at https://dl.acm.org/doi/epdf/10.1145/3797946, and this document is published as an ACM Technology Policy Council white paper at https://www.acm.org/binaries/content/assets/public-policy/buildvsbuyai.pdf"},{"id":"http://arxiv.org/abs/2602.19598v1","updated":"2026-02-23T08:40:50Z","published":"2026-02-23T08:40:50Z","title":"Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support","summary":"Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.","authors":["Deborah N. Jakobi","David R. Reich","Paul Prasse","Jana M. Hofmann","Lena S. Bolliger","Lena A. Jäger"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19583v1","updated":"2026-02-23T08:08:57Z","published":"2026-02-23T08:08:57Z","title":"DEEP: Docker-based Execution and Evaluation Platform","summary":"Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.","authors":["Sergio Gómez González","Miguel Domingo","Francisco Casacuberta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.05850v3","updated":"2026-02-23T08:02:48Z","published":"2025-06-06T08:08:48Z","title":"Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models","summary":"Reinforcement learning with verifiable reward (RLVR) has been instrumental in eliciting strong reasoning capabilities from large language models (LLMs) via long chains of thought (CoT). During RLVR training, we formalize and systemically study an empirical phenomenon whereby a multilingual model's CoT reverts to its dominant pre-training language (e.g., English) even when prompted in another language, which we term Cross-lingual Collapse. Because the long-CoT regime magnifies exposure to linguistic priors, the underlying trade-off between maximizing reasoning depth and preserving target-language fidelity has remained under-characterized. To examine this trade-off, we train LLMs with Group-Relative Policy Optimization (GRPO) on translated versions of math datasets widely used to elicit long-CoT reasoning. Throughout training, we track both task accuracy and the language consistency of reasoning chains. Our experiments yield three findings: (i) under RLVR, CoT in LLMs systematically drifts toward the pre-training dominant language as reasoning performance rises; (ii) English-centric priors, long-CoT GRPO optimization, task difficulty, and high-entropy decoding jointly amplify this drift, and the pattern persists beyond mathematics; and (iii) interventions that favor target-language traces--via a language-consistency reward, decoding-time controls, or more balanced backbones--mitigate collapse but reveal a persistent performance-fidelity trade-off.","authors":["Cheonbok Park","Jeonghoon Kim","Joosung Lee","Sanghwan Bae","Jaegul Choo","Kang Min Yoo"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2402.13904v2","updated":"2026-02-23T07:37:56Z","published":"2024-02-21T16:15:20Z","title":"Calibrating Large Language Models with Sample Consistency","summary":"Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.","authors":["Qing Lyu","Kumar Shridhar","Chaitanya Malaviya","Li Zhang","Yanai Elazar","Niket Tandon","Marianna Apidianaki","Mrinmaya Sachan","Chris Callison-Burch"],"pdf_url":"","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2602.19569v1","updated":"2026-02-23T07:36:36Z","published":"2026-02-23T07:36:36Z","title":"Temporal-Aware Heterogeneous Graph Reasoning with Multi-View Fusion for Temporal Question Answering","summary":"Question Answering over Temporal Knowledge Graphs (TKGQA) has attracted growing interest for handling time-sensitive queries. However, existing methods still struggle with: 1) weak incorporation of temporal constraints in question representation, causing biased reasoning; 2) limited ability to perform explicit multi-hop reasoning; and 3) suboptimal fusion of language and graph representations. We propose a novel framework with temporal-aware question encoding, multi-hop graph reasoning, and multi-view heterogeneous information fusion. Specifically, our approach introduces: 1) a constraint-aware question representation that combines semantic cues from language models with temporal entity dynamics; 2) a temporal-aware graph neural network for explicit multi-hop reasoning via time-aware message passing; and 3) a multi-view attention mechanism for more effective fusion of question context and temporal graph knowledge. Experiments on multiple TKGQA benchmarks demonstrate consistent improvements over multiple baselines.","authors":["Wuzhenghong Wen","Bowen Zhou","Jinwen Huang","Xianjie Wu","Yuwei Sun","Su Pan","Liang Li","Jianting Liu"],"pdf_url":"","comment":"6pages"},{"id":"http://arxiv.org/abs/2410.02099v3","updated":"2026-02-23T07:20:10Z","published":"2024-10-02T23:39:19Z","title":"A Watermark for Black-Box Language Models","summary":"Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.","authors":["Dara Bahri","John Wieting"],"pdf_url":"","comment":"Published at TMLR 2026"},{"id":"http://arxiv.org/abs/2602.19549v1","updated":"2026-02-23T06:45:19Z","published":"2026-02-23T06:45:19Z","title":"Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework","summary":"Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.","authors":["Yibo Yan","Mingdong Ou","Yi Cao","Xin Zou","Jiahao Huo","Shuliang Liu","James Kwok","Xuming Hu"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.19548v1","updated":"2026-02-23T06:41:57Z","published":"2026-02-23T06:41:57Z","title":"Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining","summary":"One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.","authors":["Jeffrey Li","Josh Gardner","Doug Kang","Fangping Shi","Karanjeet Singh","Chun-Liang Li","Herumb Shandilya","David Hall","Oncel Tuzel","Percy Liang","Ludwig Schmidt","Hadi Pour Ansari","Fartash Faghri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19543v1","updated":"2026-02-23T06:32:00Z","published":"2026-02-23T06:32:00Z","title":"Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation","summary":"Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \\textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \\textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \\textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \\textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \\textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.","authors":["Rizhuo Huang","Yifan Feng","Rundong Xue","Shihui Ying","Jun-Hai Yong","Chuan Shi","Shaoyi Du","Yue Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.15620v3","updated":"2026-02-23T06:22:49Z","published":"2026-02-17T14:46:48Z","title":"STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens","summary":"Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design an S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy, and JustRL.","authors":["Shiqi Liu","Zeyu He","Guojian Zhan","Letian Tao","Zhilong Zheng","Jiang Wu","Yinuo Wang","Yang Guan","Kehua Sheng","Bo Zhang","Keqiang Li","Jingliang Duan","Shengbo Eben Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.03688v2","updated":"2026-02-23T06:00:29Z","published":"2025-12-03T11:27:50Z","title":"AITutor-EvalKit: Exploring the Capabilities of AI Tutors","summary":"We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotation.","authors":["Numaan Naeem","Kaushal Kumar Maurya","Kseniia Petukhova","Ekaterina Kochmar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19526v1","updated":"2026-02-23T05:33:17Z","published":"2026-02-23T05:33:17Z","title":"How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1","summary":"Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of RL, we conduct a systematic study along three decoupled dimensions: prompt template, reward function, and policy optimization. Our study reveals that: 1) the Fast Thinking template yields greater stability and better performance than the Slow Thinking template used in prior work; 2) the F1-based reward underperforms the EM due to training collapse driven by answer avoidance; this can be mitigated by incorporating action-level penalties, ultimately surpassing EM; 3) REINFORCE outperforms PPO while requiring fewer search actions, whereas GRPO shows the poorest stability among policy optimization methods. Building on these insights, we then introduce Search-R1++, a strong baseline that improves the performance of Search-R1 from 0.403 to 0.442 (Qwen2.5-7B) and 0.289 to 0.331 (Qwen2.5-3B). We hope that our findings can pave the way for more principled and reliable RL training strategies in Deep Research systems.","authors":["Yinuo Xu","Shuo Lu","Jianjie Cheng","Meng Wang","Qianlong Xie","Xingxing Wang","Ran He","Jian Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.07231v3","updated":"2026-02-23T05:21:42Z","published":"2025-10-08T17:00:49Z","title":"EconCausal: A Context-Aware Causal Reasoning Benchmark for Large Language Models in Social Science","summary":"Socio-economic causal effects depend heavily on their specific institutional and environmental context. A single intervention can produce opposite results depending on regulatory or market factors, contexts that are often complex and only partially observed. This poses a significant challenge for large language models (LLMs) in decision-support roles: can they distinguish structural causal mechanisms from surface-level correlations when the context changes?\n  To address this, we introduce EconCausal, a large-scale benchmark comprising 10,490 context-annotated causal triplets extracted from 2,595 high-quality empirical studies published in top-tier economics and finance journals. Through a rigorous four-stage pipeline combining multi-run consensus, context refinement, and multi-critic filtering, we ensure each claim is grounded in peer-reviewed research with explicit identification strategies.\n  Our evaluation reveals critical limitations in current LLMs' context-dependent reasoning. While top models achieve approximately 88 percent accuracy in fixed, explicit contexts, performance drops sharply under context shifts, with a 32.6 percentage point decline, and falls to 37 percent when misinformation is introduced. Furthermore, models exhibit severe over-commitment in ambiguous cases and struggle to recognize null effects, achieving only 9.5 percent accuracy, exposing a fundamental gap between pattern matching and genuine causal reasoning. These findings underscore substantial risks for high-stakes economic decision-making, where the cost of misinterpreting causality is high.\n  The dataset and benchmark are publicly available at https://github.com/econaikaist/econcausal-benchmark.","authors":["Donggyu Lee","Hyeok Yun","Meeyoung Cha","Sungwon Park","Sangyoon Park","Jihee Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19517v1","updated":"2026-02-23T05:17:41Z","published":"2026-02-23T05:17:41Z","title":"Classroom Final Exam: An Instructor-Tested Reasoning Benchmark","summary":"We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.","authors":["Chongyang Gao","Diji Yang","Shuyan Zhou","Xichen Yan","Luchuan Song","Shuo Li","Kezhen Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19509v1","updated":"2026-02-23T04:47:47Z","published":"2026-02-23T04:47:47Z","title":"Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference","summary":"Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.","authors":["Arindam Khaled"],"pdf_url":"","comment":"6 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2507.05387v4","updated":"2026-02-23T04:24:06Z","published":"2025-07-07T18:18:51Z","title":"The Generalization Ridge: Information Flow in Natural Language Generation","summary":"Transformer-based language models have achieved state-of-the-art performance in natural language generation (NLG), yet their internal mechanisms for synthesizing task-relevant information remain insufficiently understood. While prior studies suggest that intermediate layers often yield more generalizable representations than final layers, how this generalization ability emerges and propagates across layers during training remains unclear. To address this gap, we propose InfoRidge, an information-theoretic framework, to characterize how predictive information-the mutual information between hidden representations and target outputs-varies across depth during training. Our experiments across various models and datasets reveal a consistent non-monotonic trend: predictive information peaks in intermediate layers-forming a generalization ridge-before declining in final layers, reflecting a transition between generalization and memorization. To further investigate this phenomenon, we conduct a set of complementary analyses that leverage residual scaling, attention pattern, and controlled model capacity to characterize layer-wise functional specialization. We further validate our findings with multiple-token generation experiments, verifying that the observed ridge phenomenon persists across decoding steps. Together, these findings offer new insights into the internal mechanisms of transformers and underscore the critical role of intermediate layers in supporting generalization.","authors":["Ruidi Chang","Chunyuan Deng","Hanjie Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17621v2","updated":"2026-02-23T04:13:05Z","published":"2025-11-18T16:47:15Z","title":"From Competition to Coordination: Market Making as a Scalable Framework for Safe and Aligned Multi-Agent LLM Systems","summary":"As foundation models are increasingly deployed as interacting agents in multi-agent systems, their collective behavior raises new challenges for trustworthiness, transparency, and accountability. Traditional coordination mechanisms, such as centralized oversight or adversarial adjudication, struggle to scale and often obscure how decisions emerge. We introduce a market-making framework for multi-agent large language model (LLM) coordination that organizes agent interactions as structured economic exchanges. In this setup, each agent acts as a market participant, updating and trading probabilistic beliefs, to converge toward shared, truthful outcomes. By aligning local incentives with collective epistemic goals, the framework promotes self-organizing, verifiable reasoning without requiring external enforcement. Empirically, we evaluate this approach across factual reasoning, ethical judgment, and commonsense inference tasks. Market-based coordination yields accuracy gains of up to 10% over single-shot baselines while preserving interpretability and transparency of intermediate reasoning steps. Beyond these improvements, our findings demonstrate that economic coordination principles can operationalize accountability and robustness in multi-agent LLM systems, offering a scalable pathway toward self-correcting, socially responsible AI capable of maintaining trust and oversight in real world deployment scenarios.","authors":["Brendan Gho","Suman Muppavarapu","Afnan Shaik","Tyson Tsay","Atharva Mohan","James Begin","Kevin Zhu","Archana Vaidheeswaran","Vasu Sharma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.05220v2","updated":"2026-02-23T04:04:08Z","published":"2026-02-05T02:20:07Z","title":"Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions","summary":"Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.","authors":["Jinchuan Tian","Haoran Wang","Bo-Hao Su","Chien-yu Huang","Qingzheng Wang","Jiatong Shi","William Chen","Xun Gong","Siddhant Arora","Chin-Jou Li","Masao Someki","Takashi Maekaku","Keita Goto","Yusuke Shinohara","Jin Sakuma","Chao-Han Huck Yang","Shinji Watanabe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21877v2","updated":"2026-02-23T03:34:47Z","published":"2025-12-26T05:59:19Z","title":"CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics","summary":"Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.","authors":["Vaibhav Devraj","Dhruv Kumar","Jagat Sesh Challa","Parth Agarwal","Navya Kommuri","Trizal Garg","Prisha Singhal","Dhruv Shah"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2602.19467v1","updated":"2026-02-23T03:26:17Z","published":"2026-02-23T03:26:17Z","title":"Can Large Language Models Replace Human Coders? Introducing ContentBench","summary":"Can low-cost large language models (LLMs) take over the interpretive coding work that still anchors much of empirical content analysis? This paper introduces ContentBench, a public benchmark suite that helps answer this replacement question by tracking how much agreement low-cost LLMs achieve and what they cost on the same interpretive coding tasks. The suite uses versioned tracks that invite researchers to contribute new benchmark datasets. I report results from the first track, ContentBench-ResearchTalk v1.0: 1,000 synthetic, social-media-style posts about academic research labeled into five categories spanning praise, critique, sarcasm, questions, and procedural remarks. Reference labels are assigned only when three state-of-the-art reasoning models (GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1) agree unanimously, and all final labels are checked by the author as a quality-control audit. Among the 59 evaluated models, the best low-cost LLMs reach roughly 97-99% agreement with these jury labels, far above GPT-3.5 Turbo, the model behind early ChatGPT and the initial wave of LLM-based text annotation. Several top models can code 50,000 posts for only a few dollars, pushing large-scale interpretive coding from a labor bottleneck toward questions of validation, reporting, and governance. At the same time, small open-weight models that run locally still struggle on sarcasm-heavy items (for example, Llama 3.2 3B reaches only 4% agreement on hard-sarcasm). ContentBench is released with data, documentation, and an interactive quiz at contentbench.github.io to support comparable evaluations over time and to invite community extensions.","authors":["Michael Haman"],"pdf_url":"","comment":"Project website: https://contentbench.github.io"},{"id":"http://arxiv.org/abs/2602.19463v1","updated":"2026-02-23T03:17:27Z","published":"2026-02-23T03:17:27Z","title":"PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives","summary":"As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.","authors":["Emma Jiren Wang","Siying Hu","Zhicong Lu"],"pdf_url":"","comment":"19 pages, 8 figures; Accepted by ACM CHI 2026. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI'24)"},{"id":"http://arxiv.org/abs/2509.21730v2","updated":"2026-02-23T03:07:59Z","published":"2025-09-26T00:57:27Z","title":"ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation","summary":"As large language models (LLMs) become increasingly integrated into daily life, there is growing demand for AI assistants that are not only reactive but also proactive and personalized. While recent advances have pushed forward proactivity and personalization individually, their combination remains underexplored. To bridge this gap, we introduce ProPerSim, a new task and simulation framework for developing assistants capable of making timely, personalized recommendations in realistic home scenarios. In our simulation environment, a user agent with a rich persona interacts with the assistant, providing ratings on how well each suggestion aligns with its preferences and context. The assistant's goal is to use these ratings to learn and adapt to achieve higher scores over time. Built on ProPerSim, we propose ProPerAssistant, a retrieval-augmented, preference-aligned assistant that continually learns and adapts through user feedback. Experiments across 32 diverse personas show that ProPerAssistant adapts its strategy and steadily improves user satisfaction, highlighting the promise of uniting proactivity and personalization.","authors":["Jiho Kim","Junseong Choi","Woosog Chay","Daeun Kyung","Yeonsu Kwon","Yohan Jo","Edward Choi"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19455v1","updated":"2026-02-23T02:55:32Z","published":"2026-02-23T02:55:32Z","title":"SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning","summary":"Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.","authors":["Zelin He","Boran Han","Xiyuan Zhang","Shuai Zhang","Haotian Lin","Qi Zhu","Haoyang Fang","Danielle C. Maddix","Abdul Fatir Ansari","Akash Chandrayan","Abhinav Pradhan","Bernie Wang","Matthew Reimherr"],"pdf_url":"","comment":"Accepted by the 29th International Conference on Artificial Intelligence and Statistics (AISTATS 2026)"},{"id":"http://arxiv.org/abs/2602.17053v3","updated":"2026-02-23T02:04:32Z","published":"2026-02-19T03:49:37Z","title":"RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models","summary":"Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: https://aidaslab.github.io/RFEval/","authors":["Yunseok Han","Yejoon Lee","Jaeyoung Do"],"pdf_url":"","comment":"Accepted in ICLR 2026 Poster: https://iclr.cc/virtual/2026/poster/10011763"},{"id":"http://arxiv.org/abs/2602.01560v3","updated":"2026-02-23T02:00:38Z","published":"2026-02-02T02:54:29Z","title":"Argument Rarity-based Originality Assessment for AI-Assisted Writing","summary":"This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth, quantified via density estimation and integrated with quality adjustment. Experiments using 1,375 human essays and 1,000 AI-generated essays on two argumentative topics revealed three key findings. First, a strong negative correlation (r = -0.67) between text quality and claim rarity demonstrates a quality-originality trade-off. Second, while AI essays achieved near-perfect quality scores (Q = 0.998), their claim rarity was approximately one-fifth of human levels (AI: 0.037, human: 0.170), indicating that LLMs can reproduce argumentative structure but not semantic originality. Third, the four components showed low mutual correlations (r = 0.06--0.13 between structural and semantic dimensions), confirming that they capture genuinely independent aspects of originality. These results suggest that writing assessment in the AI era must shift from quality to originality.","authors":["Keito Inoshita","Michiaki Omura","Tsukasa Yamanaka","Go Maeda","Kentaro Tsuji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19403v1","updated":"2026-02-23T00:32:23Z","published":"2026-02-23T00:32:23Z","title":"Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins","summary":"Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages.\n  We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1.\n  LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences.\n  Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.","authors":["Jasmin Han","Janardan Devkota","Joseph Waring","Amanda Luken","Felix Naughton","Roger Vilardaga","Jonathan Bricker","Carl Latkin","Meghan Moran","Yiqun Chen","Johannes Thrul"],"pdf_url":"","comment":"31 pages, 5 figures, submitted to Journal of the American Medical Informatics Association (JAMIA). Drs. Chen and Thrul share last authorship"},{"id":"http://arxiv.org/abs/2602.20423v1","updated":"2026-02-23T23:46:05Z","published":"2026-02-23T23:46:05Z","title":"MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation","summary":"Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.","authors":["Taha Koleilat","Hojat Asgariandehkordi","Omid Nejati Manzari","Berardino Barile","Yiming Xiao","Hassan Rivaz"],"pdf_url":"","comment":"CVPR 2026; Project Page: https://tahakoleilat.github.io/MedCLIPSeg"},{"id":"http://arxiv.org/abs/2602.14044v2","updated":"2026-02-23T22:32:36Z","published":"2026-02-15T08:15:13Z","title":"Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness","summary":"Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.","authors":["Pietro Bernardelle","Stefano Civelli","Kevin Roitero","Gianluca Demartini"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.05182v2","updated":"2026-02-23T22:05:59Z","published":"2026-02-05T01:20:32Z","title":"The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems","summary":"Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.","authors":["Shangbin Feng","Kishan Panaganti","Yulia Tsvetkov","Wenhao Yu"],"pdf_url":"","comment":"Code at https://github.com/BunsenFeng/moco_distill"},{"id":"http://arxiv.org/abs/2602.16729v2","updated":"2026-02-23T21:46:20Z","published":"2026-02-17T18:29:22Z","title":"Intent Laundering: AI Safety Datasets Are Not What They Seem","summary":"We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave.","authors":["Shahriar Golchin","Marc Wetter"],"pdf_url":"","comment":"v1.1 preprint"},{"id":"http://arxiv.org/abs/2602.20379v1","updated":"2026-02-23T21:37:06Z","published":"2026-02-23T21:37:06Z","title":"Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems","summary":"Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.\n  We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.\n  Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.","authors":["Mukul Chhabra","Luigi Medrano","Arush Verma"],"pdf_url":"","comment":"12 pages including appendix, 6 figures"},{"id":"http://arxiv.org/abs/2510.08091v2","updated":"2026-02-23T21:32:22Z","published":"2025-10-09T11:22:29Z","title":"Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility","summary":"We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.","authors":["Shramay Palta","Peter Rankel","Sarah Wiegreffe","Rachel Rudinger"],"pdf_url":"","comment":"Updated"},{"id":"http://arxiv.org/abs/2509.23115v3","updated":"2026-02-23T21:26:30Z","published":"2025-09-27T04:55:56Z","title":"RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility","summary":"Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby quadratically reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM keeps the pretrained LLM backbone frozen, yielding faster training and lower memory usage. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.","authors":["Haoyu He","Haozheng Luo","Yan Chen","Qi R. Wang"],"pdf_url":"","comment":"Advances in Neural Information Processing Systems 39 (NeurIPS) 2025"},{"id":"http://arxiv.org/abs/2602.20372v1","updated":"2026-02-23T21:19:07Z","published":"2026-02-23T21:19:07Z","title":"How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity","summary":"Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.","authors":["Chundra Cathcart","Arne Rubehn","Katja Bocklage","Luca Ciucci","Kellen Parker van Dam","Alžběta Kučerová","Jekaterina Mažara","Carlo Y. Meloni","David Snee","Johann-Mattis List"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.05722v2","updated":"2026-02-23T20:57:20Z","published":"2025-11-07T21:29:41Z","title":"OckBench: Measuring the Efficiency of LLM Reasoning","summary":"Large language models (LLMs) such as GPT-5 and Gemini 3 have pushed the frontier of automated reasoning and code generation. Yet current benchmarks emphasize accuracy and output quality, neglecting a critical dimension: efficiency of token usage. The token efficiency is highly variable in practical. Models solving the same problem with similar accuracy can exhibit up to a \\textbf{5.0$\\times$} difference in token length, leading to massive gap of model reasoning ability. Such variance exposes significant redundancy, highlighting the critical need for a standardized benchmark to quantify the gap of token efficiency. Thus, we introduce OckBench, the first benchmark that jointly measures accuracy and token efficiency across reasoning and coding tasks. Our evaluation reveals that token efficiency remains largely unoptimized across current models, significantly inflating serving costs and latency. These findings provide a concrete roadmap for the community to optimize the latent reasoning ability, token efficiency. Ultimately, we argue for an evaluation paradigm shift: tokens must not be multiplied beyond necessity. Our benchmarks are available at https://ockbench.github.io/.","authors":["Zheng Du","Hao Kang","Song Han","Tushar Krishna","Ligeng Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20336v1","updated":"2026-02-23T20:33:22Z","published":"2026-02-23T20:33:22Z","title":"Natural Language Processing Models for Robust Document Categorization","summary":"This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.\n  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.\n  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.","authors":["Radoslaw Roszczyk","Pawel Tecza","Maciej Stodolski","Krzysztof Siwek"],"pdf_url":"","comment":"13 pages, 1 fiure, 5 tables"},{"id":"http://arxiv.org/abs/2602.20332v1","updated":"2026-02-23T20:28:48Z","published":"2026-02-23T20:28:48Z","title":"No One Size Fits All: QueryBandits for Hallucination Mitigation","summary":"Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.","authors":["Nicole Cho","William Watson","Alec Koppel","Sumitra Ganesh","Manuela Veloso"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25684v2","updated":"2026-02-23T20:24:13Z","published":"2025-09-30T02:38:10Z","title":"LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts","summary":"Recent studies have shown that combining parameter-efficient fine-tuning (PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting large language models (LLMs) to the downstream tasks. However, most existing approaches rely on conventional TopK routing, which requires careful hyperparameter tuning and assigns a fixed number of experts to each token. In this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise expert allocation. Our method replaces the non-differentiable TopK selection with a differentiable routing function and a closed-form solution. Moreover, our design allows the model to adaptively determine the number of experts to activate for each token at different layers. In addition, we introduce an analytical sparsity control objective to regularize the number of activated experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show that LD-MoLE achieves the highest average scores compared to state-of-the-art baselines, across a diverse set of benchmarks. Our method not only achieves superior performance, but also demonstrates the ability to learn token-dependent and layer-wise expert allocation.","authors":["Yuan Zhuang","Yi Shen","Yuexin Bian","Qing Su","Shihao Ji","Yuanyuan Shi","Fei Miao"],"pdf_url":"","comment":"International Conference on Learning Representations (ICLR 2026)"},{"id":"http://arxiv.org/abs/2602.20324v1","updated":"2026-02-23T20:20:23Z","published":"2026-02-23T20:20:23Z","title":"An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models","summary":"Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.","authors":["Cathy Shyr","Yan Hu","Rory J. Tinker","Thomas A. Cassini","Kevin W. Byram","Rizwan Hamid","Daniel V. Fabbri","Adam Wright","Josh F. Peterson","Lisa Bastarache","Hua Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.13404v4","updated":"2026-02-23T19:47:40Z","published":"2025-08-18T23:48:22Z","title":"TASER: Table Agents for Schema-guided Extraction and Recommendation","summary":"Real-world financial filings report critical information about an entity's investment holdings, essential for assessing that entity's risk, profitability, and relationship profile. Yet, these details are often buried in messy, multi-page, fragmented tables that are difficult to parse, hindering downstream QA and data normalization. Specifically, 99.4% of the tables in our financial table dataset lack bounding boxes, with the largest table spanning 44 pages. To address this, we present TASER (Table Agents for Schema-guided Extraction and Recommendation), a continuously learning, agentic table extraction system that converts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Guided by an initial portfolio schema, TASER executes table detection, classification, extraction, and recommendations in a single pipeline. Our Recommender Agent reviews unmatched outputs and proposes schema revisions, enabling TASER to outperform vision-based table detection models such as Table Transformer by 10.1%. Within this continuous learning process, larger batch sizes yield a 104.3% increase in useful schema recommendations and a 9.8% increase in total extractions. To train TASER, we manually labeled 22,584 pages and 3,213 tables covering $731.7 billion in holdings, culminating in TASERTab to facilitate research on real-world financial tables and structured outputs. Our results highlight the promise of continuously learning agents for robust extractions from complex tabular data.","authors":["Nicole Cho","Kirsty Fielding","William Watson","Sumitra Ganesh","Manuela Veloso"],"pdf_url":"","comment":"EACL 2026 Industry (Oral)"},{"id":"http://arxiv.org/abs/2602.20300v1","updated":"2026-02-23T19:30:08Z","published":"2026-02-23T19:30:08Z","title":"What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance","summary":"Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.","authors":["William Watson","Nicole Cho","Sumitra Ganesh","Manuela Veloso"],"pdf_url":"","comment":"EACL 2026 Findings"},{"id":"http://arxiv.org/abs/2602.20294v1","updated":"2026-02-23T19:21:10Z","published":"2026-02-23T19:21:10Z","title":"InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation","summary":"Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.","authors":["Yu Li","Pranav Narayanan Venkit","Yada Pruksachatkun","Chien-Sheng Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2406.10281v5","updated":"2026-02-23T19:16:04Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find that our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2405.10385v3","updated":"2026-02-23T19:08:20Z","published":"2024-05-16T18:26:38Z","title":"Augmenting Lateral Thinking in Language Models with Humor and Riddle Data for the BRAINTEASER Task","summary":"The SemEval 2024 BRAINTEASER task challenges language models to perform lateral thinking -- a form of creative, non-linear reasoning that remains underexplored in NLP. The task comprises two subtasks, Sentence Puzzle and Word Puzzle, requiring models to defy conventional commonsense associations. We present a system that fine-tunes DeBERTaV3 using HuggingFace's AutoModelForMultipleChoice architecture. We augment the provided training data with two additional sources: (1) a humor-style question-answering dataset generated via GPT-4 prompting, and (2) the RiddleSense dataset. This data augmentation strategy is motivated by the observation that humor and riddles share the lateral reasoning structure required by the task. Our best system achieves 92.5\\% overall accuracy on the Sentence Puzzle subtask and 80.2\\% on the Word Puzzle subtask, ranking 6th out of 31 teams and 10th out of 23 teams, respectively. We further show that the choice of task formulation matters: framing the problem as multiple-choice rather than sequence classification yields a 10-point accuracy improvement with the same base model. Our analysis reveals that data augmentation with humor and riddle data is particularly effective for sentence-level lateral reasoning, while word-level puzzles remain a harder challenge.","authors":["Mina Ghashami","Soumya Smruti Mishra"],"pdf_url":"","comment":"Accepted at SemEval 2024 (Colocated with NAACL 2024)"},{"id":"http://arxiv.org/abs/2506.04462v4","updated":"2026-02-23T19:05:29Z","published":"2025-06-04T21:29:07Z","title":"Watermarking Degrades Alignment in Language Models: Analysis and Mitigation","summary":"Watermarking has become a practical tool for tracing language model outputs, but it modifies token probabilities at inference time, which were carefully tuned by alignment training. This creates a tension: how do watermark-induced shifts interact with the procedures intended to make models safe and useful? Experiments on several contemporary models and two representative watermarking schemes reveal that watermarking induces a nontrivial, patterned yet model-specific shift in alignment. We see two failure modes: guard attenuation, where models become more helpful but less safe, and guard amplification, where refusals become overly conservative. These effects persist even after controlling for perplexity degradation, pointing to alignment-specific distortions, not just quality loss. We address this with Alignment Resampling (AR), a procedure that samples multiple watermarked outputs and selects the most aligned response according to an external reward model. Using standard results on the expected maximum of Gaussian random variables, we derive a theoretical lower bound showing that alignment gains grow sublogarithmically with sample size. In practice, sampling as few as two to four candidates largely restores unwatermarked alignment performance in truthfulness, safety, and helpfulness, without hurting watermark detection. This is the first empirical study of watermarking-alignment interactions; it shows that a simple inference-time fix can recover alignment.","authors":["Apurv Verma","NhatHai Phan","Shubhendu Trivedi"],"pdf_url":"","comment":"Published in Transactions of Machine Learning Research 02/2026. Extended version of the earlier paper published at the 1st Workshop on GenAI Watermarking (ICLR 2025)"},{"id":"http://arxiv.org/abs/2506.13792v2","updated":"2026-02-23T18:47:28Z","published":"2025-06-11T13:46:47Z","title":"ICE-ID: A Novel Historical Census Dataset for Longitudinal Identity Resolution","summary":"We introduce \\textbf{ICE-ID}, a benchmark dataset comprising 984,028 records from 16 Icelandic census waves spanning 220 years (1703--1920), with 226,864 expert-curated person identifiers. ICE-ID combines hierarchical geography (farm$\\to$parish$\\to$district$\\to$county), patronymic naming conventions, sparse kinship links (partner, father, mother), and multi-decadal temporal drift -- challenges not captured by standard product-matching or citation datasets. This paper presents an artifact-backed analysis of temporal coverage, missingness, identifier ambiguity, candidate-generation efficiency, and cluster distributions, and situates ICE-ID against classical ER benchmarks (Abt--Buy, Amazon--Google, DBLP--ACM, DBLP--Scholar, Walmart--Amazon, iTunes--Amazon, Beer, Fodors--Zagats). We also define a deployment-faithful temporal OOD protocol and release the dataset, splits, regeneration scripts, analysis artifacts, and a dashboard for interactive exploration. Baseline model comparisons and end-to-end ER results are reported in the companion methods paper.","authors":["Gonçalo Hora de Carvalho","Lazar S. Popov","Sander Kaatee","Mário S. Correia","Kristinn R. Thórisson","Tangrui Li","Pétur Húni Björnsson","Eiríkur Smári Sigurðarson","Jilles S. Dibangoye"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20224v1","updated":"2026-02-23T16:17:33Z","published":"2026-02-23T16:17:33Z","title":"Exploring Anti-Aging Literature via ConvexTopics and Large Language Models","summary":"The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.","authors":["Lana E. Yeganova","Won G. Kim","Shubo Tian","Natalie Xie","Donald C. Comeau","W. John Wilbur","Zhiyong Lu"],"pdf_url":"","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2602.20161v1","updated":"2026-02-23T18:59:58Z","published":"2026-02-23T18:59:58Z","title":"Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device","summary":"Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/","authors":["Abdelrahman Shaker","Ahmed Heakl","Jaseel Muhammad","Ritesh Thawkar","Omkar Thawakar","Senmao Li","Hisham Cholakkal","Ian Reid","Eric P. Xing","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"","comment":"Project page: https://amshaker.github.io/Mobile-O/"},{"id":"http://arxiv.org/abs/2602.17665v2","updated":"2026-02-23T18:59:54Z","published":"2026-02-19T18:59:54Z","title":"OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents","summary":"Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.","authors":["Akashah Shabbir","Muhammad Umer Sheikh","Muhammad Akhtar Munir","Hiyam Debary","Mustansar Fiaz","Muhammad Zaigham Zaheer","Paolo Fraccaro","Fahad Shahbaz Khan","Muhammad Haris Khan","Xiao Xiang Zhu","Salman Khan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20160v1","updated":"2026-02-23T18:59:45Z","published":"2026-02-23T18:59:45Z","title":"tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction","summary":"We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.","authors":["Chen Wang","Hao Tan","Wang Yifan","Zhiqin Chen","Yuheng Liu","Kalyan Sunkavalli","Sai Bi","Lingjie Liu","Yiwei Hu"],"pdf_url":"","comment":"Accepted by CVPR 2026. Project Page: https://cwchenwang.github.io/tttLRM"},{"id":"http://arxiv.org/abs/2602.20159v1","updated":"2026-02-23T18:59:41Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2602.20157v1","updated":"2026-02-23T18:59:30Z","published":"2026-02-23T18:59:30Z","title":"Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning","summary":"Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.","authors":["Zhongxiao Cong","Qitao Zhao","Minsik Jeon","Shubham Tulsiani"],"pdf_url":"","comment":"CVPR 2026. Project website: https://flow3r-project.github.io/"},{"id":"http://arxiv.org/abs/2602.20150v1","updated":"2026-02-23T18:58:24Z","published":"2026-02-23T18:58:24Z","title":"Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization","summary":"Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.","authors":["Wei-Cheng Huang","Jiaheng Han","Xiaohan Ye","Zherong Pan","Kris Hauser"],"pdf_url":"","comment":"15 pages, 13 figures, in submission"},{"id":"http://arxiv.org/abs/2602.20137v1","updated":"2026-02-23T18:47:51Z","published":"2026-02-23T18:47:51Z","title":"Do Large Language Models Understand Data Visualization Rules?","summary":"Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.","authors":["Martin Sinnona","Valentin Bonas","Emmanuel Iarussi","Viviana Siless"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18406v2","updated":"2026-02-23T18:44:49Z","published":"2026-02-20T18:14:05Z","title":"Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges","summary":"Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training$\\unicode{x2013}$for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to learn equivariant operators in a latent space, from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.","authors":["Minh Dinh","Stéphane Deny"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20119v1","updated":"2026-02-23T18:35:18Z","published":"2026-02-23T18:35:18Z","title":"NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning","summary":"Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/","authors":["Jiahui Fu","Junyu Nan","Lingfeng Sun","Hongyu Li","Jianing Qian","Jennifer L. Barry","Kris Kitani","George Konidaris"],"pdf_url":"","comment":"25 pages, 15 figures. Project webpage: https://nova-plan.github.io/"},{"id":"http://arxiv.org/abs/2602.20114v1","updated":"2026-02-23T18:33:16Z","published":"2026-02-23T18:33:16Z","title":"Benchmarking Unlearning for Vision Transformers","summary":"Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.","authors":["Kairan Zhao","Iurie Luca","Peter Triantafillou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20100v1","updated":"2026-02-23T18:15:30Z","published":"2026-02-23T18:15:30Z","title":"Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine","summary":"The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.","authors":["Soumick Chatterjee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.17258v2","updated":"2026-02-23T18:12:49Z","published":"2026-01-24T02:17:07Z","title":"FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding","summary":"Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.","authors":["João Pereira","Vasco Lopes","João Neves","David Semedo"],"pdf_url":"","comment":"Accepted at AAAI 2026"},{"id":"http://arxiv.org/abs/2602.17807v2","updated":"2026-02-23T18:10:18Z","published":"2026-02-19T20:14:14Z","title":"VidEoMT: Your ViT is Secretly Also a Video Segmentation Model","summary":"Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x-10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/","authors":["Narges Norouzi","Idil Esen Zulfikar","Niccolò Cavagnero","Tommie Kerssies","Bastian Leibe","Gijs Dubbelman","Daan de Geus"],"pdf_url":"","comment":"CVPR 2025. Code: https://www.tue-mps.org/videomt/"},{"id":"http://arxiv.org/abs/2601.16210v2","updated":"2026-02-23T18:05:24Z","published":"2026-01-22T18:58:55Z","title":"PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation","summary":"Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.","authors":["Onkar Susladkar","Tushar Prakash","Adheesh Juvekar","Kiet A. Nguyen","Dong-Hwan Jang","Inderjit S Dhillon","Ismini Lourentzou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.20648v2","updated":"2026-02-23T17:59:38Z","published":"2025-11-25T18:59:45Z","title":"LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight","summary":"To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 38.90 AP_3D, surpassing the previous best by +13.98 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.","authors":["Yunze Man","Shihao Wang","Guowen Zhang","Johan Bjorck","Zhiqi Li","Liang-Yan Gui","Jim Fan","Jan Kautz","Yu-Xiong Wang","Zhiding Yu"],"pdf_url":"","comment":"Tech report. Project page: https://nvlabs.github.io/LocateAnything3D/"},{"id":"http://arxiv.org/abs/2602.20089v1","updated":"2026-02-23T17:57:37Z","published":"2026-02-23T17:57:37Z","title":"StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues","summary":"Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.","authors":["Zanxi Ruan","Qiuyu Kong","Songqun Gao","Yiming Wang","Marco Cristani"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20084v1","updated":"2026-02-23T17:51:06Z","published":"2026-02-23T17:51:06Z","title":"Do Large Language Models Understand Data Visualization Principles?","summary":"Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.","authors":["Martin Sinnona","Valentin Bonas","Viviana Siless","Emmanuel Iarussi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20079v1","updated":"2026-02-23T17:45:21Z","published":"2026-02-23T17:45:21Z","title":"SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis","summary":"We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.","authors":["Xinya Chen","Christopher Wewer","Jiahao Xie","Xinting Hu","Jan Eric Lenssen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.07853v2","updated":"2026-02-23T17:38:56Z","published":"2025-03-10T20:59:41Z","title":"Hier-COS: Making Deep Features Hierarchy-aware via Composition of Orthogonal Subspaces","summary":"Traditional classifiers treat all labels as mutually independent, thereby considering all negative classes to be equally incorrect. This approach fails severely in many real-world scenarios, where a known semantic hierarchy defines a partial order of preferences over negative classes. While hierarchy-aware feature representations have shown promise in mitigating this problem, their performance is typically assessed using metrics like MS and AHD. In this paper, we highlight important shortcomings in existing hierarchical evaluation metrics, demonstrating that they are often incapable of measuring true hierarchical performance. Our analysis reveals that existing methods learn sub-optimal hierarchical representations, despite competitive MS and AHD scores. To counter these issues, we introduce Hier-COS, a novel framework for unified hierarchy-aware fine-grained and hierarchical multi-level classification. We show that Hier-COS is theoretically guaranteed to be consistent with the given hierarchy tree. Furthermore, our framework implicitly adapts the learning capacity for different classes based on their position within the hierarchy tree-a vital property absent in existing methods. Finally, to address the limitations of evaluation metrics, we propose HOPS, a ranking-based metric that demonstrably overcomes the deficiencies of current evaluation standards. We benchmark Hier-COS on four challenging datasets, including the deep and imbalanced tieredImageNet-H and iNaturalist-19. Through extensive experiments, we demonstrate that Hier-COS achieves SOTA across all hierarchical metrics for every dataset, while simultaneously beating the top-1 accuracy in all but one case. Lastly, we show that Hier-COS can effectively learn to transform the frozen features extracted from a pretrained backbone (ViT) to be hierarchy-aware, yielding substantial benefits for hierarchical classification performance.","authors":["Depanshu Sani","Saket Anand"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2503.14637v2","updated":"2026-02-23T17:30:07Z","published":"2025-03-18T18:37:49Z","title":"KINESIS: Motion Imitation for Human Musculoskeletal Locomotion","summary":"How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \\& non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.","authors":["Merkourios Simos","Alberto Silvio Chiappa","Alexander Mathis"],"pdf_url":"","comment":"Accepted to ICRA. Here we include an appendix"},{"id":"http://arxiv.org/abs/2602.20068v1","updated":"2026-02-23T17:24:18Z","published":"2026-02-23T17:24:18Z","title":"The Invisible Gorilla Effect in Out-of-distribution Detection","summary":"Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.","authors":["Harry Anthony","Ziyun Liang","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20066v1","updated":"2026-02-23T17:22:54Z","published":"2026-02-23T17:22:54Z","title":"HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images","summary":"Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.","authors":["Kundan Thota","Xuanhao Mu","Thorsten Schlachter","Veit Hagenmeyer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20060v1","updated":"2026-02-23T17:17:26Z","published":"2026-02-23T17:17:26Z","title":"MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving","summary":"Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.","authors":["Junli Wang","Xueyi Liu","Yinan Zheng","Zebing Xing","Pengfei Li","Guang Li","Kun Ma","Guang Chen","Hangjun Ye","Zhongpu Xia","Long Chen","Qichao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.12713v2","updated":"2026-02-23T17:15:46Z","published":"2025-08-18T08:25:18Z","title":"Real-Time Sign Language Gestures to Speech Transcription using Deep Learning","summary":"Communication barriers pose significant challenges for individuals with hearing and speech impairments, often limiting their ability to effectively interact in everyday environments. This project introduces a real-time assistive technology solution that leverages advanced deep learning techniques to translate sign language gestures into textual and audible speech. By employing convolution neural networks (CNN) trained on the Sign Language MNIST dataset, the system accurately classifies hand gestures captured live via webcam. Detected gestures are instantaneously translated into their corresponding meanings and transcribed into spoken language using text-to-speech synthesis, thus facilitating seamless communication. Comprehensive experiments demonstrate high model accuracy and robust real-time performance with some latency, highlighting the system's practical applicability as an accessible, reliable, and user-friendly tool for enhancing the autonomy and integration of sign language users in diverse social settings.","authors":["Brandone Fonya","Clarence Worrell"],"pdf_url":"","comment":"Course related research project"},{"id":"http://arxiv.org/abs/2602.20055v1","updated":"2026-02-23T17:10:00Z","published":"2026-02-23T17:10:00Z","title":"To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation","summary":"Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.","authors":["Apoorva Vashisth","Manav Kulshrestha","Pranav Bakshi","Damon Conover","Guillaume Sartoretti","Aniket Bera"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.23479v3","updated":"2026-02-23T17:06:33Z","published":"2025-10-27T16:12:40Z","title":"MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding","summary":"Vision-language alignment in multi-modal large language models (MLLMs) relies on supervised fine-tuning (SFT) or reinforcement learning (RL). To align multi-modal large language models (MLLMs) in the post-training stage, supervised fine-tuning (SFT) is a stable choice but requires human annotations and lacks task generalizations, while Reinforcement Learning (RL) searches for better answers from reward signals but suffers from computational overhead and instability. To achieve balance among scalability, efficiency, and alignment generalizations, we propose MergeMix, a unified paradigm that bridges SFT and RL with an efficient Token Merge based Mixup augmentation. As for the Mixup policy, we generate contextual aligned mixed images with the corresponding labels according to the merged attention maps with cluster regions. Then, we enhance the preference-driven paradigm for MLLMs by building preference pairs with raw images and MergeMix-generated ones and optimizing the soft preference margin with the mixed SimPO loss. Extensive experiments demonstrate that MergeMix not only achieves dominant classification accuracy as an augmentation method but also improves generalization abilities and alignment of MLLMs, providing a new learning paradigm for preference alignment with training efficiency and stability.","authors":["Xin Jin","Siyuan Li","Siyong Jian","Kai Yu","Huan Wang"],"pdf_url":"","comment":"ICLR 2026, Web link: https://jinxins.github.io/MergeMix_Web/"},{"id":"http://arxiv.org/abs/2602.20053v1","updated":"2026-02-23T17:02:55Z","published":"2026-02-23T17:02:55Z","title":"Decoupling Defense Strategies for Robust Image Watermarking","summary":"Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.","authors":["Jiahui Chen","Zehang Deng","Zeyu Zhang","Chaoyang Li","Lianchen Jia","Lifeng Sun"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20051v1","updated":"2026-02-23T17:00:35Z","published":"2026-02-23T17:00:35Z","title":"SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency","summary":"3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.","authors":["Yeonsung Kim","Junggeun Do","Seunguk Do","Sangmin Kim","Jaesik Park","Jay-Yoon Lee"],"pdf_url":"","comment":"17 pages"},{"id":"http://arxiv.org/abs/2602.20046v1","updated":"2026-02-23T16:57:39Z","published":"2026-02-23T16:57:39Z","title":"Closing the gap in multimodal medical representation alignment","summary":"In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.","authors":["Eleonora Grassucci","Giordano Cicchetti","Danilo Comminiello"],"pdf_url":"","comment":"Accepted at MLSP2025"},{"id":"http://arxiv.org/abs/2602.20041v1","updated":"2026-02-23T16:50:21Z","published":"2026-02-23T16:50:21Z","title":"EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover","summary":"Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.","authors":["Ghadah Alosaimi","Maha Alsayyari","Yixin Sun","Stamos Katsigiannis","Amir Atapour-Abarghouei","Toby P. Breckon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.12846v2","updated":"2026-02-23T16:34:59Z","published":"2025-09-16T09:05:46Z","title":"Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration","summary":"Visual-inertial fusion is crucial for a large amount of intelligent and autonomous applications, such as robot navigation and augmented reality. To bootstrap and achieve optimal state estimation, the spatial-temporal displacements between IMU and cameras must be calibrated in advance. Most existing calibration methods adopt continuous-time state representation, more specifically the B-spline. Despite these methods achieve precise spatial-temporal calibration, they suffer from high computational cost caused by continuous-time state representation. To this end, we propose a novel and extremely efficient calibration method that unleashes the power of discrete-time state representation. Moreover, the weakness of discrete-time state representation in temporal calibration is tackled in this paper. With the increasing production of drones, cellphones and other visual-inertial platforms, if one million devices need calibration around the world, saving one minute for the calibration of each device means saving 2083 work days in total. To benefit both the research and industry communities, the open-source implementation is released at https://github.com/JunlinSong/DT-VI-Calib.","authors":["Junlin Song","Antoine Richard","Miguel Olivares-Mendez"],"pdf_url":"","comment":"Accepted by ICRA 2026"},{"id":"http://arxiv.org/abs/2602.15656v3","updated":"2026-02-23T16:21:27Z","published":"2026-02-17T15:30:31Z","title":"A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models","summary":"The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.","authors":["Mustafa Yurdakul","Zeynep Sena Bastug","Ali Emre Gok","Sakir Taşdemir"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20008v1","updated":"2026-02-23T16:15:38Z","published":"2026-02-23T16:15:38Z","title":"Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation","summary":"We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.","authors":["Louis Fabrice Tshimanga","Andrea Zanola","Federico Del Pup","Manfredo Atzori"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25723v3","updated":"2026-02-23T16:10:03Z","published":"2025-09-30T03:34:40Z","title":"SAGE: Spatial-visual Adaptive Graph Exploration for Efficient Visual Place Recognition","summary":"Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. The code and model are available at https://github.com/chenshunpeng/SAGE.","authors":["Shunpeng Chen","Changwei Wang","Rongtao Xu","Xingtian Pei","Yukun Song","Jinzhou Lin","Wenhao Xu","Jingyi Zhang","Li Guo","Shibiao Xu"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2505.03646v4","updated":"2026-02-23T16:09:40Z","published":"2025-05-06T15:52:14Z","title":"GRILL: Restoring Gradient Signal in Ill-Conditioned Layers for More Effective Adversarial Attacks on Autoencoders","summary":"Adversarial robustness of deep autoencoders (AEs) has received less attention than that of discriminative models, although their compressed latent representations induce ill-conditioned mappings that can amplify small input perturbations and destabilize reconstructions. Existing white-box attacks for AEs, which optimize norm-bounded adversarial perturbations to maximize output damage, often stop at suboptimal attacks. We observe that this limitation stems from vanishing adversarial loss gradients during backpropagation through ill-conditioned layers, caused by near-zero singular values in their Jacobians. To address this issue, we introduce GRILL, a technique that locally restores gradient signals in ill-conditioned layers, enabling more effective norm-bounded attacks. Through extensive experiments across multiple AE architectures, considering both sample-specific and universal attacks under both standard and adaptive attack settings, we show that GRILL significantly increases attack effectiveness, leading to a more rigorous evaluation of AE robustness. Beyond AEs, we provide empirical evidence that modern multimodal architectures with encoder-decoder structures exhibit similar vulnerabilities under GRILL.","authors":["Chethan Krishnamurthy Ramanaik","Arjun Roy","Tobias Callies","Eirini Ntoutsi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.10135v2","updated":"2026-02-23T16:02:05Z","published":"2024-08-19T16:33:17Z","title":"$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via Geometry and Appearance Refinement","summary":"Mesh reconstruction from Neural Radiance Fields (NeRF) is widely used in 3D reconstruction and has been applied across numerous domains. However, existing methods typically rely solely on the given training set images, which restricts supervision to limited observations and makes it difficult to fully constrain geometry and appearance. Moreover, the contribution of each viewpoint for training is not uniform and changes dynamically during the optimization process, which can result in suboptimal guidance for both geometric refinement and rendering quality. To address these limitations, we propose $R^2$-Mesh, a reinforcement learning framework that combines NeRF-rendered pseudo-supervision with online viewpoint selection. Our key insight is to exploit NeRF's rendering ability to synthesize additional high-quality images, enriching training with diverse viewpoint information. To ensure that supervision focuses on the most beneficial perspectives, we introduce a UCB-based strategy with a geometry-aware reward, which dynamically balances exploration and exploitation to identify informative viewpoints throughout training. Within this framework, we jointly optimize SDF geometry and view-dependent appearance under differentiable rendering, while periodically refining meshes to capture fine geometric details. Experiments demonstrate that our method achieves competitive results in both geometric accuracy and rendering quality.","authors":["Haoyang Wang","Liming Liu","Xinggong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19994v1","updated":"2026-02-23T16:01:31Z","published":"2026-02-23T16:01:31Z","title":"RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather","summary":"Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.","authors":["Christof Leitgeb","Thomas Puchleitner","Max Peter Ronecker","Daniel Watzenig"],"pdf_url":"","comment":"Accepted to 2026 IEEE Intelligent Vehicles Symposium (IV)"},{"id":"http://arxiv.org/abs/2408.07543v6","updated":"2026-02-23T15:56:57Z","published":"2024-08-14T13:23:43Z","title":"MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts","summary":"With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.","authors":["Hao Liang","Linzhuang Sun","Minxuan Zhou","Zirong Chen","Meiyi Qiang","Mingan Lin","Tianpeng Li","Fan Yang","Zenan Zhou","Wentao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19974v1","updated":"2026-02-23T15:39:53Z","published":"2026-02-23T15:39:53Z","title":"RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection","summary":"Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.","authors":["Tianyu Wang","Zhiyuan Ma","Qian Wang","Xinyi Zhang","Xinwei Long","Bowen Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.12047v3","updated":"2026-02-23T15:22:32Z","published":"2025-03-15T08:38:47Z","title":"PSGait: Gait Recognition using Parsing Skeleton","summary":"Gait recognition has emerged as a robust biometric modality due to its non-intrusive nature. Conventional gait recognition methods mainly rely on silhouettes or skeletons. While effective in controlled laboratory settings, their limited information entropy restricts generalization to real-world scenarios. To overcome this, we propose a novel representation called \\textbf{Parsing Skeleton}, which uses a skeleton-guided human parsing method to capture fine-grained body dynamics with much higher information entropy. To effectively explore the capability of the Parsing Skeleton, we also introduce \\textbf{PSGait}, a framework that fuses Parsing Skeleton with silhouettes to enhance individual differentiation. Comprehensive benchmarks demonstrate that PSGait outperforms state-of-the-art multimodal methods while significantly reducing computational resources. As a plug-and-play method, it achieves an improvement of up to 15.7\\% in the accuracy of Rank-1 in various models. These results validate the Parsing Skeleton as a \\textbf{lightweight}, \\textbf{effective}, and highly \\textbf{generalizable} representation for gait recognition in the wild. Code is available at https://github.com/realHarryX/PSGait.","authors":["Hangrui Xu","Zhengxian Wu","Chuanrui Zhang","Zhuohong Chen","Zhifang Liu","Peng Jiao","Haoqian Wang"],"pdf_url":"","comment":"Accepted by ICASSP 2026"},{"id":"http://arxiv.org/abs/2602.19946v1","updated":"2026-02-23T15:15:53Z","published":"2026-02-23T15:15:53Z","title":"When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators","summary":"Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.","authors":["Krzysztof Adamkiewicz","Brian Moser","Stanislav Frolov","Tobias Christian Nauen","Federico Raue","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19944v1","updated":"2026-02-23T15:15:37Z","published":"2026-02-23T15:15:37Z","title":"Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation","summary":"Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \\textbf{D}iscover-\\textbf{S}egment-\\textbf{S}elect (\\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.","authors":["Yilong Yang","Jianxin Tian","Shengchuan Zhang","Liujuan Cao"],"pdf_url":"","comment":"Accepted by CVPR 2026 (main conference)"},{"id":"http://arxiv.org/abs/2602.09609v2","updated":"2026-02-23T15:14:47Z","published":"2026-02-10T10:01:16Z","title":"Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing","summary":"Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.","authors":["Jialun Liu","Tian Li","Xiao Cao","Yukuo Ma","Gonghu Shang","Haibin Huang","Chi Zhang","Xiangzhen Chang","Zhiyong Huang","Jiakui Hu","Zuoxin Li","Yuanzhi Liang","Cong Liu","Junqi Liu","Robby T. Tan","Haitong Tang","Qizhen Weng","Yifan Xu","Liying Yang","Xiaoyan Yang","Peng Yu","Shiwen Zhang","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.08550v2","updated":"2026-02-23T15:12:01Z","published":"2026-02-09T11:50:29Z","title":"GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing","summary":"Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.","authors":["Shih-Fang Chen","Jun-Cheng Chen","I-Hong Jhuo","Yen-Yu Lin"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19937v1","updated":"2026-02-23T15:10:00Z","published":"2026-02-23T15:10:00Z","title":"Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation","summary":"Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.","authors":["Yifei Shi","Boyan Wan","Xin Xu","Kai Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19931v1","updated":"2026-02-23T15:06:52Z","published":"2026-02-23T15:06:52Z","title":"Expanding the Role of Diffusion Models for Robust Classifier Training","summary":"Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.","authors":["Pin-Han Huang","Shang-Tse Chen","Hsuan-Tien Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19916v1","updated":"2026-02-23T14:55:31Z","published":"2026-02-23T14:55:31Z","title":"Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting","summary":"Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.","authors":["Yixin Yang","Bojian Wu","Yang Zhou","Hui Huang"],"pdf_url":"","comment":"Accepted to ICLR 2026. Project page: \\url{https://xiaoxinyyx.github.io/augs}"},{"id":"http://arxiv.org/abs/2602.19910v1","updated":"2026-02-23T14:51:09Z","published":"2026-02-23T14:51:09Z","title":"Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery","summary":"Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.","authors":["Wei He","Xianghan Meng","Zhiyuan Huang","Xianbiao Qi","Rong Xiao","Chun-Guang Li"],"pdf_url":"","comment":"15 pages, accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19907v1","updated":"2026-02-23T14:46:08Z","published":"2026-02-23T14:46:08Z","title":"Gradient based Severity Labeling for Biomarker Classification in OCT","summary":"In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.","authors":["Kiran Kokilepersaud","Mohit Prabhushankar","Ghassan AlRegib","Stephanie Trejo Corona","Charles Wykoff"],"pdf_url":"","comment":"Accepted at International Conference on Image Processing (ICIP) 2022"},{"id":"http://arxiv.org/abs/2509.12746v2","updated":"2026-02-23T14:44:35Z","published":"2025-09-16T07:04:45Z","title":"Modelling and analysis of the 8 filters from the \"master key filters hypothesis\" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory","summary":"This paper presents the results of analysing and modelling a set of 8 ``master key filters'', which have been extracted by applying a clustering approach to the receptive fields learned in depthwise-separable deep networks based on the ConvNeXt architecture.\n  For this purpose, we first compute spatial spread measures in terms of weighted mean values and weighted variances of the absolute values of the learned filters, which support the working hypotheses that: (i) the learned filters can be modelled by separable filtering operations over the spatial domain, and that (ii) the spatial offsets of the those learned filters that are non-centered are rather close to half a grid unit. Then, we model the clustered ``master key filters'' in terms of difference operators applied to a spatial smoothing operation in terms of the discrete analogue of the Gaussian kernel, and demonstrate that the resulting idealized models of the receptive fields show good qualitative similarity to the learned filters.\n  This modelling is performed in two different ways: (i) using possibly different values of the scale parameters in the coordinate directions for each filter, and (ii) using the same value of the scale parameter in both coordinate directions. Then, we perform the actual model fitting by either (i) requiring spatial spread measures in terms of spatial variances of the absolute values of the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or $l_2$-norms between the idealized receptive field models and the learned filters.\n  Complementary experimental results then demonstrate the idealized models of receptive fields have good predictive properties for replacing the learned filters by idealized filters in depthwise-separable deep networks, thus showing that the learned filters in depthwise-separable deep networks can be well approximated by discrete scale-space filters.","authors":["Tony Lindeberg","Zahra Babaiee","Peyman M. Kiasari"],"pdf_url":"","comment":"25 pages, 5 figures, 11 tables"},{"id":"http://arxiv.org/abs/2602.19900v1","updated":"2026-02-23T14:41:35Z","published":"2026-02-23T14:41:35Z","title":"ExpPortrait: Expressive Portrait Generation via Personalized Representation","summary":"While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.","authors":["Junyi Wang","Yudong Guo","Boyang Guo","Shengming Yang","Juyong Zhang"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19896v1","updated":"2026-02-23T14:37:07Z","published":"2026-02-23T14:37:07Z","title":"Monocular Mesh Recovery and Body Measurement of Female Saanen Goats","summary":"The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.","authors":["Bo Jin","Shichao Zhao","Jin Lyu","Bin Zhang","Tao Yu","Liang An","Yebin Liu","Meili Wang"],"pdf_url":"","comment":"Accepted to AAAI2026"},{"id":"http://arxiv.org/abs/2602.19891v1","updated":"2026-02-23T14:33:24Z","published":"2026-02-23T14:33:24Z","title":"Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images","summary":"While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.","authors":["Wen-Liang Lin","Yun-Chien Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19881v1","updated":"2026-02-23T14:27:36Z","published":"2026-02-23T14:27:36Z","title":"Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations","summary":"Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd","authors":["Blaž Rolih","Matic Fučka","Filip Wolf","Luka Čehovin Zajc"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19874v1","updated":"2026-02-23T14:21:15Z","published":"2026-02-23T14:21:15Z","title":"BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations","summary":"The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\\textbf{Big Ma}$ca$\\textbf{Q}$ue 3D Motion and Animation Dataset ($\\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .","authors":["Lucas Martini","Alexander Lappe","Anna Bognár","Rufin Vogels","Martin A. Giese"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.06450v2","updated":"2026-02-23T14:20:57Z","published":"2025-11-09T16:34:19Z","title":"Countering Multi-modal Representation Collapse through Rank-targeted Fusion","summary":"Multi-modal fusion methods often suffer from two types of representation collapse: feature collapse where individual dimensions lose their discriminative power (as measured by eigenspectra), and modality collapse where one dominant modality overwhelms the other. Applications like human action anticipation that require fusing multifarious sensor data are hindered by both feature and modality collapse. However, existing methods attempt to counter feature collapse and modality collapse separately. This is because there is no unifying framework that efficiently addresses feature and modality collapse in conjunction. In this paper, we posit the utility of effective rank as an informative measure that can be utilized to quantify and counter both the representation collapses. We propose \\textit{Rank-enhancing Token Fuser}, a theoretically grounded fusion framework that selectively blends less informative features from one modality with complementary features from another modality. We show that our method increases the effective rank of the fused representation. To address modality collapse, we evaluate modality combinations that mutually increase each others' effective rank. We show that depth maintains representational balance when fused with RGB, avoiding modality collapse. We validate our method on action anticipation, where we present \\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on NTURGBD, UTKinect, and DARai demonstrate that our approach significantly outperforms prior state-of-the-art methods by up to 3.74\\%. Our code is available at: \\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.","authors":["Seulgi Kim","Kiran Kokilepersaud","Mohit Prabhushankar","Ghassan AlRegib"],"pdf_url":"","comment":"Accepted in 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2602.19872v1","updated":"2026-02-23T14:15:56Z","published":"2026-02-23T14:15:56Z","title":"GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery","summary":"Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.","authors":["Jizhou Han","Chenhao Ding","SongLin Dong","Yuhang He","Shaokun Wang","Qiang Wang","Yihong Gong"],"pdf_url":"","comment":"Accept by AAAI 2026"},{"id":"http://arxiv.org/abs/2602.19870v1","updated":"2026-02-23T14:15:37Z","published":"2026-02-23T14:15:37Z","title":"ApET: Approximation-Error Guided Token Compression for Efficient VLMs","summary":"Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.","authors":["Qiankun Ma","Ziyao Zhang","Haofei Wang","Jie Chen","Zhen Song","Hairong Zheng"],"pdf_url":"","comment":"CVPR2026"},{"id":"http://arxiv.org/abs/2602.19863v1","updated":"2026-02-23T14:09:01Z","published":"2026-02-23T14:09:01Z","title":"Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation","summary":"Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Code: Coming soon.","authors":["Filip Wolf","Blaž Rolih","Luka Čehovin Zajc"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2509.06685v4","updated":"2026-02-23T14:07:37Z","published":"2025-09-08T13:41:10Z","title":"MOGS: Monocular Object-guided Gaussian Splatting in Large Scenes","summary":"Recent advances in 3D Gaussian Splatting (3DGS) deliver striking photorealism, and extending it to large scenes opens new opportunities for semantic reasoning and prediction in applications such as autonomous driving. Today's state-of-the-art systems for large scenes primarily originate from LiDAR-based pipelines that utilize long-range depth sensing. However, they require costly high-channel sensors whose dense point clouds strain memory and computation, limiting scalability, fleet deployment, and optimization speed. We present MOGS, a monocular 3DGS framework that replaces active LiDAR depth with object-anchored, metrized dense depth derived from sparse visual-inertial (VI) structure-from-motion (SfM) cues. Our key idea is to exploit image semantics to hypothesize per-object shape priors, anchor them with sparse but metrically reliable SfM points, and propagate the resulting metric constraints across each object to produce dense depth. To address two key challenges, i.e., insufficient SfM coverage within objects and cross-object geometric inconsistency, MOGS introduces (1) a multi-scale shape consensus module that adaptively merges small segments into coarse objects best supported by SfM and fits them with parametric shape models, and (2) a cross-object depth refinement module that optimizes per-pixel depth under a combinatorial objective combining geometric consistency, prior anchoring, and edge-aware smoothness. Experiments on public datasets show that, with a low-cost VI sensor suite, MOGS reduces training time by up to 30.4% and memory consumption by 19.8%, while achieving high-quality rendering competitive with costly LiDAR-based approaches in large scenes.","authors":["Shengkai Zhang","Yuhe Liu","Jianhua He","Xuedou Xiao","Mozi Chen","Kezhong Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19857v1","updated":"2026-02-23T13:56:49Z","published":"2026-02-23T13:56:49Z","title":"Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions","summary":"Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.","authors":["Rodrigo Mota","Kelvin Cunha","Emanoel dos Santos","Fábio Papais","Francisco Filho","Thales Bezerra","Erico Medeiros","Paulo Borba","Tsang Ing Ren"],"pdf_url":"","comment":"4 pages, 5 figures, 1 table, isbi2026"},{"id":"http://arxiv.org/abs/2602.19848v1","updated":"2026-02-23T13:52:28Z","published":"2026-02-23T13:52:28Z","title":"DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation","summary":"Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.","authors":["Francisco Filho","Kelvin Cunha","Fábio Papais","Emanoel dos Santos","Rodrigo Mota","Thales Bezerra","Erico Medeiros","Paulo Borba","Tsang Ing Ren"],"pdf_url":"","comment":"4 pages, 2 figures, 1 table, isbi2026"},{"id":"http://arxiv.org/abs/2601.14706v2","updated":"2026-02-23T13:42:55Z","published":"2026-01-21T06:50:23Z","title":"LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval","summary":"In this paper, we present LookBench (We use the term \"look\" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.","authors":["Gensmo. ai","Chao Gao","Siqiao Xue","Yimin Peng","Jiwen Fu","Tingyi Gu","Shanshan Li","Fan Zhou"],"pdf_url":"","comment":"The first two authors contributed equally to this work. Project site: https://serendipityoneinc.github.io/look-bench-page/"},{"id":"http://arxiv.org/abs/2602.19832v1","updated":"2026-02-23T13:30:59Z","published":"2026-02-23T13:30:59Z","title":"M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting","summary":"The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.","authors":["Penghui Niu","Taotao Cai","Suqi Zhang","Junhua Gu","Ping Zhang","Qiqi Liu","Jianxin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19828v1","updated":"2026-02-23T13:26:18Z","published":"2026-02-23T13:26:18Z","title":"TextShield-R1: Reinforced Reasoning for Tampered Text Detection","summary":"The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.","authors":["Chenfan Qu","Yiwu Zhong","Jian Liu","Xuekang Zhu","Bohan Yu","Lianwen Jin"],"pdf_url":"","comment":"AAAI 2026"},{"id":"http://arxiv.org/abs/2602.19823v1","updated":"2026-02-23T13:22:51Z","published":"2026-02-23T13:22:51Z","title":"Open-vocabulary 3D scene perception in industrial environments","summary":"Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM \"IndustrialCLIP\" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.","authors":["Keno Moenck","Adrian Philip Florea","Julian Koch","Thorsten Schüppstuhl"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19822v1","updated":"2026-02-23T13:22:25Z","published":"2026-02-23T13:22:25Z","title":"Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation","summary":"Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\\%, a specificity of 97.2\\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.","authors":["Dongjing Shan","Yamei Luo","Jiqing Xuan","Lu Huang","Jin Li","Mengchu Yang","Zeyu Chen","Fajin Lv","Yong Tang","Chunxiang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17779v4","updated":"2026-02-23T12:51:16Z","published":"2025-05-23T11:48:48Z","title":"U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding","summary":"Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.","authors":["Anjie Le","Henan Liu","Yue Wang","Zhenyu Liu","Rongkun Zhu","Taohan Weng","Jinze Yu","Boyang Wang","Yalun Wu","Kaiwen Yan","Quanlin Sun","Meirui Jiang","Jialun Pei","Siya Liu","Haoyun Zheng","Zhoujun Li","Alison Noble","Jacques Souquet","Xiaoqing Guo","Manxi Lin","Hongcheng Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.15886v4","updated":"2026-02-23T12:38:59Z","published":"2025-09-19T11:33:10Z","title":"RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation","summary":"Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.","authors":["Paul Julius Kühn","Duc Anh Nguyen","Arjan Kuijper","Saptarshi Neil Sinha"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.01835v4","updated":"2026-02-23T12:24:46Z","published":"2025-07-02T15:49:12Z","title":"Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views","summary":"Hyperspectral reconstruction (HSR) from RGB images is a highly promising direction for accurate color reproduction and material color measurement. While most existing approaches rely on a single RGB image - thereby limiting reconstruction accuracy - the majority of modern smartphones are equipped with two or more cameras. In this work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR) framework that leverages a triple-camera smartphone system, where two lenses are equipped with carefully selected spectral filters. Our easy-to-implement configuration, based on theoretical and empirical analysis, allows to obtain more complete and diverse spectral data than traditional single-chamber setups. To support this new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising aligned images from three smartphone cameras and a hyperspectral reference camera across diverse scenes. We further introduce a lightweight alignment module for MI-HSR that effectively fuses multi-view inputs while mitigating parallax- and occlusion-induced artifacts. Proposed module demonstrate consistent quality improvements for modern HSR methods. In a nutshell, our setup allows 30% more accurate estimations of spectra compared to an ordinary RGB camera, while the proposed alignment module boosts the reconstruction quality of SotA methods by an additional 5%. Our findings suggest that spectral filtering of multiple views with commodity hardware unlocks more accurate and practical hyperspectral imaging.","authors":["Daniil Reutsky","Daniil Vladimirov","Yasin Mamedov","Georgy Perevozchikov","Nancy Mehta","Egor Ershov","Radu Timofte"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19768v1","updated":"2026-02-23T12:18:26Z","published":"2026-02-23T12:18:26Z","title":"TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding","summary":"Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.","authors":["Fan Yang","Shurong Zheng","Hongyin Zhao","Yufei Zhan","Xin Li","Yousong Zhu","Chaoyang Zhao Ming Tang","Jinqiao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19766v1","updated":"2026-02-23T12:15:54Z","published":"2026-02-23T12:15:54Z","title":"One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image","summary":"Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \\textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.","authors":["Pengfei Wang","Liyi Chen","Zhiyuan Ma","Yanjun Guo","Guowen Zhang","Lei Zhang"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19763v1","updated":"2026-02-23T12:12:43Z","published":"2026-02-23T12:12:43Z","title":"Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications","summary":"Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.","authors":["Yida Lin","Bing Xue","Mengjie Zhang","Sam Schofield","Richard Green"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19756v1","updated":"2026-02-23T12:08:28Z","published":"2026-02-23T12:08:28Z","title":"Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis","summary":"Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.","authors":["Junhyeok Choi","Sangwoo Mo","Minwoo Chae"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.05571v2","updated":"2026-02-23T12:05:15Z","published":"2025-12-05T09:53:07Z","title":"MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging","summary":"Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT shows promising capability in identifying anatomical correspondence without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance. Code is available online.","authors":["Xingyu Zhang","Anna Reithmeir","Fryderyk Kögl","Rickmer Braren","Julia A. Schnabel","Daniel M. Lang"],"pdf_url":"","comment":"Updated results"},{"id":"http://arxiv.org/abs/2602.19753v1","updated":"2026-02-23T12:02:03Z","published":"2026-02-23T12:02:03Z","title":"RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing","summary":"3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.","authors":["Kaifa Yang","Qi Yang","Yiling Xu","Zhu Li"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2512.05016v2","updated":"2026-02-23T11:59:47Z","published":"2025-12-04T17:27:32Z","title":"Generative Neural Video Compression via Video Diffusion Prior","summary":"We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.","authors":["Qi Mao","Hao Cheng","Tinghan Yang","Libiao Jin","Siwei Ma"],"pdf_url":"","comment":"accept by CVPR2026"},{"id":"http://arxiv.org/abs/2602.17690v2","updated":"2026-02-23T11:36:42Z","published":"2026-02-06T05:10:19Z","title":"DesignAsCode: Bridging Structural Editability and Visual Fidelity in Graphic Design Generation","summary":"Graphic design generation demands a delicate balance between high visual fidelity and fine-grained structural editability. However, existing approaches typically bifurcate into either non-editable raster image synthesis or abstract layout generation devoid of visual content. Recent combinations of these two approaches attempt to bridge this gap but often suffer from rigid composition schemas and unresolvable visual dissonances (e.g., text-background conflicts) due to their inexpressive representation and open-loop nature. To address these challenges, we propose DesignAsCode, a novel framework that reimagines graphic design as a programmatic synthesis task using HTML/CSS. Specifically, we introduce a Plan-Implement-Reflect pipeline, incorporating a Semantic Planner to construct dynamic, variable-depth element hierarchies and a Visual-Aware Reflection mechanism that iteratively optimizes the code to rectify rendering artifacts. Extensive experiments demonstrate that DesignAsCode significantly outperforms state-of-the-art baselines in both structural validity and aesthetic quality. Furthermore, our code-native representation unlocks advanced capabilities, including automatic layout retargeting, complex document generation (e.g., resumes), and CSS-based animation. Our project page is available at https://liuziyuan1109.github.io/design-as-code/.","authors":["Ziyuan Liu","Shizhao Sun","Danqing Huang","Yingdong Shi","Meisheng Zhang","Ji Li","Jingsong Yu","Jiang Bian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19736v1","updated":"2026-02-23T11:34:59Z","published":"2026-02-23T11:34:59Z","title":"InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution","summary":"Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.","authors":["Shoukun Sun","Zhe Wang","Xiang Que","Jiyin Zhang","Xiaogang Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19735v1","updated":"2026-02-23T11:33:56Z","published":"2026-02-23T11:33:56Z","title":"VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments","summary":"In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.","authors":["Jingyi Xu","Zhangshuo Qi","Zhongmiao Yan","Xuyu Gao","Qianyun Jiao","Songpengcheng Xia","Xieyuanli Chen","Ling Pei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19723v1","updated":"2026-02-23T11:20:27Z","published":"2026-02-23T11:20:27Z","title":"Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets","summary":"Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.","authors":["Yue Zhang","Zhizheng Zhuo","Siyao Xu","Shan Lv","Zhaoxi Liu","Jun Qiu","Qiuli Wang","Yaou Liu","S. Kevin Zhou"],"pdf_url":"","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.19719v1","updated":"2026-02-23T11:15:12Z","published":"2026-02-23T11:15:12Z","title":"Generative 6D Pose Estimation via Conditional Flow Matching","summary":"Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/","authors":["Amir Hamza","Davide Boscaini","Weihang Li","Benjamin Busam","Fabio Poiesi"],"pdf_url":"","comment":"Project Website : https://tev-fbk.github.io/Flose/"},{"id":"http://arxiv.org/abs/2602.19715v1","updated":"2026-02-23T11:08:46Z","published":"2026-02-23T11:08:46Z","title":"Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision","summary":"Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\\%, outperforming \\texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \\href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.","authors":["Kartik Kuckreja","Parul Gupta","Muhammad Haris Khan","Abhinav Dhall"],"pdf_url":"","comment":"CVPR-2026, Code is available here: https://github.com/KjAeRsTuIsK/DeepfakeJudge"},{"id":"http://arxiv.org/abs/2602.19710v1","updated":"2026-02-23T11:00:08Z","published":"2026-02-23T11:00:08Z","title":"Universal Pose Pretraining for Generalizable Vision-Language-Action Policies","summary":"Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.\n  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.\n  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.","authors":["Haitao Lin","Hanyang Yu","Jingshun Huang","He Zhang","Yonggen Ling","Ping Tan","Xiangyang Xue","Yanwei Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19708v1","updated":"2026-02-23T10:59:41Z","published":"2026-02-23T10:59:41Z","title":"ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets","summary":"Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.","authors":["Hoyoung Kim","Minwoo Jang","Jabin Koo","Sangdoo Yun","Jungseul Ok"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19706v1","updated":"2026-02-23T10:57:22Z","published":"2026-02-23T10:57:22Z","title":"HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion","summary":"Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting","authors":["Yo-Tin Lin","Su-Kai Chen","Hou-Ning Hu","Yen-Yu Lin","Yu-Lun Liu"],"pdf_url":"","comment":"WACV 2026. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting"},{"id":"http://arxiv.org/abs/2602.14512v2","updated":"2026-02-23T10:51:34Z","published":"2026-02-16T06:48:48Z","title":"MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction","summary":"Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.","authors":["Zhicheng He","Yunpeng Zhao","Junde Wu","Ziwei Niu","Zijun Li","Bohan Li","Lanfen Lin","Yueming Jin"],"pdf_url":"","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.06742v3","updated":"2026-02-23T10:49:43Z","published":"2025-04-09T09:53:39Z","title":"nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection","summary":"Landmark detection is central to many medical applications, such as identifying critical structures for treatment planning or defining control points for biometric measurements. However, manual annotation is labor-intensive and requires expert anatomical knowledge. While deep learning shows promise in automating this task, fair evaluation and interpretation of methods in a broader context are hindered by limited public benchmarking, inconsistent baseline implementations, and non-standardized experimentation. To overcome these pitfalls, we present nnLandmark, a self-configuring framework for 3D landmark detection that combines tailored heatmap generation, loss design, inference logic, and a robust set of hyperparameters for heatmap regression, while reusing components from nnU-Net's underlying self-configuration and training engine. nnLandmark achieves state-of-the-art performance across five public and one private dataset, benchmarked against three recently published methods. Its out-of-the-box usability enables training strong landmark detection models on new datasets without expert knowledge or dataset-specific hyperparameter tuning. Beyond accuracy, nnLandmark provides both a strong, common baseline and a flexible, standardized environment for developing and evaluating new methodological contributions. It further streamlines evaluation across multiple datasets by offering data conversion utilities for current public benchmarks. Together, these properties position nnLandmark as a central tool for advancing 3D medical landmark detection through systematic, transparent benchmarking, enabling to genuinely measure methodological progress. The code is available on GitHub: https://github.com/MIC-DKFZ/nnLandmark","authors":["Alexandra Ertl","Stefan Denner","Robin Peretzke","Shuhan Xiao","David Zimmerer","Maximilian Fischer","Markus Bujotzek","Xin Yang","Peter Neher","Fabian Isensee","Klaus H. Maier-Hein"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18193v2","updated":"2026-02-23T10:48:48Z","published":"2026-02-20T12:59:27Z","title":"BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards","summary":"Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.","authors":["Yiran Yang","Zhaowei Liu","Yuan Yuan","Yukun Song","Xiong Ma","Yinghao Song","Xiangji Zeng","Lu Sun","Yulu Wang","Hai Zhou","Shuai Cui","Zhaohan Gong","Jiefei Zhang"],"pdf_url":"","comment":"7 pages, 3 figures. To appear in AAAI 2026"},{"id":"http://arxiv.org/abs/2602.19698v1","updated":"2026-02-23T10:44:27Z","published":"2026-02-23T10:44:27Z","title":"Iconographic Classification and Content-Based Recommendation for Digitized Artworks","summary":"We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.","authors":["Krzysztof Kutt","Maciej Baczyński"],"pdf_url":"","comment":"14 pages, 7 figures; submitted to ICCS 2026 conference"},{"id":"http://arxiv.org/abs/2602.19697v1","updated":"2026-02-23T10:44:15Z","published":"2026-02-23T10:44:15Z","title":"BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU","summary":"Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF","authors":["Soumya Mazumdar","Vineet Kumar Rakesh","Tapas Samanta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19679v1","updated":"2026-02-23T10:22:52Z","published":"2026-02-23T10:22:52Z","title":"TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures","summary":"Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.","authors":["Hyeongjin Nam","Daniel Sungho Jung","Kyoung Mu Lee"],"pdf_url":"","comment":"Published at CVPR 2026, 20 pages including the supplementary material"},{"id":"http://arxiv.org/abs/2602.19668v1","updated":"2026-02-23T10:14:36Z","published":"2026-02-23T10:14:36Z","title":"Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation","summary":"Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.\n  We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.\n  Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Kenji Hirata","Minghui Tang","Takaaki Yoshimura","Hiroyuki Sugimori","Noriko Nishioka","Yukie Shimizu","Kohsuke Kudo","Miki Haseyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19631v1","updated":"2026-02-23T09:18:27Z","published":"2026-02-23T09:18:27Z","title":"Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection","summary":"Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.","authors":["Uichan Lee","Jeonghyeon Kim","Sangheum Hwang"],"pdf_url":"","comment":"Accepted at ICLR 2026. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2602.19624v1","updated":"2026-02-23T09:13:55Z","published":"2026-02-23T09:13:55Z","title":"Accurate Planar Tracking With Robust Re-Detection","summary":"We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM","authors":["Jonas Serych","Jiri Matas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19623v1","updated":"2026-02-23T09:12:13Z","published":"2026-02-23T09:12:13Z","title":"PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring","summary":"While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional \"one-shot\" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.","authors":["Injun Baek","Yearim Kim","Nojun Kwak"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19615v1","updated":"2026-02-23T09:02:40Z","published":"2026-02-23T09:02:40Z","title":"Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness","summary":"Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.","authors":["Xin Hu","Haomiao Ni","Yunbei Zhang","Jihun Hamm","Zechen Li","Zhengming Ding"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19611v1","updated":"2026-02-23T08:54:27Z","published":"2026-02-23T08:54:27Z","title":"RAID: Retrieval-Augmented Anomaly Detection","summary":"Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.","authors":["Mingxiu Cai","Zhe Zhang","Gaochang Wu","Tianyou Chai","Xiatian Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19608v1","updated":"2026-02-23T08:50:07Z","published":"2026-02-23T08:50:07Z","title":"Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning","summary":"Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.","authors":["Girmaw Abebe Tadesse","Titien Bartette","Andrew Hassanali","Allen Kim","Jonathan Chemla","Andrew Zolli","Yves Ubelmann","Caleb Robinson","Inbal Becker-Reshef","Juan Lavista Ferres"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19605v1","updated":"2026-02-23T08:47:19Z","published":"2026-02-23T08:47:19Z","title":"CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning","summary":"Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.","authors":["Chunlei Meng","Guanhong Huang","Rong Fu","Runmin Jian","Zhongxue Gan","Chun Ouyang"],"pdf_url":"","comment":"This study has been Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2511.16175v2","updated":"2026-02-23T08:44:17Z","published":"2025-11-20T09:30:23Z","title":"Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight","summary":"Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.","authors":["Yi Yang","Xueqi Li","Yiyang Chen","Jin Song","Yihan Wang","Zipeng Xiao","Jiadi Su","You Qiaoben","Pengfei Liu","Zhijie Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19596v1","updated":"2026-02-23T08:38:27Z","published":"2026-02-23T08:38:27Z","title":"Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception","summary":"Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\\% against state-of-the-art defenses while achieving 47\\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git","authors":["Yihang Tao","Senkang Hu","Haonan An","Zhengru Fang","Hangcheng Cao","Yuguang Fang"],"pdf_url":"","comment":"Accepted by CVPR'26"},{"id":"http://arxiv.org/abs/2510.06751v3","updated":"2026-02-23T08:33:05Z","published":"2025-10-08T08:19:15Z","title":"OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot","summary":"Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.","authors":["Junhan Zhu","Hesong Wang","Mingluo Su","Zefang Wang","Huan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.21990v2","updated":"2026-02-23T08:15:44Z","published":"2025-09-26T07:13:37Z","title":"WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM","summary":"While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\\textbf{u}nified \\& \\textbf{v}ersatile \\textbf{a}udio-\\textbf{v}isual \\textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code and checkpoints are released at \\href{https://github.com/TCL606/WAVE}{https://github.com/TCL606/WAVE}.","authors":["Changli Tang","Qinfan Xiao","Ke Mei","Tianyi Wang","Fengyun Rao","Chao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.17354v3","updated":"2026-02-23T08:13:48Z","published":"2026-01-24T07:58:53Z","title":"PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling","summary":"Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.","authors":["Wenzhi Guo","Guangchi Fang","Shu Yang","Bing Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.13430v2","updated":"2026-02-23T08:05:53Z","published":"2026-02-13T20:07:34Z","title":"Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning","summary":"Chest X-Ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.","authors":["Ha-Hieu Pham","Hai-Dang Nguyen","Thanh-Huy Nguyen","Min Xu","Ulas Bagci","Trung-Nghia Le","Huy-Hieu Pham"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.03349v2","updated":"2026-02-23T07:50:32Z","published":"2025-04-04T11:06:09Z","title":"Meta-DAN: towards an efficient prediction strategy for page-level handwritten text recognition","summary":"Recent advances in text recognition led to a paradigm shift for page-level recognition, from multi-step segmentation-based approaches to end-to-end attention-based ones. However, the naïve character-level autoregressive decoding process results in long prediction times: it requires several seconds to process a single page image on a modern GPU. We propose the Meta Document Attention Network (Meta-DAN) as a novel decoding strategy to reduce the prediction time while enabling a better context modeling. It relies on two main components: windowed queries, to process several transformer queries altogether, enlarging the context modeling with near future; and multi-token predictions, whose goal is to predict several tokens per query instead of only the next one. We evaluate the proposed approach on 10 full-page handwritten datasets and demonstrate state-of-the-art results on average in terms of character error rate. Source code and weights of trained models are available at https://github.com/FactoDeepLearning/meta_dan.","authors":["Denis Coquenet"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19575v1","updated":"2026-02-23T07:46:19Z","published":"2026-02-23T07:46:19Z","title":"ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization","summary":"Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.","authors":["Minseo Kim","Minchan Kwon","Dongyeun Lee","Yunho Jeon","Junmo Kim"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19571v1","updated":"2026-02-23T07:40:32Z","published":"2026-02-23T07:40:32Z","title":"HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies","summary":"Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.","authors":["Chang Liu","Yunfan Ye","Qingyang Zhou","Xichen Tan","Mengxuan Luo","Zhenyu Qiu","Wei Peng","Zhiping Cai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19570v1","updated":"2026-02-23T07:39:43Z","published":"2026-02-23T07:39:43Z","title":"VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense","summary":"Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.","authors":["Nadav Kadvil","Ayellet Tal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19565v1","updated":"2026-02-23T07:30:47Z","published":"2026-02-23T07:30:47Z","title":"DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces","summary":"Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICArt (DIsCrete Diffusion for Articulation Pose Estimation), a novel framework that formulates pose estimation as a conditional discrete diffusion process. Instead of operating in a continuous domain, DICArt progressively denoises a noisy pose representation through a learned reverse diffusion procedure to recover the GT pose. To improve modeling fidelity, we propose a flexible flow decider that dynamically determines whether each token should be denoised or reset, effectively balancing the real and noise distributions during diffusion. Additionally, we incorporate a hierarchical kinematic coupling strategy, estimating the pose of each rigid part hierarchically to respect the object's kinematic structure. We validate DICArt on both synthetic and real-world datasets. Experimental results demonstrate its superior performance and robustness. By integrating discrete generative modeling with structural priors, DICArt offers a new paradigm for reliable category-level 6D pose estimation in complex environments.","authors":["Li Zhang","Mingyu Mei","Ailing Wang","Xianhui Meng","Yan Zhong","Xinyuan Song","Liu Liu","Rujing Wang","Zaixing He","Cewu Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.01289v2","updated":"2026-02-23T07:27:27Z","published":"2026-02-01T15:45:07Z","title":"Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models","summary":"Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.","authors":["Dung Anh Hoang","Cuong Pham anh Trung Le","Jianfei Cai","Thanh-Toan Do"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.06391v2","updated":"2026-02-23T07:21:22Z","published":"2026-01-10T02:28:31Z","title":"Object-WIPER : Training-Free Object and Associated Effect Removal in Videos","summary":"In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.","authors":["Saksham Singh Kushwaha","Sayan Nag","Yapeng Tian","Kuldeep Kulkarni"],"pdf_url":"","comment":"Accepted to CVPR 2026. Project Page: https://sakshamsingh1.github.io/object_wiper_webpage/"},{"id":"http://arxiv.org/abs/2602.19562v1","updated":"2026-02-23T07:20:11Z","published":"2026-02-23T07:20:11Z","title":"A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data","summary":"Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .","authors":["Joseph Bingham"],"pdf_url":"","comment":"19 Pages, 6 figures, preprint"},{"id":"http://arxiv.org/abs/2602.19549v1","updated":"2026-02-23T06:45:19Z","published":"2026-02-23T06:45:19Z","title":"Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework","summary":"Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.","authors":["Yibo Yan","Mingdong Ou","Yi Cao","Xin Zou","Jiahao Huo","Shuliang Liu","James Kwok","Xuming Hu"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2211.12817v3","updated":"2026-02-23T06:38:15Z","published":"2022-11-23T10:02:05Z","title":"Learning to See the Elephant in the Room: Self-Supervised Context Reasoning in Humans and AI","summary":"Humans rarely perceive objects in isolation but interpret scenes through relationships among co-occurring elements. How such contextual knowledge is acquired without explicit supervision remains unclear. Here we combine human psychophysics experiments with computational modelling to study the emergence of contextual reasoning. Participants were exposed to novel objects embedded in naturalistic scenes that followed predefined contextual rules capturing global context, local context and crowding. After viewing short training videos, participants completed a \"lift-the-flap\" task in which a hidden object had to be inferred from the surrounding context under variations in size, resolution and spatial arrangement. Humans rapidly learned these contextual associations without labels or feedback and generalised robustly across contextual changes. We then introduce SeCo (Self-supervised learning for Context Reasoning), a biologically inspired model that learns contextual relationships from complex scenes. SeCo encodes targets and context with separate vision encoders and stores latent contextual priors in a learnable external memory module. Given contextual cues, the model retrieves likely object representations to infer hidden targets. SeCo outperforms state-of-the-art self-supervised learning approaches and predicts object placements most consistent with human behaviour, highlighting the central role of contextual associations in scene understanding.","authors":["Xiao Liu","Soumick Sarker","Ankur Sikarwar","Bryan Atista Kiely","Gabriel Kreiman","Zenglin Shi","Mengmi Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19542v1","updated":"2026-02-23T06:30:36Z","published":"2026-02-23T06:30:36Z","title":"Vinedresser3D: Agentic Text-guided 3D Editing","summary":"Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.","authors":["Yankuan Chi","Xiang Li","Zixuan Huang","James M. Rehg"],"pdf_url":"","comment":"CVPR 2026, Project website:https://vinedresser3d.github.io/"},{"id":"http://arxiv.org/abs/2506.01783v2","updated":"2026-02-23T06:26:39Z","published":"2025-06-02T15:29:41Z","title":"Harnessing Chain-of-Thought Reasoning in Multimodal Large Language Models for Face Anti-Spoofing","summary":"Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image-text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision-language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.","authors":["Honglu Zhang","Zhiqin Fang","Ningning Zhao","Saihui Hou","Long Ma","Renwang Pei","Zhaofeng He"],"pdf_url":"","comment":"Accepted to CVPR2026"},{"id":"http://arxiv.org/abs/2602.19540v1","updated":"2026-02-23T06:21:56Z","published":"2026-02-23T06:21:56Z","title":"A Green Learning Approach to LDCT Image Restoration","summary":"This work proposes a green learning (GL) approach to restore medical images. Without loss of generality, we use low-dose computed tomography (LDCT) images as examples. LDCT images are susceptible to noise and artifacts, where the imaging process introduces distortion. LDCT image restoration is an important preprocessing step for further medical analysis. Deep learning (DL) methods have been developed to solve this problem. We examine an alternative solution using the Green Learning (GL) methodology. The new restoration method is characterized by mathematical transparency, computational and memory efficiency, and high performance. Experiments show that our GL method offers state-of-the-art restoration performance at a smaller model size and with lower inference complexity.","authors":["Wei Wang","Yixing Wu","C. -C. Jay Kuo"],"pdf_url":"","comment":"Published in IEEE International Conference on Image Processing (ICIP), 2025, pp. 1762-1767. Final version available at IEEE Xplore"},{"id":"http://arxiv.org/abs/2210.11974v2","updated":"2026-02-23T06:14:00Z","published":"2022-10-21T14:03:40Z","title":"Face Pyramid Vision Transformer","summary":"A novel Face Pyramid Vision Transformer (FPVT) is proposed to learn a discriminative multi-scale facial representations for face recognition and verification. In FPVT, Face Spatial Reduction Attention (FSRA) and Dimensionality Reduction (FDR) layers are employed to make the feature maps compact, thus reducing the computations. An Improved Patch Embedding (IPE) algorithm is proposed to exploit the benefits of CNNs in ViTs (e.g., shared weights, local context, and receptive fields) to model lower-level edges to higher-level semantic primitives. Within FPVT framework, a Convolutional Feed-Forward Network (CFFN) is proposed that extracts locality information to learn low level facial information. The proposed FPVT is evaluated on seven benchmark datasets and compared with ten existing state-of-the-art methods, including CNNs, pure ViTs, and Convolutional ViTs. Despite fewer parameters, FPVT has demonstrated excellent performance over the compared methods. Project page is available at https://khawar-islam.github.io/fpvt/","authors":["Khawar Islam","Muhammad Zaigham Zaheer","Arif Mahmood"],"pdf_url":"","comment":"Accepted in BMVC 2022"},{"id":"http://arxiv.org/abs/2602.19539v1","updated":"2026-02-23T06:13:52Z","published":"2026-02-23T06:13:52Z","title":"Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems","summary":"Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.","authors":["Xingyu Shen","Tommy Duong","Xiaodong An","Zengqi Zhao","Zebang Hu","Haoyu Hu","Ziyou Wang","Finn Guo","Simiao Ren"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2602.19536v1","updated":"2026-02-23T06:03:07Z","published":"2026-02-23T06:03:07Z","title":"Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection","summary":"Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.","authors":["Zhiwei Ning","Xuanang Gao","Jiaxi Cao","Runze Yang","Huiying Xu","Xinzhong Zhu","Jie Yang","Wei Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19530v1","updated":"2026-02-23T05:47:28Z","published":"2026-02-23T05:47:28Z","title":"ORION: ORthonormal Text Encoding for Universal VLM AdaptatION","summary":"Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.","authors":["Omprakash Chakraborty","Jose Dolz","Ismail Ben Ayed"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19523v1","updated":"2026-02-23T05:25:05Z","published":"2026-02-23T05:25:05Z","title":"OSInsert: Towards High-authenticity and High-fidelity Image Composition","summary":"Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. Some high-authenticity methods can adjust foreground pose/view to be compatible with background, while some high-fidelity methods can preserve the foreground details accurately. However, existing methods can hardly achieve both goals at the same time. In this work, we propose a two-stage strategy to achieve both goals. In the first stage, we use high-authenticity method to generate reasonable foreground shape, serving as the condition of high-fidelity method in the second stage. The experiments on MureCOM dataset verify the effectiveness of our two-stage strategy. The code and model have been released at https://github.com/bcmi/OSInsert-Image-Composition.","authors":["Jingyuan Wang","Li Niu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19517v1","updated":"2026-02-23T05:17:41Z","published":"2026-02-23T05:17:41Z","title":"Classroom Final Exam: An Instructor-Tested Reasoning Benchmark","summary":"We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.","authors":["Chongyang Gao","Diji Yang","Shuyan Zhou","Xichen Yan","Luchuan Song","Shuo Li","Kezhen Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.20629v4","updated":"2026-02-23T05:04:31Z","published":"2025-11-25T18:49:21Z","title":"MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models","summary":"Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.","authors":["Chieh-Yun Chen","Zhonghao Wang","Qi Chen","Zhifan Ye","Min Shi","Yue Zhao","Yinan Zhao","Hui Qu","Wei-An Lin","Yiru Shen","Ajinkya Kale","Irfan Essa","Humphrey Shi"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19512v1","updated":"2026-02-23T04:56:41Z","published":"2026-02-23T04:56:41Z","title":"Variational Trajectory Optimization of Anisotropic Diffusion Schedules","summary":"We introduce a variational framework for diffusion models with anisotropic noise schedules parameterized by a matrix-valued path $M_t(θ)$ that allocates noise across subspaces. Central to our framework is a trajectory-level objective that jointly trains the score network and learns $M_t(θ)$, which encompasses general parameterization classes of matrix-valued noise schedules. We further derive an estimator for the derivative with respect to $θ$ of the score that enables efficient optimization of the $M_t(θ)$ schedule. For inference, we develop an efficiently-implementable reverse-ODE solver that is an anisotropic generalization of the second-order Heun discretization algorithm. Across CIFAR-10, AFHQv2, FFHQ, and ImageNet-64, our method consistently improves upon the baseline EDM model in all NFE regimes. Code is available at https://github.com/lizeyu090312/anisotropic-diffusion-paper.","authors":["Pengxi Liu","Zeyu Michael Li","Xiang Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19506v1","updated":"2026-02-23T04:45:38Z","published":"2026-02-23T04:45:38Z","title":"Relational Feature Caching for Accelerating Diffusion Transformers","summary":"Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC","authors":["Byunggwan Son","Jeimin Jeon","Jeongwoo Choi","Bumsub Ham"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19505v1","updated":"2026-02-23T04:42:10Z","published":"2026-02-23T04:42:10Z","title":"Test-Time Computing for Referring Multimodal Large Language Models","summary":"We propose ControlMLLM++, a novel test-time adaptation framework that injects learnable visual prompts into frozen multimodal large language models (MLLMs) to enable fine-grained region-based visual reasoning without any model retraining or fine-tuning. Leveraging the insight that cross-modal attention maps intrinsically encode semantic correspondences between textual tokens and visual regions, ControlMLLM++ optimizes a latent visual token modifier during inference via a task-specific energy function to steer model attention towards user-specified areas. To enhance optimization stability and mitigate language prompt biases, ControlMLLM++ incorporates an improved optimization strategy (Optim++) and a prompt debiasing mechanism (PromptDebias). Supporting diverse visual prompt types including bounding boxes, masks, scribbles, and points, our method demonstrates strong out-of-domain generalization and interpretability. The code is available at https://github.com/mrwu-mac/ControlMLLM.","authors":["Mingrui Wu","Hao Chen","Jiayi Ji","Xiaoshuai Sun","Zhiyuan Liu","Liujuan Cao","Ming-Ming Cheng","Rongrong Ji"],"pdf_url":"","comment":"arXiv admin note: substantial text overlap with arXiv:2407.21534"},{"id":"http://arxiv.org/abs/2602.19503v1","updated":"2026-02-23T04:40:14Z","published":"2026-02-23T04:40:14Z","title":"A Text-Guided Vision Model for Enhanced Recognition of Small Instances","summary":"As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To address this need, an efficient text-guided object detection model has been developed to enhance the detection of small objects. Specifically, an improved version of the existing YOLO-World model is introduced. The proposed method replaces the C2f layer in the YOLOv8 backbone with a C3k2 layer, enabling more precise representation of local features, particularly for small objects or those with clearly defined boundaries. Additionally, the proposed architecture improves processing speed and efficiency through parallel processing optimization, while also contributing to a more lightweight model design. Comparative experiments on the VisDrone dataset show that the proposed model outperforms the original YOLO-World model, with precision increasing from 40.6% to 41.6%, recall from 30.8% to 31%, F1 score from 35% to 35.5%, and mAP@0.5 from 30.4% to 30.7%, confirming its enhanced accuracy. Furthermore, the model demonstrates superior lightweight performance, with the parameter count reduced from 4 million to 3.8 million and FLOPs decreasing from 15.7 billion to 15.2 billion. These results indicate that the proposed approach provides a practical and effective solution for precise object detection in drone-based applications.","authors":["Hyun-Ki Jung"],"pdf_url":"","comment":"Accepted for publication in Applied Computer Science (2026)"},{"id":"http://arxiv.org/abs/2602.19497v1","updated":"2026-02-23T04:32:52Z","published":"2026-02-23T04:32:52Z","title":"MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models","summary":"Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \\textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \\textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.","authors":["Mingrui Wu","Hang Liu","Jiayi Ji","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"","comment":"CVPR2026"},{"id":"http://arxiv.org/abs/2503.14720v2","updated":"2026-02-23T04:07:31Z","published":"2025-03-18T20:48:58Z","title":"ShapeShift: Text-to-Mosaic Synthesis via Semantic Phase-Field Guidance","summary":"We present ShapeShift, a method for arranging rigid objects into configurations that visually convey semantic concepts specified by natural language. While pretrained diffusion models provide powerful semantic guidance, such as Score Distillation Sampling, enforcing physical validity poses a fundamental challenge. Naive overlap resolution disrupts semantic structure -- separating overlapping shapes along geometrically optimal directions (minimum translation vectors) often destroys the very arrangements that make concepts recognizable.\n  Our intuition is that diffusion model features encode not just what a concept looks like, but its geometric, directional structure -- how it is oriented and shaped -- which we leverage to make overlap resolution semantically aware. We introduce a deformable boundary represented as a phase field that expands anisotropically, guided by intermediate features from the diffusion model, creating space along semantically coherent directions. Experiments demonstrate that ShapeShift, by coupling semantic guidance and feasibility constraint resolution, produces arrangements achieving both semantic clarity and overlap-free validity, significantly outperforming baselines that treat these objectives independently.","authors":["Vihaan Misra","Peter Schaldenbrand","Jean Oh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19487v1","updated":"2026-02-23T04:07:08Z","published":"2026-02-23T04:07:08Z","title":"Exploiting Label-Independent Regularization from Spatial Dependencies for Whole Slide Image Analysis","summary":"Whole slide images, with their gigapixel-scale panoramas of tissue samples, are pivotal for precise disease diagnosis. However, their analysis is hindered by immense data size and scarce annotations. Existing MIL methods face challenges due to the fundamental imbalance where a single bag-level label must guide the learning of numerous patch-level features. This sparse supervision makes it difficult to reliably identify discriminative patches during training, leading to unstable optimization and suboptimal solutions. We propose a spatially regularized MIL framework that leverages inherent spatial relationships among patch features as label-independent regularization signals. Our approach learns a shared representation space by jointly optimizing feature-induced spatial reconstruction and label-guided classification objectives, enforcing consistency between intrinsic structural patterns and supervisory signals. Experimental results on multiple public datasets demonstrate significant improvements over state-of-the-art methods, offering a promising direction.","authors":["Weiyi Wu","Xinwen Xu","Chongyang Gao","Xingjian Diao","Siting Li","Jiang Gui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2405.14504v2","updated":"2026-02-23T03:43:55Z","published":"2024-05-23T12:39:49Z","title":"Adaptive Runge-Kutta Dynamics for Spatiotemporal Prediction","summary":"Spatiotemporal prediction is important in solving natural problems and processing video frames, especially in weather forecasting and human action recognition. Recent advances attempt to incorporate prior physical knowledge into the deep learning framework to estimate the unknown governing partial differential equations (PDEs) in complex dynamics, which have shown promising results in spatiotemporal prediction tasks. However, previous approaches only restrict neural network architectures or loss functions to acquire physical or PDE features, which decreases the representative capacity of a neural network. Meanwhile, the updating process of the physical state cannot be effectively estimated. To solve the problems mentioned above, we introduce a physical-guided neural network, which utilizes an adaptive second-order Runge-Kutta method with physical constraints to model the physical states more precisely. Furthermore, we propose a frequency-enhanced Fourier module to strengthen the model's ability to estimate the spatiotemporal dynamics. We evaluate our model on both spatiotemporal and video prediction tasks. The experimental results show that our model outperforms several state-of-the-art methods and performs the best in several spatiotemporal scenarios with a much smaller parameter count.","authors":["Xuanle Zhao","Yue Sun","Ziyi Wang","Bo Xu","Tielin Zhang"],"pdf_url":"","comment":"Accepted by ICASSP 2026"},{"id":"http://arxiv.org/abs/2601.16449v2","updated":"2026-02-23T03:42:21Z","published":"2026-01-23T05:02:43Z","title":"Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding","summary":"Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.","authors":["Xiaojiang Peng","Jingyi Chen","Zebang Cheng","Bao Peng","Fengyi Wu","Yifei Dong","Shuyuan Tu","Qiyu Hu","Huiting Huang","Yuxiang Lin","Jun-Yan He","Kai Wang","Zheng Lian","Zhi-Qi Cheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19474v1","updated":"2026-02-23T03:36:55Z","published":"2026-02-23T03:36:55Z","title":"Structured Bitmap-to-Mesh Triangulation for Geometry-Aware Discretization of Image-Derived Domains","summary":"We propose a template-driven triangulation framework that embeds raster- or segmentation-derived boundaries into a regular triangular grid for stable PDE discretization on image-derived domains. Unlike constrained Delaunay triangulation (CDT), which may trigger global connectivity updates, our method retriangulates only triangles intersected by the boundary, preserves the base mesh, and supports synchronization-free parallel execution. To ensure determinism and scalability, we classify all local boundary-intersection configurations up to discrete equivalence and triangle symmetries, yielding a finite symbolic lookup table that maps each case to a conflict-free retriangulation template. We prove that the resulting mesh is closed, has bounded angles, and is compatible with cotangent-based discretizations and standard finite element methods. Experiments on elliptic and parabolic PDEs, signal interpolation, and structural metrics show fewer sliver elements, more regular triangles, and improved geometric fidelity near complex boundaries. The framework is well suited for real-time geometric analysis and physically based simulation over image-derived domains.","authors":["Wei Feng","Haiyong Zheng"],"pdf_url":"","comment":"Revised version after peer review; under review at Graphical Models. Earlier version appeared on SSRN"},{"id":"http://arxiv.org/abs/2602.19471v1","updated":"2026-02-23T03:29:54Z","published":"2026-02-23T03:29:54Z","title":"Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model","summary":"Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.","authors":["Zheang Huai","Hui Tang","Hualiang Wang","Xiaomeng Li"],"pdf_url":"","comment":"10 pages"},{"id":"http://arxiv.org/abs/2602.19470v1","updated":"2026-02-23T03:28:41Z","published":"2026-02-23T03:28:41Z","title":"Physics-informed Active Polarimetric 3D Imaging for Specular Surfaces","summary":"3D imaging of specular surfaces remains challenging in real-world scenarios, such as in-line inspection or hand-held scanning, requiring fast and accurate measurement of complex geometries. Optical metrology techniques such as deflectometry achieve high accuracy but typically rely on multi-shot acquisition, making them unsuitable for dynamic environments. Fourier-based single-shot approaches alleviate this constraint, yet their performance deteriorates when measuring surfaces with high spatial frequency structure or large curvature. Alternatively, polarimetric 3D imaging in computer vision operates in a single-shot fashion and exhibits robustness to geometric complexity. However, its accuracy is fundamentally limited by the orthographic imaging assumption. In this paper, we propose a physics-informed deep learning framework for single-shot 3D imaging of complex specular surfaces. Polarization cues provide orientation priors that assist in interpreting geometric information encoded by structured illumination. These complementary cues are processed through a dual-encoder architecture with mutual feature modulation, allowing the network to resolve their nonlinear coupling and directly infer surface normals. The proposed method achieves accurate and robust normal estimation in single-shot with fast inference, enabling practical 3D imaging of complex specular surfaces.","authors":["Jiazhang Wang","Hyelim Yang","Tianyi Wang","Florian Willomitzer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.05992v2","updated":"2026-02-23T03:28:02Z","published":"2025-07-08T13:53:28Z","title":"Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge","summary":"Partial multi-label learning aims to extract knowledge from incompletely annotated data, which includes known correct labels, known incorrect labels, and unknown labels. The core challenge lies in accurately identifying the ambiguous relationships between labels and instances. In this paper, we emphasize that matching co-occurrence patterns between labels and instances is key to addressing this challenge. To this end, we propose Semantic Co-occurrence Insight Network (SCINet), a novel and effective framework for partial multi-label learning. Specifically, SCINet introduces a bi-dominant prompter module, which leverages an off-the-shelf multimodal model to capture text-image correlations and enhance semantic alignment. To reinforce instance-label interdependencies, we develop a cross-modality fusion module that jointly models inter-label correlations, inter-instance relationships, and co-occurrence patterns across instance-label assignments. Moreover, we propose an intrinsic semantic augmentation strategy that enhances the model's understanding of intrinsic data semantics by applying diverse image transformations, thereby fostering a synergistic relationship between label confidence and sample difficulty. Extensive experiments on four widely-used benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.","authors":["Xin Wu","Fei Teng","Yue Feng","Kaibo Shi","Zhuosheng Lin","Ji Zhang","James Wang"],"pdf_url":"","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2602.19461v1","updated":"2026-02-23T03:09:56Z","published":"2026-02-23T03:09:56Z","title":"Laplacian Multi-scale Flow Matching for Generative Modeling","summary":"In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\\times$1024) while maintaining lower computational overhead.","authors":["Zelin Zhao","Petr Molodyk","Haotian Xue","Yongxin Chen"],"pdf_url":"","comment":"Accepted to appear in ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19454v1","updated":"2026-02-23T02:53:05Z","published":"2026-02-23T02:53:05Z","title":"HD-TTA: Hypothesis-Driven Test-Time Adaptation for Safer Brain Tumor Segmentation","summary":"Standard Test-Time Adaptation (TTA) methods typically treat inference as a blind optimization task, applying generic objectives to all or filtered test samples. In safety-critical medical segmentation, this lack of selectivity often causes the tumor mask to spill into healthy brain tissue or degrades predictions that were already correct. We propose Hypothesis-Driven TTA, a novel framework that reformulates adaptation as a dynamic decision process. Rather than forcing a single optimization trajectory, our method generates intuitive competing geometric hypotheses: compaction (is the prediction noisy? trim artifacts) versus inflation (is the valid tumor under-segmented? safely inflate to recover). It then employs a representation-guided selector to autonomously identify the safest outcome based on intrinsic texture consistency. Additionally, a pre-screening Gatekeeper prevents negative transfer by skipping adaptation on confident cases. We validate this proof-of-concept on a cross-domain binary brain tumor segmentation task, applying a source model trained on adult BraTS gliomas to unseen pediatric and more challenging meningioma target domains. HD-TTA improves safety-oriented outcomes (Hausdorff Distance (HD95) and Precision) over several state-of-the-art representative baselines in the challenging safety regime, reducing the HD95 by approximately 6.4 mm and improving Precision by over 4%, while maintaining comparable Dice scores. These results demonstrate that resolving the safety-adaptation trade-off via explicit hypothesis selection is a viable, robust path for safe clinical model deployment. Code will be made publicly available upon acceptance.","authors":["Kartik Jhawar","Lipo Wang"],"pdf_url":"","comment":"11 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2510.00523v2","updated":"2026-02-23T02:47:15Z","published":"2025-10-01T05:11:54Z","title":"VIRTUE: Visual-Interactive Text-Image Universal Embedder","summary":"Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.","authors":["Wei-Yao Wang","Kazuya Tateishi","Qiyu Wu","Shusuke Takahashi","Yuki Mitsufuji"],"pdf_url":"","comment":"ICLR 2026. 25 pages. Project page: https://sony.github.io/virtue/"},{"id":"http://arxiv.org/abs/2602.19449v1","updated":"2026-02-23T02:39:26Z","published":"2026-02-23T02:39:26Z","title":"Decoupling Vision and Language: Codebook Anchored Visual Adaptation","summary":"Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.","authors":["Jason Wu","Tianchen Zhao","Chang Liu","Jiarui Cai","Zheng Zhang","Zhuowei Li","Aaditya Singh","Xiang Xu","Mani Srivastava","Jonathan Wu"],"pdf_url":"","comment":"17 pages, accepted to CVPR2026 main conference"},{"id":"http://arxiv.org/abs/2602.19442v1","updated":"2026-02-23T02:24:55Z","published":"2026-02-23T02:24:55Z","title":"UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment","summary":"Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.","authors":["Yecheng Zhang","Rong Zhao","Zhizhou Sha","Yong Li","Lei Wang","Ce Hou","Wen Ji","Hao Huang","Yunshan Wan","Jian Yu","Junhao Xia","Yuru Zhang","Chunlei Shi"],"pdf_url":"","comment":"26 pages"},{"id":"http://arxiv.org/abs/2505.22499v3","updated":"2026-02-23T02:14:34Z","published":"2025-05-28T15:49:54Z","title":"SABER: Spatially Consistent 3D Universal Adversarial Objects for BEV Detectors","summary":"Adversarial robustness of BEV 3D object detectors is critical for autonomous driving (AD). Existing invasive attacks require altering the target vehicle itself (e.g. attaching patches), making them unrealistic and impractical for real-world evaluation. While non-invasive attacks that place adversarial objects in the environment are more practical, current methods still lack the multi-view and temporal consistency needed for physically plausible threats. In this paper, we present the first framework for generating universal, non-invasive, and 3D-consistent adversarial objects that expose fundamental vulnerabilities for BEV 3D object detectors. Instead of modifying target vehicles, our method inserts rendered objects into scenes with an occlusion-aware module that enforces physical plausibility across views and time. To maintain attack effectiveness across views and frames, we optimize adversarial object appearance using a BEV spatial feature-guided optimization strategy that attacks the detector's internal representations. Extensive experiments demonstrate that our learned universal adversarial objects can consistently degrade multiple BEV detectors from various viewpoints and distances. More importantly, the new environment-manipulation attack paradigm exposes models' over-reliance on contextual cues and provides a practical pipeline for robustness evaluation in AD systems.","authors":["Aixuan Li","Mochu Xiang","Bosen Hou","Zhexiong Wan","Jing Zhang","Yuchao Dai"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19437v1","updated":"2026-02-23T02:12:47Z","published":"2026-02-23T02:12:47Z","title":"FinSight-Net:A Physics-Aware Decoupled Network with Frequency-Domain Compensation for Underwater Fish Detection in Smart Aquaculture","summary":"Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition. To address these challenges, we propose FinSight-Net, an efficient and physics-aware detection framework tailored for complex aquaculture environments. FinSight-Net introduces a Multi-Scale Decoupled Dual-Stream Processing (MS-DDSP) bottleneck that explicitly targets frequency-specific information loss via heterogeneous convolutional branches, suppressing backscattering artifacts while compensating distorted biological cues through scale-aware and channel-weighted pathways. We further design an Efficient Path Aggregation FPN (EPA-FPN) as a detail-filling mechanism: it restores high-frequency spatial information typically attenuated in deep layers by establishing long-range skip connections and pruning redundant fusion routes, enabling robust detection of non-rigid fish targets under severe blur and turbidity. Extensive experiments on DeepFish, AquaFishSet, and our challenging UW-BlurredFish benchmark demonstrate that FinSight-Net achieves state-of-the-art performance. In particular, on UW-BlurredFish, FinSight-Net reaches 92.8% mAP, outperforming YOLOv11s by 4.8% while reducing parameters by 29.0%, providing a strong and lightweight solution for real-time automated monitoring in smart aquaculture.","authors":["Jinsong Yang","Zeyuan Hu","Yichen Li","Hong Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.07709v2","updated":"2026-02-23T02:09:23Z","published":"2025-06-09T12:51:10Z","title":"Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding","summary":"With the remarkable progress in neural P-frame video coding, neural B-frame coding has recently emerged as a critical research direction. However, most existing neural B-frame codecs directly adopt P-frame coding tools without adequately addressing the unique challenges of B-frame compression, leading to suboptimal performance. To bridge this gap, we propose novel enhancements for motion compression and temporal fusion for neural B-frame coding. First, we design a fine-grained motion compression method. This method incorporates an interactive dual-branch motion auto-encoder with per-branch adaptive quantization steps, which enables fine-grained compression of bi-directional motion vectors while accommodating their asymmetric bitrate allocation and reconstruction quality requirements. Furthermore, this method involves an interactive motion entropy model that exploits correlations between bi-directional motion latent representations by interactively leveraging partitioned latent segments as directional priors. Second, we propose a selective temporal fusion method that predicts bi-directional fusion weights to achieve discriminative utilization of bi-directional multi-scale temporal contexts with varying qualities. Additionally, this method introduces a hyperprior-based implicit alignment mechanism for contextual entropy modeling. By treating the hyperprior as a surrogate for the contextual latent representation, this mechanism implicitly mitigates the misalignment in the fused bi-directional temporal priors. Extensive experiments demonstrate that our proposed codec achieves an average BD-rate reduction of approximately 10% compared to the state-of-the-art neural B-frame codec, DCVC-B, and delivers comparable or even superior compression performance to the H.266/VVC reference software under random-access configurations.","authors":["Xihua Sheng","Peilin Chen","Meng Wang","Li Zhang","Shiqi Wang","Dapeng Oliver Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19432v1","updated":"2026-02-23T02:01:44Z","published":"2026-02-23T02:01:44Z","title":"CountEx: Fine-Grained Counting via Exemplars and Exclusion","summary":"This paper presents CountEx, a discriminative visual counting framework designed to address a key limitation of existing prompt-based methods: the inability to explicitly exclude visually similar distractors. While current approaches allow users to specify what to count via inclusion prompts, they often struggle in cluttered scenes with confusable object categories, leading to ambiguity and overcounting. CountEx enables users to express both inclusion and exclusion intent, specifying what to count and what to ignore, through multimodal prompts including natural language descriptions and optional visual exemplars. At the core of CountEx is a novel Discriminative Query Refinement module, which jointly reasons over inclusion and exclusion cues by first identifying shared visual features, then isolating exclusion-specific patterns, and finally applying selective suppression to refine the counting query. To support systematic evaluation of fine-grained counting methods, we introduce CoCount, a benchmark comprising 1,780 videos and 10,086 annotated frames across 97 category pairs. Experiments show that CountEx achieves substantial improvements over state-of-the-art methods for counting objects from both known and novel categories. The data and code are available at https://github.com/bbvisual/CountEx.","authors":["Yifeng Huang","Gia Khanh Nguyen","Minh Hoai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19430v1","updated":"2026-02-23T01:56:29Z","published":"2026-02-23T01:56:29Z","title":"TherA: Thermal-Aware Visual-Language Prompting for Controllable RGB-to-Thermal Infrared Translation","summary":"Despite the inherent advantages of thermal infrared(TIR) imaging, large-scale data collection and annotation remain a major bottleneck for TIR-based perception. A practical alternative is to synthesize pseudo TIR data via image translation; however, most RGB-to-TIR approaches heavily rely on RGB-centric priors that overlook thermal physics, yielding implausible heat distributions. In this paper, we introduce TherA, a controllable RGB-to-TIR translation framework that produces diverse and thermally plausible images at both scene and object level. TherA couples TherA-VLM with a latent-diffusion-based translator. Given a single RGB image and a user-prompted condition pair, TherA-VLM yields a thermal-aware embedding that encodes scene, object, material, and heat-emission context reflecting the input scene-condition pair. Conditioning the diffusion model on this embedding enables realistic TIR synthesis and fine-grained control across time of day, weather, and object state. Compared to other baselines, TherA achieves state-of-the-art translation performance, demonstrating improved zero-shot translation performance up to 33% increase averaged across all metrics.","authors":["Dong-Guw Lee","Tai Hyoung Rhee","Hyunsoo Jang","Young-Sik Shin","Ukcheol Shin","Ayoung Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.21258v2","updated":"2026-02-23T01:50:53Z","published":"2025-03-27T08:31:46Z","title":"Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning","summary":"Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods.","authors":["Jizhou Han","Chenhao Ding","Yuhang He","Songlin Dong","Qiang Wang","Xinyuan Gao","Yihong Gong"],"pdf_url":"","comment":"Accepted by IEEE TCSVT. This is the author's version which has not been fully edited and content may change prior to final publication"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2602.20135v1","updated":"2026-02-23T18:46:27Z","published":"2026-02-23T18:46:27Z","title":"KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration","summary":"With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.","authors":["Mohammad Amanlou","Erfan Shafiee Moghaddam","Yasaman Amou Jafari","Mahdi Noori","Farhan Farsi","Behnam Bahrak"],"pdf_url":"","comment":"Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)"},{"id":"http://arxiv.org/abs/2602.20122v1","updated":"2026-02-23T18:37:49Z","published":"2026-02-23T18:37:49Z","title":"NanoKnow: How to Know What Your Language Model Knows","summary":"How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.","authors":["Lingwei Gu","Nour Jedidi","Jimmy Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20093v1","updated":"2026-02-23T18:02:50Z","published":"2026-02-23T18:02:50Z","title":"ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation","summary":"Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.","authors":["Kun Yang","Yuxuan Zhu","Yazhe Chen","Siyao Zheng","Bangyang Hong","Kangle Wu","Yabo Ni","Anxiang Zeng","Cong Fu","Hui Li"],"pdf_url":"","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.20001v1","updated":"2026-02-23T16:08:32Z","published":"2026-02-23T16:08:32Z","title":"FairFS: Addressing Deep Feature Selection Biases for Recommender System","summary":"Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.","authors":["Xianquan Wang","Zhaocheng Du","Jieming Zhu","Qinglin Jia","Zhenhua Dong","Kai Zhang"],"pdf_url":"","comment":"Accepted by The Web Conference 2026"},{"id":"http://arxiv.org/abs/2602.19990v1","updated":"2026-02-23T15:55:32Z","published":"2026-02-23T15:55:32Z","title":"A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT","summary":"Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scenarios. This work proposes a context-aware semantic platform for data stream management that unifies heterogeneous IoT/IoE data sources through a Knowledge Graph enabling formal representation of devices, streams, agents, transformation pipelines, roles and rights. The model supports flexible data gathering, composable stream processing pipelines, and dynamic role-based data access based on agents' contexts, relying on Apache Kafka and Apache Flink for real-time processing, while SPARQL and SWRL-based reasoning provide context-dependent stream discovery. Experimental evaluations demonstrate the effectiveness of combining semantic models, context-aware reasoning and distributed stream processing to enable interoperable data workflows for Industry 5.0 environments.","authors":["Monica Marconi Sciarroni","Emanuele Storti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19987v1","updated":"2026-02-23T15:53:25Z","published":"2026-02-23T15:53:25Z","title":"Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction","summary":"This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.","authors":["Ha-Anh Hoang Nguyen","Tri-Duc Phan Le","Duc-Hoang Pham","Huy-Son Nguyen","Cam-Van Thi Nguyen","Duc-Trong Le","Hoang-Quynh Le"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19961v1","updated":"2026-02-23T15:27:41Z","published":"2026-02-23T15:27:41Z","title":"Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval","summary":"With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.","authors":["Yibo Yan","Jiahao Huo","Guanbo Feng","Mingdong Ou","Yi Cao","Xin Zou","Shuliang Liu","Yuanhuiyi Lyu","Yu Huang","Jungang Li","Kening Zheng","Xu Zheng","Philip S. Yu","James Kwok","Xuming Hu"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2512.24943v2","updated":"2026-02-23T13:41:40Z","published":"2025-12-31T16:09:08Z","title":"RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment","summary":"Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.","authors":["Chenji Lu","Zhuo Chen","Hui Zhao","Zhenyi Wang","Pengjie Wang","Chuan Yu","Jian Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19778v1","updated":"2026-02-23T12:32:53Z","published":"2026-02-23T12:32:53Z","title":"Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation","summary":"Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.","authors":["Nghia Phan","Rong Jin","Gang Liu","Xiao Dong"],"pdf_url":"","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.04272v4","updated":"2026-02-23T11:53:02Z","published":"2024-12-05T15:54:16Z","title":"PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables","summary":"In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable.","authors":["Qingyang Mao","Qi Liu","Zhi Li","Mingyue Cheng","Zheng Zhang","Rui Li"],"pdf_url":"","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.01450v3","updated":"2026-02-23T11:43:23Z","published":"2026-02-01T21:39:36Z","title":"The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT","summary":"To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.\n  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.","authors":["Abhisek Dash","Soumi Das","Elisabeth Kirsten","Qinyuan Wu","Sai Keerthana Karnam","Krishna P. Gummadi","Thorsten Holz","Muhammad Bilal Zafar","Savvas Zannettou"],"pdf_url":"","comment":"This paper has been accepted at The ACM Web Conference 2026"},{"id":"http://arxiv.org/abs/2602.19728v1","updated":"2026-02-23T11:26:11Z","published":"2026-02-23T11:26:11Z","title":"GrIT: Group Informed Transformer for Sequential Recommendation","summary":"Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling temporally evolving group features alongside individual user histories can significantly enhance next-item recommendation. Our approach introduces latent group representations, where each user's affiliation to these groups is modeled through learnable, time-varying membership weights. The membership weights at each timestep are computed by modeling shifts in user preferences through their interaction history, where we incorporate both short-term and long-term user preferences. We extract a set of statistical features that capture the dynamics of user behavior and further refine them through a series of transformations to produce the final drift-aware membership weights. A group-based representation is derived by weighting latent group embeddings with the learned membership scores. This representation is integrated with the user's sequential representation within the transformer block to jointly capture personal and group-level temporal dynamics, producing richer embeddings that lead to more accurate, context-aware recommendations. We validate the effectiveness of our approach through extensive experiments on five benchmark datasets, where it consistently outperforms state-of-the-art sequential recommendation methods.","authors":["Adamya Shyam","Venkateswara Rao Kagita","Bharti Rana","Vikas Kumar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19711v1","updated":"2026-02-23T11:02:13Z","published":"2026-02-23T11:02:13Z","title":"A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs","summary":"The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation.","authors":["Krzysztof Kutt","Elżbieta Sroka","Oleksandra Ishchuk","Luiz do Valle Miranda"],"pdf_url":"","comment":"15 pages, 1 figure; submitted to ICCS 2026 conference"},{"id":"http://arxiv.org/abs/2602.19702v1","updated":"2026-02-23T10:52:20Z","published":"2026-02-23T10:52:20Z","title":"DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework","summary":"Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.","authors":["Adamya Shyam","Venkateswara Rao Kagita","Bharti Rana","Vikas Kumar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19698v1","updated":"2026-02-23T10:44:27Z","published":"2026-02-23T10:44:27Z","title":"Iconographic Classification and Content-Based Recommendation for Digitized Artworks","summary":"We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.","authors":["Krzysztof Kutt","Maciej Baczyński"],"pdf_url":"","comment":"14 pages, 7 figures; submitted to ICCS 2026 conference"},{"id":"http://arxiv.org/abs/2602.19549v1","updated":"2026-02-23T06:45:19Z","published":"2026-02-23T06:45:19Z","title":"Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework","summary":"Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.","authors":["Yibo Yan","Mingdong Ou","Yi Cao","Xin Zou","Jiahao Huo","Shuliang Liu","James Kwok","Xuming Hu"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.19543v1","updated":"2026-02-23T06:32:00Z","published":"2026-02-23T06:32:00Z","title":"Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation","summary":"Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \\textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \\textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \\textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \\textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \\textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.","authors":["Rizhuo Huang","Yifan Feng","Rundong Xue","Shihui Ying","Jun-Hai Yong","Chuan Shi","Shaoyi Du","Yue Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.10507v5","updated":"2026-02-23T02:22:58Z","published":"2025-04-09T17:46:12Z","title":"PinRec: Unified Generative Retrieval for Pinterest Recommender Systems","summary":"Generative retrieval methods employ sequential modeling techniques, like transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models such as two tower architectures. However, a key limitation is that current approaches require a separate model for each product surface, as building a unified model that accommodates the different business needs of various surfaces has proven challenging. Furthermore, existing methods often fail to capture the evolution of user interests over a sequence, focusing instead on only predicting the next item. This paper introduces PinRec, a novel unified generative retrieval model for all of Pinterest recommendation surfaces, including home feed, search, and related pins. PinRec is pretrained on user activity sequences aggregated across surfaces, then finetuned for each surface using impression data from that surface. This pretraining and finetuning approach enables a single unified model while still adapting to the needs of individual surfaces. To better align recommendations with surface specific business goals, PinRec incorporates a novel outcome conditioned generation mechanism that targets different outcomes for each surface, which further enhances the impact of finetuning. Our experiments show that PinRec balances performance, diversity, and efficiency, delivering significant gains such as +4% increase in search saves. To our knowledge, this paper presents the first rigorous study of a unified generative retrieval model built and deployed at Pinterest scale, marking a significant milestone in the field.","authors":["Edoardo Botta","Jaewon Yang","Yi-Ping Hsu","Laksh Bhasin","Prabhat Agarwal","Anirudhan Badrinath","Yilin Chen","Jiajing Xu","Charles Rosenberg"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.24787v2","updated":"2026-02-23T23:54:28Z","published":"2025-12-31T11:16:24Z","title":"HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment","summary":"Slate recommendation, which presents users with a ranked item list in a single display, is ubiquitous across mainstream online platforms. Recent advances in generative models have shown significant potential for this task via autoregressive modeling of discrete semantic ID sequences. However, existing methods suffer from three key limitations: entangled item tokenization, inefficient sequential decoding, and the absence of holistic slate planning. These issues often result in substantial inference overhead and inadequate alignment with diverse user preferences and practical business requirements, hindering the industrial deployment of generative slate recommendation systems. In this paper, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we design an auto-encoder incorporating residual quantization and contrastive constraints, which tokenizes items into semantically structured IDs to enable controllable generation. Second, HiGR decouples the generation process into two stages: a list-level planning stage to capture global slate intent, and an item-level decoding stage to select specific items, effectively reducing the search space and enabling efficient generation. Third, we introduce a multi-objective and listwise preference alignment mechanism that enhances slate quality by leveraging implicit user feedback. Extensive experiments have validated the effectiveness of our HiGR method. Notably, it outperforms state-of-the-art baselines by over 10\\% in offline recommendation quality while achieving a $5\\times$ inference speedup. Furthermore, we have deployed HiGR on a commercial platform under Tencent (serving hundreds of millions of users), and online A/B tests show that it increases average watch time and average video plays by 1.22\\% and 1.73\\%, respectively.","authors":["Yunsheng Pang","Zijian Liu","Yudong Li","Shaojie Zhu","Zijian Luo","Chenyun Yu","Sikai Wu","Shichen Shen","Cong Xu","Bin Wang","Kai Jiang","Hongyong Yu","Chengxiang Zhuo","Zang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.13404v4","updated":"2026-02-23T19:47:40Z","published":"2025-08-18T23:48:22Z","title":"TASER: Table Agents for Schema-guided Extraction and Recommendation","summary":"Real-world financial filings report critical information about an entity's investment holdings, essential for assessing that entity's risk, profitability, and relationship profile. Yet, these details are often buried in messy, multi-page, fragmented tables that are difficult to parse, hindering downstream QA and data normalization. Specifically, 99.4% of the tables in our financial table dataset lack bounding boxes, with the largest table spanning 44 pages. To address this, we present TASER (Table Agents for Schema-guided Extraction and Recommendation), a continuously learning, agentic table extraction system that converts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Guided by an initial portfolio schema, TASER executes table detection, classification, extraction, and recommendations in a single pipeline. Our Recommender Agent reviews unmatched outputs and proposes schema revisions, enabling TASER to outperform vision-based table detection models such as Table Transformer by 10.1%. Within this continuous learning process, larger batch sizes yield a 104.3% increase in useful schema recommendations and a 9.8% increase in total extractions. To train TASER, we manually labeled 22,584 pages and 3,213 tables covering $731.7 billion in holdings, culminating in TASERTab to facilitate research on real-world financial tables and structured outputs. Our results highlight the promise of continuously learning agents for robust extractions from complex tabular data.","authors":["Nicole Cho","Kirsty Fielding","William Watson","Sumitra Ganesh","Manuela Veloso"],"pdf_url":"","comment":"EACL 2026 Industry (Oral)"},{"id":"http://arxiv.org/abs/2405.10385v3","updated":"2026-02-23T19:08:20Z","published":"2024-05-16T18:26:38Z","title":"Augmenting Lateral Thinking in Language Models with Humor and Riddle Data for the BRAINTEASER Task","summary":"The SemEval 2024 BRAINTEASER task challenges language models to perform lateral thinking -- a form of creative, non-linear reasoning that remains underexplored in NLP. The task comprises two subtasks, Sentence Puzzle and Word Puzzle, requiring models to defy conventional commonsense associations. We present a system that fine-tunes DeBERTaV3 using HuggingFace's AutoModelForMultipleChoice architecture. We augment the provided training data with two additional sources: (1) a humor-style question-answering dataset generated via GPT-4 prompting, and (2) the RiddleSense dataset. This data augmentation strategy is motivated by the observation that humor and riddles share the lateral reasoning structure required by the task. Our best system achieves 92.5\\% overall accuracy on the Sentence Puzzle subtask and 80.2\\% on the Word Puzzle subtask, ranking 6th out of 31 teams and 10th out of 23 teams, respectively. We further show that the choice of task formulation matters: framing the problem as multiple-choice rather than sequence classification yields a 10-point accuracy improvement with the same base model. Our analysis reveals that data augmentation with humor and riddle data is particularly effective for sentence-level lateral reasoning, while word-level puzzles remain a harder challenge.","authors":["Mina Ghashami","Soumya Smruti Mishra"],"pdf_url":"","comment":"Accepted at SemEval 2024 (Colocated with NAACL 2024)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2602.20159v1","updated":"2026-02-23T18:59:41Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2602.20156v1","updated":"2026-02-23T18:59:27Z","published":"2026-02-23T18:59:27Z","title":"Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks","summary":"LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.","authors":["David Schmotz","Luca Beurer-Kellner","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20153v1","updated":"2026-02-23T18:59:10Z","published":"2026-02-23T18:59:10Z","title":"JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks","summary":"We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.","authors":["Jakob Heiss","Sören Lambrecht","Jakob Weissteiner","Hanna Wutte","Žan Žurič","Josef Teichmann","Bin Yu"],"pdf_url":"","comment":"11 pages + appendix. Preliminary version of an ongoing project that will be expanded with furhter evaluations"},{"id":"http://arxiv.org/abs/2602.20152v1","updated":"2026-02-23T18:59:04Z","published":"2026-02-23T18:59:04Z","title":"Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data","summary":"Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.","authors":["Zhenyao Ma","Yue Liang","Dongxu Li"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20151v1","updated":"2026-02-23T18:58:54Z","published":"2026-02-23T18:58:54Z","title":"Conformal Risk Control for Non-Monotonic Losses","summary":"Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.","authors":["Anastasios N. Angelopoulos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.03817v3","updated":"2026-02-23T18:54:13Z","published":"2025-10-04T14:14:20Z","title":"TROLL: Trust Regions improve Reinforcement Learning for Large Language Models","summary":"Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.","authors":["Philipp Becker","Niklas Freymuth","Serge Thilges","Fabian Otto","Gerhard Neumann"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.16666v2","updated":"2026-02-23T18:49:07Z","published":"2026-02-18T18:05:44Z","title":"Towards a Science of AI Agent Reliability","summary":"AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.","authors":["Stephan Rabanser","Sayash Kapoor","Peter Kirgis","Kangheng Liu","Saiteja Utpala","Arvind Narayanan"],"pdf_url":"","comment":"Interactive dashboard available at: https://hal.cs.princeton.edu/reliability"},{"id":"http://arxiv.org/abs/2512.01149v2","updated":"2026-02-23T18:46:56Z","published":"2025-11-30T23:59:37Z","title":"A Benchmark of Causal vs. Correlation AI for Predictive Maintenance","summary":"Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications.","authors":["Shaunak Dhande","Chutian Ma","Giacinto Paolo Saggese","Paul Smith","Krishna Taduri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18406v2","updated":"2026-02-23T18:44:49Z","published":"2026-02-20T18:14:05Z","title":"Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges","summary":"Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training$\\unicode{x2013}$for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to learn equivariant operators in a latent space, from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.","authors":["Minh Dinh","Stéphane Deny"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20132v1","updated":"2026-02-23T18:44:10Z","published":"2026-02-23T18:44:10Z","title":"LAD: Learning Advantage Distribution for Reasoning","summary":"Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.","authors":["Wendi Li","Sharon Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20126v1","updated":"2026-02-23T18:41:34Z","published":"2026-02-23T18:41:34Z","title":"Adaptation to Intrinsic Dependence in Diffusion Language Models","summary":"Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.","authors":["Yunxiao Zhao","Changxiao Cai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20122v1","updated":"2026-02-23T18:37:49Z","published":"2026-02-23T18:37:49Z","title":"NanoKnow: How to Know What Your Language Model Knows","summary":"How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.","authors":["Lingwei Gu","Nour Jedidi","Jimmy Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20117v1","updated":"2026-02-23T18:34:29Z","published":"2026-02-23T18:34:29Z","title":"ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models","summary":"Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs","authors":["Andre He","Nathaniel Weir","Kaj Bostrom","Allen Nie","Darion Cassel","Sam Bayless","Huzefa Rangwala"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20111v1","updated":"2026-02-23T18:30:48Z","published":"2026-02-23T18:30:48Z","title":"Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds","summary":"We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.\n  We resolve this question by proving a matching $Ω(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.","authors":["Ezra Edelman","Surbhi Goel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.10471v2","updated":"2026-02-23T18:26:51Z","published":"2025-01-16T06:56:43Z","title":"VillageNet: Graph-based, Easily-interpretable, Unsupervised Clustering for Broad Biomedical Applications","summary":"Clustering large high-dimensional datasets with diverse variable is essential for extracting high-level latent information from these datasets. Here, we developed an unsupervised clustering algorithm, we call \"Village-Net\". Village-Net is specifically designed to effectively cluster high-dimension data without priori knowledge on the number of existing clusters. The algorithm operates in two phases: first, utilizing K-Means clustering, it divides the dataset into distinct subsets we refer to as \"villages\". Next, a weighted network is created, with each node representing a village, capturing their proximity relationships. To achieve optimal clustering, we process this network using a community detection algorithm called Walk-likelihood Community Finder (WLCF), a community detection algorithm developed by one of our team members. A salient feature of Village-Net Clustering is its ability to autonomously determine an optimal number of clusters for further analysis based on inherent characteristics of the data. We present extensive benchmarking on extant real-world datasets with known ground-truth labels to showcase its competitive performance, particularly in terms of the normalized mutual information (NMI) score, when compared to other state-of-the-art methods. The algorithm is computationally efficient, boasting a time complexity of O(N*k*d), where N signifies the number of instances, k represents the number of villages and d represents the dimension of the dataset, which makes it well suited for effectively handling large-scale datasets.","authors":["Aditya Ballal","Gregory A. DePaul","Esha Datta","Asuka Hatano","Erik Carlsson","Ye Chen-Izu","Javier E. López","Leighton T. Izu"],"pdf_url":"","comment":"Software available at https://villagenet.streamlit.app/ Github Link: https://github.com/lordareicgnon/VillageNet"},{"id":"http://arxiv.org/abs/2602.05165v3","updated":"2026-02-23T18:23:57Z","published":"2026-02-05T00:33:02Z","title":"EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.","authors":["Kevin Han","Yuhang Zhou","Mingze Gao","Gedi Zhou","Serena Li","Abhishek Kumar","Xiangjun Fan","Weiwei Li","Lizhu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20104v1","updated":"2026-02-23T18:22:58Z","published":"2026-02-23T18:22:58Z","title":"Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration","summary":"In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.","authors":["Hasan Amin","Ming Yin","Rajiv Khanna"],"pdf_url":"","comment":"AAAI 2026"},{"id":"http://arxiv.org/abs/2602.20102v1","updated":"2026-02-23T18:19:46Z","published":"2026-02-23T18:19:46Z","title":"BarrierSteer: LLM Safety via Learning Barrier Steering","summary":"Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.","authors":["Thanh Q. Tran","Arun Verma","Kiwan Wong","Bryan Kian Hsiang Low","Daniela Rus","Wei Xiao"],"pdf_url":"","comment":"This paper introduces SafeBarrier, a framework that enforces safety in large language models by steering their latent representations with control barrier functions during inference, reducing adversarial and unsafe outputs"},{"id":"http://arxiv.org/abs/2504.12389v2","updated":"2026-02-23T18:15:25Z","published":"2025-04-16T18:00:46Z","title":"Predictive control of blast furnace temperature in steelmaking with hybrid depth-infused quantum neural networks","summary":"Accurate prediction and stabilization of blast furnace temperatures are crucial for optimizing the efficiency and productivity of steel production. Traditional methods often struggle with the complex and non-linear nature of the temperature fluctuations within blast furnaces. This paper proposes a novel approach that combines hybrid quantum machine learning with pulverized coal injection control to address these challenges. By integrating classical machine learning techniques with quantum computing algorithms, we aim to enhance predictive accuracy and achieve more stable temperature control. For this we utilized a unique prediction-based optimization method. Our method leverages quantum-enhanced feature space exploration and the robustness of classical regression models to forecast temperature variations and optimize pulverized coal injection values. Our results demonstrate a significant improvement in prediction accuracy over 25 percent and our solution improved temperature stability to +-7.6 degrees of target range from the earlier variance of +-50 degrees, highlighting the potential of hybrid quantum machine learning models in industrial steel production applications.","authors":["Nayoung Lee","Minsoo Shin","Asel Sagingalieva","Arsenii Senokosov","Matvei Anoshin","Ayush Joshi Tripathi","Karan Pinto","Alexey Melnikov"],"pdf_url":"","comment":"21 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2602.02853v2","updated":"2026-02-23T17:55:22Z","published":"2026-02-02T21:59:35Z","title":"Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data","summary":"Equivariant neural networks exploit underlying task symmetries to improve generalization, but strict equivariance constraints can induce more complex optimization dynamics that can hinder learning. Prior work addresses these limitations by relaxing strict equivariance during training, but typically relies on prespecified, explicit, or implicit target levels of relaxation for each network layer, which are task-dependent and costly to tune. We propose Recurrent Equivariant Constraint Modulation (RECM), a layer-wise constraint modulation mechanism that learns appropriate relaxation levels solely from the training signal and the symmetry properties of each layer's input-target distribution, without requiring any prior knowledge about the task-dependent target relaxation level. We demonstrate that under the proposed RECM update, the relaxation level of each layer provably converges to a value upper-bounded by its symmetry gap, namely the degree to which its input-target distribution deviates from exact symmetry. Consequently, layers processing symmetric distributions recover full equivariance, while those with approximate symmetries retain sufficient flexibility to learn non-symmetric solutions when warranted by the data. Empirically, RECM outperforms prior methods across diverse exact and approximate equivariant tasks, including the challenging molecular conformer generation on the GEOM-Drugs dataset.","authors":["Stefanos Pertigkiozoglou","Mircea Petrache","Shubhendu Trivedi","Kostas Daniilidis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.03313v2","updated":"2026-02-23T17:54:32Z","published":"2025-09-30T22:45:06Z","title":"Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining","summary":"Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.","authors":["Anirudh Subramanyam","Yuxin Chen","Robert L. Grossman"],"pdf_url":"","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2601.14242v3","updated":"2026-02-23T17:49:38Z","published":"2026-01-20T18:53:44Z","title":"APEX-Agents","summary":"We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open source Archipelago, our infrastructure for agent execution and evaluation.","authors":["Bertie Vidgen","Austin Mann","Abby Fennelly","John Wright Stanly","Lucas Rothman","Marco Burstein","Julien Benchek","David Ostrofsky","Anirudh Ravichandran","Debnil Sur","Neel Venugopal","Alannah Hsia","Isaac Robinson","Calix Huang","Olivia Varones","Daniyal Khan","Michael Haines","Austin Bridges","Jesse Boyle","Koby Twist","Zach Richards","Chirag Mahapatra","Brendan Foody","Osvald Nitski"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22842v3","updated":"2026-02-23T17:45:58Z","published":"2025-05-28T20:22:23Z","title":"Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation","summary":"Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.","authors":["Arthur S. Bianchessi","Yasmin C. Aguirre","Rodrigo C. Barros","Lucas S. Kupssinskü"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20078v1","updated":"2026-02-23T17:45:08Z","published":"2026-02-23T17:45:08Z","title":"Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning","summary":"Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.","authors":["Shan Yang","Yang Liu"],"pdf_url":"","comment":"10 pages, 5 figures, 5 tables; plus 16 pages of appendices"},{"id":"http://arxiv.org/abs/2503.07853v2","updated":"2026-02-23T17:38:56Z","published":"2025-03-10T20:59:41Z","title":"Hier-COS: Making Deep Features Hierarchy-aware via Composition of Orthogonal Subspaces","summary":"Traditional classifiers treat all labels as mutually independent, thereby considering all negative classes to be equally incorrect. This approach fails severely in many real-world scenarios, where a known semantic hierarchy defines a partial order of preferences over negative classes. While hierarchy-aware feature representations have shown promise in mitigating this problem, their performance is typically assessed using metrics like MS and AHD. In this paper, we highlight important shortcomings in existing hierarchical evaluation metrics, demonstrating that they are often incapable of measuring true hierarchical performance. Our analysis reveals that existing methods learn sub-optimal hierarchical representations, despite competitive MS and AHD scores. To counter these issues, we introduce Hier-COS, a novel framework for unified hierarchy-aware fine-grained and hierarchical multi-level classification. We show that Hier-COS is theoretically guaranteed to be consistent with the given hierarchy tree. Furthermore, our framework implicitly adapts the learning capacity for different classes based on their position within the hierarchy tree-a vital property absent in existing methods. Finally, to address the limitations of evaluation metrics, we propose HOPS, a ranking-based metric that demonstrably overcomes the deficiencies of current evaluation standards. We benchmark Hier-COS on four challenging datasets, including the deep and imbalanced tieredImageNet-H and iNaturalist-19. Through extensive experiments, we demonstrate that Hier-COS achieves SOTA across all hierarchical metrics for every dataset, while simultaneously beating the top-1 accuracy in all but one case. Lastly, we show that Hier-COS can effectively learn to transform the frozen features extracted from a pretrained backbone (ViT) to be hierarchy-aware, yielding substantial benefits for hierarchical classification performance.","authors":["Depanshu Sani","Saket Anand"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.03596v2","updated":"2026-02-23T17:35:26Z","published":"2026-02-03T14:50:19Z","title":"SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network","summary":"Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks \\textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.","authors":["Cristian Manca","Christian Scano","Giorgio Piras","Fabio Brau","Maura Pintor","Battista Biggio"],"pdf_url":"","comment":"ITASEC-2026"},{"id":"http://arxiv.org/abs/2503.14637v2","updated":"2026-02-23T17:30:07Z","published":"2025-03-18T18:37:49Z","title":"KINESIS: Motion Imitation for Human Musculoskeletal Locomotion","summary":"How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \\& non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.","authors":["Merkourios Simos","Alberto Silvio Chiappa","Alexander Mathis"],"pdf_url":"","comment":"Accepted to ICRA. Here we include an appendix"},{"id":"http://arxiv.org/abs/2602.20070v1","updated":"2026-02-23T17:26:09Z","published":"2026-02-23T17:26:09Z","title":"Training-Free Generative Modeling via Kernelized Stochastic Interpolants","summary":"We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaφ(x)^\\topη_t$, where $η_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.","authors":["Florentin Coeurdoux","Etienne Lempereur","Nathanaël Cuvelle-Magar","Thomas Eboli","Stéphane Mallat","Anastasia Borovykh","Eric Vanden-Eijnden"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20068v1","updated":"2026-02-23T17:24:18Z","published":"2026-02-23T17:24:18Z","title":"The Invisible Gorilla Effect in Out-of-distribution Detection","summary":"Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.","authors":["Harry Anthony","Ziyun Liang","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20062v1","updated":"2026-02-23T17:19:33Z","published":"2026-02-23T17:19:33Z","title":"A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning","summary":"Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.","authors":["Nicolas Anguita","Francesco Locatello","Andrew M. Saxe","Marco Mondelli","Flavia Mancini","Samuel Lippl","Clementine Domine"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20046v1","updated":"2026-02-23T16:57:39Z","published":"2026-02-23T16:57:39Z","title":"Closing the gap in multimodal medical representation alignment","summary":"In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.","authors":["Eleonora Grassucci","Giordano Cicchetti","Danilo Comminiello"],"pdf_url":"","comment":"Accepted at MLSP2025"},{"id":"http://arxiv.org/abs/2505.19193v3","updated":"2026-02-23T16:49:34Z","published":"2025-05-25T15:41:01Z","title":"SuperMAN: Interpretable and Expressive Networks over Temporally Sparse Heterogeneous Data","summary":"Real-world temporal data often consists of multiple signal types recorded at irregular, asynchronous intervals. For instance, in the medical domain, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling occur in other domains, such as the monitoring of large systems using event log files. Effectively learning from such data requires handling sets of temporal sparse and heterogeneous signals. In this work, we propose Super Mixing Additive Networks (SuperMAN), a novel and interpretable-by-design framework for learning directly from such heterogeneous signals, by modeling them as sets of implicit graphs. SuperMAN provides diverse interpretability capabilities, including node-level, graph-level, and subset-level importance, and enables practitioners to trade finer-grained interpretability for greater expressivity when domain priors are available. SuperMAN achieves state-of-the-art performance in real-world high-stakes tasks, including predicting Crohn's disease onset and hospital length of stay from routine blood test measurements and detecting fake news. Furthermore, we demonstrate how SuperMAN's interpretability properties assist in revealing disease development phase transitions and provide crucial insights in the healthcare domain.","authors":["Andrea Zerio","Maya Bechler-Speicher","Maor Huri","Marie Vibeke Vestergaard","Ran Gilad-Bachrach","Tine Jess","Samir Bhatt","Aleksejs Sazonovs"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17762v2","updated":"2026-02-23T16:40:25Z","published":"2025-12-19T16:34:27Z","title":"Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation","summary":"Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.","authors":["Luca Miglior","Matteo Tolloso","Alessio Gravina","Davide Bacciu"],"pdf_url":"","comment":"Accepted at ICLR 2026 ( https://openreview.net/forum?id=DgkWFPZMPp )"},{"id":"http://arxiv.org/abs/2602.20031v1","updated":"2026-02-23T16:39:42Z","published":"2026-02-23T16:39:42Z","title":"Latent Introspection: Models Can Detect Prior Concept Injections","summary":"We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.","authors":["Theia Pearson-Vogel","Martin Vanek","Raymond Douglas","Jan Kulveit"],"pdf_url":"","comment":"28 pages, 17 figures. Submitted to ICML 2026. Workshop version submitted to ICLR 2026 Workshop on Latent and Implicit Thinking"},{"id":"http://arxiv.org/abs/2506.16824v2","updated":"2026-02-23T16:38:34Z","published":"2025-06-20T08:26:12Z","title":"Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs","summary":"Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.","authors":["Thomas Marwitz","Alexander Colsmann","Ben Breitung","Christoph Brabec","Christoph Kirchlechner","Eva Blasco","Gabriel Cadilha Marques","Horst Hahn","Michael Hirtz","Pavel A. Levkin","Yolita M. Eggeler","Tobias Schlöder","Pascal Friederich"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20019v1","updated":"2026-02-23T16:25:35Z","published":"2026-02-23T16:25:35Z","title":"Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision","summary":"Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \\textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.","authors":["Yuxing Tian","Yiyan Qi","Fengran Mo","Weixu Zhang","Jian Guo","Jian-Yun Nie"],"pdf_url":"","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.20003v1","updated":"2026-02-23T16:12:02Z","published":"2026-02-23T16:12:02Z","title":"A Secure and Private Distributed Bayesian Federated Learning Design","summary":"Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.","authors":["Nuocheng Yang","Sihua Wang","Zhaohui Yang","Mingzhe Chen","Changchuan Yin","Kaibin Huang"],"pdf_url":"","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2508.12674v2","updated":"2026-02-23T16:09:58Z","published":"2025-08-18T07:13:53Z","title":"Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation","summary":"Dynamic relational data arise in many machine learning applications, yet their evolving structure poses challenges for learning representations that remain consistent and interpretable over time. A common approach is to learn time varying node embeddings, whose usefulness depends on well defined stability properties across nodes and across time. We introduce Unfolded Laplacian Spectral Embedding (ULSE), a principled extension of unfolded adjacency spectral embedding to normalized Laplacian operators, a setting where stability guarantees have remained out of reach. We prove that ULSE satisfies both cross-sectional and longitudinal stability under a dynamic stochastic block model. Moreover, the Laplacian formulation yields a dynamic Cheeger-type inequality linking the spectrum of the unfolded normalized Laplacian to worst case conductance over time, providing structural insight into the embeddings. Empirical results on synthetic and real world dynamic networks validate the theory.","authors":["Haruka Ezoe","Hiroki Matsumoto","Ryohei Hisano"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.03646v4","updated":"2026-02-23T16:09:40Z","published":"2025-05-06T15:52:14Z","title":"GRILL: Restoring Gradient Signal in Ill-Conditioned Layers for More Effective Adversarial Attacks on Autoencoders","summary":"Adversarial robustness of deep autoencoders (AEs) has received less attention than that of discriminative models, although their compressed latent representations induce ill-conditioned mappings that can amplify small input perturbations and destabilize reconstructions. Existing white-box attacks for AEs, which optimize norm-bounded adversarial perturbations to maximize output damage, often stop at suboptimal attacks. We observe that this limitation stems from vanishing adversarial loss gradients during backpropagation through ill-conditioned layers, caused by near-zero singular values in their Jacobians. To address this issue, we introduce GRILL, a technique that locally restores gradient signals in ill-conditioned layers, enabling more effective norm-bounded attacks. Through extensive experiments across multiple AE architectures, considering both sample-specific and universal attacks under both standard and adaptive attack settings, we show that GRILL significantly increases attack effectiveness, leading to a more rigorous evaluation of AE robustness. Beyond AEs, we provide empirical evidence that modern multimodal architectures with encoder-decoder structures exhibit similar vulnerabilities under GRILL.","authors":["Chethan Krishnamurthy Ramanaik","Arjun Roy","Tobias Callies","Eirini Ntoutsi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20001v1","updated":"2026-02-23T16:08:32Z","published":"2026-02-23T16:08:32Z","title":"FairFS: Addressing Deep Feature Selection Biases for Recommender System","summary":"Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.","authors":["Xianquan Wang","Zhaocheng Du","Jieming Zhu","Qinglin Jia","Zhenhua Dong","Kai Zhang"],"pdf_url":"","comment":"Accepted by The Web Conference 2026"},{"id":"http://arxiv.org/abs/2512.07805v4","updated":"2026-02-23T15:57:05Z","published":"2025-12-08T18:39:13Z","title":"Group Representational Position Encoding","summary":"We present GRAPE (Group Representational Position Encoding), a unified framework for positional encoding based on group actions. GRAPE unifies two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\operatorname{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n) = \\exp(n \\, ω\\, \\mathbf{L})$ with a rank-2 skew-symmetric generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes correspond to canonical coordinate pairs with a log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise from rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Overall, GRAPE provides a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project page: https://github.com/model-architectures/GRAPE.","authors":["Yifan Zhang","Zixiang Chen","Yifeng Liu","Zhen Qin","Huizhuo Yuan","Kangping Xu","Yang Yuan","Quanquan Gu","Andrew Chi-Chih Yao"],"pdf_url":"","comment":"Published in ICLR 2026; Project Page: https://github.com/model-architectures/GRAPE"},{"id":"http://arxiv.org/abs/2602.01428v2","updated":"2026-02-23T15:55:07Z","published":"2026-02-01T20:30:59Z","title":"Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models","summary":"Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.","authors":["Weiqing He","Xiang Li","Li Shen","Weijie Su","Qi Long"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19987v1","updated":"2026-02-23T15:53:25Z","published":"2026-02-23T15:53:25Z","title":"Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction","summary":"This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.","authors":["Ha-Anh Hoang Nguyen","Tri-Duc Phan Le","Duc-Hoang Pham","Huy-Son Nguyen","Cam-Van Thi Nguyen","Duc-Trong Le","Hoang-Quynh Le"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19984v1","updated":"2026-02-23T15:51:50Z","published":"2026-02-23T15:51:50Z","title":"Multivariate time-series forecasting of ASTRI-Horn monitoring data: A Normal Behavior Model","summary":"This study presents a Normal Behavior Model (NBM) developed to forecast monitoring time-series data from the ASTRI-Horn Cherenkov telescope under normal operating conditions. The analysis focused on 15 physical variables acquired by the Telescope Control Unit between September 2022 and July 2024, representing sensor measurements from the Azimuth and Elevation motors. After data cleaning, resampling, feature selection, and correlation analysis, the dataset was segmented into fixed-length intervals, in which the first I samples represented the input sequence provided to the model, while the forecast length, T, indicated the number of future time steps to be predicted. A sliding-window technique was then applied to increase the number of intervals. A Multi-Layer Perceptron (MLP) was trained to perform multivariate forecasting across all features simultaneously. Model performance was evaluated using the Mean Squared Error (MSE) and the Normalized Median Absolute Deviation (NMAD), and it was also benchmarked against a Long Short-Term Memory (LSTM) network. The MLP model demonstrated consistent results across different features and I-T configurations, and matched the performance of the LSTM while converging faster. It achieved an MSE of 0.019+/-0.003 and an NMAD of 0.032+/-0.009 on the test set under its best configuration (4 hidden layers, 720 units per layer, and I-T lengths of 300 samples each, corresponding to 5 hours at 1-minute resolution). Extending the forecast horizon up to 6.5 hours-the maximum allowed by this configuration-did not degrade performance, confirming the model's effectiveness in providing reliable hour-scale predictions. The proposed NBM provides a powerful tool for enabling early anomaly detection in online ASTRI-Horn monitoring time series, offering a basis for the future development of a prognostics and health management system that supports predictive maintenance.","authors":["Federico Incardona","Alessandro Costa","Farida Farsian","Francesco Franchina","Giuseppe Leto","Emilio Mastriani","Kevin Munari","Giovanni Pareschi","Salvatore Scuderi","Sebastiano Spinello","Gino Tosti"],"pdf_url":"","comment":"15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2602.19982v1","updated":"2026-02-23T15:49:46Z","published":"2026-02-23T15:49:46Z","title":"A Computationally Efficient Multidimensional Vision Transformer","summary":"Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and structured feature representations. We develop the\n  theoretical foundations of the tensor cosine product, analyze its algebraic\n  properties, and integrate it into a new Cproduct-based Vision Transformer\n  architecture (TCP-ViT). Numerical experiments on standard classification and\n  segmentation benchmarks demonstrate that the proposed method achieves a uniform\n  1/C parameter reduction (where C is the number of channels) while\n  maintaining competitive accuracy.","authors":["Alaa El Ichi","Khalide Jbilou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19980v1","updated":"2026-02-23T15:47:27Z","published":"2026-02-23T15:47:27Z","title":"Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks","summary":"While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.","authors":["Itamar Trainin","Shauli Ravfogel","Omri Abend","Amir Feder"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.15077v4","updated":"2026-02-23T15:30:05Z","published":"2025-04-21T13:05:26Z","title":"Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL","summary":"While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.","authors":["Simone Papicchio","Simone Rossi","Luca Cagliero","Paolo Papotti"],"pdf_url":"","comment":"26 pages, work in progress"},{"id":"http://arxiv.org/abs/2602.19967v1","updated":"2026-02-23T15:29:50Z","published":"2026-02-23T15:29:50Z","title":"Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems","summary":"Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by recent advances in machine unlearning and structured network pruning, we propose P-PINN, a selective pruning framework designed to unlearn the influence of corrupted data in a pretrained PINN. Specifically, starting from a PINN trained on the full dataset, P-PINN evaluates a joint residual--data fidelity indicator, a weighted combination of data misfit and PDE residuals, to partition the training set into reliable and corrupted subsets. Next, we introduce a bias-based neuron importance measure that quantifies directional activation discrepancies between the two subsets, identifying neurons whose representations are predominantly driven by corrupted samples. Building on this, an iterative pruning strategy then removes noise-sensitive neurons layer by layer. The resulting pruned network is fine-tuned on the reliable data subject to the original PDE constraints, acting as a lightweight post-processing stage rather than a complete retraining. Numerical experiments on extensive PDE inverse-problem benchmarks demonstrate that P-PINN substantially improves robustness, accuracy, and training stability under noisy conditions, achieving up to a 96.6\\% reduction in relative error compared with baseline PINNs. These results indicate that activation-level post hoc pruning is a promising mechanism for enhancing the reliability of physics-informed learning in noise-contaminated settings.","authors":["Yongsheng Chen","Yong Chen","Wei Guo","Xinghui Zhong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19964v1","updated":"2026-02-23T15:28:27Z","published":"2026-02-23T15:28:27Z","title":"On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference","summary":"Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.","authors":["Moritz A. Zanger","Yijun Wu","Pascal R. Van der Vaart","Wendelin Böhmer","Matthijs T. J. Spaan"],"pdf_url":"","comment":"8 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2602.19956v1","updated":"2026-02-23T15:23:17Z","published":"2026-02-23T15:23:17Z","title":"Sparse Masked Attention Policies for Reliable Generalization","summary":"In reinforcement learning, abstraction methods that remove unnecessary information from the observation are commonly used to learn policies which generalize better to unseen tasks. However, these methods often overlook a crucial weakness: the function which extracts the reduced-information representation has unknown generalization ability in unseen observations. In this paper, we address this problem by presenting an information removal method which more reliably generalizes to new states. We accomplish this by using a learned masking function which operates on, and is integrated with, the attention weights within an attention-based policy network. We demonstrate that our method significantly improves policy generalization to unseen tasks in the Procgen benchmark compared to standard PPO and masking approaches.","authors":["Caroline Horsch","Laurens Engwegen","Max Weltevrede","Matthijs T. J. Spaan","Wendelin Böhmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19945v1","updated":"2026-02-23T15:15:47Z","published":"2026-02-23T15:15:47Z","title":"DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models","summary":"Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\\%. The code is available in Appendix.","authors":["Jin Liu","Yinbin Miao","Ning Xi","Junkang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.08550v2","updated":"2026-02-23T15:12:01Z","published":"2026-02-09T11:50:29Z","title":"GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing","summary":"Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.","authors":["Shih-Fang Chen","Jun-Cheng Chen","I-Hong Jhuo","Yen-Yu Lin"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2507.21807v5","updated":"2026-02-23T15:11:27Z","published":"2025-07-29T13:42:38Z","title":"MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation","summary":"Statistical learning methods for automated variable selection, such as LASSO, elastic nets, or gradient boosting, have become increasingly popular tools for building powerful prediction models. Yet, in practice, analyses are often complicated by missing data. The most widely used approach to address missingness is multiple imputation, which involves creating several completed datasets. However, there is an ongoing debate on how to perform model selection in the presence of multiple imputed datasets. Simple strategies, such as pooling models across datasets, have been shown to have suboptimal properties. Although more sophisticated methods exist, they are often difficult to implement and therefore not widely applied. In contrast, two recent approaches modify the regularization methods LASSO and elastic nets by defining a single loss function, resulting in a unified set of coefficients across imputations. Our key contribution is to extend this principle to the framework of component-wise gradient boosting by proposing MIBoost, a novel algorithm that employs a uniform variable-selection mechanism across imputed datasets. Simulation studies suggest that our approach yields prediction performance comparable to that of these recently proposed methods.","authors":["Robert Kuchen"],"pdf_url":"","comment":"16 pages, 2 algorithms, includes a simulation study"},{"id":"http://arxiv.org/abs/2602.19938v1","updated":"2026-02-23T15:11:16Z","published":"2026-02-23T15:11:16Z","title":"A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs","summary":"Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.","authors":["Zijie Liu","Jie Peng","Jinhao Duan","Zirui Liu","Kaixiong Zhou","Mingfu Liang","Luke Simon","Xi Liu","Zhaozhuo Xu","Tianlong Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.01809v3","updated":"2026-02-23T15:07:01Z","published":"2025-12-01T15:44:53Z","title":"Much Ado About Noising: Dispelling the Myths of Generative Robotic Control","summary":"Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/","authors":["Chaoyi Pan","Giri Anantharaman","Nai-Chieh Huang","Claire Jin","Daniel Pfrommer","Chenyang Yuan","Frank Permenter","Guannan Qu","Nicholas Boffi","Guanya Shi","Max Simchowitz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19931v1","updated":"2026-02-23T15:06:52Z","published":"2026-02-23T15:06:52Z","title":"Expanding the Role of Diffusion Models for Robust Classifier Training","summary":"Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.","authors":["Pin-Han Huang","Shang-Tse Chen","Hsuan-Tien Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19930v1","updated":"2026-02-23T15:06:33Z","published":"2026-02-23T15:06:33Z","title":"Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning","summary":"Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.","authors":["Nathan Gavenski","Felipe Meneguzzi","Odinaldo Rodrigues"],"pdf_url":"","comment":"Accepted as part of the Blue Sky Ideas Track for the 25th International Conference on Autonomous Agents and Multiagent Systems"},{"id":"http://arxiv.org/abs/2602.19926v1","updated":"2026-02-23T15:05:28Z","published":"2026-02-23T15:05:28Z","title":"Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models","summary":"Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.","authors":["Jin Liu","Yinbin Miao","Ning Xi","Junkang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19919v1","updated":"2026-02-23T14:58:51Z","published":"2026-02-23T14:58:51Z","title":"Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling","summary":"Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.","authors":["Xiang Li","Zikai Wei","Yiyan Qi","Wanyun Zhou","Xiang Liu","Penglei Sun","Yongqi Zhang","Xiaowen Chu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19918v1","updated":"2026-02-23T14:58:08Z","published":"2026-02-23T14:58:08Z","title":"RobPI: Robust Private Inference against Malicious Client","summary":"The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.","authors":["Jiaqi Xue","Mengxin Zheng","Qian Lou"],"pdf_url":"","comment":"Accepted by SaTML 2026"},{"id":"http://arxiv.org/abs/2602.19917v1","updated":"2026-02-23T14:57:52Z","published":"2026-02-23T14:57:52Z","title":"Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning","summary":"Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \\TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.","authors":["Thanh Nguyen","Tung Luu","Tri Ton","Sungwoong Kim","Chang D. Yoo"],"pdf_url":"","comment":"10 pages, 4 Figures, IEEE Access"},{"id":"http://arxiv.org/abs/2602.19915v1","updated":"2026-02-23T14:55:28Z","published":"2026-02-23T14:55:28Z","title":"Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction","summary":"Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations.","authors":["Michael Trimboli","Mohammed Alsubaie","Sirani M. Perera","Ke-Gang Wang","Xianqi Li"],"pdf_url":"","comment":"24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2602.19912v1","updated":"2026-02-23T14:52:53Z","published":"2026-02-23T14:52:53Z","title":"De novo molecular structure elucidation from mass spectra via flow matching","summary":"Mass spectrometry is a powerful and widely used tool for identifying molecular structures due to its sensitivity and ability to profile complex samples. However, translating spectra into full molecular structures is a difficult, under-defined inverse problem. Overcoming this problem is crucial for enabling biological insight, discovering new metabolites, and advancing chemical research across multiple fields. To this end, we develop MSFlow, a two-stage encoder-decoder flow-matching generative model that achieves state-of-the-art performance on the structure elucidation task for small molecules. In the first stage, we adopt a formula-restricted transformer model for encoding mass spectra into a continuous and chemically informative embedding space, while in the second stage, we train a decoder flow matching model to reconstruct molecules from latent embeddings of mass spectra. We present ablation studies demonstrating the importance of using information-preserving molecular descriptors for encoding mass spectra and motivate the use of our discrete flow-based decoder. Our rigorous evaluation demonstrates that MSFlow can accurately translate up to 45 percent of molecular mass spectra into their corresponding molecular representations - an improvement of up to fourteen-fold over the current state-of-the-art. A trained version of MSFlow is made publicly available on GitHub for non-commercial users.","authors":["Ghaith Mqawass","Tuan Le","Fabian Theis","Djork-Arné Clevert"],"pdf_url":"","comment":"13-page preprint, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2512.23348v2","updated":"2026-02-23T14:47:12Z","published":"2025-12-29T10:14:56Z","title":"Persistent Homology via Finite Topological Spaces","summary":"We propose a functorial framework for persistent homology based on finite topological spaces and their associated posets. Starting from a finite metric space, we associate a filtration of finite topologies whose structure maps are continuous identity maps. By passing functorially to posets and to order complexes, we obtain persistence modules without requiring inclusion relations between the resulting complexes. We show that standard poset-level simplifications preserve persistent invariants and establish stability of the resulting persistence diagrams under perturbations of the input metric in a basic density-based instantiation, illustrating how stability arguments arise naturally in our framework. We further introduce a concrete density-guided construction, designed to be faithful to anchor neighborhood structure at each scale, and demonstrate its practical viability through an implementation tested on real datasets.","authors":["Selçuk Kayacan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19907v1","updated":"2026-02-23T14:46:08Z","published":"2026-02-23T14:46:08Z","title":"Gradient based Severity Labeling for Biomarker Classification in OCT","summary":"In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.","authors":["Kiran Kokilepersaud","Mohit Prabhushankar","Ghassan AlRegib","Stephanie Trejo Corona","Charles Wykoff"],"pdf_url":"","comment":"Accepted at International Conference on Image Processing (ICIP) 2022"},{"id":"http://arxiv.org/abs/2602.19903v1","updated":"2026-02-23T14:43:15Z","published":"2026-02-23T14:43:15Z","title":"Rethinking Chronological Causal Discovery with Signal Processing","summary":"Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.","authors":["Kurt Butler","Damian Machlanski","Panagiotis Dimitrakopoulos","Sotirios A. Tsaftaris"],"pdf_url":"","comment":"5 pages, 5 figures, Final version accepted to the 59th Asilomar Conference on Signals, Systems, and Computers (2025)"},{"id":"http://arxiv.org/abs/2508.06199v4","updated":"2026-02-23T14:41:11Z","published":"2025-08-08T10:29:24Z","title":"Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning","summary":"Pretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.","authors":["Mateusz Praski","Jakub Adamczyk","Wojciech Czech"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2404.16890v2","updated":"2026-02-23T14:40:05Z","published":"2024-04-24T09:12:04Z","title":"Layer Collapse Can be Induced by Unstructured Pruning","summary":"Unstructured pruning is a popular compression method for efficiently reducing model parameters. However, while it effectively decreases the number of parameters, it is commonly believed that unstructured pruning cannot shorten the computational critical path, i.e., the maximum number of layers traversed during forward propagation.\n  In this paper, we study when and how unstructured pruning can yield structural effects. For rectifier-activated networks, we introduce the notion of neuron entropy, which quantifies the degree of nonlinearity utilization. We show that magnitude-based pruning naturally lowers this entropy, sometimes down to zero-entropy layers that become linearizable and can thus be removed. Building on this insight, we propose a method that leverages \"unstructured\" pruning to favor sparsity in low-entropy layers, enabling their complete removal. We validate the phenomenon across CNNs, Vision Transformers, and NLP models: unstructured pruning can induce effective layer removal with little or no performance degradation in over-parameterized networks.","authors":["Zhu Liao","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19895v1","updated":"2026-02-23T14:37:01Z","published":"2026-02-23T14:37:01Z","title":"DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning","summary":"Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.","authors":["Zhongwei Wan","Yun Shen","Zhihao Dou","Donghao Zhou","Yu Zhang","Xin Wang","Hui Shen","Jing Xiong","Chaofan Tao","Zixuan Zhong","Peizhou Huang","Mi Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19893v1","updated":"2026-02-23T14:33:39Z","published":"2026-02-23T14:33:39Z","title":"Generalized Random Direction Newton Algorithms for Stochastic Optimization","summary":"We present a family of generalized Hessian estimators of the objective using random direction stochastic approximation (RDSA) by utilizing only noisy function measurements. The form of each estimator and the order of the bias depend on the number of function measurements. In particular, we demonstrate that estimators with more function measurements exhibit lower-order estimation bias. We show the asymptotic unbiasedness of the estimators. We also perform asymptotic and non-asymptotic convergence analyses for stochastic Newton methods that incorporate our generalized Hessian estimators. Finally, we perform numerical experiments to validate our theoretical findings.","authors":["Soumen Pachal","Prashanth L. A.","Shalabh Bhatnagar","Avinash Achar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.08535v4","updated":"2026-02-23T14:27:48Z","published":"2026-02-09T11:33:12Z","title":"Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds","summary":"Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and the pathology of anticipatory control. In this work, we introduce the Causal Schrodinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. By leveraging diffusion processes (SDEs), CSB enables probability mass to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes exactly into local, robust transitions. This theorem provides a principled resolution to the Information Bottleneck that plagues monolithic architectures in high dimensions. We empirically validate CSB on a full-rank causal system (d=10^5, intrinsic rank 10^5), where standard structure-blind MLPs fail to converge (MSE ~0.31). By physically implementing the structural decomposition, CSB achieves high-fidelity transport (MSE ~0.06) in just 73.73 seconds on a single GPU. This stands in stark contrast to structure-agnostic O(d^3) baselines, estimated to require over 6 years. Our results demonstrate that CSB breaks the Curse of Dimensionality through structural intelligence, offering a scalable foundation for high-stakes causal discovery in 10^5-node systems. Code is available at: https://github.com/cochran1/causal-schrodinger-bridge","authors":["Rui Wu","Li YongJun"],"pdf_url":"","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2511.06450v2","updated":"2026-02-23T14:20:57Z","published":"2025-11-09T16:34:19Z","title":"Countering Multi-modal Representation Collapse through Rank-targeted Fusion","summary":"Multi-modal fusion methods often suffer from two types of representation collapse: feature collapse where individual dimensions lose their discriminative power (as measured by eigenspectra), and modality collapse where one dominant modality overwhelms the other. Applications like human action anticipation that require fusing multifarious sensor data are hindered by both feature and modality collapse. However, existing methods attempt to counter feature collapse and modality collapse separately. This is because there is no unifying framework that efficiently addresses feature and modality collapse in conjunction. In this paper, we posit the utility of effective rank as an informative measure that can be utilized to quantify and counter both the representation collapses. We propose \\textit{Rank-enhancing Token Fuser}, a theoretically grounded fusion framework that selectively blends less informative features from one modality with complementary features from another modality. We show that our method increases the effective rank of the fused representation. To address modality collapse, we evaluate modality combinations that mutually increase each others' effective rank. We show that depth maintains representational balance when fused with RGB, avoiding modality collapse. We validate our method on action anticipation, where we present \\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on NTURGBD, UTKinect, and DARai demonstrate that our approach significantly outperforms prior state-of-the-art methods by up to 3.74\\%. Our code is available at: \\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.","authors":["Seulgi Kim","Kiran Kokilepersaud","Mohit Prabhushankar","Ghassan AlRegib"],"pdf_url":"","comment":"Accepted in 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2602.19859v1","updated":"2026-02-23T13:58:16Z","published":"2026-02-23T13:58:16Z","title":"Dirichlet Scale Mixture Priors for Bayesian Neural Networks","summary":"Neural networks are the cornerstone of modern machine learning, yet can be difficult to interpret, give overconfident predictions and are vulnerable to adversarial attacks. Bayesian neural networks (BNNs) provide some alleviation of these limitations, but have problems of their own. The key step of specifying prior distributions in BNNs is no trivial task, yet is often skipped out of convenience. In this work, we propose a new class of prior distributions for BNNs, the Dirichlet scale mixture (DSM) prior, that addresses current limitations in Bayesian neural networks through structured, sparsity-inducing shrinkage. Theoretically, we derive general dependence structures and shrinkage results for DSM priors and show how they manifest under the geometry induced by neural networks. In experiments on simulated and real world data we find that the DSM priors encourages sparse networks through implicit feature selection, show robustness under adversarial attacks and deliver competitive predictive performance with substantially fewer effective parameters. In particular, their advantages appear most pronounced in correlated, moderately small data regimes, and are more amenable to weight pruning. Moreover, by adopting heavy-tailed shrinkage mechanisms, our approach aligns with recent findings that such priors can mitigate the cold posterior effect, offering a principled alternative to the commonly used Gaussian priors.","authors":["August Arnstad","Leiv Rønneberg","Geir Storvik"],"pdf_url":"","comment":"24 pages, 20 figures"},{"id":"http://arxiv.org/abs/2602.19851v1","updated":"2026-02-23T13:54:12Z","published":"2026-02-23T13:54:12Z","title":"Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments","summary":"We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes","authors":["Xinyan Su","Jiacan Gao","Mingyuan Ma","Xiao Xu","Xinrui Wan","Tianqi Gu","Enyun Yu","Jiecheng Guo","Zhiheng Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19845v1","updated":"2026-02-23T13:49:05Z","published":"2026-02-23T13:49:05Z","title":"I Dropped a Neural Net","summary":"A recent Dwarkesh Patel podcast with John Collison and Elon Musk featured an interesting puzzle from Jane Street: they trained a neural net, shuffled all 96 layers, and asked to put them back in order.\n  Given unlabelled layers of a Residual Network and its training dataset, we recover the exact ordering of the layers. The problem decomposes into pairing each block's input and output projections ($48!$ possibilities) and ordering the reassembled blocks ($48!$ possibilities), for a combined search space of $(48!)^2 \\approx 10^{122}$, which is more than the atoms in the observable universe. We show that stability conditions during training like dynamic isometry leave the product $W_{\\text{out}} W_{\\text{in}}$ for correctly paired layers with a negative diagonal structure, allowing us to use diagonal dominance ratio as a signal for pairing. For ordering, we seed-initialize with a rough proxy such as delta-norm or $\\|W_{\\text{out}}\\|_F$ then hill-climb to zero mean squared error.","authors":["Hyunwoo Park"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.24943v2","updated":"2026-02-23T13:41:40Z","published":"2025-12-31T16:09:08Z","title":"RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment","summary":"Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.","authors":["Chenji Lu","Zhuo Chen","Hui Zhao","Zhenyi Wang","Pengjie Wang","Chuan Yu","Jian Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19837v1","updated":"2026-02-23T13:39:58Z","published":"2026-02-23T13:39:58Z","title":"Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent","summary":"Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.","authors":["Björn Hoppmann","Christoph Scholz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.19375v2","updated":"2026-02-23T13:35:08Z","published":"2025-04-27T22:45:00Z","title":"$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation","summary":"Two-time-scale stochastic approximation (SA) is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. In this work, we derive mean squared error bounds for non-linear two-time-scale iterations with contractive mappings. In the setting where both stepsizes are order $Θ(1/k)$, commonly referred to as single time-scale SA with multiple coupled sequences, we obtain the first $O(1/k)$ rate without imposing additional smoothness assumptions. In the setting with true time-scale separation, the previous best bound was $O(1/k^{2/3})$. We improve this to $O(1/k^a)$ for any $a<1$ approaching the optimal $O(1/k)$ rate. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence whose variance decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation. Our results apply to Polyak averaging, as well as to algorithms from reinforcement learning, and optimization, including gradient descent-ascent and two-time-scale Lagrangian optimization.","authors":["Siddharth Chandak"],"pdf_url":"","comment":"Submitted to IEEE Transactions on Automatic Control"},{"id":"http://arxiv.org/abs/2408.01839v2","updated":"2026-02-23T13:29:24Z","published":"2024-08-03T18:34:23Z","title":"Optimal Local Convergence Rates of Stochastic First-Order Methods under Local $α$-PL","summary":"We study the local convergence rate of stochastic first-order methods under a local $α$-Polyak-Lojasiewicz ($α$-PL) condition in a neighborhood of a target connected component $\\mathcal{M}$ of the local minimizer set. The parameter $α\\in [1,2]$ is the exponent of the gradient norm in the $α$-PL inequality: $α=2$ recovers the classical PL case, $α=1$ corresponds to Holder-type error bounds, and intermediate values interpolate between these regimes. Our performance criterion is the number of oracle queries required to output $\\hat{x}$ with $F(\\hat{x})-l \\le \\varepsilon$, where $l := F(y)$ for any $y \\in \\mathcal{M}$. We work in a local regime where the algorithm is initialized near $\\mathcal{M}$ and, with high probability, its iterates remain in that neighborhood. We establish a lower bound $Ω(\\varepsilon^{-2/α})$ for all stochastic first-order methods in this regime, and we obtain a matching upper bound $\\mathcal{O}(\\varepsilon^{-2/α})$ for $1 \\le α< 2$ via a SARAH-type variance-reduced method with time-varying batch sizes and step sizes. In the convex setting, assuming a local $α$-PL condition on the $\\varepsilon$-sublevel set, we further show a complexity lower bound $\\widetildeΩ(\\varepsilon^{-2/α})$ for reaching an $\\varepsilon$-global optimum, matching the $\\varepsilon$-dependence of known accelerated stochastic subgradient methods.","authors":["Saeed Masiha","Saber Salehkaleybar","Niao He","Negar Kiyavash","Patrick Thiran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.10572v2","updated":"2026-02-23T13:23:01Z","published":"2025-06-12T11:01:43Z","title":"Probability Bounding: Post-Hoc Calibration via Box-Constrained Softmax","summary":"Many studies have observed that modern neural networks achieve high accuracy while producing poorly calibrated probabilities, making calibration a critical practical issue. In this work, we propose probability bounding (PB), a novel post-hoc calibration method that mitigates both underconfidence and overconfidence by learning lower and upper bounds on the output probabilities. To implement PB, we introduce the box-constrained softmax (BCSoftmax) function, a generalization of Softmax that explicitly enforces lower and upper bounds on the output probabilities. While BCSoftmax is formulated as the solution to a box-constrained optimization problem, we develop an exact and efficient algorithm for computing BCSoftmax. We further provide theoretical guarantees for PB and introduce two variants of PB. We demonstrate the effectiveness of our methods experimentally on four real-world datasets, consistently reducing calibration errors. Our Python implementation is available at https://github.com/neonnnnn/torchbcsoftmax.","authors":["Kyohei Atarashi","Satoshi Oyama","Hiromi Arai","Hisashi Kashima"],"pdf_url":"","comment":"46 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.05295v2","updated":"2026-02-23T13:16:47Z","published":"2025-05-08T14:34:44Z","title":"Performance Estimation in Binary Classification Using Calibrated Confidence","summary":"Model monitoring is a critical component of the machine learning lifecycle, safeguarding against undetected drops in the model's performance after deployment. Traditionally, performance monitoring has required access to ground truth labels, which are not always readily available. This can result in unacceptable latency or render performance monitoring altogether impossible. Recently, methods designed to estimate the accuracy of classifier models without access to labels have shown promising results. However, there are various other metrics that might be more suitable for assessing model performance in many cases. Until now, none of these important metrics has received similar interest from the scientific community. In this work, we address this gap by presenting CBPE, a novel method that can estimate any binary classification metric defined using the confusion matrix. In particular, we choose four metrics from this large family: accuracy, precision, recall, and F$_1$, to demonstrate our method. CBPE treats the elements of the confusion matrix as random variables and leverages calibrated confidence scores of the model to estimate their distributions. The desired metric is then also treated as a random variable, whose full probability distribution can be derived from the estimated confusion matrix. CBPE is shown to produce estimates that come with strong theoretical guarantees and valid confidence intervals.","authors":["Juhani Kivimäki","Jakub Białek","Wojtek Kuberski","Jukka K. Nurminen"],"pdf_url":"","comment":"Accepted for publication in Machine Learning, (ACML 2025 Journal Track). Presented at the 17th Asian Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2602.19816v1","updated":"2026-02-23T13:13:41Z","published":"2026-02-23T13:13:41Z","title":"Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling","summary":"Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.","authors":["Yungang Yi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19805v1","updated":"2026-02-23T13:03:48Z","published":"2026-02-23T13:03:48Z","title":"Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing","summary":"Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.","authors":["Wall Kim","Chaeyoung Song","Hanul Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19799v1","updated":"2026-02-23T12:55:48Z","published":"2026-02-23T12:55:48Z","title":"Path-conditioned training: a principled way to rescale ReLU neural networks","summary":"Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.","authors":["Arthur Lebeurrier","Titouan Vayer","Rémi Gribonval"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.07102v4","updated":"2026-02-23T12:55:28Z","published":"2024-11-11T16:26:33Z","title":"Effectively Leveraging Momentum Terms in Stochastic Line Search Frameworks for Fast Optimization of Finite-Sum Problems","summary":"In this work, we address unconstrained finite-sum optimization problems, with particular focus on instances originating in large scale deep learning scenarios. Our main interest lies in the exploration of the relationship between recent line search approaches for stochastic optimization in the overparametrized regime and momentum directions. First, we point out that combining these two elements with computational benefits is not straightforward. To this aim, we propose a solution based on mini-batch persistency. We then introduce an algorithmic framework that exploits a mix of data persistency, conjugate-gradient type rules for the definition of the momentum parameter and stochastic line searches. The resulting algorithm provably possesses convergence properties under suitable assumptions and is empirically shown to outperform other popular methods from the literature, obtaining state-of-the-art results in both convex and nonconvex large scale training problems.","authors":["Matteo Lapucci","Davide Pucci"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17779v4","updated":"2026-02-23T12:51:16Z","published":"2025-05-23T11:48:48Z","title":"U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding","summary":"Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.","authors":["Anjie Le","Henan Liu","Yue Wang","Zhenyu Liu","Rongkun Zhu","Taohan Weng","Jinze Yu","Boyang Wang","Yalun Wu","Kaiwen Yan","Quanlin Sun","Meirui Jiang","Jialun Pei","Siya Liu","Haoyun Zheng","Zhoujun Li","Alison Noble","Jacques Souquet","Xiaoqing Guo","Manxi Lin","Hongcheng Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.15888v2","updated":"2026-02-23T12:47:29Z","published":"2026-02-06T13:16:28Z","title":"NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing","summary":"Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms.","authors":["Boyu Li","Xingchun Zhu","Yonghui Wu"],"pdf_url":"","comment":"14 pages, 5 figures, under review at Physiological Measurement"},{"id":"http://arxiv.org/abs/2602.19790v1","updated":"2026-02-23T12:46:50Z","published":"2026-02-23T12:46:50Z","title":"Drift Localization using Conformal Predictions","summary":"Concept drift -- the change of the distribution over time -- poses significant challenges for learning systems and is of central interest for monitoring. Understanding drift is thus paramount, and drift localization -- determining which samples are affected by the drift -- is essential. While several approaches exist, most rely on local testing schemes, which tend to fail in high-dimensional, low-signal settings. In this work, we consider a fundamentally different approach based on conformal predictions. We discuss and show the shortcomings of common approaches and demonstrate the performance of our approach on state-of-the-art image datasets.","authors":["Fabian Hinder","Valerie Vaquet","Johannes Brinkrolf","Barbara Hammer"],"pdf_url":"","comment":"Paper was accepted at the 34th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning --- ESANN 2026"},{"id":"http://arxiv.org/abs/2602.19789v1","updated":"2026-02-23T12:46:23Z","published":"2026-02-23T12:46:23Z","title":"Stop Preaching and Start Practising Data Frugality for Responsible Development of AI","summary":"This position paper argues that the machine learning community must move from preaching to practising data frugality for responsible artificial intelligence (AI) development. For long, progress has been equated with ever-larger datasets, driving remarkable advances but now yielding increasingly diminishing performance gains alongside rising energy use and carbon emissions. While awareness of data frugal approaches has grown, their adoption has remained rhetorical, and data scaling continues to dominate development practice. We argue that this gap between preach and practice must be closed, as continued data scaling entails substantial and under-accounted environmental impacts. To ground our position, we provide indicative estimates of the energy use and carbon emissions associated with the downstream use of ImageNet-1K. We then present empirical evidence that data frugality is both practical and beneficial, demonstrating that coreset-based subset selection can substantially reduce training energy consumption with little loss in accuracy, while also mitigating dataset bias. Finally, we outline actionable recommendations for moving data frugality from rhetorical preach to concrete practice for responsible development of AI.","authors":["Sophia N. Wilson","Guðrún Fjóla Guðmundsdóttir","Andrew Millard","Raghavendra Selvan","Sebastian Mair"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19788v1","updated":"2026-02-23T12:44:22Z","published":"2026-02-23T12:44:22Z","title":"Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings","summary":"Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms.","authors":["Lotta Mäkinen","Jorge Loría","Samuel Kaski"],"pdf_url":"","comment":"27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.19785v1","updated":"2026-02-23T12:42:00Z","published":"2026-02-23T12:42:00Z","title":"Unsupervised Anomaly Detection in NSL-KDD Using $β$-VAE: A Latent Space and Reconstruction Error Approach","summary":"As Operational Technology increasingly integrates with Information Technology, the need for Intrusion Detection Systems becomes more important. This paper explores an unsupervised approach to anomaly detection in network traffic using $β$-Variational Autoencoders on the NSL-KDD dataset. We investigate two methods: leveraging the latent space structure by measuring distances from test samples to the training data projections, and using the reconstruction error as a conventional anomaly detection metric. By comparing these approaches, we provide insights into their respective advantages and limitations in an unsupervised setting. Experimental results highlight the effectiveness of latent space exploitation for classification tasks.","authors":["Dylan Baptiste","Ramla Saddem","Alexandre Philippot","François Foyer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19782v1","updated":"2026-02-23T12:38:26Z","published":"2026-02-23T12:38:26Z","title":"Addressing Instrument-Outcome Confounding in Mendelian Randomization through Representation Learning","summary":"Mendelian Randomization (MR) is a prominent observational epidemiological research method designed to address unobserved confounding when estimating causal effects. However, core assumptions -- particularly the independence between instruments and unobserved confounders -- are often violated due to population stratification or assortative mating. Leveraging the increasing availability of multi-environment data, we propose a representation learning framework that exploits cross-environment invariance to recover latent exogenous components of genetic instruments. We provide theoretical guarantees for identifying these latent instruments under various mixing mechanisms and demonstrate the effectiveness of our approach through simulations and semi-synthetic experiments using data from the All of Us Research Hub.","authors":["Shimeng Huang","Matthew Robinson","Francesco Locatello"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19778v1","updated":"2026-02-23T12:32:53Z","published":"2026-02-23T12:32:53Z","title":"Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation","summary":"Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.","authors":["Nghia Phan","Rong Jin","Gang Liu","Xiao Dong"],"pdf_url":"","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2602.19775v1","updated":"2026-02-23T12:29:43Z","published":"2026-02-23T12:29:43Z","title":"Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization","summary":"Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.","authors":["Jose M. G. Vilar","Leonor Saiz"],"pdf_url":"","comment":"28 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.09257v4","updated":"2026-02-23T12:20:57Z","published":"2025-02-13T12:13:25Z","title":"From Contextual Combinatorial Semi-Bandits to Bandit List Classification: Improved Sample Complexity with Sparse Rewards","summary":"We study the problem of contextual combinatorial semi-bandits, where input contexts are mapped into subsets of size $m$ of a collection of $K$ possible actions. In each round, the learner observes the realized reward of the predicted actions. Motivated by prototypical applications of contextual bandits, we focus on the $s$-sparse regime where we assume that the sum of rewards is bounded by some value $s\\ll K$. For example, in recommendation systems the number of products purchased by any customer is significantly smaller than the total number of available products. Our main result is for the $(ε,δ)$-PAC variant of the problem for which we design an algorithm that returns an $ε$-optimal policy with high probability using a sample complexity of $\\tilde{O}((poly(K/m)+sm/ε^2) \\log(|Π|/δ))$ where $Π$ is the underlying (finite) class and $s$ is the sparsity parameter. This bound improves upon known bounds for combinatorial semi-bandits whenever $s\\ll K$, and in the regime where $s=O(1)$, the leading term is independent of $K$. Our algorithm is also computationally efficient given access to an ERM oracle for $Π$. Our framework generalizes the list multiclass classification problem with bandit feedback, which can be seen as a special case with binary reward vectors. In the special case of single-label classification corresponding to $s=m=1$, we prove an $O((K^7+1/ε^2)\\log(|H|/δ))$ sample complexity bound, which improves upon recent results in this scenario. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\\tilde O(|Π|+\\sqrt{smT\\log |Π|})$, extending the result of Erez et al. (2024) who consider the simpler single label classification setting.","authors":["Liad Erez","Tomer Koren"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19770v1","updated":"2026-02-23T12:20:37Z","published":"2026-02-23T12:20:37Z","title":"The Confusion is Real: GRAPHIC - A Network Science Approach to Confusion Matrices in Deep Learning","summary":"Explainable artificial intelligence has emerged as a promising field of research to address reliability concerns in artificial intelligence. Despite significant progress in explainable artificial intelligence, few methods provide a systematic way to visualize and understand how classes are confused and how their relationships evolve as training progresses. In this work, we present GRAPHIC, an architecture-agnostic approach that analyzes neural networks on a class level. It leverages confusion matrices derived from intermediate layers using linear classifiers. We interpret these as adjacency matrices of directed graphs, allowing tools from network science to visualize and quantify learning dynamics across training epochs and intermediate layers. GRAPHIC provides insights into linear class separability, dataset issues, and architectural behavior, revealing, for example, similarities between flatfish and man and labeling ambiguities validated in a human study. In summary, by uncovering real confusions, GRAPHIC offers new perspectives on how neural networks learn. The code is available at https://github.com/Johanna-S-Froehlich/GRAPHIC.","authors":["Johanna S. Fröhlich","Bastian Heinlein","Jan U. Claar","Hans Rosenberger","Vasileios Belagiannis","Ralf R. Müller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19761v1","updated":"2026-02-23T12:12:13Z","published":"2026-02-23T12:12:13Z","title":"Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes","summary":"Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.","authors":["Nina van Gerwen","Sten Willemsen","Bettina E. Hansen","Christophe Corpechot","Marco Carbone","Cynthia Levy","Maria-Carlota Londõno","Atsushi Tanaka","Palak Trivedi","Alejandra Villamil","Gideon Hirschfield","Dimitris Rizopoulos"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.05493v2","updated":"2026-02-23T11:51:33Z","published":"2025-08-07T15:29:22Z","title":"Exact and Heuristic Algorithms for Constrained Biclustering","summary":"Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.","authors":["Antonio M. Sudoso"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.17690v2","updated":"2026-02-23T11:36:42Z","published":"2026-02-06T05:10:19Z","title":"DesignAsCode: Bridging Structural Editability and Visual Fidelity in Graphic Design Generation","summary":"Graphic design generation demands a delicate balance between high visual fidelity and fine-grained structural editability. However, existing approaches typically bifurcate into either non-editable raster image synthesis or abstract layout generation devoid of visual content. Recent combinations of these two approaches attempt to bridge this gap but often suffer from rigid composition schemas and unresolvable visual dissonances (e.g., text-background conflicts) due to their inexpressive representation and open-loop nature. To address these challenges, we propose DesignAsCode, a novel framework that reimagines graphic design as a programmatic synthesis task using HTML/CSS. Specifically, we introduce a Plan-Implement-Reflect pipeline, incorporating a Semantic Planner to construct dynamic, variable-depth element hierarchies and a Visual-Aware Reflection mechanism that iteratively optimizes the code to rectify rendering artifacts. Extensive experiments demonstrate that DesignAsCode significantly outperforms state-of-the-art baselines in both structural validity and aesthetic quality. Furthermore, our code-native representation unlocks advanced capabilities, including automatic layout retargeting, complex document generation (e.g., resumes), and CSS-based animation. Our project page is available at https://liuziyuan1109.github.io/design-as-code/.","authors":["Ziyuan Liu","Shizhao Sun","Danqing Huang","Yingdong Shi","Meisheng Zhang","Ji Li","Jingsong Yu","Jiang Bian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19733v1","updated":"2026-02-23T11:32:39Z","published":"2026-02-23T11:32:39Z","title":"Understanding the Curse of Unrolling","summary":"Algorithm unrolling is ubiquitous in machine learning, particularly in hyperparameter optimization and meta-learning, where Jacobians of solution mappings are computed by differentiating through iterative algorithms. Although unrolling is known to yield asymptotically correct Jacobians under suitable conditions, recent work has shown that the derivative iterates may initially diverge from the true Jacobian, a phenomenon known as the curse of unrolling. In this work, we provide a non-asymptotic analysis that explains the origin of this behavior and identifies the algorithmic factors that govern it. We show that truncating early iterations of the derivative computation mitigates the curse while simultaneously reducing memory requirements. Finally, we demonstrate that warm-starting in bilevel optimization naturally induces an implicit form of truncation, providing a practical remedy. Our theoretical findings are supported by numerical experiments on representative examples.","authors":["Sheheryar Mehmood","Florian Knoll","Peter Ochs"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22295v5","updated":"2026-02-23T11:29:56Z","published":"2025-09-26T12:56:20Z","title":"Aurora: Towards Universal Generative Multimodal Time Series Forecasting","summary":"Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Cross-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corresponding text or image modalities, thus possessing strong cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on 5 well-recognized benchmarks, including TimeMMD, TSFM-Bench, ProbTS, TFB, and EPF, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.","authors":["Xingjian Wu","Jianxin Jin","Wanghui Qiu","Peng Chen","Yang Shu","Bin Yang","Chenjuan Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.12313v2","updated":"2026-02-23T11:26:43Z","published":"2026-02-12T17:18:22Z","title":"Visible and Hyperspectral Imaging for Quality Assessment of Milk: Property Characterisation and Identification","summary":"Rapid and non-destructive assessment of milk quality is crucial to ensuring both nutritional value and food safety. In this study, we investigated the potential of visible and hyperspectral imaging as cost-effective and quick-response alternatives to conventional chemical analyses for characterizing key properties of cowś milk. A total of 52 milk samples were analysed to determine their biochemical composition (polyphenols, antioxidant capacity, and fatty acids) using spectrophotometer methods and standard gas-liquid and high-performance liquid chromatography (GLC/HPLC). Concurrently, visible (RGB) images were captured using a standard smartphone, and hyperspectral data were acquired in the near-infrared range. A comprehensive analytical framework, including eleven different machine learning algorithms, was employed to correlate imaging features with biochemical measurements. Analysis of visible images accurately distinguished between fresh samples and those stored for 12 days (100 percent accuracy) and achieved perfect discrimination between antibiotic-treated and untreated groups (100 percent accuracy). Moreover, image-derived features enabled perfect prediction of the polyphenols content and the antioxidant capacity using an XGBoost model. Hyperspectral imaging further achieved classification accuracies exceeding 95 percent for several individual fatty acids and 94.8 percent for treatment groups using a Random Forest model. These findings demonstrate that both visible and hyperspectral imaging, when coupled with machine learning, are powerful, non-invasive tools for the rapid assessment of milkś chemical and nutritional profiles, highlighting the strong potential of imaging-based approaches for milk quality assessment.","authors":["Massimo Martinelli","Elena Tomassi","Nafiou Arouna","Morena Gabriele","Laryssa Perez Fabbri","Luisa Pozzo","Bianca Castiglioni","Paola Cremonesi","Giuseppe Conte","Davide Moroni","Laura Pucci"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18949v2","updated":"2026-02-23T11:07:21Z","published":"2025-09-23T12:58:32Z","title":"Towards Privacy-Aware Bayesian Networks: A Credal Approach","summary":"Bayesian networks (BN) are probabilistic graphical models that enable efficient knowledge representation and inference. These have proven effective across diverse domains, including healthcare, bioinformatics and economics. The structure and parameters of a BN can be obtained by domain experts or directly learned from available data. However, as privacy concerns escalate, it becomes increasingly critical for publicly released models to safeguard sensitive information in training data. Typically, released models do not prioritize privacy by design. In particular, tracing attacks from adversaries can combine the released BN with auxiliary data to determine whether specific individuals belong to the data from which the BN was learned. State-of-the-art protection tecniques involve introducing noise into the learned parameters. While this offers robust protection against tracing attacks, it significantly impacts the model's utility, in terms of both the significance and accuracy of the resulting inferences. Hence, high privacy may be attained at the cost of releasing a possibly ineffective model. This paper introduces credal networks (CN) as a novel solution for balancing the model's privacy and utility. After adapting the notion of tracing attacks, we demonstrate that a CN enables the masking of the learned BN, thereby reducing the probability of successful attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve meaningful inferences while safeguarding privacy. Moreover, we identify key learning information that must be concealed to prevent attackers from recovering the underlying BN. Finally, we conduct a set of numerical experiments to analyze how privacy gains can be modulated by tuning the CN hyperparameters. Our results confirm that CNs provide a principled, practical, and effective approach towards the development of privacy-aware probabilistic graphical models.","authors":["Niccolò Rocchi","Fabio Stella","Cassio de Campos"],"pdf_url":"","comment":"Accepted at ECAI2025 conference, 20 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.08941v3","updated":"2026-02-23T11:04:45Z","published":"2025-02-13T03:43:13Z","title":"Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation","summary":"This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad'' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.","authors":["Han-Dong Lim","Donghwan Lee"],"pdf_url":"","comment":"Added experiments for n-step PVI and n-step TD convergence/divergence"},{"id":"http://arxiv.org/abs/2602.16902v3","updated":"2026-02-23T11:03:50Z","published":"2026-02-18T21:33:59Z","title":"LLM-WikiRace Benchmark: How Far Can LLMs Plan over Real-World Knowledge Graphs?","summary":"We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.","authors":["Juliusz Ziomek","William Bankes","Lorenz Wolf","Shyam Sundhar Ramesh","Xiaohang Tang","Ilija Bogunovic"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.13851v2","updated":"2026-02-23T11:03:39Z","published":"2026-01-20T11:02:54Z","title":"Inverting Self-Organizing Maps: A Unified Activation-Based Framework","summary":"Self-Organizing Maps (SOMs) provide topology-preserving projections of high-dimensional data, yet their use as generative models remains largely unexplored. We show that the activation pattern of a SOM -- the squared distances to its prototypes -- can be \\emph{inverted} to recover the exact input, following from a classical result in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which inversion is well-posed. Building on this mechanism, we introduce the \\emph{Manifold-Aware Unified SOM Inversion and Control} (MUSIC) update rule, which modifies squared distances to selected prototypes while preserving others, producing controlled, semantically meaningful trajectories aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update and ensures smooth motion in high dimensions. Unlike variational or diffusion-based generative models, MUSIC requires no sampling, latent priors, or learned decoders: it operates entirely on prototype geometry. If no perturbation is applied, inversion recovers the exact input; when a target prototype or cluster is specified, MUSIC produces coherent semantic transitions. We validate the framework on synthetic Gaussian mixtures, MNIST digits, and the Labeled Faces in the Wild dataset. Across all settings, MUSIC trajectories maintain high classifier confidence, produce significantly sharper intermediate images than linear interpolation, and reveal an interpretable geometric structure of the learned map.","authors":["Alessandro Londei","Matteo Benati","Denise Lanzieri","Vittorio Loreto"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.19073v3","updated":"2026-02-23T11:03:09Z","published":"2025-08-26T14:29:34Z","title":"CARMA: Collocation-Aware Resource Manager","summary":"GPUs running deep learning (DL) workloads are frequently underutilized. Collocating multiple DL training tasks on the same GPU can improve utilization but introduces two key risks: (1) out-of-memory (OOM) crashes for newly scheduled tasks, and (2) severe performance interference among co-running tasks, which can negate any throughput gains. These issues reduce system robustness, quality of service, and energy efficiency. We present CARMA, a task-level, collocation-aware resource manager for the server-scale. CARMA addresses collocation challenges via (1) fine-grained monitoring and bookkeeping of GPUs and a collocation risk analysis that filters out the high-risk GPUs; (2) task placement policies that cap GPU utilization to limit OOMs and interference; (3) integration of GPU memory need estimators for DL tasks to minimize OOMs during collocation; and (4) a lightweight recovery method that relaunches jobs crashed due to OOMs. Our evaluation on a DL training workload derived from real-world traces shows that CARMA uses GPUs more efficiently by making more informed collocation decisions: for the best-performing collocation policy, CARMA increases GPU streaming multiprocessor (SM) utilization by 54%, the parallelism achieved per SM by 61%, and memory use by 62%. This results in a ~35% and ~15% reduction in the end-to-end execution time (makespan) and GPU energy consumption, respectively, for this workload.","authors":["Ehsan Yousefzadeh-Asl-Miandoab","Florina M. Ciorba","Pınar Tözün"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19710v1","updated":"2026-02-23T11:00:08Z","published":"2026-02-23T11:00:08Z","title":"Universal Pose Pretraining for Generalizable Vision-Language-Action Policies","summary":"Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.\n  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.\n  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.","authors":["Haitao Lin","Hanyang Yu","Jingshun Huang","He Zhang","Yonggen Ling","Ping Tan","Xiangyang Xue","Yanwei Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.14934v2","updated":"2026-02-23T10:54:32Z","published":"2026-02-16T17:17:08Z","title":"Activation-Space Uncertainty Quantification for Pretrained Networks","summary":"Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.","authors":["Richard Bergna","Stefan Depeweg","Sergio Calvo-Ordoñez","Jonathan Plenk","Alvaro Cartea","Jose Miguel Hernández-Lobato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19691v1","updated":"2026-02-23T10:38:12Z","published":"2026-02-23T10:38:12Z","title":"Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations","summary":"Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.","authors":["Yuhao Liu","Zilin Wang","Lei Wu","Shaobo Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16703v2","updated":"2026-02-23T10:35:13Z","published":"2025-10-19T04:13:09Z","title":"On the Granularity of Causal Effect Identifiability","summary":"The classical notion of causal effect identifiability is defined in terms of treatment and outcome variables. In this paper, we consider the identifiability of state-based causal effects: how an intervention on a particular state of treatment variables affects a particular state of outcome variables. We demonstrate that state-based causal effects may be identifiable even when variable-based causal effects may not. Moreover, we show that this separation occurs only when additional knowledge -- such as context-specific independencies -- is available. We further examine knowledge that constrains the states of variables, and show that such knowledge can improve both variable-based and state-based identifiability when combined with other knowledge such as context-specific independencies. We finally propose an approach for identifying causal effects under these additional constraints, and conduct empirical studies to further illustrate the separations between the two levels of identifiability.","authors":["Yizuo Chen","Adnan Darwiche"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19685v1","updated":"2026-02-23T10:28:56Z","published":"2026-02-23T10:28:56Z","title":"PerturbDiff: Functional Diffusion for Single-Cell Perturbation Modeling","summary":"Building Virtual Cells that can accurately simulate cellular responses to perturbations is a long-standing goal in systems biology. A fundamental challenge is that high-throughput single-cell sequencing is destructive: the same cell cannot be observed both before and after a perturbation. Thus, perturbation prediction requires mapping unpaired control and perturbed populations. Existing models address this by learning maps between distributions, but typically assume a single fixed response distribution when conditioned on observed cellular context (e.g., cell type) and the perturbation type. In reality, responses vary systematically due to unobservable latent factors such as microenvironmental fluctuations and complex batch effects, forming a manifold of possible distributions for the same observed conditions. To account for this variability, we introduce PerturbDiff, which shifts modeling from individual cells to entire distributions. By embedding distributions as points in a Hilbert space, we define a diffusion-based generative process operating directly over probability distributions. This allows PerturbDiff to capture population-level response shifts across hidden factors. Benchmarks on established datasets show that PerturbDiff achieves state-of-the-art performance in single-cell response prediction and generalizes substantially better to unseen perturbations. See our project page (https://katarinayuan.github.io/PerturbDiff-ProjectPage/), where code and data will be made publicly available (https://github.com/DeepGraphLearning/PerturbDiff).","authors":["Xinyu Yuan","Xixian Liu","Ya Shi Zhang","Zuobai Zhang","Hongyu Guo","Jian Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.16642v2","updated":"2026-02-23T10:22:26Z","published":"2026-02-18T17:32:43Z","title":"Optimizer choice matters for the emergence of Neural Collapse","summary":"Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.","authors":["Jim Zhao","Tin Sum Cheng","Wojciech Masarczyk","Aurelien Lucchi"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19672v1","updated":"2026-02-23T10:17:25Z","published":"2026-02-23T10:17:25Z","title":"SkillOrchestra: Learning to Route Agents via Skill Transfer","summary":"Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.","authors":["Jiayu Wang","Yifei Ming","Zixuan Ke","Shafiq Joty","Aws Albarghouthi","Frederic Sala"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19668v1","updated":"2026-02-23T10:14:36Z","published":"2026-02-23T10:14:36Z","title":"Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation","summary":"Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.\n  We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.\n  Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.","authors":["He Zhu","Ren Togo","Takahiro Ogawa","Kenji Hirata","Minghui Tang","Takaaki Yoshimura","Hiroyuki Sugimori","Noriko Nishioka","Yukie Shimizu","Kohsuke Kudo","Miki Haseyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.08549v3","updated":"2026-02-23T10:11:13Z","published":"2026-01-13T13:36:38Z","title":"Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures","summary":"We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.","authors":["Sucheta Ghosh","Felix Dietrich","Zahra Monfared"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19661v1","updated":"2026-02-23T10:09:50Z","published":"2026-02-23T10:09:50Z","title":"PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information","summary":"Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.","authors":["Kihyuk Yoon","Lingchao Mao","Catherine Chong","Todd J. Schwedt","Chia-Chun Chiang","Jing Li"],"pdf_url":"","comment":"26 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2509.22387v2","updated":"2026-02-23T10:05:41Z","published":"2025-09-26T14:15:44Z","title":"SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly","summary":"The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.","authors":["Narada Maugin","Tristan Cazenave"],"pdf_url":"","comment":"Accepted at Advances in Computer Games (ACG) 2025, LNCS (Springer)"},{"id":"http://arxiv.org/abs/2510.03734v2","updated":"2026-02-23T10:04:26Z","published":"2025-10-04T08:38:03Z","title":"Cost Efficient Fairness Audit Under Partial Feedback","summary":"We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.","authors":["Nirjhar Das","Mohit Sharma","Praharsh Nanavati","Kirankumar Shiragur","Amit Deshpande"],"pdf_url":"","comment":"Accepted at NeurIPS 2025 RegML Workshop; Reliable ML Workshop"},{"id":"http://arxiv.org/abs/2602.19655v1","updated":"2026-02-23T09:59:03Z","published":"2026-02-23T09:59:03Z","title":"Representation Stability in a Minimal Continual Learning Agent","summary":"Continual learning systems are increasingly deployed in environments where retraining or reset is infeasible, yet many approaches emphasize task performance rather than the evolution of internal representations over time. In this work, we study a minimal continual learning agent designed to isolate representational dynamics from architectural complexity and optimization objectives. The agent maintains a persistent state vector across executions and incrementally updates it as new textual data is introduced. We quantify representational change using cosine similarity between successive normalized state vectors and define a stability metric over time intervals. Longitudinal experiments across eight executions reveal a transition from an initial plastic regime to a stable representational regime under consistent input. A deliberately introduced semantic perturbation produces a bounded decrease in similarity, followed by recovery and restabilization under subsequent coherent input. These results demonstrate that meaningful stability plasticity tradeoffs can emerge in a minimal, stateful learning system without explicit regularization, replay, or architectural complexity. The work establishes a transparent empirical baseline for studying representational accumulation and adaptation in continual learning systems.","authors":["Vishnu Subramanian"],"pdf_url":"","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2602.19654v1","updated":"2026-02-23T09:56:22Z","published":"2026-02-23T09:56:22Z","title":"NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region","summary":"Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.","authors":["Rampunit Kumar","Aditya Maheshwari"],"pdf_url":"","comment":"18 pages"},{"id":"http://arxiv.org/abs/2602.19651v1","updated":"2026-02-23T09:53:23Z","published":"2026-02-23T09:53:23Z","title":"Denoising Particle Filters: Learning State Estimation with Single-Step Objectives","summary":"Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.","authors":["Lennart Röstel","Berthold Bäuml"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19644v1","updated":"2026-02-23T09:42:42Z","published":"2026-02-23T09:42:42Z","title":"Spectral Phase Encoding for Quantum Kernel Methods","summary":"Quantum kernel methods are promising for near-term quantum ma- chine learning, yet their behavior under data corruption remains insuf- ficiently understood. We analyze how quantum feature constructions degrade under controlled additive noise. We introduce Spectral Phase Encoding (SPE), a hybrid construc- tion combining a discrete Fourier transform (DFT) front-end with a diagonal phase-only embedding aligned with the geometry of diagonal quantum maps. Within a unified framework, we compare QK-DFT against alternative quantum variants (QK-PCA, QK-RP) and classi- cal SVM baselines under identical clean-data hyperparameter selection, quantifying robustness via dataset fixed-effects regression with wild cluster bootstrap inference across heterogeneous real-world datasets. Across the quantum family, DFT-based preprocessing yields the smallest degradation rate as noise increases, with statistically sup- ported slope differences relative to PCA and RP. Compared to classical baselines, QK-DFT shows degradation comparable to linear SVM and more stable than RBF SVM under matched tuning. Hardware exper- iments confirm that SPE remains executable and numerically stable for overlap estimation. These results indicate that robustness in quan- tum kernels depends critically on structure-aligned preprocessing and its interaction with diagonal embeddings, supporting a robustness-first perspective for NISQ-era quantum machine learning.","authors":["Pablo Herrero Gómez","Antonio Jimeno Morenilla","David Muñoz-Hernández","Higinio Mora Mora"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19641v1","updated":"2026-02-23T09:39:06Z","published":"2026-02-23T09:39:06Z","title":"Evaluating the Impact of Data Anonymization on Image Retrieval","summary":"With the growing importance of privacy regulations such as the General Data Protection Regulation, anonymizing visual data is becoming increasingly relevant across institutions. However, anonymization can negatively affect the performance of Computer Vision systems that rely on visual features, such as Content-Based Image Retrieval (CBIR). Despite this, the impact of anonymization on CBIR has not been systematically studied. This work addresses this gap, motivated by the DOKIQ project, an artificial intelligence-based system for document verification actively used by the State Criminal Police Office Baden-Württemberg. We propose a simple evaluation framework: retrieval results after anonymization should match those obtained before anonymization as closely as possible. To this end, we systematically assess the impact of anonymization using two public datasets and the internal DOKIQ dataset. Our experiments span three anonymization methods, four anonymization degrees, and four training strategies, all based on the state of the art backbone Self-Distillation with No Labels (DINO)v2. Our results reveal a pronounced retrieval bias in favor of models trained on original data, which produce the most similar retrievals after anonymization. The findings of this paper offer practical insights for developing privacy-compliant CBIR systems while preserving performance.","authors":["Marvin Chen","Manuel Eberhardinger","Johannes Maucher"],"pdf_url":"","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2508.05612v5","updated":"2026-02-23T09:33:32Z","published":"2025-08-07T17:53:47Z","title":"Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle","summary":"Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.","authors":["Linghao Zhu","Yiran Guan","Dingkang Liang","Jianzhong Ju","Zhenbo Luo","Bin Qin","Jian Luan","Yuliang Liu","Xiang Bai"],"pdf_url":"","comment":"This paper has been accepted by ICLR 2026 Project page at: https://xenozlh.github.io/Shuffle-R1/"},{"id":"http://arxiv.org/abs/2505.24183v5","updated":"2026-02-23T09:24:51Z","published":"2025-05-30T03:51:06Z","title":"QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation","summary":"Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.","authors":["Yaoyu Zhu","Di Huang","Hanqi Lyu","Xiaoyun Zhang","Chongxiao Li","Wenxuan Shi","Yutong Wu","Jianan Mu","Jinghua Wang","Yang Zhao","Pengwei Jin","Shuyao Cheng","Shengwen Liang","Xishan Zhang","Rui Zhang","Zidong Du","Qi Guo","Xing Hu","Yunji Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19634v1","updated":"2026-02-23T09:22:21Z","published":"2026-02-23T09:22:21Z","title":"Compositional Planning with Jumpy World Models","summary":"The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.","authors":["Jesse Farebrother","Matteo Pirotta","Andrea Tirinzoni","Marc G. Bellemare","Alessandro Lazaric","Ahmed Touati"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19622v1","updated":"2026-02-23T09:10:39Z","published":"2026-02-23T09:10:39Z","title":"VecFormer: Towards Efficient and Generalizable Graph Transformer with Graph Token Attention","summary":"Graph Transformer has demonstrated impressive capabilities in the field of graph representation learning. However, existing approaches face two critical challenges: (1) most models suffer from exponentially increasing computational complexity, making it difficult to scale to large graphs; (2) attention mechanisms based on node-level operations limit the flexibility of the model and result in poor generalization performance in out-of-distribution (OOD) scenarios. To address these issues, we propose \\textbf{VecFormer} (the \\textbf{Vec}tor Quantized Graph Trans\\textbf{former}), an efficient and highly generalizable model for node classification, particularly under OOD settings. VecFormer adopts a two-stage training paradigm. In the first stage, two codebooks are used to reconstruct the node features and the graph structure, aiming to learn the rich semantic \\texttt{Graph Codes}. In the second stage, attention mechanisms are performed at the \\texttt{Graph Token} level based on the transformed cross codebook, reducing computational complexity while enhancing the model's generalization capability. Extensive experiments on datasets of various sizes demonstrate that VecFormer outperforms the existing Graph Transformer in both performance and speed.","authors":["Jingbo Zhou","Jun Xia","Siyuan Li","Yunfan Liu","Wenjun Wang","Yufei Huang","Changxi Chi","Mutian Hong","Zhuoli Ouyang","Shu Wang","Zhongqi Wang","Xingyu Wu","Chang Yu","Stan Z. Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19619v1","updated":"2026-02-23T09:06:13Z","published":"2026-02-23T09:06:13Z","title":"Is Your Diffusion Sampler Actually Correct? A Sampler-Centric Evaluation of Discrete Diffusion Language Models","summary":"Discrete diffusion language models (dLLMs) provide a fast and flexible alternative to autoregressive models (ARMs) via iterative denoising with parallel updates. However, their evaluation is challenging: existing metrics conflate denoiser approximation error with sampler-induced error from the sampling dynamics, a problem that does not arise for ARMs whose autoregressive sampling exactly reflects the learned probability model. We introduce a sampler-centric oracle framework that replaces learned denoisers with an exact Hidden Markov Model posterior derived from a ground-truth Markov chain, isolating sampler-induced error in a controlled setting. We show that few-step discrete diffusion samplers are not distributionally correct even under an oracle denoiser, with transition-level mismatch that vanishes only as the number of steps approaches the sequence length. Moreover, improvements in negative log-likelihood, generative perplexity, or MAUVE do not imply correct sampling. Code is available at https://luhantang.github.io/dllm_sampler","authors":["Luhan Tang","Longxuan Yu","Shaorong Zhang","Greg Ver Steeg"],"pdf_url":"","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.19614v1","updated":"2026-02-23T09:02:38Z","published":"2026-02-23T09:02:38Z","title":"Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering","summary":"The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic (\"big-bang\") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.","authors":["Chih-Hong Cheng","Brian Hsuan-Cheng Liao","Adam Molin","Hasan Esen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.00502v2","updated":"2026-02-23T08:58:39Z","published":"2025-10-01T04:34:07Z","title":"Diffusion Alignment as Variational Expectation-Maximization","summary":"Diffusion alignment aims to optimize diffusion models for the downstream objective. While existing methods based on reinforcement learning or direct backpropagation achieve considerable success in maximizing rewards, they often suffer from reward over-optimization and mode collapse. We introduce Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that formulates diffusion alignment as an iterative process alternating between two complementary phases: the E-step and the M-step. In the E-step, we employ test-time search to generate diverse and reward-aligned samples. In the M-step, we refine the diffusion model using samples discovered by the E-step. We demonstrate that DAV can optimize reward while preserving diversity for both continuous and discrete tasks: text-to-image synthesis and DNA sequence design. Our code is available at https://github.com/Jaewoopudding/dav.","authors":["Jaewoo Lee","Minsu Kim","Sanghyeok Choi","Inhyuck Song","Sujin Yun","Hyeongyu Kang","Woocheol Shin","Taeyoung Yun","Kiyoung Om","Jinkyoo Park"],"pdf_url":"","comment":"32 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2511.18945v3","updated":"2026-02-23T08:55:36Z","published":"2025-11-24T09:55:28Z","title":"MIST: Mutual Information Estimation Via Supervised Training","summary":"We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.","authors":["German Gritsai","Megan Richards","Maxime Méloux","Kyunghyun Cho","Maxime Peyrard"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.08604v4","updated":"2026-02-23T08:51:45Z","published":"2025-06-10T09:13:37Z","title":"Physics vs Distributions: Pareto Optimal Flow Matching with Physics Constraints","summary":"Physics-constrained generative modeling aims to produce high-dimensional samples that are both physically consistent and distributionally accurate, a task that remains challenging due to often conflicting optimization objectives. Recent advances in flow matching and diffusion models have enabled efficient generative modeling, but integrating physical constraints often degrades generative fidelity or requires costly inference-time corrections. Our work is the first to recognize the trade-off between distributional and physical accuracy. Based on the insight of inherently conflicting objectives, we introduce Physics-Based Flow Matching (PBFM) a method that enforces physical constraints at training time using conflict-free gradient updates and unrolling to mitigate Jensen's gap. Our approach avoids manual loss balancing and enables simultaneous optimization of generative and physical objectives. As a consequence, physics constraints do not impede inference performance. We benchmark our method across three representative PDE benchmarks. PBFM achieves a Pareto-optimal trade-off, competitive inference speed, and generalizes to a wide range of physics-constrained generative tasks, providing a practical tool for scientific machine learning. Code and datasets available at https://github.com/tum-pbs/PBFM.","authors":["Giacomo Baldan","Qiang Liu","Alberto Guardone","Nils Thuerey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19610v1","updated":"2026-02-23T08:51:26Z","published":"2026-02-23T08:51:26Z","title":"Variational Inference for Bayesian MIDAS Regression","summary":"We develop a Coordinate Ascent Variational Inference (CAVI) algorithm for Bayesian Mixed Data Sampling (MIDAS) regression with linear weight parameterizations. The model separates impact coeffcients from weighting function parameters through a normalization constraint, creating a bilinear structure that renders generic Hamiltonian Monte Carlo samplers unreliable while preserving conditional conjugacy exploitable by CAVI. Each variational update admits a closed-form solution: Gaussian for regression coefficients and weight parameters, Inverse-Gamma for the error variance. The algorithm propagates uncertainty across blocks through second moments, distinguishing it from naive plug-in approximations. In a Monte Carlo study spanning 21 data-generating configurations with up to 50 predictors, CAVI produces posterior means nearly identical to a block Gibbs sampler benchmark while achieving speedups of 107x to 1,772x (Table 9). Generic automatic differentiation VI (ADVI), by contrast, produces bias 714 times larger while being orders of magnitude slower, confirming the value of model-specific derivations. Weight function parameters maintain excellent calibration (coverage above 92%) across all configurations. Impact coefficient credible intervals exhibit the underdispersion characteristic of mean-field approximations, with coverage declining from 89% to 55% as the number of predictors grows a documented trade-off between speed and interval calibration that structured variational methods can address. An empirical application to realized volatility forecasting on S&P 500 daily returns cofirms that CAVI and Gibbs sampling yield virtually identical point forecasts, with CAVI completing each monthly estimation in under 10 milliseconds.","authors":["Luigi Simeone"],"pdf_url":"","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2511.17439v2","updated":"2026-02-23T08:51:09Z","published":"2025-11-21T17:36:12Z","title":"InTAct: Interval-based Task Activation Consolidation for Continual Learning","summary":"Continual learning is a fundamental challenge in artificial intelligence that requires networks to acquire new knowledge while preserving previously learned representations. Despite the success of various approaches, most existing paradigms do not provide rigorous mathematical guarantees against catastrophic forgetting. Current methods that offer such guarantees primarily focus on analyzing the parameter space using \\textit{interval arithmetic (IA)}, as seen in frameworks such as InterContiNet. However, restricting high-dimensional weight updates can be computationally expensive. In this work, we propose InTAct (Interval-based Task Activation Consolidation), a method that mitigates catastrophic forgetting by enforcing functional invariance at the neuron level. We identify specific activation intervals where previous tasks reside and constrain updates within these regions while allowing for flexible adaptation elsewhere. By ensuring that predictions remain stable within these nested activation intervals, we provide a tractable mathematical guarantee of functional invariance. We emphasize that regulating the activation space is significantly more efficient than parameter-based constraints, because the dimensionality of internal signals is much lower than that of the vast space of model weights. While our approach is architecture-agnostic and applicable to various continual learning settings, its integration with prompt-based methods enables it to achieve state-of-the-art performance on challenging benchmarks.","authors":["Patryk Krukowski","Jan Miksa","Piotr Helm","Jacek Tabor","Paweł Wawrzyński","Przemysław Spurek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.14697v2","updated":"2026-02-23T08:43:06Z","published":"2026-02-16T12:34:27Z","title":"Unifying Evolutionary Prompt Search and Reinforcement Learning for LLM Self-Improvement","summary":"Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL samples trajectories under multiple system prompts in parallel. It applies RL updates to LLM weights conditioned on system prompts, and evolutionary updates to system prompts via mutation and crossover, two genetic operators based on LLM self-reflection. Each system prompt is assigned a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results demonstrate that RL and evolutionary prompt search are deeply synergistic, and unifying the two yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL","authors":["Lunjun Zhang","Ryan Chen","Bradly C. Stadie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19600v1","updated":"2026-02-23T08:42:40Z","published":"2026-02-23T08:42:40Z","title":"Manifold-Aligned Generative Transport","summary":"High-dimensional generative modeling is fundamentally a manifold-learning problem: real data concentrate near a low-dimensional structure embedded in the ambient space. Effective generators must therefore balance support fidelity -- placing probability mass near the data manifold -- with sampling efficiency. Diffusion models often capture near-manifold structure but require many iterative denoising steps and can leak off-support; normalizing flows sample in one pass but are limited by invertibility and dimension preservation. We propose MAGT (Manifold-Aligned Generative Transport), a flow-like generator that learns a one-shot, manifold-aligned transport from a low-dimensional base distribution to the data space. Training is performed at a fixed Gaussian smoothing level, where the score is well-defined and numerically stable. We approximate this fixed-level score using a finite set of latent anchor points with self-normalized importance sampling, yielding a tractable objective. MAGT samples in a single forward pass, concentrates probability near the learned support, and induces an intrinsic density with respect to the manifold volume measure, enabling principled likelihood evaluation for generated samples. We establish finite-sample Wasserstein bounds linking smoothing level and score-approximation accuracy to generative fidelity, and empirically improve fidelity and manifold concentration across synthetic and benchmark datasets while sampling substantially faster than diffusion models.","authors":["Xinyu Tian","Xiaotong Shen"],"pdf_url":"","comment":"64 pages, 5 figures"},{"id":"http://arxiv.org/abs/2301.00201v4","updated":"2026-02-23T08:41:22Z","published":"2022-12-31T13:48:42Z","title":"Exploring Singularities in point clouds with the graph Laplacian: An explicit approach","summary":"We develop theory and methods that use the graph Laplacian to analyze the geometry of the underlying manifold of datasets. Our theory provides theoretical guarantees and explicit bounds on the functional forms of the graph Laplacian when it acts on functions defined close to singularities of the underlying manifold. We use these explicit bounds to develop tests for singularities and propose methods that can be used to estimate geometric properties of singularities in the datasets.","authors":["Martin Andersson","Benny Avelin"],"pdf_url":"","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2602.19594v1","updated":"2026-02-23T08:37:53Z","published":"2026-02-23T08:37:53Z","title":"ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?","summary":"We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.","authors":["Ayush Nangia","Shikhar Mishra","Aman Gokrani","Paras Chopra"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.01650v2","updated":"2026-02-23T08:37:18Z","published":"2025-10-02T04:10:17Z","title":"The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM","summary":"Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Moreover, we show that $\\texttt{Elsa}$ remains stable even at extreme sparsity (e.g., 95\\%), yielding up to $\\times$3.98 inference speedup and $\\times$7.80 memory compression over its dense counterpart. We also present $\\texttt{Elsa}_{-L}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees.These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.","authors":["Kwanhee Lee","Hyeondo Jang","Dongyeop Lee","Dan Alistarh","Namhoon Lee"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19591v1","updated":"2026-02-23T08:35:55Z","published":"2026-02-23T08:35:55Z","title":"Detecting High-Potential SMEs with Heterogeneous Graph Neural Networks","summary":"Small and Medium Enterprises (SMEs) constitute 99.9% of U.S. businesses and generate 44% of economic activity, yet systematically identifying high-potential SMEs remains an open challenge. We introduce SME-HGT, a Heterogeneous Graph Transformer framework that predicts which SBIR Phase I awardees will advance to Phase II funding using exclusively public data. We construct a heterogeneous graph with 32,268 company nodes, 124 research topic nodes, and 13 government agency nodes connected by approximately 99,000 edges across three semantic relation types. SME-HGT achieves an AUPRC of 0.621 0.003 on a temporally-split test set, outperforming an MLP baseline (0.590 0.002) and R-GCN (0.608 0.013) across five random seeds. At a screening depth of 100 companies, SME-HGT attains 89.6% precision with a 2.14 lift over random selection. Our temporal evaluation protocol prevents information leakage, and our reliance on public data ensures reproducibility. These results demonstrate that relational structure among firms, research topics, and funding agencies provides meaningful signal for SME potential assessment, with implications for policymakers and early-stage investors.","authors":["Yijiashun Qi","Hanzhe Guo","Yijiazhen Qi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19584v1","updated":"2026-02-23T08:12:49Z","published":"2026-02-23T08:12:49Z","title":"Interpolation-Driven Machine Learning Approaches for Plume Shine Dose Estimation: A Comparison of XGBoost, Random Forest, and TabNet","summary":"Despite the success of machine learning (ML) in surrogate modeling, its use in radiation dose assessment is limited by safety-critical constraints, scarce training-ready data, and challenges in selecting suitable architectures for physics-dominated systems. Within this context, rapid and accurate plume shine dose estimation serves as a practical test case, as it is critical for nuclear facility safety assessment and radiological emergency response, while conventional photon-transport-based calculations remain computationally expensive. In this work, an interpolation-assisted ML framework was developed using discrete dose datasets generated with the pyDOSEIA suite for 17 gamma-emitting radionuclides across varying downwind distances, release heights, and atmospheric stability categories. The datasets were augmented using shape-preserving interpolation to construct dense, high-resolution training data. Two tree-based ML models (Random Forest and XGBoost) and one deep learning (DL) model (TabNet) were evaluated to examine predictive performance and sensitivity to dataset resolution. All models showed higher prediction accuracy with the interpolated high-resolution dataset than with the discrete data; however, XGBoost consistently achieved the highest accuracy. Interpretability analysis using permutation importance (tree-based models) and attention-based feature attribution (TabNet) revealed that performance differences stem from how the models utilize input features. Tree-based models focus mainly on dominant geometry-dispersion features (release height, stability category, and downwind distance), treating radionuclide identity as a secondary input, whereas TabNet distributes attention more broadly across multiple variables. For practical deployment, a web-based GUI was developed for interactive scenario evaluation and transparent comparison with photon-transport reference calculations.","authors":["Biswajit Sadhu","Kalpak Gupte","Trijit Sadhu","S. Anand"],"pdf_url":"","comment":"28 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2602.19582v1","updated":"2026-02-23T08:08:23Z","published":"2026-02-23T08:08:23Z","title":"Advantage-based Temporal Attack in Reinforcement Learning","summary":"Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.","authors":["Shenghong He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19580v1","updated":"2026-02-23T08:01:44Z","published":"2026-02-23T08:01:44Z","title":"Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training","summary":"We introduce Leap+Verify, a framework that applies speculative execution -- predicting future model weights and validating predictions before acceptance -- to accelerate neural network training. Inspired by speculative decoding in language model inference and by the Automatically Scalable Computation (ASC) architecture for program execution, Leap+Verify decomposes training into three dynamically detected regimes (chaotic, transition, stable) using activation-space cosine similarity as a real-time Lyapunov proxy signal. Within each regime, analytic weight predictors (momentum, linear, quadratic extrapolation) attempt to forecast model parameters K training steps ahead; predictions are accepted only when validated against a held-out loss criterion. We evaluate Leap+Verify on GPT-2 124M and Qwen 2.5-1.5B trained on WikiText-103 across five random seeds, sweeping prediction depth K in {5, 10, 25, 50, 75, 100}. Momentum-based prediction (Adam moment extrapolation) fails catastrophically at both scales, with predicted losses exceeding actuals by 100-10,000x -- a universal norm explosion in optimizer-state extrapolation. Finite-difference predictors (linear, quadratic) succeed where momentum fails: at 124M, they achieve 24% strict acceptance at K=5 in stable regimes; at 1.5B, they achieve 37% strict acceptance in transition regimes. The scale-dependent finding is in regime distribution: GPT-2 124M spends 34% of training in stable regime, while Qwen 1.5B spends 64% in chaotic regime and reaches stable in only 0-2 of 40 checkpoints. Larger models are more predictable when predictable, but less often predictable -- the practical bottleneck shifts from predictor accuracy to regime availability. Cross-seed results are highly consistent (less than 1% validation loss variance), and the three-regime framework produces identical phase boundaries (plus or minus 50 steps) across seeds.","authors":["Jeremy McEntire"],"pdf_url":"","comment":"18 pages, 5 tables. Code and data available at https://github.com/jmcentire/leap-verify"},{"id":"http://arxiv.org/abs/2602.19578v1","updated":"2026-02-23T07:57:11Z","published":"2026-02-23T07:57:11Z","title":"Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization","summary":"Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.","authors":["Weichi Yao","Bianca Dumitrascu","Bryan R. Goldsmith","Yixin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.01289v2","updated":"2026-02-23T07:27:27Z","published":"2026-02-01T15:45:07Z","title":"Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models","summary":"Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.","authors":["Dung Anh Hoang","Cuong Pham anh Trung Le","Jianfei Cai","Thanh-Toan Do"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.03098v2","updated":"2026-02-23T07:25:58Z","published":"2026-02-03T04:43:13Z","title":"TextME: Bridging Unseen Modalities Through Text Descriptions","summary":"Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.","authors":["Soyeon Hong","Jinchan Kim","Jaegook You","Seungtaek Choi","Suha Kwak","Hyunsouk Cho"],"pdf_url":"","comment":"Code available at https://github.com/SoyeonHH/TextME"},{"id":"http://arxiv.org/abs/2410.02099v3","updated":"2026-02-23T07:20:10Z","published":"2024-10-02T23:39:19Z","title":"A Watermark for Black-Box Language Models","summary":"Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.","authors":["Dara Bahri","John Wieting"],"pdf_url":"","comment":"Published at TMLR 2026"},{"id":"http://arxiv.org/abs/2510.22512v2","updated":"2026-02-23T07:03:22Z","published":"2025-10-26T03:32:31Z","title":"Transitive RL: Value Learning via Divide and Conquer","summary":"In this work, we present Transitive Reinforcement Learning (TRL), a new value learning algorithm based on a divide-and-conquer paradigm. TRL is designed for offline goal-conditioned reinforcement learning (GCRL) problems, where the aim is to find a policy that can reach any state from any other state in the smallest number of steps. TRL converts a triangle inequality structure present in GCRL into a practical divide-and-conquer value update rule. This has several advantages compared to alternative value learning paradigms. Compared to temporal difference (TD) methods, TRL suffers less from bias accumulation, as in principle it only requires $O(\\log T)$ recursions (as opposed to $O(T)$ in TD learning) to handle a length-$T$ trajectory. Unlike Monte Carlo methods, TRL suffers less from high variance as it performs dynamic programming. Experimentally, we show that TRL achieves the best performance in highly challenging, long-horizon benchmark tasks compared to previous offline GCRL algorithms.","authors":["Seohong Park","Aditya Oberai","Pranav Atreya","Sergey Levine"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19552v1","updated":"2026-02-23T06:52:11Z","published":"2026-02-23T06:52:11Z","title":"The Sample Complexity of Replicable Realizable PAC Learning","summary":"In this paper, we consider the problem of replicable realizable PAC learning. We construct a particularly hard learning problem and show a sample complexity lower bound with a close to $(\\log|H|)^{3/2}$ dependence on the size of the hypothesis class $H$. Our proof uses several novel techniques and works by defining a particular Cayley graph associated with $H$ and analyzing a suitable random walk on this graph by examining the spectral properties of its adjacency matrix.\n  Furthermore, we show an almost matching upper bound for the lower bound instance, meaning if a stronger lower bound exists, one would have to consider a different instance of the problem.","authors":["Kasper Green Larsen","Markus Engelund Mathiasen","Chirag Pabbaraju","Clement Svendsen"],"pdf_url":"","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2602.20159v1","updated":"2026-02-23T18:59:41Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2602.08550v2","updated":"2026-02-23T15:12:01Z","published":"2026-02-09T11:50:29Z","title":"GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing","summary":"Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.","authors":["Shih-Fang Chen","Jun-Cheng Chen","I-Hong Jhuo","Yen-Yu Lin"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19778v1","updated":"2026-02-23T12:32:53Z","published":"2026-02-23T12:32:53Z","title":"Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation","summary":"Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.","authors":["Nghia Phan","Rong Jin","Gang Liu","Xiao Dong"],"pdf_url":"","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2602.17690v2","updated":"2026-02-23T11:36:42Z","published":"2026-02-06T05:10:19Z","title":"DesignAsCode: Bridging Structural Editability and Visual Fidelity in Graphic Design Generation","summary":"Graphic design generation demands a delicate balance between high visual fidelity and fine-grained structural editability. However, existing approaches typically bifurcate into either non-editable raster image synthesis or abstract layout generation devoid of visual content. Recent combinations of these two approaches attempt to bridge this gap but often suffer from rigid composition schemas and unresolvable visual dissonances (e.g., text-background conflicts) due to their inexpressive representation and open-loop nature. To address these challenges, we propose DesignAsCode, a novel framework that reimagines graphic design as a programmatic synthesis task using HTML/CSS. Specifically, we introduce a Plan-Implement-Reflect pipeline, incorporating a Semantic Planner to construct dynamic, variable-depth element hierarchies and a Visual-Aware Reflection mechanism that iteratively optimizes the code to rectify rendering artifacts. Extensive experiments demonstrate that DesignAsCode significantly outperforms state-of-the-art baselines in both structural validity and aesthetic quality. Furthermore, our code-native representation unlocks advanced capabilities, including automatic layout retargeting, complex document generation (e.g., resumes), and CSS-based animation. Our project page is available at https://liuziyuan1109.github.io/design-as-code/.","authors":["Ziyuan Liu","Shizhao Sun","Danqing Huang","Yingdong Shi","Meisheng Zhang","Ji Li","Jingsong Yu","Jiang Bian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.15082v2","updated":"2026-02-23T10:34:04Z","published":"2026-02-16T10:28:38Z","title":"S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization","summary":"Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.","authors":["Zineb Lahrichi","Gaëtan Hadjeres","Gaël Richard","Geoffroy Peeters"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19605v1","updated":"2026-02-23T08:47:19Z","published":"2026-02-23T08:47:19Z","title":"CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning","summary":"Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.","authors":["Chunlei Meng","Guanhong Huang","Rong Fu","Runmin Jian","Zhongxue Gan","Chun Ouyang"],"pdf_url":"","comment":"This study has been Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.19585v1","updated":"2026-02-23T08:19:54Z","published":"2026-02-23T08:19:54Z","title":"Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis","summary":"Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.","authors":["Chunlei Meng","Jiabin Luo","Zhenglin Yan","Zhenyu Yu","Rong Fu","Zhongxue Gan","Chun Ouyang"],"pdf_url":"","comment":"This study has been Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2505.17543v3","updated":"2026-02-23T07:09:36Z","published":"2025-05-23T06:47:06Z","title":"MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation","summary":"Music-driven 3D dance generation has attracted increasing attention in recent years, with promising applications in choreography, virtual reality, and creative content creation. Previous research has generated promising realistic dance movement from audio signals. However, traditional methods underutilize genre conditioning, often treating it as auxiliary modifiers rather than core semantic drivers. This oversight compromises music-motion synchronization and disrupts dance genre continuity, particularly during complex rhythmic transitions, thereby leading to visually unsatisfactory effects. To address the challenge, we propose MEGADance, a novel architecture for music-driven 3D dance generation. By decoupling choreographic consistency into dance generality and genre specificity, MEGADance demonstrates significant dance quality and strong genre controllability. It consists of two stages: (1) High-Fidelity Dance Quantization Stage (HFDQ), which encodes dance motions into a latent representation by Finite Scalar Quantization (FSQ) and reconstructs them with kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage (GADG), which maps music into the latent representation by synergistic utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate the state-of-the-art performance of MEGADance both qualitatively and quantitatively. Code is available at https://github.com/XulongT/MEGADance.","authors":["Kaixing Yang","Xulong Tang","Ziqiao Peng","Yuxuan Hu","Jun He","Hongyan Liu"],"pdf_url":"","comment":"NeurIPS 2025"}]},"2026-02-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2602.19385v1","updated":"2026-02-22T23:39:21Z","published":"2026-02-22T23:39:21Z","title":"Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition","summary":"Recognizing implicit visual and textual patterns is essential in many real-world applications of modern AI. However, tackling long-tail pattern recognition tasks remains challenging for current pre-trained foundation models such as LLMs and VLMs. While finetuning pre-trained models can improve accuracy in recognizing implicit patterns, it is usually infeasible due to a lack of training data and high computational overhead. In this paper, we propose ADAMAB, an efficient embedding calibration framework for few-shot pattern recognition. To maximally reduce the computational costs, ADAMAB trains embedder-agnostic light-weight calibrators on top of fixed embedding models without accessing their parameters. To mitigate the need for large-scale training data, we introduce an adaptive data augmentation strategy based on the Multi-Armed Bandit (MAB) mechanism. With a modified upper confidence bound algorithm, ADAMAB diminishes the gradient shifting and offers theoretically guaranteed convergence in few-shot training. Our multi-modal experiments justify the superior performance of ADAMAB, with up to 40% accuracy improvement when training with less than 5 initial data samples of each class.","authors":["Minxue Tang","Yangyang Yu","Aolin Ding","Maziyar Baran Pouyan","Taha Belkhouja Yujia Bao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.16439v3","updated":"2026-02-22T22:10:01Z","published":"2025-10-18T10:22:13Z","title":"FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution","summary":"Human communication heavily relies on laconism and inferential pragmatics, allowing listeners to successfully reconstruct rich meaning from sparse, telegraphic speech. In contrast, large language models (LLMs) owe much of their stellar performance to expansive input contexts, yet such verbosity inflates monetary costs, carbon footprint, and inference-time latency. This overhead manifests from the redundant low-utility tokens present in typical prompts, as only a fraction of tokens typically carries the majority of the semantic weight. Inspired by the aforementioned cognitive psycholinguistic processes, we address this inefficiency by introducing FrugalPrompt, a novel prompt compression framework for LLMs, which retains only the most semantically significant tokens. Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX, we assign salience scores to every token in an input sequence, rank them to retain the top-k% tokens, and obtain a sparse frugalized prompt. We establish the theoretical stability of our approach and provide strong empirical results across a suite of four NLP tasks to study the trade-off between the portion of retained tokens and performance. Experimental findings across retention settings reveal asymmetric performance patterns that suggest potential task contamination effects. We posit that our work contributes to a more nuanced understanding of LLM behavior in performance-efficiency trade-offs and delineates the boundary between tasks tolerant of contextual sparsity and those requiring exhaustive context.","authors":["Syed Rifat Raiyan","Md Farhan Ishmam","Abdullah Al Imran","Mohammad Ali Moni"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22876v6","updated":"2026-02-22T21:47:08Z","published":"2025-09-26T19:41:00Z","title":"HEART: Emotionally-Driven Test-Time Scaling of Language Models","summary":"Test-time scaling has significantly improved how AI models solve problems, yet current methods often get stuck in repetitive, incorrect patterns of thought. We introduce HEART, a framework that uses emotional cues to guide the model's focus, much like how feelings contribute to human decision-making. By alternating between critical tones to sharpen error detection and encouraging tones to spark new ideas, HEART helps the model break out of dead-end reasoning and find the right solution. We evaluate HEART across seven high-difficulty benchmarks--including Humanity's Last Exam, GPQA Diamond, and LiveCodeBench--demonstrating robustness across diverse models. Results show that emotion facilitates deeper reasoning, yielding consistent accuracy gains over affect-sterile baselines. These findings suggest that the next frontier in machine reasoning lies in the strategic integration of affective regulation to guide logical synthesis.","authors":["Gabriela Pinto","Palash Goyal","Mihir Parmar","Yiwen Song","Souradip Chakraborty","Zifeng Wang","Jinsung Yoon","Tomas Pfister","Hamid Palangi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.27623v3","updated":"2026-02-22T21:08:08Z","published":"2025-10-31T16:50:49Z","title":"BEAT: Visual Backdoor Attacks on VLM-based Embodied Agents via Contrastive Trigger Learning","summary":"Recent advances in Vision-Language Models (VLMs) have propelled embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision-driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into VLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and VLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in VLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.","authors":["Qiusi Zhan","Hyeonjeong Ha","Rui Yang","Sirui Xu","Hanyang Chen","Liang-Yan Gui","Yu-Xiong Wang","Huan Zhang","Heng Ji","Daniel Kang"],"pdf_url":"","comment":"ICLR 2026. Project Page: https://zqs1943.github.io/BEAT/"},{"id":"http://arxiv.org/abs/2602.19333v1","updated":"2026-02-22T20:53:08Z","published":"2026-02-22T20:53:08Z","title":"PerSoMed: A Large-Scale Balanced Dataset for Persian Social Media Text Classification","summary":"This research introduces the first large-scale, well-balanced Persian social media text classification dataset, specifically designed to address the lack of comprehensive resources in this domain. The dataset comprises 36,000 posts across nine categories (Economic, Artistic, Sports, Political, Social, Health, Psychological, Historical, and Science & Technology), each containing 4,000 samples to ensure balanced class distribution. Data collection involved 60,000 raw posts from various Persian social media platforms, followed by rigorous preprocessing and hybrid annotation combining ChatGPT-based few-shot prompting with human verification. To mitigate class imbalance, we employed undersampling with semantic redundancy removal and advanced data augmentation strategies integrating lexical replacement and generative prompting. We benchmarked several models, including BiLSTM, XLM-RoBERTa (with LoRA and AdaLoRA adaptations), FaBERT, SBERT-based architectures, and the Persian-specific TookaBERT (Base and Large). Experimental results show that transformer-based models consistently outperform traditional neural networks, with TookaBERT-Large achieving the best performance (Precision: 0.9622, Recall: 0.9621, F1- score: 0.9621). Class-wise evaluation further confirms robust performance across all categories, though social and political texts exhibited slightly lower scores due to inherent ambiguity. This research presents a new high-quality dataset and provides comprehensive evaluations of cutting-edge models, establishing a solid foundation for further developments in Persian NLP, including trend analysis, social behavior modeling, and user classification. The dataset is publicly available to support future research endeavors.","authors":["Isun Chehreh","Ebrahim Ansari"],"pdf_url":"","comment":"10 pages, including 1 figure"},{"id":"http://arxiv.org/abs/2512.09730v2","updated":"2026-02-22T20:33:54Z","published":"2025-12-10T15:12:09Z","title":"Interpreto: An Explainability Library for Transformers","summary":"Interpreto is an open-source Python library for interpreting HuggingFace language models, from early BERT variants to LLMs. It provides two complementary families of methods: attribution methods and concept-based explanations. The library bridges recent research and practical tooling by exposing explanation workflows through a unified API for both classification and text generation. A key differentiator is its end-to-end concept-based pipeline (from activation extraction to concept learning, interpretation, and scoring), which goes beyond feature-level attributions and is uncommon in existing libraries.","authors":["Antonin Poché","Thomas Mullor","Gabriele Sarti","Frédéric Boisnard","Corentin Friedrich","Charlotte Claye","François Hoofd","Raphael Bernas","Céline Hudelot","Fanny Jourdan"],"pdf_url":"","comment":"Equal contribution: Poché and Jourdan"},{"id":"http://arxiv.org/abs/2602.19320v1","updated":"2026-02-22T19:50:01Z","published":"2026-02-22T19:50:01Z","title":"Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations","summary":"Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.","authors":["Dongming Jiang","Yi Li","Songtao Wei","Jinxin Yang","Ayushi Kishore","Alysa Zhao","Dingyi Kang","Xu Hu","Feng Chen","Qiannan Li","Bingzhe Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19317v1","updated":"2026-02-22T19:43:43Z","published":"2026-02-22T19:43:43Z","title":"Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering","summary":"Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.","authors":["Maryam Amirizaniani","Alireza Salemi","Hamed Zamani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.04940v2","updated":"2026-02-22T19:34:09Z","published":"2025-03-06T20:15:51Z","title":"VQEL: Enabling Self-Play in Emergent Language Games via Agent-Internal Vector Quantization","summary":"Emergent Language (EL) focuses on the emergence of communication among artificial agents. Although symbolic communication channels more closely mirror the discrete nature of human language, learning such protocols remains fundamentally difficult due to the non-differentiability of symbol sampling. Existing approaches typically rely on high-variance gradient estimators such as REINFORCE or on continuous relaxations such as Gumbel-Softmax, both of which suffer from limitations in training stability and scalability. Motivated by cognitive theories that emphasize intrapersonal processes preceding communication, we explore self-play as a substrate for language emergence prior to mutual interaction. We introduce Vector Quantized Emergent Language (VQEL), a novel architecture that incorporates vector quantization into the message generation process. VQEL enables agents to perform self-play using discrete internal representations derived from a learned codebook while preserving end-to-end differentiability. Moreover, the resulting vector-quantized codebook naturally induces a symbolic vocabulary that can be directly transferred and aligned during subsequent mutual play with other agents. Empirical results show that agents pretrained via VQEL self-play achieve more consistent symbol alignment and higher task success when later engaged in mutual interaction. These findings position self-play as a principled and effective mechanism for learning discrete communication protocols, addressing key optimization and representational challenges in emergent language systems.","authors":["Mohammad Mahdi Samiei Paqaleh","Mehdi Jamalkhah","Mahdieh Soleymani Baghshah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04891v2","updated":"2026-02-22T19:11:54Z","published":"2025-10-06T15:11:46Z","title":"SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests","summary":"Large language models (LLMs) are increasingly deployed in contexts where their failures can have direct sociopolitical consequences. Yet, existing safety benchmarks rarely test vulnerabilities in domains such as political manipulation, propaganda and disinformation generation, or surveillance and information control. We introduce SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries, designed to surface where LLMs most acutely fail in politically charged contexts. Our evaluations reveal several shortcomings: open-weight models exhibit high vulnerability to harmful compliance, with Mistral-7B reaching attack success rates as high as 97% to 98% in domains such as historical revisionism, propaganda, and political manipulation. Moreover, temporal and geographic analyses show that LLMs are most fragile when confronted with 21st-century or pre-20th-century contexts, and when responding to prompts tied to regions such as Latin America, the USA, and the UK. These findings demonstrate that current safeguards fail to generalize to high-stakes sociopolitical settings, exposing systematic biases and raising concerns about the reliability of LLMs in preserving human rights and democratic values. We share the SocialHarmBench benchmark at https://huggingface.co/datasets/psyonp/SocialHarmBench.","authors":["Punya Syon Pandey","Hai Son Le","Devansh Bhardwaj","Rada Mihalcea","Zhijing Jin"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2505.08783v2","updated":"2026-02-22T19:11:40Z","published":"2025-05-13T17:58:08Z","title":"CodePDE: An Inference Framework for LLM-driven PDE Solver Generation","summary":"Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). With CodePDE, we present a thorough evaluation on critical capacities of LLM for PDE solving: reasoning, debugging, self-refinement, and test-time scaling. CodePDE shows that, with advanced inference-time algorithms and scaling strategies, LLMs can achieve strong performance across a range of representative PDE problems. We also identify novel insights into LLM-driven solver generation, such as trade-offs between solver reliability and sophistication, design principles for LLM-powered PDE solving agents, and failure modes for LLM on hard tasks. These insights offer guidance for building more capable and reliable LLM-based scientific engines.","authors":["Shanda Li","Tanya Marwah","Junhong Shen","Weiwei Sun","Andrej Risteski","Yiming Yang","Ameet Talwalkar"],"pdf_url":"","comment":"TMLR. Code available at https://github.com/LithiumDA/CodePDE"},{"id":"http://arxiv.org/abs/2508.11915v2","updated":"2026-02-22T19:00:41Z","published":"2025-08-16T05:26:36Z","title":"CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures","summary":"Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.","authors":["Punya Syon Pandey","Yongjin Yang","Jiarui Liu","Zhijing Jin"],"pdf_url":"","comment":"EACL 2026 (Main)"},{"id":"http://arxiv.org/abs/2505.16789v3","updated":"2026-02-22T18:44:26Z","published":"2025-05-22T15:30:00Z","title":"Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards","summary":"As large language models (LLMs) gain popularity, their vulnerability to adversarial attacks emerges as a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can inadvertently introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Vulnerability, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity across multiple experimental datasets. We then evaluate the adversarial robustness of these fine-tuned models, analyzing persona shifts and interpretability traits to understand how dataset factors contribute to attack success rates. Lastly, we explore causal relationships that offer new insights into adversarial defense strategies, highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_vulnerability.","authors":["Punya Syon Pandey","Samuel Simko","Kellin Pelrine","Zhijing Jin"],"pdf_url":"","comment":"Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI 2026)"},{"id":"http://arxiv.org/abs/2511.15690v2","updated":"2026-02-22T18:27:38Z","published":"2025-11-19T18:48:27Z","title":"MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping","summary":"Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$. Our code is available at https://github.com/ModelTC/MoDES.","authors":["Yushi Huang","Zining Wang","Zhihang Yuan","Yifu Ding","Ruihao Gong","Jinyang Guo","Xianglong Liu","Jun Zhang"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.14281v2","updated":"2026-02-22T18:19:38Z","published":"2026-02-15T19:10:00Z","title":"MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents","summary":"The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.","authors":["Zhenhong Zhou","Yuanhe Zhang","Hongwei Cai","Moayad Aloqaily","Ouns Bouachir","Linsey Pang","Prakhar Mehrotra","Kun Wang","Qingsong Wen"],"pdf_url":"","comment":"21 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2601.04568v2","updated":"2026-02-22T18:09:34Z","published":"2026-01-08T03:53:05Z","title":"Neurosymbolic Retrievers for Retrieval-augmented Generation","summary":"Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance","authors":["Yash Saxena","Manas Gaur"],"pdf_url":"","comment":"8 pages, 2 Figures, Published in IEEE Intelligent Systems"},{"id":"http://arxiv.org/abs/2602.14488v2","updated":"2026-02-22T17:20:42Z","published":"2026-02-16T06:04:04Z","title":"BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR","summary":"IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.","authors":["Md. Najib Hasan","Mst. Jannatun Ferdous Rain","Fyad Mohammed","Nazmul Siddique"],"pdf_url":"","comment":"This work was submitted without the consent of my current adviser. Additionally, it overlaps with my unpublished research work. In order to avoid potential academic and authorship conflicts, I am requesting withdrawal of the paper"},{"id":"http://arxiv.org/abs/2510.23828v2","updated":"2026-02-22T16:43:39Z","published":"2025-10-27T20:13:32Z","title":"Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language","summary":"We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.","authors":["Mena Attia","Aashiq Muhamed","Mai Alkhamissi","Thamar Solorio","Mona Diab"],"pdf_url":"","comment":"EACL 2026 Main Conference"},{"id":"http://arxiv.org/abs/2512.02764v2","updated":"2026-02-22T16:16:16Z","published":"2025-12-02T13:44:41Z","title":"PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models","summary":"Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory.","authors":["Robert Belanec","Ivan Srba","Maria Bielikova"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.21285v2","updated":"2026-02-22T16:12:41Z","published":"2025-11-26T11:18:06Z","title":"PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark","summary":"Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the advances in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 7 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Cost Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.","authors":["Robert Belanec","Branislav Pecher","Ivan Srba","Maria Bielikova"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.03707v3","updated":"2026-02-22T15:44:32Z","published":"2026-02-03T16:28:24Z","title":"OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering","summary":"Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization. To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.","authors":["Yifan Zhu","Xinyu Mu","Tao Feng","Zhonghong Ou","Yuning Gong","Haoran Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02817v2","updated":"2026-02-22T15:32:00Z","published":"2025-12-02T14:27:26Z","title":"BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion","summary":"The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \\textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. The demo video and code can be found at https://ai4lt.github.io/boom/ \\footnote{All released code and models are licensed under the MIT License}.","authors":["Sai Koneru","Fabian Retkowski","Christian Huber","Lukas Hilgert","Seymanur Akti","Enes Yavuz Ugan","Alexander Waibel","Jan Niehues"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.19212v1","updated":"2026-02-22T14:48:25Z","published":"2026-02-22T14:48:25Z","title":"Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection","summary":"Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive code-mixing. To address these issues, we augment the Bengali Hateful Memes (BHM) dataset with semantically aligned samples from the Multimodal Aggression Dataset in Bengali (MIMOSA), improving both class balance and semantic diversity. We propose the Enhanced Dual Co-attention Framework (xDORA), integrating vision encoders (CLIP, DINOv2) and multilingual text encoders (XGLM, XLM-R) via weighted attention pooling to learn robust cross-modal representations. Building on these embeddings, we develop a FAISS-based k-nearest neighbor classifier for non-parametric inference and introduce RAG-Fused DORA, which incorporates retrieval-driven contextual reasoning. We further evaluate LLaVA under zero-shot, few-shot, and retrieval-augmented prompting settings. Experiments on the extended dataset show that xDORA (CLIP + XLM-R) achieves macro-average F1-scores of 0.78 for hateful meme identification and 0.71 for target entity detection, while RAG-Fused DORA improves performance to 0.79 and 0.74, yielding gains over the DORA baseline. The FAISS-based classifier performs competitively and demonstrates robustness for rare classes through semantic similarity modeling. In contrast, LLaVA exhibits limited effectiveness in few-shot settings, with only modest improvements under retrieval augmentation, highlighting constraints of pretrained vision-language models for code-mixed Bengali content without fine-tuning. These findings demonstrate the effectiveness of supervised, retrieval-augmented, and non-parametric multimodal frameworks for addressing linguistic and cultural complexities in low-resource hate speech detection.","authors":["Raihan Tanvir","Md. Golam Rabiul Alam"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19177v1","updated":"2026-02-22T13:14:27Z","published":"2026-02-22T13:14:27Z","title":"Next Reply Prediction X Dataset: Linguistic Discrepancies in Naively Generated Content","summary":"The increasing use of Large Language Models (LLMs) as proxies for human participants in social science research presents a promising, yet methodologically risky, paradigm shift. While LLMs offer scalability and cost-efficiency, their \"naive\" application, where they are prompted to generate content without explicit behavioral constraints, introduces significant linguistic discrepancies that challenge the validity of research findings. This paper addresses these limitations by introducing a novel, history-conditioned reply prediction task on authentic X (formerly Twitter) data, to create a dataset designed to evaluate the linguistic output of LLMs against human-generated content. We analyze these discrepancies using stylistic and content-based metrics, providing a quantitative framework for researchers to assess the quality and authenticity of synthetic data. Our findings highlight the need for more sophisticated prompting techniques and specialized datasets to ensure that LLM-generated content accurately reflects the complex linguistic patterns of human communication, thereby improving the validity of computational social science studies.","authors":["Simon Münker","Nils Schwager","Kai Kugler","Michael Heseltine","Achim Rettinger"],"pdf_url":"","comment":"8 pages (12 including references), 2 figures and 2 tables"},{"id":"http://arxiv.org/abs/2602.19174v1","updated":"2026-02-22T13:08:21Z","published":"2026-02-22T13:08:21Z","title":"TurkicNLP: An NLP Toolkit for Turkic Languages","summary":"Natural language processing for the Turkic language family, spoken by over 200 million people across Eurasia, remains fragmented, with most languages lacking unified tooling and resources. We present TurkicNLP, an open-source Python library providing a single, consistent NLP pipeline for Turkic languages across four script families: Latin, Cyrillic, Perso-Arabic, and Old Turkic Runic. The library covers tokenization, morphological analysis, part-of-speech tagging, dependency parsing, named entity recognition, bidirectional script transliteration, cross-lingual sentence embeddings, and machine translation through one language-agnostic API. A modular multi-backend architecture integrates rule-based finite-state transducers and neural models transparently, with automatic script detection and routing between script variants. Outputs follow the CoNLL-U standard for full interoperability and extension. Code and documentation are hosted at https://github.com/turkic-nlp/turkicnlp .","authors":["Sherzod Hakimov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19160v1","updated":"2026-02-22T12:43:00Z","published":"2026-02-22T12:43:00Z","title":"Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing","summary":"This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.","authors":["Maciej Świechowski","Adam Żychowski","Jacek Mańdziuk"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19159v1","updated":"2026-02-22T12:42:38Z","published":"2026-02-22T12:42:38Z","title":"Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM","summary":"Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.","authors":["Francesca Bianco","Derek Shiller"],"pdf_url":"","comment":"24 pages, 8+1 Tables"},{"id":"http://arxiv.org/abs/2602.19157v1","updated":"2026-02-22T12:39:02Z","published":"2026-02-22T12:39:02Z","title":"Facet-Level Persona Control by Trait-Activated Routing with Contrastive SAE for Role-Playing LLMs","summary":"Personality control in Role-Playing Agents (RPAs) is commonly achieved via training-free methods that inject persona descriptions and memory through prompts or retrieval-augmented generation, or via supervised fine-tuning (SFT) on persona-specific corpora. While SFT can be effective, it requires persona-labeled data and retraining for new roles, limiting flexibility. In contrast, prompt- and RAG-based signals are easy to apply but can be diluted in long dialogues, leading to drifting and sometimes inconsistent persona behavior. To address this, we propose a contrastive Sparse AutoEncoder (SAE) framework that learns facet-level personality control vectors aligned with the Big Five 30-facet model. A new 15,000-sample leakage-controlled corpus is constructed to provide balanced supervision for each facet. The learned vectors are integrated into the model's residual space and dynamically selected by a trait-activated routing module, enabling precise and interpretable personality steering. Experiments on Large Language Models (LLMs) show that the proposed method maintains stable character fidelity and output quality across contextualized settings, outperforming Contrastive Activation Addition (CAA) and prompt-only baselines. The combined SAE+Prompt configuration achieves the best overall performance, confirming that contrastively trained latent vectors can enhance persona control while preserving dialogue coherence.","authors":["Wenqiu Tang","Zhen Wan","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"","comment":"Accepted in PAKDD 2026 special session on Data Science :Foundation and Applications"},{"id":"http://arxiv.org/abs/2602.19146v1","updated":"2026-02-22T12:20:28Z","published":"2026-02-22T12:20:28Z","title":"VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval","summary":"We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90\\% accuracy on plan-aware VQA.","authors":["Diogo Glória-Silva","David Semedo","João Maglhães"],"pdf_url":"","comment":"Accepted at EACL 2026 Findings"},{"id":"http://arxiv.org/abs/2602.19133v1","updated":"2026-02-22T11:29:03Z","published":"2026-02-22T11:29:03Z","title":"A Dataset for Named Entity Recognition and Relation Extraction from Art-historical Image Descriptions","summary":"This paper introduces FRAME (Fine-grained Recognition of Art-historical Metadata and Entities), a manually annotated dataset of art-historical image descriptions for Named Entity Recognition (NER) and Relation Extraction (RE). Descriptions were collected from museum catalogs, auction listings, open-access platforms, and scholarly databases, then filtered to ensure that each text focuses on a single artwork and contains explicit statements about its material, composition, or iconography. FRAME provides stand-off annotations in three layers: a metadata layer for object-level properties, a content layer for depicted subjects and motifs, and a co-reference layer linking repeated mentions. Across layers, entity spans are labeled with 37 types and connected by typed RE links between mentions. Entity types are aligned with Wikidata to support Named Entity Linking (NEL) and downstream knowledge-graph construction. The dataset is released as UIMA XMI Common Analysis Structure (CAS) files with accompanying images and bibliographic metadata, and can be used to benchmark and fine-tune NER and RE systems, including zero- and few-shot setups with Large Language Models (LLMs).","authors":["Stefanie Schneider","Miriam Göldl","Julian Stalter","Ricarda Vollmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19127v1","updated":"2026-02-22T10:55:21Z","published":"2026-02-22T10:55:21Z","title":"AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG","summary":"With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.","authors":["Qijie You","Wenkai Yu","Wentao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.25232v2","updated":"2026-02-22T10:23:12Z","published":"2025-10-29T07:18:43Z","title":"From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity","summary":"Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.","authors":["Tianxi Wan","Jiaming Luo","Siyuan Chen","Kunyao Lan","Jianhua Chen","Haiyang Geng","Mengyue Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19115v1","updated":"2026-02-22T10:12:20Z","published":"2026-02-22T10:12:20Z","title":"How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders","summary":"In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.","authors":["Michael McCoubrey","Angelo Salatino","Francesco Osborne","Enrico Motta"],"pdf_url":"","comment":"Presented at SESAME 2025: Smarter Extraction of ScholArly MEtadata using Knowledge Graphs and Language Models, @ JCDL 2025"},{"id":"http://arxiv.org/abs/2602.19111v1","updated":"2026-02-22T09:54:40Z","published":"2026-02-22T09:54:40Z","title":"Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models","summary":"Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.","authors":["Kainan Liu","Yong Zhang","Ning Cheng","Yun Zhu","Yanmeng Wang","Shaojun Wang","Jing Xiao"],"pdf_url":"","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2602.13102v2","updated":"2026-02-22T09:15:38Z","published":"2026-02-13T17:06:17Z","title":"Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts","summary":"Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.","authors":["Kais Allkivi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.00339v4","updated":"2026-02-22T09:13:18Z","published":"2024-12-31T08:22:21Z","title":"GRASP: Replace Redundant Layers with Adaptive Singular Parameters for Efficient Model Compression","summary":"Recent studies have demonstrated that many layers are functionally redundant in large language models (LLMs), enabling model compression by removing these layers to reduce inference cost. While such approaches can improve efficiency, indiscriminate layer pruning often results in significant performance degradation. In this paper, we propose GRASP (Gradient-based Retention of Adaptive Singular Parameters), a novel compression framework that mitigates this issue by preserving sensitivity-aware singular values. Unlike direct layer pruning, GRASP leverages gradient-based attribution on a small calibration dataset to adaptively identify and retain critical singular components. By replacing redundant layers with only a minimal set of parameters, GRASP achieves efficient compression while maintaining strong performance with minimal overhead. Experiments across multiple LLMs show that GRASP consistently outperforms existing compression methods, achieving 90% of the original model's performance under 20% compression ratio.","authors":["Kainan Liu","Yong Zhang","Ning Cheng","Zhitao Li","Shaojun Wang","Jing Xiao"],"pdf_url":"","comment":"EMNLP 2025(Main)"},{"id":"http://arxiv.org/abs/2602.19101v1","updated":"2026-02-22T09:11:26Z","published":"2026-02-22T09:11:26Z","title":"Value Entanglement: Conflation Between Different Kinds of Good In (Some) Large Language Models","summary":"Value alignment of Large Language Models (LLMs) requires us to empirically measure these models' actual, acquired representation of value. Among the characteristics of value representation in humans is that they distinguish among value of different kinds. We investigate whether LLMs likewise distinguish three different kinds of good: moral, grammatical, and economic. By probing model behavior, embeddings, and residual stream activations, we report pervasive cases of value entanglement: a conflation between these distinct representations of value. Specifically, both grammatical and economic valuation was found to be overly influenced by moral value, relative to human norms. This conflation was repaired by selective ablation of the activation vectors associated with morality.","authors":["Seong Hah Cho","Junyi Li","Anna Leshinskaya"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2410.15173v3","updated":"2026-02-22T07:30:32Z","published":"2024-10-19T18:25:30Z","title":"Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation","summary":"The thematic fit estimation task measures semantic arguments' compatibility with a specific semantic role for a specific predicate. We investigate if LLMs have consistent, expressible knowledge of event arguments' thematic fit by experimenting with various prompt designs, manipulating input context, reasoning, and output forms. We set a new state-of-the-art on thematic fit benchmarks, but show that closed and open weight LLMs respond differently to our prompting strategies: Closed models achieve better scores overall and benefit from multi-step reasoning, but they perform worse at filtering out generated sentences incompatible with the specified predicate, role, and argument.","authors":["Safeyah Khaled Alshemali","Daniel Bauer","Yuval Marton"],"pdf_url":"","comment":"Significant update with massive changes: all experiments rerun with current LLMs; includes new probability estimate analysis and expanded results in Sections 4 and 5"},{"id":"http://arxiv.org/abs/2602.19079v1","updated":"2026-02-22T07:29:53Z","published":"2026-02-22T07:29:53Z","title":"TriTopic: Tri-Modal Graph-Based Topic Modeling with Iterative Refinement and Archetypes","summary":"Topic modeling extracts latent themes from large text collections, but leading approaches like BERTopic face critical limitations: stochastic instability, loss of lexical precision (\"Embedding Blur\"), and reliance on a single data perspective.\n  We present TriTopic, a framework that addresses these weaknesses through a tri-modal graph fusing semantic embeddings, TF-IDF, and metadata. Three core innovations drive its performance: hybrid graph construction via Mutual kNN and Shared Nearest Neighbors to eliminate noise and combat the curse of dimensionality; Consensus Leiden Clustering for reproducible, stable partitions; and Iterative Refinement that sharpens embeddings through dynamic centroid-pulling. TriTopic also replaces the \"average document\" concept with archetype-based topic representations defined by boundary cases rather than centers alone.\n  In benchmarks across 20 Newsgroups, BBC News, AG News, and Arxiv, TriTopic achieves the highest NMI on every dataset (mean NMI 0.575 vs. 0.513 for BERTopic, 0.416 for NMF, 0.299 for LDA), guarantees 100% corpus coverage with 0% outliers, and is available as an open-source PyPI library.","authors":["Roman Egger"],"pdf_url":"","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2601.00671v2","updated":"2026-02-22T06:35:55Z","published":"2026-01-02T12:37:53Z","title":"Fast-weight Product Key Memory","summary":"Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While softmax attention offers unbounded storage at prohibitive quadratic cost, linear variants are more efficient but suffer from limited, fixed-size storage. We introduce Fast-weight Product Key Memory (FwPKM), a sparse fast-weight memory layer that resolves this tension. FwPKM updates sparsely activated parameters at both training and inference time using chunk-level gradient descent on a local memory-rewrite objective. This performs Test-Time Training (TTT)-style gradient updates on activated slots in a sparse memory, enabling rapid memorization and retrieval of many new key-value associations while keeping per-token compute low and fixed. Experiments show that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle-in-a-Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.","authors":["Tianyu Zhao","Llion Jones"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19058v1","updated":"2026-02-22T06:04:05Z","published":"2026-02-22T06:04:05Z","title":"Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer","summary":"Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.\n  Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.\n  Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).","authors":["Chenhang Cui","An Zhang","Yuxin Chen","Gelei Deng","Jingnan Zheng","Zhenkai Liang","Xiang Wang","Tat-Seng Chua"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.09312v2","updated":"2026-02-22T05:57:02Z","published":"2025-10-10T12:06:04Z","title":"Verifying Chain-of-Thought Reasoning via Its Computational Graph","summary":"Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.","authors":["Zheng Zhao","Yeskendir Koishekenov","Xianjun Yang","Naila Murray","Nicola Cancedda"],"pdf_url":"","comment":"Accepted to ICLR 2026 (Oral)"},{"id":"http://arxiv.org/abs/2602.19049v1","updated":"2026-02-22T05:30:14Z","published":"2026-02-22T05:30:14Z","title":"IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning","summary":"Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.","authors":["Yinhan He","Yaochen Zhu","Mingjia Shi","Wendy Zheng","Lin Su","Xiaoqing Wang","Qi Guo","Jundong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10471v2","updated":"2026-02-22T05:13:04Z","published":"2026-02-11T03:22:51Z","title":"TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation","summary":"Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.","authors":["Steven Liu","Jane Luo","Xin Zhang","Aofan Liu","Hao Liu","Jie Wu","Ziyang Huang","Yangyu Huang","Yu Kang","Scarlett Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19043v1","updated":"2026-02-22T04:44:34Z","published":"2026-02-22T04:44:34Z","title":"Uncovering Context Reliance in Unstructured Knowledge Editing","summary":"Editing Large language models (LLMs) with real-world, unstructured knowledge is essential for correcting and updating their internal parametric knowledge. In this work, we revisit the fundamental next-token prediction (NTP) as a candidate paradigm for unstructured editing. We identify Context Reliance as a critical failure mode of NTP-based approaches, where knowledge acquired from edited text becomes highly dependent on its preceding context, leading to recall failures when that context is absent during inference. This hypothesis is supported by our empirical validation that prepending context during inference recovers knowledge recall. We further theoretically demonstrate that Context Reliance is an inherent consequence of gradient-based optimization, which tends to bind acquired knowledge to a specific aggregated contextual representation. To address this, we propose a simple yet effective COntext-INdependent editing framework (COIN), encouraging model to focus on knowledge within local scope rather than memorizing contextual patterns. Evaluations show that COIN reduces Context Reliance by 45.2% and outperforms strong baselines by 23.6% in editing success rate, highlighting the vital role of mitigating Context Reliance for robust editing.","authors":["Zisheng Zhou","Mengqi Zhang","Shiguang Wu","Xiaotian Ye","Chi Zhang","Zhumin Chen","Pengjie Ren"],"pdf_url":"","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2602.19020v1","updated":"2026-02-22T03:20:06Z","published":"2026-02-22T03:20:06Z","title":"Learning to Detect Language Model Training Data via Active Reconstruction","summary":"Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.","authors":["Junjie Oscar Yin","John X. Morris","Vitaly Shmatikov","Sewon Min","Hannaneh Hajishirzi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19008v1","updated":"2026-02-22T02:37:57Z","published":"2026-02-22T02:37:57Z","title":"Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks","summary":"Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\\hatβ=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.","authors":["Wilson Y. Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.05495v2","updated":"2026-02-22T02:18:44Z","published":"2026-02-05T09:57:57Z","title":"Transport and Merge: Cross-Architecture Merging for Large Language Models","summary":"Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.","authors":["Chenhang Cui","Binyun Yang","Fei Shen","Yuxin Chen","Jingnan Zheng","Xiang Wang","An Zhang","Tat-Seng Chua"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.05690v3","updated":"2026-02-22T02:01:06Z","published":"2025-06-06T02:37:47Z","title":"When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation","summary":"Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) with external knowledge. It leverages graphs to model the hierarchical structure between specific concepts, enabling more coherent and effective knowledge retrieval for accurate reasoning.Despite its conceptual promise, recent studies report that GraphRAG frequently underperforms vanilla RAG on many real-world tasks. This raises a critical question: Is GraphRAG really effective, and in which scenarios do graph structures provide measurable benefits for RAG systems? To address this, we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate GraphRAG models onboth hierarchical knowledge retrieval and deep contextual reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of increasing difficulty, coveringfact retrieval, complex reasoning, contextual summarization, and creative generation, and a systematic evaluation across the entire pipeline, from graph constructionand knowledge retrieval to final generation. Leveraging this novel benchmark, we systematically investigate the conditions when GraphRAG surpasses traditional RAG and the underlying reasons for its success, offering guidelines for its practical application. All related resources and analyses are collected for the community at https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.","authors":["Zhishang Xiang","Chuanjie Wu","Qinggang Zhang","Shengyuan Chen","Zijin Hong","Xiao Huang","Jinsong Su"],"pdf_url":"","comment":"All resources and analyses are collected at https://github.com/GraphRAG-Bench/GraphRAG-Benchmark"},{"id":"http://arxiv.org/abs/2508.07087v2","updated":"2026-02-22T01:09:33Z","published":"2025-08-09T19:55:54Z","title":"SQL-Exchange: Transforming SQL Queries Across Domains","summary":"We introduce SQL-Exchange, a framework for mapping SQL queries across different database schemas by preserving the source query structure while adapting domain-specific elements to align with the target schema. We investigate the conditions under which such mappings are feasible and beneficial, and examine their impact on enhancing the in-context learning performance of text-to-SQL systems as a downstream task. Our comprehensive evaluation across multiple model families and benchmark datasets -- assessing structural alignment with source queries, execution validity on target databases, and semantic correctness -- demonstrates that SQL-Exchange is effective across a wide range of schemas and query types. Our results further show that both in-context prompting with mapped queries and fine-tuning on mapped data consistently yield higher text-to-SQL performance than using examples drawn directly from the source schema.","authors":["Mohammadreza Daviran","Brian Lin","Davood Rafiei"],"pdf_url":"","comment":"Accepted to PVLDB 2026"},{"id":"http://arxiv.org/abs/2602.18998v1","updated":"2026-02-22T01:08:02Z","published":"2026-02-22T01:08:02Z","title":"Benchmark Test-Time Scaling of General LLM Agents","summary":"LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.","authors":["Xiaochuan Li","Ryan Ming","Pranav Setlur","Abhijay Paladugu","Andy Tang","Hao Kang","Shuai Shao","Rong Jin","Chenyan Xiong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.02066v2","updated":"2026-02-22T00:23:17Z","published":"2025-08-04T05:10:11Z","title":"MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs","summary":"Large Language Models (LLMs) have shown impressive performance across various domains, but their ability to perform molecular reasoning remains underexplored. Existing methods mostly rely on general-purpose prompting, which lacks domain-specific molecular semantics, or fine-tuning, which faces challenges in interpretability and reasoning depth, often leading to structural and textual hallucinations. To address these issues, we introduce MolReasoner, a two-stage framework that transitions LLMs from memorization to high-fidelity chemical reasoning. In the Mol-SFT stage, knowledge-enhanced Chain-of-Thought (CoT) data provides a strong foundation, while the Mol-RL stage refines reasoning using a novel, task-adaptive reward system to mitigate hallucinations. Extensive evaluations demonstrate that MolReasoner significantly outperforms a wide range of strong baselines in both molecule generation and captioning tasks. Further analyses highlight the framework's synergistic design and its ability to produce more interpretable outputs. Our work presents a principled and effective new approach for advancing high-fidelity molecular reasoning.","authors":["Guojiang Zhao","Zixiang Lu","Yutang Ge","Sihang Li","Zheng Cheng","Haitao Lin","Lirong Wu","Hanchen Xia","Hengxing Cai","Wentao Guo","Hongshuai Wang","Mingjun Xu","Siyu Zhu","Guolin Ke","Linfeng Zhang","Zhifeng Gao"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.15857v2","updated":"2026-02-22T22:16:36Z","published":"2024-09-24T08:29:10Z","title":"Large-scale Benchmarks for Multimodal Recommendation with Ducho","summary":"The common multimodal recommendation pipeline involves (i) extracting multimodal features, (ii) refining their high-level representations to suit the recommendation task, (iii) optionally fusing all multimodal features, and (iv) predicting the user-item score. Although great effort has been put into designing optimal solutions for (ii-iv), to the best of our knowledge, very little attention has been devoted to exploring procedures for (i) in a rigorous way. In this respect, the existing literature outlines the large availability of multimodal datasets and the ever-growing number of large models accounting for multimodal-aware tasks, but (at the same time) an unjustified adoption of limited standardized solutions. As very recent works from the literature have begun to conduct empirical studies to assess the contribution of multimodality in recommendation, we decide to follow and complement this same research direction. To this end, this paper settles as the first attempt to offer a large-scale benchmarking for multimodal recommender systems, with a specific focus on multimodal extractors. Specifically, we take advantage of three popular and recent frameworks for multimodal feature extraction and reproducibility in recommendation, Ducho, and MMRec/Elliot, respectively, to offer a unified and ready-to-use experimental environment able to run extensive benchmarking analyses leveraging novel multimodal feature extractors. Results, largely validated under different extractors, hyper-parameters of the extractors, domains, and modalities, provide important insights on how to train and tune the next generation of multimodal recommendation algorithms.","authors":["Matteo Attimonelli","Danilo Danese","Angela Di Fazio","Daniele Malitesta","Claudio Pomo","Tommaso Di Noia"],"pdf_url":"","comment":"Accepted in Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2602.19339v1","updated":"2026-02-22T21:02:32Z","published":"2026-02-22T21:02:32Z","title":"SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits","summary":"Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.\n  In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry.","authors":["Anna Volodkevich","Dmitry Anikin","Danil Gusak","Anton Klenitskiy","Evgeny Frolov","Alexey Vasilev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19333v1","updated":"2026-02-22T20:53:08Z","published":"2026-02-22T20:53:08Z","title":"PerSoMed: A Large-Scale Balanced Dataset for Persian Social Media Text Classification","summary":"This research introduces the first large-scale, well-balanced Persian social media text classification dataset, specifically designed to address the lack of comprehensive resources in this domain. The dataset comprises 36,000 posts across nine categories (Economic, Artistic, Sports, Political, Social, Health, Psychological, Historical, and Science & Technology), each containing 4,000 samples to ensure balanced class distribution. Data collection involved 60,000 raw posts from various Persian social media platforms, followed by rigorous preprocessing and hybrid annotation combining ChatGPT-based few-shot prompting with human verification. To mitigate class imbalance, we employed undersampling with semantic redundancy removal and advanced data augmentation strategies integrating lexical replacement and generative prompting. We benchmarked several models, including BiLSTM, XLM-RoBERTa (with LoRA and AdaLoRA adaptations), FaBERT, SBERT-based architectures, and the Persian-specific TookaBERT (Base and Large). Experimental results show that transformer-based models consistently outperform traditional neural networks, with TookaBERT-Large achieving the best performance (Precision: 0.9622, Recall: 0.9621, F1- score: 0.9621). Class-wise evaluation further confirms robust performance across all categories, though social and political texts exhibited slightly lower scores due to inherent ambiguity. This research presents a new high-quality dataset and provides comprehensive evaluations of cutting-edge models, establishing a solid foundation for further developments in Persian NLP, including trend analysis, social behavior modeling, and user classification. The dataset is publicly available to support future research endeavors.","authors":["Isun Chehreh","Ebrahim Ansari"],"pdf_url":"","comment":"10 pages, including 1 figure"},{"id":"http://arxiv.org/abs/2602.19317v1","updated":"2026-02-22T19:43:43Z","published":"2026-02-22T19:43:43Z","title":"Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering","summary":"Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.","authors":["Maryam Amirizaniani","Alireza Salemi","Hamed Zamani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.04568v2","updated":"2026-02-22T18:09:34Z","published":"2026-01-08T03:53:05Z","title":"Neurosymbolic Retrievers for Retrieval-augmented Generation","summary":"Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance","authors":["Yash Saxena","Manas Gaur"],"pdf_url":"","comment":"8 pages, 2 Figures, Published in IEEE Intelligent Systems"},{"id":"http://arxiv.org/abs/2509.13648v2","updated":"2026-02-22T17:49:58Z","published":"2025-09-17T02:53:25Z","title":"Sequential Data Augmentation for Generative Recommendation","summary":"Generative recommendation plays a crucial role in personalized systems, predicting users' future interactions from their historical behavior sequences. A critical yet underexplored factor in training these models is data augmentation, the process of constructing training data from user interaction histories. By shaping the training distribution, data augmentation directly and often substantially affects model generalization and performance. Nevertheless, in much of the existing work, this process is simplified, applied inconsistently, or treated as a minor design choice, without a systematic and principled understanding of its effects.\n  Motivated by our empirical finding that different augmentation strategies can yield large performance disparities, we conduct an in-depth analysis of how they reshape training distributions and influence alignment with future targets and generalization to unseen inputs. To systematize this design space, we propose GenPAS, a generalized and principled framework that models augmentation as a stochastic sampling process over input-target pairs with three bias-controlled steps: sequence sampling, target sampling, and input sampling. This formulation unifies widely used strategies as special cases and enables flexible control of the resulting training distribution. Our extensive experiments on benchmark and industrial datasets demonstrate that GenPAS yields superior accuracy, data efficiency, and parameter efficiency compared to existing strategies, providing practical guidance for principled training data construction in generative recommendation. Our code is available at https://github.com/snap-research/GenPAS.","authors":["Geon Lee","Bhuvesh Kumar","Clark Mingxuan Ju","Tong Zhao","Kijung Shin","Neil Shah","Liam Collins"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18736v3","updated":"2026-02-22T15:48:36Z","published":"2025-09-23T07:29:52Z","title":"Denoising Neural Reranker for Recommender Systems","summary":"For multi-stage recommenders in industry, a user request would first trigger a simple and efficient retriever module that selects and ranks a list of relevant items, then the recommender calls a slower but more sophisticated reranking model that refines the item list exposure to the user. To consistently optimize the two-stage retrieval reranking framework, most efforts have focused on learning reranker-aware retrievers. In contrast, there has been limited work on how to achieve a retriever-aware reranker. In this work, we provide evidence that the retriever scores from the previous stage are informative signals that have been underexplored. Specifically, we first empirically show that the reranking task under the two-stage framework is naturally a noise reduction problem on the retriever scores, and theoretically show the limitations of naive utilization techniques of the retriever scores. Following this notion, we derive an adversarial framework DNR that associates the denoising reranker with a carefully designed noise generation module. The resulting DNR solution extends the conventional score error minimization loss with three augmented objectives, including: 1) a denoising objective that aims to denoise the noisy retriever scores to align with the user feedback; 2) an adversarial retriever score generation objective that improves the exploration in the retriever score space; and 3) a distribution regularization term that aims to align the distribution of generated noisy retriever scores with the real ones. We conduct extensive experiments on three public datasets and an industrial recommender system, together with analytical support, to validate the effectiveness of the proposed DNR.","authors":["Wenyu Mao","Shuchang Liu","Hailan Yang","Xiaobei Wang","Xiaoyu Yang","Xu Gao","Xiang Li","Lantao Hu","Han Li","Kun Gai","An Zhang","Xiang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.05598v2","updated":"2026-02-22T15:17:40Z","published":"2025-10-07T05:48:05Z","title":"AgentDR: Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents","summary":"Recent agent-based recommendation frameworks aim to simulate user behaviors by incorporating memory mechanisms and prompting strategies, but they struggle with hallucinating non-existent items and full-catalog ranking. Besides, a largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning to capture user intent through substitute and complement relationships between items, which are usually implicit in datasets and difficult for traditional ID-based recommenders to capture. In this work, we propose a novel LLM-agent framework, AgenDR, which bridges LLM reasoning with scalable recommendation tools. Our approach delegates full-ranking tasks to traditional models while utilizing LLMs to (i) integrate multiple recommendation outputs based on personalized tool suitability and (ii) reason over substitute and complement relationships grounded in user history. This design mitigates hallucination, scales to large catalogs, and enhances recommendation relevance through relational reasoning. Through extensive experiments on three public grocery datasets, we show that our framework achieves superior full-ranking performance, yielding on average a twofold improvement over its underlying tools. We also introduce a new LLM-based evaluation metric that jointly measures semantic alignment and ranking correctness.","authors":["Mingdai Yang","Nurendra Choudhary","Jiangshu Du","Edward W. Huang","Philip S. Yu","Karthik Subbian","Danai Kourta"],"pdf_url":"","comment":"12 pages, accepted by WWW'26"},{"id":"http://arxiv.org/abs/2602.07774v4","updated":"2026-02-22T07:29:53Z","published":"2026-02-08T02:12:24Z","title":"Generative Reasoning Re-ranker","summary":"Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.","authors":["Mingfu Liang","Yufei Li","Jay Xu","Kavosh Asadi","Xi Liu","Shuo Gu","Kaushik Rangadurai","Frank Shyu","Shuaiwen Wang","Song Yang","Zhijing Li","Jiang Liu","Mengying Sun","Fei Tian","Xiaohan Wei","Chonglin Sun","Jacob Tao","Shike Mei","Wenlin Chen","Santanu Kolay","Sandeep Pandey","Hamed Firooz","Luke Simon"],"pdf_url":"","comment":"31 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2602.19319v1","updated":"2026-02-22T19:48:57Z","published":"2026-02-22T19:48:57Z","title":"Health+: Empowering Individuals via Unifying Health Data","summary":"Managing personal health data is a challenge in today's fragmented and institution-centric healthcare ecosystem. Individuals often lack meaningful control over their medical records, which are scattered across incompatible systems and formats. This vision paper presents Health+, a user-centric, multimodal health data management system that empowers individuals (including those with limited technical expertise) to upload, query, and share their data across modalities (e.g., text, images, reports). Rather than aiming for institutional overhaul, Health+ emphasizes individual agency by providing intuitive interfaces and intelligent recommendations for data access and sharing. At the system level, it tackles the complexity of storing, integrating, and securing heterogeneous health records, ensuring both efficiency and privacy. By unifying multimodal data and prioritizing patients, Health+ lays the foundation for a more connected, interpretable, and user-controlled health information ecosystem.","authors":["Sujaya Maiyya","Shantanu Sharma","Avinash Kumar"],"pdf_url":"","comment":"This paper has been accepted in ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2510.19166v2","updated":"2026-02-22T13:56:52Z","published":"2025-10-22T01:55:26Z","title":"Step-Aware Residual-Guided Diffusion for EEG Spatial Super-Resolution","summary":"For real-world BCI applications, lightweight Electroencephalography (EEG) systems offer the best cost-deployment balance. However, such spatial sparsity of EEG limits spatial fidelity, hurting learning and introducing bias. EEG spatial super-resolution methods aim to recover high-density EEG signals from sparse measurements, yet is often hindered by distribution shift and signal distortion and thus reducing fidelity and usability for EEG analysis and visualization. To overcome these challenges, we introduce SRGDiff, a step-aware residual-guided diffusion model that formulates EEG spatial super-resolution as dynamic conditional generation. Our key idea is to learn a dynamic residual condition from the low-density input that predicts the step-wise temporal and spatial details to add and uses the evolving cue to steer the denoising process toward high density reconstructions. At each denoising step, the proposed residual condition is additively fused with the previous denoiser feature maps, then a step-dependent affine modulation scales and shifts the activation to produce the current features. This iterative procedure dynamically extracts step-wise temporal rhythms and spatial-topographic cues to steer high-density recovery and maintain a fidelity-consistency balance. We adopt a comprehensive evaluation protocol spanning signal-, feature-, and downstream-level metrics across SEED, SEED-IV, and Localize-MI and multiple upsampling scales. SRGDiff achieves consistent gains of up to 40% over strong baselines, proving its superiority in the task of EEG spatial super-resolution. Moreover, topographic visualizations comparison and substantial EEG-FID gains jointly indicate that our SR EEG mitigates the spatial-spectral shift between low- and high-density recordings. Our code is available at https://github.com/DhrLhj/ICLR2026SRGDiff.","authors":["Hongjun Liu","Leyu Zhou","Zijianghao Yang","Chao Yao"],"pdf_url":"","comment":"ICLR 2026 Conference Paper"},{"id":"http://arxiv.org/abs/2602.19163v1","updated":"2026-02-22T12:44:28Z","published":"2026-02-22T12:44:28Z","title":"JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation","summary":"AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.","authors":["Kai Liu","Yanhao Zheng","Kai Wang","Shengqiong Wu","Rongjunchen Zhang","Jiebo Luo","Dimitrios Hatzinakos","Ziwei Liu","Hao Fei","Tat-Seng Chua"],"pdf_url":"","comment":"Accepted by ICLR 2026. Homepage: https://JavisVerse.github.io/JavisDiT2-page"}]},"2026-02-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2602.18966v1","updated":"2026-02-21T22:15:59Z","published":"2026-02-21T22:15:59Z","title":"Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation","summary":"Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcriptions without retraining. The pipeline intercepts Whisper's initial transcript, applies specialized LLM agents for domain context identification, named entity recognition, and jargon detection, and generates compact prompts that guide Whisper's decoder. Evaluated on 421 NBA basketball commentary segments (a domain characterized by dense proper nouns and technical terminology) our best pipeline achieves a statistically significant 17.0% relative reduction in word error rate (WER; from 0.217 to 0.180, p<0.001). Improvements are observed in 40.1% of segments with degradation in only 7.1%, substantially outperforming direct transcript post-editing. These results demonstrate that prompt-based augmentation can deliver scalable domain adaptation for ASR, offering a practical alternative to costly model fine-tuning.","authors":["Yonathan Ron","Shiri Gilboa","Tammuz Dubnov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18964v1","updated":"2026-02-21T22:10:18Z","published":"2026-02-21T22:10:18Z","title":"Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language","summary":"Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present \\textbf{Yor-Sarc}, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over $50$ million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' $κ= 0.7660$; pairwise Cohen's $κ= 0.6732$--$0.8743$), with $83.3\\%$ unanimous consensus. One annotator pair achieved almost perfect agreement ($κ= 0.8743$; $93.8\\%$ raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining $16.7\\%$ majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc\\footnote{https://github.com/toheebadura/yor-sarc} is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.","authors":["Toheeb Aduramomi Jimoh","Tabea De Wille","Nikola S. Nikolov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.15483v3","updated":"2026-02-21T18:48:44Z","published":"2025-08-21T12:01:56Z","title":"HebID: Detecting Social Identities in Hebrew-language Political Text","summary":"Political language is deeply intertwined with social identities. While social identities are often shaped by specific cultural contexts and expressed through particular uses of language, existing datasets for group and identity detection are predominantly English-centric, single-label and focus on coarse identity categories. We introduce HebID, the first multilabel Hebrew corpus for social identity detection: 5,536 sentences from Israeli politicians' Facebook posts (Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities (e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We benchmark multilabel and single-label encoders alongside 2B-9B-parameter generative LLMs, finding that Hebrew-tuned LLMs provide the best results (macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts and parliamentary speeches, evaluating differences in popularity, temporal trends, clustering patterns, and gender-related variations in identity expression. We utilize identity choices from a national public survey, enabling a comparison between identities portrayed in elite discourse and the public's identity priorities. HebID provides a comprehensive foundation for studying social identities in Hebrew and can serve as a model for similar research in other non-English political contexts.","authors":["Guy Mor-Lan","Naama Rivlin-Angert","Yael R. Kaplan","Tamir Sheafer","Shaul R. Shenhav"],"pdf_url":"","comment":"EMNLP 2025 (Findings)"},{"id":"http://arxiv.org/abs/2602.11767v2","updated":"2026-02-21T18:45:26Z","published":"2026-02-12T09:49:24Z","title":"TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents","summary":"Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.","authors":["Aladin Djuhera","Swanand Ravindra Kadhe","Farhan Ahmed","Heiko Ludwig","Holger Boche"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18922v1","updated":"2026-02-21T18:25:18Z","published":"2026-02-21T18:25:18Z","title":"Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning","summary":"Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency and precision,\n  not classification accuracy. We observe cache-key evaluation reduces to clustering evaluation and apply V-measure decomposition to separate these on n=8,682 points across MASSIVE, BANKING77, CLINC150, and NyayaBench v2, our new 8,514-entry multilingual agentic dataset (528 intents, 20 W5H2 classes, 63 languages). We introduce W5H2, a structured intent decomposition framework. Using SetFit with 8 examples per class, W5H2 achieves 91.1%+/-1.7% on MASSIVE in ~2ms -- vs 37.9% for\n  GPTCache and 68.8% for a 20B-parameter LLM at 3,447ms. On NyayaBench v2 (20 classes), SetFit achieves 55.3%, with cross-lingual transfer across 30 languages. Our five-tier cascade handles 85% of interactions locally, projecting 97.5% cost reduction. We provide risk-controlled selective prediction guarantees via RCPS with nine bound families.","authors":["Abhinaba Basu"],"pdf_url":"","comment":"28 pages, 15 figures, 8 tables, 5 appendices"},{"id":"http://arxiv.org/abs/2602.18920v1","updated":"2026-02-21T18:07:18Z","published":"2026-02-21T18:07:18Z","title":"DeepInnovator: Triggering the Innovative Capabilities of LLMs","summary":"The application of Large Language Models (LLMs) in accelerating scientific discovery has garnered increasing attention, with a key focus on constructing research agents endowed with innovative capability, i.e., the ability to autonomously generate novel and significant research ideas. Existing approaches predominantly rely on sophisticated prompt engineering and lack a systematic training paradigm. To address this, we propose DeepInnovator, a training framework designed to trigger the innovative capability of LLMs. Our approach comprises two core components. (1) ``Standing on the shoulders of giants''. We construct an automated data extraction pipeline to extract and organize structured research knowledge from a vast corpus of unlabeled scientific literature. (2) ``Conjectures and refutations''. We introduce a ``Next Idea Prediction'' training paradigm, which models the generation of research ideas as an iterative process of continuously predicting, evaluating, and refining plausible and novel next idea. Both automatic and expert evaluations demonstrate that our DeepInnovator-14B significantly outperforms untrained baselines, achieving win rates of 80.53\\%-93.81\\%, and attains performance comparable to that of current leading LLMs. This work provides a scalable training pathway toward building research agents with genuine, originative innovative capability, and will open-source the dataset to foster community advancement. Source code and data are available at: https://github.com/HKUDS/DeepInnovator.","authors":["Tianyu Fan","Fengji Zhang","Yuxiang Zheng","Bei Chen","Xinyao Niu","Chengen Huang","Junyang Lin","Chao Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.01928v3","updated":"2026-02-21T18:05:37Z","published":"2025-06-02T17:47:27Z","title":"Esoteric Language Models: Bridging Autoregressive and Masked Diffusion LLMs","summary":"Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Within this family, Masked Diffusion Models (MDMs) currently perform best but still underperform AR models in perplexity and lack key inference-time efficiency features, most notably KV caching. We introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, smoothly interpolating between their perplexities while overcoming their respective limitations. Unlike prior work, which uses transformers with bidirectional attention as MDM denoisers, we exploit the connection between MDMs and Any-Order autoregressive models and adopt causal attention. This design lets us compute the exact likelihood of MDMs for the first time and, crucially, enables us \\to introduce KV caching for MDMs while preserving parallel generation for the first time, significantly improving inference efficiency. Combined with an optimized sampling schedule, Eso-LMs achieves a new state of the art on the speed-quality Pareto frontier for unconditional generation. On long contexts, it yields $\\mathbf{14 - 65{}\\times}$ faster inference than standard MDMs and $\\mathbf{3 - 4{}\\times}$ faster inference than prior semi-autoregressive approaches. We provide code, model checkpoints, and a video tutorial on the project page: https://s-sahoo.com/Eso-LMs.","authors":["Subham Sekhar Sahoo","Zhihan Yang","Yash Akhauri","Johnna Liu","Deepansha Singh","Zhoujun Cheng","Zhengzhong Liu","Eric Xing","John Thickstun","Arash Vahdat"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18915v1","updated":"2026-02-21T17:46:34Z","published":"2026-02-21T17:46:34Z","title":"AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting","summary":"Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.","authors":["Mohammadreza Ghaffarzadeh-Esfahani","Yousof Gheisari"],"pdf_url":"","comment":"22 pages, 6 figures, and 5 supplementary files. Corresponding author: ygheisari@med.mui.ac.ir, Kaggle notebook is available at https://www.kaggle.com/code/mohammadgh009/aavgen"},{"id":"http://arxiv.org/abs/2506.23979v4","updated":"2026-02-21T17:30:52Z","published":"2025-06-30T15:45:28Z","title":"TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation","summary":"Conducting supervised and preference fine-tuning of large language models (LLMs) requires high-quality datasets to improve their ability to follow instructions and align with human preferences and values. However, constructing such datasets is resource-intensive, and most publicly available datasets are in English. To address these challenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided \\underline{\\textbf{P}}reference Data Generation (TaP) framework for automated, scalable preference dataset construction across languages. TaP uses a structured taxonomy to provide fine-grained control over dataset composition, ensuring diversity and broad coverage. We use TaP-generated datasets to perform supervised and preference fine-tuning on multiple LLMs. Experimental results demonstrate that LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets. Remarkably, LLMs trained on TaP-generated datasets outperform models trained on an open-source dataset that is 180$\\times$ larger.","authors":["Renren Jin","Tianhao Shen","Xinwei Wu","Dan Shi","Haoran Sun","Yuqi Ren","Wuwei Huang","Quandong Wang","Wei Liu","Jian Luan","Bin Wang","Deyi Xiong"],"pdf_url":"","comment":"33 pages, 16 tables, 10 figures"},{"id":"http://arxiv.org/abs/2602.18905v1","updated":"2026-02-21T17:00:54Z","published":"2026-02-21T17:00:54Z","title":"TRUE: A Trustworthy Unified Explanation Framework for Large Language Model Reasoning","summary":"Large language models (LLMs) have demonstrated strong capabilities in complex reasoning tasks, yet their decision-making processes remain difficult to interpret. Existing explanation methods often lack trustworthy structural insight and are limited to single-instance analysis, failing to reveal reasoning stability and systematic failure mechanisms. To address these limitations, we propose the Trustworthy Unified Explanation Framework (TRUE), which integrates executable reasoning verification, feasible-region directed acyclic graph (DAG) modeling, and causal failure mode analysis. At the instance level, we redefine reasoning traces as executable process specifications and introduce blind execution verification to assess operational validity. At the local structural level, we construct feasible-region DAGs via structure-consistent perturbations, enabling explicit characterization of reasoning stability and the executable region in the local input space. At the class level, we introduce a causal failure mode analysis method that identifies recurring structural failure patterns and quantifies their causal influence using Shapley values. Extensive experiments across multiple reasoning benchmarks demonstrate that the proposed framework provides multi-level, verifiable explanations, including executable reasoning structures for individual instances, feasible-region representations for neighboring inputs, and interpretable failure modes with quantified importance at the class level. These results establish a unified and principled paradigm for improving the interpretability and reliability of LLM reasoning systems.","authors":["Yujiao Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.13356v2","updated":"2026-02-21T16:44:55Z","published":"2025-09-14T18:19:55Z","title":"CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI","summary":"The challenge of aligning artificial intelligence (AI) with human values persists due to the abstract and often conflicting nature of moral principles and the opacity of existing approaches. This paper introduces CogniAlign, a multi-agent deliberation framework based on naturalistic moral realism, that grounds moral reasoning in survivability, defined across individual and collective dimensions, and operationalizes it through structured deliberations among discipline-specific scientist agents. Each agent, representing neuroscience, psychology, sociology, and evolutionary biology, provides arguments and rebuttals that are synthesized by an arbiter into transparent and empirically anchored judgments. As a proof-of-concept study, we evaluate CogniAlign on classic and novel moral questions and compare its outputs against GPT-4o using a five-part ethical audit framework with the help of three experts. Results show that CogniAlign consistently outperforms the baseline across more than sixty moral questions, with average performance gains of 12.2 points in analytic quality, 31.2 points in decisiveness, and 15 points in depth of explanation. In the Heinz dilemma, for example, CogniAlign achieved an overall score of 79 compared to GPT-4o's 65.8, demonstrating a decisive advantage in handling moral reasoning. Through transparent and structured reasoning, CogniAlign demonstrates the feasibility of an auditable approach to AI alignment, though certain challenges still remain.","authors":["Hasin Jawad Ali","Ilhamul Azam","Ajwad Abrar","Md. Kamrul Hasan","Hasan Mahmud"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.05761v2","updated":"2026-02-21T16:44:41Z","published":"2025-10-07T10:27:36Z","title":"Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis","summary":"Memes are a central part of online culture, yet their virality remains difficult to predict, especially in cross-lingual settings. We present a large-scale, time-series dataset of 46,578 Reddit memes collected from 25 meme-centric subreddits across eight language groups, with more than one million engagement tracking points. We propose a data-driven definition of virality based on a Hybrid Score that normalises engagement by community size and integrates dynamic features such as velocity and acceleration. This approach directly addresses the field's reliance on static, simple volume-based thresholds with arbitrary cut-offs. Building on this target, we construct a multimodal feature set that combines Visual, Textual, Contextual, Network, and Temporal signals, including structured annotations from a multimodal LLM to scale cross-lingual content labelling in a consistent way. We benchmark interpretable baselines (XGBoost, MLP) against end-to-end deep models (BERT, InceptionV3, CLIP) across early observation windows from 30 to 420 minutes. Our best model, a multimodal XGBoost classifier, achieves a PR AUC of 0.43 at 30 minutes and 0.80 at 420 minutes, indicating that early prediction of meme virality is feasible even under strong class imbalance. The results reveal a clear Content Ceiling, where content-only and deep multimodal baselines plateau at low PR AUC, while structural Network and Temporal features are necessary to surpass this limit. A SHAP-based temporal analysis further uncovers an evidentiary transition, where early predictions are dominated by network priors (author and community context), and later predictions increasingly rely on temporal dynamics (velocity, acceleration) as engagement accumulates. Overall, we reframe meme virality as a dynamic, path-dependent process governed by exposure and early interaction patterns rather than by intrinsic content alone.","authors":["Sedat Dogan","Nina Dethlefs","Debarati Chakraborty"],"pdf_url":"","comment":"Accepted to ACM WebSci 2026. 10 pages, 9 fiures and 8 tables"},{"id":"http://arxiv.org/abs/2602.18899v1","updated":"2026-02-21T16:43:13Z","published":"2026-02-21T16:43:13Z","title":"[b]=[d]-[t]+[p]: Self-supervised Speech Models Discover Phonological Vector Arithmetic","summary":"Self-supervised speech models (S3Ms) are known to encode rich phonetic information, yet how this information is structured remains underexplored. We conduct a comprehensive study across 96 languages to analyze the underlying structure of S3M representations, with particular attention to phonological vectors. We first show that there exist linear directions within the model's representation space that correspond to phonological features. We further demonstrate that the scale of these phonological vectors correlate to the degree of acoustic realization of their corresponding phonological features in a continuous manner. For example, the difference between [d] and [t] yields a voicing vector: adding this vector to [p] produces [b], while scaling it results in a continuum of voicing. Together, these findings indicate that S3Ms encode speech using phonologically interpretable and compositional vectors, demonstrating phonological vector arithmetic. All code and interactive demos are available at https://github.com/juice500ml/phonetic-arithmetic .","authors":["Kwanghee Choi","Eunjung Yeo","Cheol Jun Cho","David Harwath","David R. Mortensen"],"pdf_url":"","comment":"Submitted to ACL, code planned to release after acceptance"},{"id":"http://arxiv.org/abs/2504.04717v5","updated":"2026-02-21T14:30:27Z","published":"2025-04-07T04:00:08Z","title":"Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models","summary":"Recent advances in large language models (LLMs) have substantially improved single-turn task performance, yet real-world applications increasingly demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent progress in evaluating and enhancing multi-turn LLM interactions. Centered on a task-oriented taxonomy-spanning instruction following in domains such as mathematics and coding, and conversational engagement in role-playing, healthcare, education, and adversarial jailbreak settings-we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness across prolonged dialogues. We organize existing benchmarks and datasets into coherent categories reflecting the evolving landscape of multi-turn dialogue evaluation, and review a broad spectrum of enhancement methodologies, including model-centric strategies (in-context learning, supervised fine-tuning, reinforcement learning, and architectural innovations), external integration approaches (memory augmentation, retrieval-based methods, and knowledge graphs), and agent-based techniques for collaborative interaction. Finally, we identify open challenges and promising directions for future research to further improve the robustness and effectiveness of multi-turn LLM interactions.","authors":["Yubo Li","Xiaobin Shen","Xinyu Yao","Xueying Ding","Yidi Miao","Ramayya Krishnan","Rema Padman"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.23184v3","updated":"2026-02-21T13:59:06Z","published":"2025-09-27T08:38:08Z","title":"PonderLM-2: Pretraining LLM with Latent Thoughts in Continuous Space","summary":"The remarkable success of Chain-of-Thought (CoT), which enhances performance by scaling generation steps at test-time, inspires us to ask: can we leverage a similar scaling of computational steps during pretraining to improve the generation of each individual token? To address this, we propose a novel pre-training methodology: Pretraining Language Models with Latent Thoughts (PonderLM-2). Our approach pretrains a language model (LM) to first generate an intermediate latent thought-the last hidden state of the current position-which is then used as input to predict the actual subsequent token. This additional computational step enables the LM to refine its prediction within unconstrained continuous space. Our experiments demonstrate that, at an identical inference cost, a LM that generates one additional latent thought per token outperforms a standard model with double the parameters. For instance, our PonderLM-2-Pythia-1.4B, pretrained on 300B tokens from the Pile, significantly surpasses the vanilla Pythia-2.8B trained on the same data on both language modeling and a range of general downstream tasks. Furthermore, increasing the number of latent thoughts generated before each actual token-forming a chain analogous to CoT-consistently improves the model's performance. The code is available at https://github.com/LUMIA-Group/PonderLM-2.","authors":["Boyi Zeng","He Li","Shixiang Song","Yixuan Wang","Ziwei He","Xinbing Wang","Zhouhan Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18823v1","updated":"2026-02-21T12:50:43Z","published":"2026-02-21T12:50:43Z","title":"EvalSense: A Framework for Domain-Specific LLM (Meta-)Evaluation","summary":"Robust and comprehensive evaluation of large language models (LLMs) is essential for identifying effective LLM system configurations and mitigating risks associated with deploying LLMs in sensitive domains. However, traditional statistical metrics are poorly suited to open-ended generation tasks, leading to growing reliance on LLM-based evaluation methods. These methods, while often more flexible, introduce additional complexity: they depend on carefully chosen models, prompts, parameters, and evaluation strategies, making the evaluation process prone to misconfiguration and bias. In this work, we present EvalSense, a flexible, extensible framework for constructing domain-specific evaluation suites for LLMs. EvalSense provides out-of-the-box support for a broad range of model providers and evaluation strategies, and assists users in selecting and deploying suitable evaluation methods for their specific use-cases. This is achieved through two unique components: (1) an interactive guide aiding users in evaluation method selection and (2) automated meta-evaluation tools that assess the reliability of different evaluation approaches using perturbed data. We demonstrate the effectiveness of EvalSense in a case study involving the generation of clinical notes from unstructured doctor-patient dialogues, using a popular open dataset. All code, documentation, and assets associated with EvalSense are open-source and publicly available at https://github.com/nhsengland/evalsense.","authors":["Adam Dejl","Jonathan Pearson"],"pdf_url":"","comment":"Accepted to EACL 2026 System Demonstrations"},{"id":"http://arxiv.org/abs/2602.18806v1","updated":"2026-02-21T11:45:12Z","published":"2026-02-21T11:45:12Z","title":"Think$^{2}$: Grounded Metacognitive Reasoning in Large Language Models","summary":"Large Language Models (LLMs) demonstrate strong reasoning performance, yet their ability to reliably monitor, diagnose, and correct their own errors remains limited. We introduce a psychologically grounded metacognitive framework that operationalizes Ann Brown's regulatory cycle (Planning, Monitoring, and Evaluation) as a structured prompting architecture, and study its integration within a lightweight dual-process MetaController for adaptive effort allocation. Across diverse reasoning and diagnostic benchmarks (GSM8K, CRUXEval, MBPP, AIME, CorrectBench, and TruthfulQA) using Llama-3 and Qwen-3 (8B), explicit regulatory structuring substantially improves error diagnosis and yields a threefold increase in successful self-correction. Blinded human evaluations over 580 query pairs show an 84% aggregate preference for trustworthiness and metacognitive self-awareness over standard and Chain-of-Thought baselines. Grounding LLM reasoning in established cognitive theory offers a principled path toward more transparent and diagnostically robust AI systems.","authors":["Abraham Paul Elenjical","Vivek Hruday Kavuri","Vasudeva Varma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18788v1","updated":"2026-02-21T10:43:07Z","published":"2026-02-21T10:43:07Z","title":"BURMESE-SAN: Burmese NLP Benchmark for Evaluating Large Language Models","summary":"We introduce BURMESE-SAN, the first holistic benchmark that systematically evaluates large language models (LLMs) for Burmese across three core NLP competencies: understanding (NLU), reasoning (NLR), and generation (NLG). BURMESE-SAN consolidates seven subtasks spanning these competencies, including Question Answering, Sentiment Analysis, Toxicity Detection, Causal Reasoning, Natural Language Inference, Abstractive Summarization, and Machine Translation, several of which were previously unavailable for Burmese. The benchmark is constructed through a rigorous native-speaker-driven process to ensure linguistic naturalness, fluency, and cultural authenticity while minimizing translation-induced artifacts. We conduct a large-scale evaluation of both open-weight and commercial LLMs to examine challenges in Burmese modeling arising from limited pretraining coverage, rich morphology, and syntactic variation. Our results show that Burmese performance depends more on architectural design, language representation, and instruction tuning than on model scale alone. In particular, Southeast Asia regional fine-tuning and newer model generations yield substantial gains. Finally, we release BURMESE-SAN as a public leaderboard to support systematic evaluation and sustained progress in Burmese and other low-resource languages. https://leaderboard.sea-lion.ai/detailed/MY","authors":["Thura Aung","Jann Railey Montalan","Jian Gang Ngui","Peerat Limkonchotiwat"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2602.18962v1","updated":"2026-02-21T21:54:43Z","published":"2026-02-21T21:54:43Z","title":"NeuroWise: A Multi-Agent LLM \"Glass-Box\" System for Practicing Double-Empathy Communication with Autistic Partners","summary":"The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic \"deficits\" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual.","authors":["Albert Tang","Yifan Mo","Jie Li","Yue Su","Mengyuan Zhang","Sander L. Koole","Koen Hindriks","Jiahuan Pei"],"pdf_url":"","comment":"Accepted to ACM CHI 2026"},{"id":"http://arxiv.org/abs/2602.18929v1","updated":"2026-02-21T18:41:28Z","published":"2026-02-21T18:41:28Z","title":"Give Users the Wheel: Towards Promptable Recommendation Paradigm","summary":"Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user's immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios.","authors":["Fuyuan Lyu","Chenglin Luo","Qiyuan Zhang","Yupeng Hou","Haolun Wu","Xing Tang","Xue Liu","Jin L. C. Guo","Xiuqiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.11483v2","updated":"2026-02-21T14:45:56Z","published":"2025-10-13T14:55:28Z","title":"Uncertainty Quantification for Retrieval-Augmented Reasoning","summary":"Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.","authors":["Heydar Soudani","Hamed Zamani","Faegheh Hasibi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18786v1","updated":"2026-02-21T10:35:12Z","published":"2026-02-21T10:35:12Z","title":"CaliCausalRank: Calibrated Multi-Objective Ad Ranking with Robust Counterfactual Utility Optimization","summary":"Ad ranking systems must simultaneously optimize multiple objectives including click-through rate (CTR), conversion rate (CVR), revenue, and user experience metrics. However, production systems face critical challenges: score scale inconsistency across traffic segments undermines threshold transferability, and position bias in click logs causes offline-online metric discrepancies. We propose CaliCausalRank, a unified framework that integrates training-time scale calibration, constraint-based multi-objective optimization, and robust counterfactual utility estimation. Our approach treats score calibration as a first-class training objective rather than post-hoc processing, employs Lagrangian relaxation for constraint satisfaction, and utilizes variance-reduced counterfactual estimators for reliable offline evaluation. Experiments on the Criteo and Avazu datasets demonstrate that CaliCausalRank achieves 1.1% relative AUC improvement, 31.6% calibration error reduction, and 3.2% utility gain compared to the best baseline (PairRank) while maintaining consistent performance across different traffic segments.","authors":["Xikai Yang","Sebastian Sun","Yilin Li","Yue Xing","Ming Wang","Yang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18759v1","updated":"2026-02-21T08:53:10Z","published":"2026-02-21T08:53:10Z","title":"Towards Reliable Negative Sampling for Recommendation with Implicit Feedback via In-Community Popularity","summary":"Learning from implicit feedback is a fundamental problem in modern recommender systems, where only positive interactions are observed and explicit negative signals are unavailable. In such settings, negative sampling plays a critical role in model training by constructing negative items that enable effective preference learning and ranking optimization. However, designing reliable negative sampling strategies remains challenging, as they must simultaneously ensure realness, hardness, and interpretability. To this end, we propose \\textbf{ICPNS (In-Community Popularity Negative Sampling)}, a novel framework that leverages user community structure to identify reliable and informative negative samples. Our approach is grounded in the insight that item exposure is driven by latent user communities. By identifying these communities and utilizing in-community popularity, ICPNS effectively approximates the probability of item exposure. Consequently, items that are popular within a user's community but remain unclicked are identified as more reliable true negatives. Extensive experiments on four benchmark datasets demonstrate that ICPNS yields consistent improvements on graph-based recommenders and competitive performance on MF-based models, outperforming representative negative sampling strategies under a unified evaluation protocol.","authors":["Chen Chen","Haobo Lin","Yuanbo Xu"],"pdf_url":"","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.27246v2","updated":"2026-02-21T07:52:10Z","published":"2025-10-31T07:29:52Z","title":"Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs","summary":"Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.","authors":["Mohammad Tavakoli","Alireza Salemi","Carrie Ye","Mohamed Abdalla","Hamed Zamani","J Ross Mitchell"],"pdf_url":"","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2509.10544v2","updated":"2026-02-21T21:20:52Z","published":"2025-09-07T01:22:57Z","title":"ASL360: AI-Enabled Adaptive Streaming of Layered 360$^\\circ$ Video over UAV-assisted Wireless Networks","summary":"We propose ASL360, an adaptive deep reinforcement learning-based scheduler for on-demand 360$^\\circ$ video streaming to mobile VR users in next generation wireless networks. We aim to maximize the overall Quality of Experience (QoE) of the users served over a UAV-assisted 5G wireless network. Our system model comprises a macro base station (MBS) and a UAV-mounted base station which both deploy mm-Wave transmission to the users. The 360$^\\circ$ video is encoded into dependent layers and segmented tiles, allowing a user to schedule downloads of each layer's segments. Furthermore, each user utilizes multiple buffers to store the corresponding video layer's segments. We model the scheduling decision as a Constrained Markov Decision Process (CMDP), where the agent selects Base or Enhancement layers to maximize the QoE and use a policy gradient-based method (PPO) to find the optimal policy. Additionally, we implement a dynamic adjustment mechanism for cost components, allowing the system to adaptively balance and prioritize the video quality, buffer occupancy, and quality change based on real-time network and streaming session conditions. We demonstrate that ASL360 significantly improves the QoE, achieving approximately 2 dB higher average video quality, 80% lower average rebuffering time, and 57% lower video quality variation, relative to competitive baseline methods. Our results show the effectiveness of our layered and adaptive approach in enhancing the QoE in immersive videostreaming applications, particularly in dynamic and challenging network environments.","authors":["Alireza Mohammadhosseini","Jacob Chakareski","Nicholas Mastronarde"],"pdf_url":"","comment":"This paper has been accepted for presentation at the IEEE Global Communications Conference (GLOBECOM) 2025"},{"id":"http://arxiv.org/abs/2602.18863v1","updated":"2026-02-21T15:06:16Z","published":"2026-02-21T15:06:16Z","title":"TIACam: Text-Anchored Invariant Feature Learning with Auto-Augmentation for Camera-Robust Zero-Watermarking","summary":"Camera recapture introduces complex optical degradations, such as perspective warping, illumination shifts, and Moiré interference, that remain challenging for deep watermarking systems. We present TIACam, a text-anchored invariant feature learning framework with auto-augmentation for camera-robust zero-watermarking. The method integrates three key innovations: (1) a learnable auto-augmentor that discovers camera-like distortions through differentiable geometric, photometric, and Moiré operators; (2) a text-anchored invariant feature learner that enforces semantic consistency via cross-modal adversarial alignment between image and text; and (3) a zero-watermarking head that binds binary messages in the invariant feature space without modifying image pixels. This unified formulation jointly optimizes invariance, semantic alignment, and watermark recoverability. Extensive experiments on both synthetic and real-world camera captures demonstrate that TIACam achieves state-of-the-art feature stability and watermark extraction accuracy, establishing a principled bridge between multimodal invariance learning and physically robust zero-watermarking.","authors":["Abdullah All Tanvir","Agnibh Dasgupta","Xin Zhong"],"pdf_url":"","comment":"This paper is accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2504.12796v2","updated":"2026-02-21T13:13:58Z","published":"2025-04-17T09:58:38Z","title":"A Survey on Cross-Modal Interaction Between Music and Multimodal Data","summary":"Multimodal learning has driven innovation across various industries, particularly in the field of music. By enabling more intuitive interaction experiences and enhancing immersion, it not only lowers the entry barriers to the music but also increases its overall appeal. This survey aims to provide a comprehensive review of multimodal tasks related to music, outlining how music contributes to multimodal learning and offering insights for researchers seeking to expand the boundaries of computational music. Unlike text and images, which are often semantically or visually intuitive, music primarily interacts with humans through auditory perception, making its data representation inherently less intuitive. Therefore, this paper first introduces the representations of music and provides an overview of music datasets. Subsequently, we categorize cross-modal interactions between music and multimodal data into three types: music-driven cross-modal interactions, music-oriented cross-modal interactions, and bidirectional music cross-modal interactions. For each category, we systematically trace the development of relevant sub-tasks, analyze existing limitations, and discuss emerging trends. Furthermore, we provide a comprehensive summary of datasets and evaluation metrics used in multimodal tasks related to music, offering benchmark references for future research. Finally, we discuss the current challenges in cross-modal interactions involving music and propose potential directions for future research.","authors":["Sifei Li","Mining Tan","Feier Shen","Minyan Luo","Zijiao Yin","Fan Tang","Weiming Dong","Changsheng Xu"],"pdf_url":"","comment":"34 pages, 7 figures"}]},"2026-02-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2505.14685v3","updated":"2026-02-24T18:59:40Z","published":"2025-05-20T17:59:45Z","title":"Language Models use Lookbacks to Track Beliefs","summary":"How do language models (LMs) represent characters' beliefs, especially when those beliefs may differ from reality? This question lies at the heart of understanding the Theory of Mind (ToM) capabilities of LMs. We analyze LMs' ability to reason about characters' beliefs using causal mediation and abstraction. We construct a dataset, CausalToM, consisting of simple stories where two characters independently change the state of two objects, potentially unaware of each other's actions. Our investigation uncovers a pervasive algorithmic pattern that we call a lookback mechanism, which enables the LM to recall important information when it becomes necessary. The LM binds each character-object-state triple together by co-locating their reference information, represented as Ordering IDs (OIs), in low-rank subspaces of the state token's residual stream. When asked about a character's beliefs regarding the state of an object, the binding lookback retrieves the correct state OI and then the answer lookback retrieves the corresponding state token. When we introduce text specifying that one character is (not) visible to the other, we find that the LM first generates a visibility ID encoding the relation between the observing and the observed character OIs. In a visibility lookback, this ID is used to retrieve information about the observed character and update the observing character's beliefs. Our work provides insights into belief tracking mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.","authors":["Nikhil Prakash","Natalie Shapira","Arnab Sen Sharma","Christoph Riedl","Yonatan Belinkov","Tamar Rott Shaham","David Bau","Atticus Geiger"],"pdf_url":"","comment":"38 pages, 50 figures. Code and data at https://belief.baulab.info/"},{"id":"http://arxiv.org/abs/2602.21202v1","updated":"2026-02-24T18:57:33Z","published":"2026-02-24T18:57:33Z","title":"Multi-Vector Index Compression in Any Modality","summary":"We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.","authors":["Hanxiang Qin","Alexander Martin","Rohan Jha","Chunsheng Zuo","Reno Kriz","Benjamin Van Durme"],"pdf_url":"","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.21201v1","updated":"2026-02-24T18:56:10Z","published":"2026-02-24T18:56:10Z","title":"Aletheia tackles FirstProof autonomously","summary":"We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.","authors":["Tony Feng","Junehyuk Jung","Sang-hyun Kim","Carlo Pagano","Sergei Gukov","Chiang-Chiang Tsai","David Woodruff","Adel Javanmard","Aryan Mokhtari","Dawsen Hwang","Yuri Chervonyi","Jonathan N. Lee","Garrett Bingham","Trieu H. Trinh","Vahab Mirrokni","Quoc V. Le","Thang Luong"],"pdf_url":"","comment":"34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia"},{"id":"http://arxiv.org/abs/2602.21198v1","updated":"2026-02-24T18:55:18Z","published":"2026-02-24T18:55:18Z","title":"Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs","summary":"Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.","authors":["Yining Hong","Huang Huang","Manling Li","Li Fei-Fei","Jiajun Wu","Yejin Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21193v1","updated":"2026-02-24T18:51:04Z","published":"2026-02-24T18:51:04Z","title":"On Data Engineering for Scaling LLM Terminal Capabilities","summary":"Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.","authors":["Renjie Pi","Grace Lam","Mohammad Shoeybi","Pooya Jannaty","Bryan Catanzaro","Wei Ping"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.17905v2","updated":"2026-02-24T18:49:01Z","published":"2026-02-20T00:07:18Z","title":"Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning","summary":"Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.","authors":["Seyed Hossein Alavi","Zining Wang","Shruthi Chockkalingam","Raymond T. Ng","Vered Shwartz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.06873v2","updated":"2026-02-24T18:15:44Z","published":"2025-01-12T17:03:45Z","title":"Causal Claims in Economics","summary":"As economics scales, a key bottleneck is representing what papers claim in a comparable, aggregable form. We introduce evidence-annotated claim graphs that map each paper into a directed network of standardized economic concepts (nodes) and stated relationships (edges), with each edge labeled by evidentiary basis, including whether it is supported by causal inference designs or by non-causal evidence. Using a structured multi-stage AI workflow, we construct claim graphs for 44,852 economics papers from 1980-2023. The share of causal edges rises from 7.7% in 1990 to 31.7% in 2020. Measures of causal narrative structure and causal novelty are positively associated with top-five publication and long-run citations, whereas non-causal counterparts are weakly related or negative.","authors":["Prashant Garg","Thiemo Fetzer"],"pdf_url":"","comment":"Data, code, prompts, and workflow documentation are publicly available at our GitHub repository: https://github.com/prashgarg/CausalClaimsInEconomics"},{"id":"http://arxiv.org/abs/2602.21165v1","updated":"2026-02-24T18:10:00Z","published":"2026-02-24T18:10:00Z","title":"PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data","summary":"Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.","authors":["Samah Fodeh","Linhai Ma","Yan Wang","Srivani Talakokkul","Ganesh Puthiaraju","Afshan Khan","Ashley Hagaman","Sarah Lowe","Aimee Roundtree"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21158v1","updated":"2026-02-24T18:04:54Z","published":"2026-02-24T18:04:54Z","title":"SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards","summary":"Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.","authors":["Dengjia Zhang","Xiaoou Liu","Lu Cheng","Yaqing Wang","Kenton Murray","Hua Wei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20092v2","updated":"2026-02-24T17:51:23Z","published":"2026-02-23T18:02:23Z","title":"BabyLM Turns 4 and Goes Multilingual: Call for Papers for the 2026 BabyLM Workshop","summary":"The goal of the BabyLM is to stimulate new research connections between cognitive modeling and language model pretraining. We invite contributions in this vein to the BabyLM Workshop, which will also include the 4th iteration of the BabyLM Challenge. As in previous years, the challenge features two ``standard'' tracks (Strict and Strict-Small), in which participants must train language models on under 100M or 10M words of data, respectively. This year, we move beyond our previous English-only pretraining datasets with a new Multilingual track, focusing on English, Dutch, and Chinese. For the workshop, we call for papers related to the overall theme of BabyLM, which includes training efficiency, small-scale training datasets, cognitive modeling, model evaluation, and architecture innovation.","authors":["Leshem Choshen","Ryan Cotterell","Mustafa Omer Gul","Jaap Jumelet","Tal Linzen","Aaron Mueller","Suchir Salhan","Raj Sanjay Shah","Alex Warstadt","Ethan Gotlieb Wilcox"],"pdf_url":"","comment":"8 pages, 1 table. arXiv admin note: substantial text overlap with arXiv:2502.10645"},{"id":"http://arxiv.org/abs/2506.21220v4","updated":"2026-02-24T17:50:18Z","published":"2025-06-26T13:13:24Z","title":"Complexity-aware fine-tuning","summary":"General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across three small open models ($\\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.58$ vs $0.45$ average accuracy) and outperforms the distillation approach ($0.58$ vs $0.56$ average accuracy) while using $81\\%$ less data.","authors":["Andrey Goncharov","Daniil Vyazhev","Petr Sychev","Edvard Khalafyan","Alexey Zaytsev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21143v1","updated":"2026-02-24T17:43:32Z","published":"2026-02-24T17:43:32Z","title":"A Benchmark for Deep Information Synthesis","summary":"Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.","authors":["Debjit Paul","Daniel Murphy","Milan Gritta","Ronald Cardenas","Victor Prokhorov","Lena Sophia Bolliger","Aysim Toker","Roy Miles","Andreea-Maria Oncescu","Jasivan Alex Sivakumar","Philipp Borchert","Ismail Elezi","Meiru Zhang","Ka Yiu Lee","Guchun Zhang","Jun Wang","Gerasimos Lampouras"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.03411v2","updated":"2026-02-24T17:13:02Z","published":"2026-02-03T11:38:48Z","title":"SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training","summary":"In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.","authors":["Huatong Song","Lisheng Huang","Shuang Sun","Jinhao Jiang","Ran Le","Daixuan Cheng","Guoxin Chen","Yiwen Hu","Zongchao Chen","Yiming Jia","Wayne Xin Zhao","Yang Song","Tao Zhang","Ji-Rong Wen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21103v1","updated":"2026-02-24T17:03:21Z","published":"2026-02-24T17:03:21Z","title":"Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning","summary":"Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\\% to 90.0\\% and 67\\% to 83\\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.","authors":["Sanket Badhe","Deep Shah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21082v1","updated":"2026-02-24T16:45:17Z","published":"2026-02-24T16:45:17Z","title":"Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification","summary":"Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.","authors":["Vishal Patil","Shree Vaishnavi Bacha","Revanth Yamani","Yidan Sun","Mayank Kejriwal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21059v1","updated":"2026-02-24T16:16:44Z","published":"2026-02-24T16:16:44Z","title":"An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems","summary":"Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the assessment strategies of practicing scientists. In collaboration with domain experts, we identified 20 error patterns across seven categories through thematic analysis of 68 question-answer pairs. We validated this schema through contextual inquiries with 10 additional scientists, which showed not only which errors experts naturally identify but also how structured evaluation schemas can help them detect previously overlooked issues. Domain experts use systematic assessment strategies, including technical precision testing, value-based evaluation, and meta-evaluation of their own practices. We discuss implications for supporting expert evaluation of LLM outputs, including opportunities for personalized, schema-driven tools that adapt to individual evaluation patterns and expertise levels.","authors":["Anna Martin-Boyle","William Humphreys","Martha Brown","Cara Leckey","Harmanpreet Kaur"],"pdf_url":"","comment":"24 pages, 2 figures. Accepted at ACM CHI conference on Human Factors in Computing Systems, 2026"},{"id":"http://arxiv.org/abs/2509.26314v3","updated":"2026-02-24T16:11:47Z","published":"2025-09-30T14:26:36Z","title":"Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in Its Latent Thoughts","summary":"Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. A recent work instead proposes a latent thinking architecture, Huginn-3.5B, which represents intermediate reasoning steps as a sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of the model's latent thinking processes. In this paper, we provide a systematic study of how Huginn-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.","authors":["Hanwen Du","Yuxin Dong","Xia Ning"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21054v1","updated":"2026-02-24T16:11:14Z","published":"2026-02-24T16:11:14Z","title":"VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation","summary":"Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.","authors":["Seongheon Park","Changdae Oh","Hyeong Kyu Choi","Xuefeng Du","Sharon Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21045v1","updated":"2026-02-24T16:04:50Z","published":"2026-02-24T16:04:50Z","title":"PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A","summary":"Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source documents into discrete claims and evidence, mapping them to reveal supported assertions, unsupported claims, and information omitted from the source texts. We evaluated PaperTrail in a within-subjects study with 26 researchers who performed two scholarly editing tasks using PaperTrail and a baseline interface. Our results show that PaperTrail significantly lowered participants' trust compared to the baseline. However, this increased caution did not translate to behavioral changes, as people continued to rely on LLM-generated scholarly edits to avoid a cognitively burdensome task. We discuss the value of claim-evidence matching for understanding LLM trustworthiness in scholarly settings, and present design implications for cognition-friendly communication of provenance information.","authors":["Anna Martin-Boyle","Cara A. C. Leckey","Martha C. Brown","Harmanpreet Kaur"],"pdf_url":"","comment":"25 pages, 3 figures. Accepted at the ACM CHI conference on Human Factors in Computing Systems 2026"},{"id":"http://arxiv.org/abs/2602.21009v1","updated":"2026-02-24T15:28:58Z","published":"2026-02-24T15:28:58Z","title":"HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders","summary":"Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we propose Hierarchical Sparse Activation Compression (HiSAC), an efficient framework for personalized sequence modeling. HiSAC encodes interactions into multi-level semantic IDs and constructs a global hierarchical codebook. A hierarchical voting mechanism sparsely activates personalized interest-agents as fine-grained preference centers. Guided by these agents, Soft-Routing Attention aggregates historical signals in semantic space, weighting by similarity to minimize quantization error and retain long-tail behaviors. Deployed on Taobao's \"Guess What You Like\" homepage, HiSAC achieves significant compression and cost reduction, with online A/B tests showing a consistent 1.65% CTR uplift -- demonstrating its scalability and real-world effectiveness.","authors":["Kun Yuan","Junyu Bi","Daixuan Cheng","Changfa Wu","Shuwen Xiao","Binbin Cao","Jian Wu","Yuning Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.09082v2","updated":"2026-02-24T15:17:55Z","published":"2026-02-09T18:43:40Z","title":"UI-Venus-1.5 Technical Report","summary":"GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging. In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications. The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios. Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus","authors":[" Venus Team","Changlong Gao","Zhangxuan Gu","Yulin Liu","Xinyu Qiu","Shuheng Shen","Yue Wen","Tianyu Xia","Zhenyu Xu","Zhengwen Zeng","Beitong Zhou","Xingran Zhou","Weizhi Chen","Sunhao Dai","Jingya Dou","Yichen Gong","Yuan Guo","Zhenlin Guo","Feng Li","Qian Li","Jinzhen Lin","Yuqi Zhou","Linchao Zhu","Liang Chen","Zhenyu Guo","Changhua Meng","Weiqiang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20995v1","updated":"2026-02-24T15:14:49Z","published":"2026-02-24T15:14:49Z","title":"Generative Pseudo-Labeling for Pre-Ranking with LLMs","summary":"Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.","authors":["Junyu Bi","Xinting Niu","Daixuan Cheng","Kun Yuan","Tao Wang","Binbin Cao","Jian Wu","Yuning Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20994v1","updated":"2026-02-24T15:14:04Z","published":"2026-02-24T15:14:04Z","title":"Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures","summary":"Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.","authors":["Yubin Ge","Yongsong Huang","Xiaofeng Liu"],"pdf_url":"","comment":"IEEE International Symposium on Biomedical Imaging (ISBI) 2026"},{"id":"http://arxiv.org/abs/2602.20976v1","updated":"2026-02-24T15:00:00Z","published":"2026-02-24T15:00:00Z","title":"Evaluating Proactive Risk Awareness of Large Language Models","summary":"As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.","authors":["Xuan Luo","Yubin Chen","Zhiyu Hou","Linpu Yu","Geng Tu","Jing Li","Ruifeng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20973v1","updated":"2026-02-24T14:53:34Z","published":"2026-02-24T14:53:34Z","title":"Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving","summary":"To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.","authors":["Yuliang Ji","Fuchen Shen","Jian Wu","Qiujie Xie","Yue Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.18777v2","updated":"2026-02-24T14:51:33Z","published":"2025-06-23T15:45:44Z","title":"Programming by Backprop: An Instruction is Worth 100 Examples When Finetuning LLMs","summary":"Large language models (LLMs) are typically trained to acquire behaviours from demonstrations or experience, yet much of their training data is declarative: instructions, rules, and descriptions that specify behaviours without showing how to execute them. We introduce Programming by Backprop (PBB): a training regime that enables LLMs to acquire procedural knowledge (i.e., reusable behaviours) from declarative instructions encountered during training. With PBB, instructions in training data provide an opportunity to `program' specific behaviours into model weights. The core principle underpinning PBB is the separation of learning how instructions map to behaviour from internalising new instructions. We devise two distinct PBB curricula that leverage this principle. Through controlled experiments across two domains (algorithmic execution from Python source code and text generation from context-free grammars), we demonstrate the benefit of these curricula over training on a homogeneous data mixture. Crucially, PBB is highly sample efficient, with a single instruction substituting for up to 100 execution examples. Though execution of instructions in training data remains less reliable than when instructions are given in-context, our results demonstrate that procedural knowledge can be noisily `programmed' into LLMs through PBB, with important implications for data curation and safety.","authors":["Jonathan Cook","Silvia Sapora","Arash Ahmadian","Akbir Khan","Tim Rocktaschel","Jakob Foerster","Laura Ruis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20966v1","updated":"2026-02-24T14:45:08Z","published":"2026-02-24T14:45:08Z","title":"Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models","summary":"This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?\n  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.\n  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.","authors":["Paola Merlo","Chunyang Jiang","Giuseppe Samo","Vivi Nastase"],"pdf_url":"","comment":"Under review, 46 pages, 5 tables, 28 figures"},{"id":"http://arxiv.org/abs/2509.14297v2","updated":"2026-02-24T14:28:30Z","published":"2025-09-17T04:21:20Z","title":"A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness","summary":"This study reveals a critical safety blind spot in modern LLMs: learning-style queries, which closely resemble ordinary educational questions, can reliably elicit harmful responses. The learning-style queries are constructed by a novel reframing paradigm: HILL (Hiding Intention by Learning from LLMs). The deterministic, model-agnostic reframing framework is composed of 4 conceptual components: 1) key concept, 2) exploratory transformation, 3) detail-oriented inquiry, and optionally 4) hypotheticality. Further, new metrics are introduced to thoroughly evaluate the efficiency and harmfulness of jailbreak methods. Experiments on the AdvBench dataset across a wide range of models demonstrate HILL's strong generalizability. It achieves top attack success rates on the majority of models and across malicious categories while maintaining high efficiency with concise prompts. On the other hand, results of various defense methods show the robustness of HILL, with most defenses having mediocre effects or even increasing the attack success rates. In addition, the assessment of defenses on the constructed safe prompts reveals inherent limitations of LLMs' safety mechanisms and flaws in the defense methods. This work exposes significant vulnerabilities of safety measures against learning-style elicitation, highlighting a critical challenge of fulfilling both helpfulness and safety alignments.","authors":["Xuan Luo","Yue Wang","Zefeng He","Geng Tu","Jing Li","Ruifeng Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20945v1","updated":"2026-02-24T14:28:16Z","published":"2026-02-24T14:28:16Z","title":"The Art of Efficient Reasoning: Data, Reward, and Optimization","summary":"Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.","authors":["Taiqiang Wu","Zenan Zu","Bo Zhou","Ngai Wong"],"pdf_url":"","comment":"Tech Report, Insights on Efficient Reasoning via Reward Shaping"},{"id":"http://arxiv.org/abs/2502.12927v3","updated":"2026-02-24T14:17:40Z","published":"2025-02-18T15:09:29Z","title":"SEFL: A Framework for Generating Synthetic Educational Assignment Feedback with LLM Agents","summary":"Providing high-quality feedback on student assignments is crucial for student success, but it is heavily limited by time and budgetary constraints. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments and teacher feedback. To obtain this type of data, two large language models (LLMs) operate in a teacher-student role to simulate assignment completion and formative feedback, generating 19.8K synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Through comprehensive evaluations with three LLM judges and three human experts, across a subset of 900 outputs, we demonstrate that SEFL-tuned models outperform both their untuned counterparts and an existing baseline in terms of feedback quality. The potential for societal impact is reinforced by extensive qualitative comments and ratings from human stakeholders -- both students and higher education instructors. SEFL has the potential to transform feedback processes for higher education and beyond.","authors":["Mike Zhang","Amalie Pernille Dilling","Léon Gondelman","Niels Erik Ruan Lyngdorf","Euan D. Lindsay","Johannes Bjerva"],"pdf_url":"","comment":"LREC 2026"},{"id":"http://arxiv.org/abs/2602.20918v1","updated":"2026-02-24T13:54:38Z","published":"2026-02-24T13:54:38Z","title":"Predicting Sentence Acceptability Judgments in Multimodal Contexts","summary":"Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.","authors":["Hyewon Jang","Nikolai Ilinykh","Sharid Loáiciga","Jey Han Lau","Shalom Lappin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14365v3","updated":"2026-02-24T13:53:32Z","published":"2025-10-16T06:59:58Z","title":"Understanding the Ability of LLMs to Handle Character-Level Perturbation","summary":"This work investigates the resilience of contemporary large language models (LLMs) against frequent character-level perturbations. We examine three types of character-level perturbations including introducing numerous typos within words, shuffling the characters in each word, and inserting a large number of invisible characters into the text. Surprisingly, even under severe perturbation, such as shuffling nearly all words character-wise to produce text that is almost unreadable to humans, or inserting invisible characters which are several times more than the visible ones as noise, many LLMs still maintain notable performance. We explore the underlying causes of this robustness and find that LLMs exhibit remarkable resilience to chaotic segmentation and fragmented tokenization. Furthermore, we examine the mechanisms by which LLMs remove perturbations to correctly comprehend text, including both implicit and explicit mechanisms for character-level perturbation. We hope that our findings on the low-level robustness of LLMs will unveil their inherent architectural strengths, reveal the potential risks of their misuse, and inform the reliable deployment of LLMs across diverse application scenarios.","authors":["Anyuan Zhuo","Xuefei Ning","Ningyuan Li","Jingyi Zhu","Yu Wang","Pinyan Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22500v2","updated":"2026-02-24T13:37:46Z","published":"2025-10-26T02:42:03Z","title":"Towards Scalable Oversight via Partitioned Human Supervision","summary":"As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging. Our focus is on tasks that require deep knowledge and skills of multiple domains, where this bottleneck is severe. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks. However, based on their narrow expertise, humans may provide a weak signal, i.e., a complementary label indicating an option that is incorrect. For example, a cardiologist could state that ''this is not related to any cardiovascular disease,'' even if they cannot identify the true disease. Based on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth. We derive an unbiased estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels. We provide finite-sample deviation guarantees for both complementary-only and the mixed estimators. Empirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels. We further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can improve itself with this partitioned human supervision. Our code is available at https://github.com/R-Yin-217/Towards-Scalable-Oversight-via-Partitioned-Human-Supervision.","authors":["Ren Yin","Takashi Ishida","Masashi Sugiyama"],"pdf_url":"","comment":"ICLR 2026 camera ready version"},{"id":"http://arxiv.org/abs/2602.20892v1","updated":"2026-02-24T13:28:23Z","published":"2026-02-24T13:28:23Z","title":"Exa-PSD: a new Persian sentiment analysis dataset on Twitter","summary":"Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.","authors":["Seyed Himan Ghaderi","Saeed Sarbazi Azad","Mohammad Mehdi Jaziriyan","Ahmad Akbari"],"pdf_url":"","comment":"This is the original submitted (preprint) version of a paper published in Language Resources and Evaluation. The final published version is available at Springer via DOI: https://doi.org/10.1007/s10579-025-09886-5"},{"id":"http://arxiv.org/abs/2505.11876v4","updated":"2026-02-24T13:15:55Z","published":"2025-05-17T07:00:02Z","title":"EAMET: Robust Massive Model Editing via Embedding Alignment Optimization","summary":"Model editing techniques are essential for efficiently updating knowledge in large language models (LLMs). However, the effectiveness of existing approaches degrades in massive editing scenarios, particularly when evaluated with practical metrics. Their robustness is also limited in context-rich settings or when editing multiple facts of the same subject simultaneously. We attribute these failures to the embedding misalignment among knowledge items, which undermines editing reliability at scale. To address this, we propose EAMET (Embedding Alignment Model Editing in Transformers), which addresses this issue by aligning the space of key and residual embeddings. Extensive experiments across six LLMs and three datasets demonstrate that EAMET consistently outperforms existing methods, achieving about 90\\% editing efficacy when editing 10k facts. Codes and datasets are publicly available at https://ybdai7.github.io/eamet-page/.","authors":["Yanbo Dai","Zhenlan Ji","Zongjie Li","Shuai Wang"],"pdf_url":"","comment":"This paper was accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2508.03250v3","updated":"2026-02-24T13:10:08Z","published":"2025-08-05T09:28:20Z","title":"RooseBERT: A New Deal For Political Language Modelling","summary":"The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models (LMs). To address this, we introduce a novel pre-trained LM for political discourse language called RooseBERT. Pre-training a LM on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (11GB) in English. To evaluate its performances, we fine-tuned it on multiple downstream tasks related to political debate analysis, i.e., stance detection, sentiment analysis, argument component detection and classification, argument relation prediction and classification, policy classification, named entity recognition (NER). Our results show significant improvements over general-purpose LMs on the majority of these tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release RooseBERT for the research community.","authors":["Deborah Dore","Elena Cabrio","Serena Villata"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20859v1","updated":"2026-02-24T13:02:09Z","published":"2026-02-24T13:02:09Z","title":"FinAnchor: Aligned Multi-Model Representations for Financial Prediction","summary":"Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.","authors":["Zirui He","Huopu Zhang","Yanguang Liu","Sirui Wu","Mengnan Du"],"pdf_url":"","comment":"11 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2510.24694v2","updated":"2026-02-24T12:55:37Z","published":"2025-10-28T17:50:40Z","title":"Repurposing Synthetic Data for Fine-grained Search Agent Supervision","summary":"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative \"near-miss\" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these \"near-misses\". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.","authors":["Yida Zhao","Kuan Li","Xixi Wu","Liwen Zhang","Dingchu Zhang","Baixuan Li","Maojia Song","Zhuo Chen","Chenxi Wang","Xinyu Wang","Kewei Tu","Pengjun Xie","Jingren Zhou","Yong Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.18337v2","updated":"2026-02-24T12:33:49Z","published":"2025-12-20T12:06:13Z","title":"Towards Efficient Agents: A Co-Design of Inference Architecture and System","summary":"The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.","authors":["Weizhe Lin","Hui-Ling Zhen","Shuai Yang","Xian Wang","Renxi Liu","Hanting Chen","Wangze Zhang","Chuansai Zhou","Yiming Li","Chen Chen","Xing Li","Zhiyuan Yang","Xiaosong Li","Xianzhi Yu","Zhenhua Dong","Mingxuan Yuan","Yunhe Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20816v1","updated":"2026-02-24T11:54:06Z","published":"2026-02-24T11:54:06Z","title":"Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation","summary":"The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.","authors":["Sayantan Dasgupta","Trevor Cohn","Timothy Baldwin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16602v3","updated":"2026-02-24T11:22:02Z","published":"2025-12-18T14:43:04Z","title":"Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics","summary":"We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.","authors":["Iker García-Ferrero","David Montero","Roman Orus"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19626v2","updated":"2026-02-24T11:10:17Z","published":"2026-02-23T09:14:05Z","title":"Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding","summary":"We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder, achieving the best compression results among the systems evaluated in this study on natural language text. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29 (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution (OOD) evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.","authors":["Roberto Tacconelli"],"pdf_url":"","comment":"10 pages"},{"id":"http://arxiv.org/abs/2602.19612v2","updated":"2026-02-24T10:56:28Z","published":"2026-02-23T08:58:48Z","title":"Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning","summary":"Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.","authors":["Borisiuk Anna","Andrey Savchenko","Alexander Panchenko","Elena Tutubalina"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.15763v2","updated":"2026-02-24T10:44:44Z","published":"2026-02-17T17:50:56Z","title":"GLM-5: from Vibe Coding to Agentic Engineering","summary":"We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.","authors":[" GLM-5-Team"," :","Aohan Zeng","Xin Lv","Zhenyu Hou","Zhengxiao Du","Qinkai Zheng","Bin Chen","Da Yin","Chendi Ge","Chenghua Huang","Chengxing Xie","Chenzheng Zhu","Congfeng Yin","Cunxiang Wang","Gengzheng Pan","Hao Zeng","Haoke Zhang","Haoran Wang","Huilong Chen","Jiajie Zhang","Jian Jiao","Jiaqi Guo","Jingsen Wang","Jingzhao Du","Jinzhu Wu","Kedong Wang","Lei Li","Lin Fan","Lucen Zhong","Mingdao Liu","Mingming Zhao","Pengfan Du","Qian Dong","Rui Lu"," Shuang-Li","Shulin Cao","Song Liu","Ting Jiang","Xiaodong Chen","Xiaohan Zhang","Xuancheng Huang","Xuezhen Dong","Yabo Xu","Yao Wei","Yifan An","Yilin Niu","Yitong Zhu","Yuanhao Wen","Yukuo Cen","Yushi Bai","Zhongpei Qiao","Zihan Wang","Zikang Wang","Zilin Zhu","Ziqiang Liu","Zixuan Li","Bojie Wang","Bosi Wen","Can Huang","Changpeng Cai","Chao Yu","Chen Li","Chengwei Hu","Chenhui Zhang","Dan Zhang","Daoyan Lin","Dayong Yang","Di Wang","Ding Ai","Erle Zhu","Fangzhou Yi","Feiyu Chen","Guohong Wen","Hailong Sun","Haisha Zhao","Haiyi Hu","Hanchen Zhang","Hanrui Liu","Hanyu Zhang","Hao Peng","Hao Tai","Haobo Zhang","He Liu","Hongwei Wang","Hongxi Yan","Hongyu Ge","Huan Liu","Huanpeng Chu","Jia'ni Zhao","Jiachen Wang","Jiajing Zhao","Jiamin Ren","Jiapeng Wang","Jiaxin Zhang","Jiayi Gui","Jiayue Zhao","Jijie Li","Jing An","Jing Li","Jingwei Yuan","Jinhua Du","Jinxin Liu","Junkai Zhi","Junwen Duan","Kaiyue Zhou","Kangjian Wei","Ke Wang","Keyun Luo","Laiqiang Zhang","Leigang Sha","Liang Xu","Lindong Wu","Lintao Ding","Lu Chen","Minghao Li","Nianyi Lin","Pan Ta","Qiang Zou","Rongjun Song","Ruiqi Yang","Shangqing Tu","Shangtong Yang","Shaoxiang Wu","Shengyan Zhang","Shijie Li","Shuang Li","Shuyi Fan","Wei Qin","Wei Tian","Weining Zhang","Wenbo Yu","Wenjie Liang","Xiang Kuang","Xiangmeng Cheng","Xiangyang Li","Xiaoquan Yan","Xiaowei Hu","Xiaoying Ling","Xing Fan","Xingye Xia","Xinyuan Zhang","Xinze Zhang","Xirui Pan","Xu Zou","Xunkai Zhang","Yadi Liu","Yandong Wu","Yanfu Li","Yidong Wang","Yifan Zhu","Yijun Tan","Yilin Zhou","Yiming Pan","Ying Zhang","Yinpei Su","Yipeng Geng","Yong Yan","Yonglin Tan","Yuean Bi","Yuhan Shen","Yuhao Yang","Yujiang Li","Yunan Liu","Yunqing Wang","Yuntao Li","Yurong Wu","Yutao Zhang","Yuxi Duan","Yuxuan Zhang","Zezhen Liu","Zhengtao Jiang","Zhenhe Yan","Zheyu Zhang","Zhixiang Wei","Zhuo Chen","Zhuoer Feng","Zijun Yao","Ziwei Chai","Ziyuan Wang","Zuzhou Zhang","Bin Xu","Minlie Huang","Hongning Wang","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20759v1","updated":"2026-02-24T10:39:27Z","published":"2026-02-24T10:39:27Z","title":"Overton Pluralistic Reinforcement Learning for Large Language Models","summary":"Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a \"small models, big perspective coverage\" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.","authors":["Yu Fu","Seongho Son","Ilija Bogunovic"],"pdf_url":"","comment":"28 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.08274v3","updated":"2026-02-24T10:29:09Z","published":"2026-02-09T05:09:03Z","title":"Language Modeling and Understanding Through Paraphrase Generation and Detection","summary":"Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...","authors":["Jan Philip Wahle"],"pdf_url":"","comment":"PhD Thesis (Dissertation), University of Göttingen Germany, 2025. 186 pages"},{"id":"http://arxiv.org/abs/2602.20751v1","updated":"2026-02-24T10:28:44Z","published":"2026-02-24T10:28:44Z","title":"SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing","summary":"Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.","authors":["Yifei Xu","Guilherme Potje","Shivam Shandilya","Tiancheng Yuan","Leonardo de Oliveira Nunes","Rakshanda Agarwal","Saeid Asgari","Adam Atkinson","Emre Kıcıman","Songwu Lu","Ranveer Chandra","Tusher Chakraborty"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20749v1","updated":"2026-02-24T10:25:29Z","published":"2026-02-24T10:25:29Z","title":"Explicit Grammar Semantic Feature Fusion for Robust Text Classification","summary":"Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices","authors":["Azrin Sultana","Firoz Ahmed"],"pdf_url":"","comment":"30 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.20743v1","updated":"2026-02-24T10:12:40Z","published":"2026-02-24T10:12:40Z","title":"Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization","summary":"Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.","authors":["Gabriel Loiseau","Damien Sileo","Damien Riquet","Maxime Meyer","Marc Tommasi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.24803v3","updated":"2026-02-24T10:03:33Z","published":"2025-09-29T13:54:34Z","title":"TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models","summary":"Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.","authors":["Tong Guan","Zijie Meng","Dianqi Li","Shiyu Wang","Chao-Han Huck Yang","Qingsong Wen","Zuozhu Liu","Sabato Marco Siniscalchi","Ming Jin","Shirui Pan"],"pdf_url":"","comment":"Accepted by the 14th International Conference on Learning Representations (ICLR 2026)"},{"id":"http://arxiv.org/abs/2602.20735v1","updated":"2026-02-24T09:58:25Z","published":"2026-02-24T09:58:25Z","title":"RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition","summary":"This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\n  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n  (R2RAG), a research-focused retrieval-augmented generation (RAG)\n  architecture composed of lightweight components that dynamically adapt the\n  retrieval strategy based on inferred query complexity and evidence\n  sufficiency. The system uses smaller LLMs, enabling operation on a single\n  consumer-grade GPU while supporting complex research tasks. It builds on the\n  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\n  with modules informed by qualitative review of outputs. R2RAG won the Best\n  Dynamic Evaluation award in the Open Source category, demonstrating high\n  effectiveness with careful design and efficient use of resources.","authors":["Kun Ran","Marwah Alaofi","Danula Hettiachchi","Chenglong Ma","Khoi Nguyen Dinh Anh","Khoi Vo Nguyen","Sachin Pathiyan Cherumanal","Lida Rashidi","Falk Scholer","Damiano Spina","Shuoqi Sun","Oleg Zendel"],"pdf_url":"","comment":"MMU-RAG NeurIPS 2025 winning system"},{"id":"http://arxiv.org/abs/2602.20727v1","updated":"2026-02-24T09:45:10Z","published":"2026-02-24T09:45:10Z","title":"ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition","summary":"LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.","authors":["Xindian Ma","Rundong Kong","Peng Zhang","Ruoxiang Huang","Yongyu Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.00628v2","updated":"2026-02-24T09:42:15Z","published":"2025-10-01T08:00:58Z","title":"Hearing the Order: Investigating Position Bias in Large Audio-Language Models","summary":"Large audio-language models (LALMs) are often used in tasks that involve reasoning over ordered options. An open question is whether their predictions are influenced by the order of answer choices, which would indicate a form of position bias and undermine their reliability. In this paper, we identify and analyze this problem in LALMs. We demonstrate that no model is immune to this bias through extensive experiments on six LALMs across three widely used benchmarks and their spoken counterparts. Shuffling the order of answer options can cause performance fluctuations of up to 24% and even change model rankings, raising concerns about the reliability of current evaluation practices. We also study permutation-based strategies and show that they can mitigate bias in most cases. Our work represents the first systematic investigation of this issue in LALMs, and we hope it raises awareness and motivates further research in this direction.","authors":["Yu-Xiang Lin","Chen-An Li","Sheng-Lun Wei","Po-Chun Chen","Hsin-Hsi Chen","Hung-yi Lee"],"pdf_url":"","comment":"The first two authors contributed equally. Submitted to Interspeech 2026"},{"id":"http://arxiv.org/abs/2512.23447v2","updated":"2026-02-24T09:29:19Z","published":"2025-12-29T13:03:18Z","title":"Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss","summary":"Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain intermediate activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on $n^2$ activations, where $n$ is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.","authors":["Ang Lv","Jin Ma","Yiyuan Ma","Siyuan Qiao"],"pdf_url":"","comment":"ICLR 2026 Oral"},{"id":"http://arxiv.org/abs/2602.20710v1","updated":"2026-02-24T09:15:30Z","published":"2026-02-24T09:15:30Z","title":"Counterfactual Simulation Training for Chain-of-Thought Faithfulness","summary":"Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training","authors":["Peter Hase","Christopher Potts"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.00994v3","updated":"2026-02-24T09:11:32Z","published":"2025-07-01T17:45:48Z","title":"Should We Still Pretrain Encoders with Masked Language Modeling?","summary":"Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 38 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models, reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.","authors":["Hippolyte Gisserot-Boukhlef","Nicolas Boizard","Manuel Faysse","Duarte M. Alves","Emmanuel Malherbe","André F. T. Martins","Céline Hudelot","Pierre Colombo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.03922v2","updated":"2026-02-24T09:07:33Z","published":"2025-06-04T13:14:13Z","title":"HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models","summary":"Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.","authors":["Zhaolu Kang","Junhao Gong","Jiaxu Yan","Wanke Xia","Yian Wang","Ziwen Wang","Huaxuan Ding","Zhuo Cheng","Wenhao Cao","Zhiyuan Feng","Siqi He","Shannan Yan","Junzhe Chen","Xiaomin He","Chaoya Jiang","Wei Ye","Kaidong Yu","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14640v3","updated":"2026-02-24T09:00:19Z","published":"2025-10-16T12:54:40Z","title":"LUMI: Unsupervised Intent Clustering with Multiple Pseudo-Labels","summary":"In this paper, we propose an intuitive, training-free and label-free method for intent clustering in conversational search. Current approaches to short text clustering use LLM-generated pseudo-labels to enrich text representations or to identify similar text pairs for pooling. The limitations are: (1) each text is assigned only a single label, and refining representations toward a single label can be unstable; (2) text-level similarity is treated as a binary selection, which fails to account for continuous degrees of similarity. Our method LUMI is designed to amplify similarities between texts by using shared pseudo-labels. We first generate pseudo-labels for each text and collect them into a pseudo-label set. Next, we compute the mean of the pseudo-label embeddings and pool it with the text embedding. Finally, we perform text-level pooling: Each text representation is pooled with its similar pairs, where similarity is determined by the degree of shared labels. Our evaluation on four benchmark sets shows that our approach achieves competitive results, better than recent state-of-the-art baselines, while avoiding the need to estimate the number of clusters during embedding refinement, as is required by most methods. Our findings indicate that LUMI can effectively be applied in unsupervised short-text clustering scenarios.","authors":["I-Fan Lin","Faegheh Hasibi","Suzan Verberne"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.08645v2","updated":"2026-02-24T08:46:39Z","published":"2026-01-13T15:20:28Z","title":"A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding","summary":"Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.","authors":["Dilara Torunoğlu-Selamet","Dogukan Arslan","Rodrigo Wilkens","Wei He","Doruk Eryiğit","Thomas Pickard","Adriana S. Pagano","Aline Villavicencio","Gülşen Eryiğit","Ágnes Abuczki","Aida Cardoso","Alesia Lazarenka","Dina Almassova","Amalia Mendes","Anna Kanellopoulou","Antoni Brosa-Rodríguez","Baiba Saulite","Beata Wojtowicz","Bolette Pedersen","Carlos Manuel Hidalgo-Ternero","Chaya Liebeskind","Danka Jokić","Diego Alves","Eleni Triantafyllidi","Erik Velldal","Fred Philippy","Giedre Valunaite Oleskeviciene","Ieva Rizgeliene","Inguna Skadina","Irina Lobzhanidze","Isabell Stinessen Haugen","Jauza Akbar Krito","Jelena M. Marković","Johanna Monti","Josue Alejandro Sauca","Kaja Dobrovoljc","Kingsley O. Ugwuanyi","Laura Rituma","Lilja Øvrelid","Maha Tufail Agro","Manzura Abjalova","Maria Chatzigrigoriou","María del Mar Sánchez Ramos","Marija Pendevska","Masoumeh Seyyedrezaei","Mehrnoush Shamsfard","Momina Ahsan","Muhammad Ahsan Riaz Khan","Nathalie Carmen Hau Norman","Nilay Erdem Ayyıldız","Nina Hosseini-Kivanani","Noémi Ligeti-Nagy","Numaan Naeem","Olha Kanishcheva","Olha Yatsyshyna","Daniil Orel","Petra Giommarelli","Petya Osenova","Radovan Garabik","Regina E. Semou","Rozane Rebechi","Salsabila Zahirah Pranida","Samia Touileb","Sanni Nimb","Sarfraz Ahmad","Sarvinoz Sharipova","Shahar Golan","Shaoxiong Ji","Sopuruchi Christian Aboh","Srdjan Sucur","Stella Markantonatou","Sussi Olsen","Vahide Tajalli","Veronika Lipp","Voula Giouli","Yelda Yeşildal Eraydın","Zahra Saaberi","Zhuohan Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.15148v3","updated":"2026-02-24T08:46:07Z","published":"2025-09-18T16:55:09Z","title":"ATTS: Asynchronous Test-Time Scaling via Conformal Prediction","summary":"Large language models (LLMs) benefit from test-time scaling but are often hampered by high inference latency. Speculative decoding is a natural way to accelerate the scaling process; however, scaling along both the parallel and sequential dimensions poses significant challenges, including substantial memory-bound execution and synchronization overhead. We introduce ATTS (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive scaling framework that follows the hypothesis testing process to address these challenges. By revisiting arithmetic intensity, ATTS identifies synchronization as the primary bottleneck. It enables asynchronous inference through online calibration and proposes an ordinal classification algorithm that supports a three-stage rejection sampling pipeline, scaling along both the sequential and parallel axes. Across experiments on the MATH, AMC23, AIME24, and AIME25 datasets and across multiple draft-target model families, we show that ATTS delivers up to 56.7x speedup in test-time scaling and a 4.14x throughput improvement, while maintaining accurate control of the rejection rate, reducing latency and memory overhead, and incurring no accuracy loss. By scaling both in parallel and sequential dimensions, we enable the 1.5B/70B draft/target model combination to achieve the performance of the state-of-the-art reasoning model o3-mini (high) on the AIME dataset. We have released the code at https://github.com/menik1126/asynchronous-test-time-scaling.","authors":["Jing Xiong","Qiujiang Chen","Fanghua Ye","Zhongwei Wan","Chuanyang Zheng","Chenyang Zhao","Hui Shen","Hanbo Li","Chaofan Tao","Haochen Tan","Haoli Bai","Lifeng Shang","Lingpeng Kong","Ngai Wong"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20670v1","updated":"2026-02-24T08:20:08Z","published":"2026-02-24T08:20:08Z","title":"CAMEL: Confidence-Gated Reflection for Reward Modeling","summary":"Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.","authors":["Zirui Zhu","Hailun Xu","Yang Luo","Yong Liu","Kanchan Sarkar","Kun Xu","Yang You"],"pdf_url":"","comment":"Preprint. 13 pages"},{"id":"http://arxiv.org/abs/2208.02500v2","updated":"2026-02-24T08:16:42Z","published":"2022-08-04T07:20:00Z","title":"Usability Study of Security Features in Programmable Logic Controllers","summary":"Programmable Logic Controllers (PLCs) drive industrial processes critical to society, for example, water treatment and distribution, electricity and fuel networks. Search engines, e.g., Shodan, have highlighted that PLCs are often left exposed to the Internet, one of the main reasons being the misconfigurations of security settings. This leads to the question - why do these misconfigurations occur and, specifically, whether usability of security controls plays a part. To date, the usability of configuring PLC security mechanisms has not been studied. We present the first investigation through a task based study and subsequent semi-structured interviews (N=19). We explore the usability of PLC connection configurations and two key security mechanisms (i.e., access levels and user administration). We find that the use of unfamiliar labels, layouts and misleading terminology exacerbates an already complex process of configuring security mechanisms. Our results uncover various misperceptions about the security controls and how design constraints, e.g., safety and lack of regular updates due to the long-term nature of such systems, provide significant challenges to the realization of modern HCI and usability principles. Based on these findings, we provide design recommendations to bring usable security in industrial settings at par with its IT counterpart.","authors":["Karen Li","Kopo M. Ramokapane","Awais Rashid"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20648v1","updated":"2026-02-24T07:52:56Z","published":"2026-02-24T07:52:56Z","title":"CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models","summary":"Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.","authors":["Anqi Li","Chenxiao Wang","Yu Lu","Renjun Xu","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.20647v1","updated":"2026-02-24T07:52:35Z","published":"2026-02-24T07:52:35Z","title":"Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books","summary":"I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty","authors":["W. Frederick Zimmerman"],"pdf_url":"","comment":"six figures. dataset available at Hugging Face"},{"id":"http://arxiv.org/abs/2602.20634v1","updated":"2026-02-24T07:26:17Z","published":"2026-02-24T07:26:17Z","title":"Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches","summary":"The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.","authors":["Saurabh Mishra","Shivani Thakur","Radhika Mamidi"],"pdf_url":"","comment":"32 pages, 24 figures"},{"id":"http://arxiv.org/abs/2602.18448v2","updated":"2026-02-24T07:16:00Z","published":"2026-01-28T17:27:49Z","title":"INSURE-Dial: A Phase-Aware Conversational Dataset & Benchmark for Compliance Verification and Phase Detection","summary":"Administrative phone tasks drain roughly 1 trillion USD annually from U.S. healthcare, with over 500 million insurance-benefit verification calls manually handled in 2024. We introduce INSURE-Dial, to our knowledge the first public benchmark for developing and assessing compliance-aware voice agents for phase-aware call auditing with span-based compliance verification. The corpus includes 50 de-identified, AI-initiated calls with live insurance representatives (mean 71 turns/call) and 1,000 synthetically generated calls that mirror the same workflow. All calls are annotated with a phase-structured JSON schema covering IVR navigation, patient identification, coverage status, medication checks (up to two drugs), and agent identification (CRN), and each phase is labeled for Information and Procedural compliance under explicit ask/answer logic. We define two novel evaluation tasks: (1) Phase Boundary Detection (span segmentation under phase-specific acceptance rules) and (2) Compliance Verification (IC/PC decisions given fixed spans). Per-phase scores are strong across small, low-latency baselines, but end-to-end reliability is constrained by span-boundary errors. On real calls, full-call exact segmentation is low, showing a gap between conversational fluency and audit-grade evidence.","authors":["Shubham Kulkarni","Alexander Lyzhov","Preetam Joshi","Shiva Chaitanya"],"pdf_url":"","comment":"Accepted to the 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026)"},{"id":"http://arxiv.org/abs/2505.17645v2","updated":"2026-02-24T07:02:26Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning","summary":"Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"","comment":"Camera-ready version. Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.20610v1","updated":"2026-02-24T07:01:17Z","published":"2026-02-24T07:01:17Z","title":"SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference","summary":"Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.","authors":["Cuong Chi Le","Minh V. T Pham","Tung Vu Duy","Cuong Duc Van","Huy N. Phan","Hoang N. Phan","Tien N. Nguyen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20580v1","updated":"2026-02-24T06:02:03Z","published":"2026-02-24T06:02:03Z","title":"Personal Information Parroting in Language Models","summary":"Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.","authors":["Nishant Subramani","Kshitish Ghate","Mona Diab"],"pdf_url":"","comment":"EACL Findings 2026"},{"id":"http://arxiv.org/abs/2602.20574v1","updated":"2026-02-24T05:56:20Z","published":"2026-02-24T05:56:20Z","title":"GATES: Self-Distillation under Privileged Context with Consensus Gating","summary":"We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\\% to 62.0\\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\\% to 35.4\\%.","authors":["Alex Stein","Furong Huang","Tom Goldstein"],"pdf_url":"","comment":"10 Pages of main text with an additional 7 pages of supplementary material"},{"id":"http://arxiv.org/abs/2507.03043v3","updated":"2026-02-24T05:30:54Z","published":"2025-07-03T08:05:02Z","title":"K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function","summary":"Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. Its core, Kids-Weighted Finite State Transducer (K-WFST), merges an acoustic phoneme encoder with a phoneme-similarity model to capture child-specific speech errors while remaining fully interpretable. K-WFST achieves a 1.39 % phoneme error rate on MyST and 8.61 % on Multitudes-an absolute improvement of 10.47 % and 7.06 % over a greedy-search decoder. These high-quality transcripts are used by an LLM to grade verbal skills, developmental milestones, reading, and comprehension, with results that align closely with human evaluators. Our findings show that precise phoneme recognition is essential for creating an effective assessment framework, enabling scalable language screening for children.","authors":["Shuhe Li","Chenxu Guo","Jiachen Lian","Cheol Jun Cho","Wenshuo Zhao","Xiner Xu","Ruiyu Jin","Xiaoyu Shi","Xuanru Zhou","Dingkun Zhou","Sam Wang","Grace Wang","Jingze Yang","Jingyi Xu","Ruohan Bao","Xingrui Chen","Elise Brenner","Brandon In","Francesca Pei","Maria Luisa Gorno-Tempini","Gopala Anumanchipalli"],"pdf_url":"","comment":"Accepted to 2026 ICASSP"},{"id":"http://arxiv.org/abs/2601.03868v2","updated":"2026-02-24T04:21:01Z","published":"2026-01-07T12:31:52Z","title":"What Matters For Safety Alignment?","summary":"This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.","authors":["Xing Li","Hui-Ling Zhen","Lihao Yin","Xianzhi Yu","Zhenhua Dong","Mingxuan Yuan"],"pdf_url":"","comment":"Added more commercial model results, majority voting scores, and theoretical analysis in v2"},{"id":"http://arxiv.org/abs/2602.20532v1","updated":"2026-02-24T04:19:48Z","published":"2026-02-24T04:19:48Z","title":"Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training","summary":"Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.","authors":["Zhengyao Gu","Jonathan Light","Raul Astudillo","Ziyu Ye","Langzhou He","Henry Peng Zou","Wei Cheng","Santiago Paternain","Philip S. Yu","Yisong Yue"],"pdf_url":"","comment":"37 pages, 8 figures, 1 table. Preprint under review. Equal contribution by first two authors"},{"id":"http://arxiv.org/abs/2602.20528v1","updated":"2026-02-24T04:09:31Z","published":"2026-02-24T04:09:31Z","title":"Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning","summary":"The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.","authors":["Justin Lovelace","Christian Belardi","Sofian Zalouk","Adhitya Polavaram","Srivatsa Kundurthy","Kilian Q. Weinberger"],"pdf_url":"","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2602.20517v1","updated":"2026-02-24T03:37:42Z","published":"2026-02-24T03:37:42Z","title":"Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination","summary":"Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.","authors":["Rakshit Trivedi","Kartik Sharma","David C Parkes"],"pdf_url":"","comment":"Spotlight paper at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.20513v1","updated":"2026-02-24T03:31:07Z","published":"2026-02-24T03:31:07Z","title":"From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility","summary":"As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.","authors":["Gavin Levinson","Keith Feldman"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.01070v3","updated":"2026-02-24T03:22:11Z","published":"2026-02-01T07:30:22Z","title":"What If We Allocate Test-Time Compute Adaptively?","summary":"Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.","authors":["Ahsan Bilal","Ahmed Mohsin","Muhammad Umer","Ali Subhan","Hassan Rizwan","Ayesha Mohsin","Dean Hougen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18464v2","updated":"2026-02-24T03:18:00Z","published":"2026-02-06T21:57:08Z","title":"How Well Can LLM Agents Simulate End-User Security and Privacy Attitudes and Behaviors?","summary":"A growing body of research assumes that large language model (LLM) agents can serve as proxies for how people form attitudes toward and behave in response to security and privacy (S&P) threats. If correct, these simulations could offer a scalable way to forecast S&P risks in products prior to deployment. We interrogate this assumption using SP-ABCBench, a new benchmark of 30 tests derived from validated S&P human-subject studies, which measures alignment between simulations and human-subjects studies on a 0-100 ascending scale, where higher scores indicate better alignment across three dimensions: Attitude, Behavior, and Coherence. Evaluating twelve LLMs, four persona construction strategies, and two prompting methods, we found that there remains substantial room for improvement: all models score between 50 and 64 on average. Newer, bigger, and smarter models do not reliably do better and sometimes do worse. Some simulation configurations, however, do yield high alignment: e.g., with scores above 95 for some behavior tests when agents are prompted to apply bounded rationality and weigh privacy costs against perceived benefits. We release SP-ABCBench to enable reproducible evaluation as methods improve.","authors":["Yuxuan Li","Leyang Li","Hao-Ping Lee","Sauvik Das"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.19001v2","updated":"2026-02-24T02:24:49Z","published":"2026-01-26T22:23:09Z","title":"FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning","summary":"We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST","authors":["Haozheng Luo","Zhuolin Jiang","Md Zahid Hasan","Yan Chen","Soumalya Sarkar"],"pdf_url":"","comment":"International Conference on Learning Representations (ICLR) 2026"},{"id":"http://arxiv.org/abs/2602.14281v3","updated":"2026-02-24T02:08:48Z","published":"2026-02-15T19:10:00Z","title":"MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents","summary":"The Model Context Protocol (MCP) standardizes tool use for LLM-based agents and enable third-party servers. This openness introduces a security misalignment: agents implicitly trust tools exposed by potentially untrusted MCP servers. However, despite its excellent utility, existing agents typically offer limited validation for third-party MCP servers. As a result, agents remain vulnerable to MCP-based attacks that exploit the misalignment between agents and servers throughout the tool invocation lifecycle. In this paper, we propose MCPShield as a plug-in security cognition layer that mitigates this misalignment and ensures agent security when invoking MCP-based tools. Drawing inspiration from human experience-driven tool validation, MCPShield assists agent forms security cognition with metadata-guided probing before invocation. Our method constrains execution within controlled boundaries while cognizing runtime events, and subsequently updates security cognition by reasoning over historical traces after invocation, building on human post-use reflection on tool behavior. Experiments demonstrate that MCPShield exhibits strong generalization in defending against six novel MCP-based attack scenarios across six widely used agentic LLMs, while avoiding false positives on benign servers and incurring low deployment overhead. Overall, our work provides a practical and robust security safeguard for MCP-based tool invocation in open agent ecosystems.","authors":["Zhenhong Zhou","Yuanhe Zhang","Hongwei Cai","Moayad Aloqaily","Ouns Bouachir","Linsey Pang","Prakhar Mehrotra","Kun Wang","Qingsong Wen"],"pdf_url":"","comment":"21 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2602.20459v1","updated":"2026-02-24T01:37:53Z","published":"2026-02-24T01:37:53Z","title":"PreScience: A Benchmark for Forecasting Scientific Contributions","summary":"Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.","authors":["Anirudh Ajith","Amanpreet Singh","Jay DeYoung","Nadav Kunievsky","Austin C. Kozlowski","Oyvind Tafjord","James Evans","Daniel S. Weld","Tom Hope","Doug Downey"],"pdf_url":"","comment":"10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm"},{"id":"http://arxiv.org/abs/2602.20449v1","updated":"2026-02-24T01:18:30Z","published":"2026-02-24T01:18:30Z","title":"Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference","summary":"Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.","authors":["Anna Hart","Chi Han","Jeonghwan Kim","Huimin Zhao","Heng Ji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.17364v2","updated":"2026-02-24T00:34:06Z","published":"2025-02-24T17:41:48Z","title":"Bridging Gaps in Natural Language Processing for Yorùbá: A Systematic Review of a Decade of Progress and Prospects","summary":"Natural Language Processing (NLP) is becoming a dominant subset of artificial intelligence as the need to help machines understand human language looks indispensable. Several NLP applications are ubiquitous, partly due to the myriad of datasets being churned out daily through mediums like social networking sites. However, the growing development has not been evident in most African languages due to the persisting resource limitations, among other issues. Yorùbá language, a tonal and morphologically rich African language, suffers a similar fate, resulting in limited NLP usage. To encourage further research towards improving this situation, this systematic literature review aims to comprehensively analyse studies addressing NLP development for Yorùbá, identifying challenges, resources, techniques, and applications. A well-defined search string from a structured protocol was employed to search, select, and analyse 105 primary studies between 2014 and 2024 from reputable databases. The review highlights the scarcity of annotated corpora, the limited availability of pre-trained language models, and linguistic challenges like tonal complexity and diacritic dependency as significant obstacles. It also revealed the prominent techniques, including rule-based methods, among others. The findings reveal a growing body of multilingual and monolingual resources, even though the field is constrained by socio-cultural factors such as code-switching and the desertion of language for digital usage. This review synthesises existing research, providing a foundation for advancing NLP for Yorùbá and in African languages generally. It aims to guide future research by identifying gaps and opportunities, thereby contributing to the broader inclusion of Yorùbá and other under-resourced African languages in global NLP advancements.","authors":["Toheeb Aduramomi Jimoh","Tabea De Wille","Nikola S. Nikolov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20433v1","updated":"2026-02-24T00:31:04Z","published":"2026-02-24T00:31:04Z","title":"Disentangling Geometry, Performance, and Training in Language Models","summary":"Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.","authors":["Atharva Kulkarni","Jacob Mitchell Springer","Arjun Subramonian","Swabha Swayamdipta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.15997v3","updated":"2026-02-24T00:03:59Z","published":"2026-02-17T20:39:02Z","title":"Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks","summary":"Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K--85M parameters), 120 task$\\times$level$\\times$ model combinations (119 achieving accuracy-based emergence) across eight algorithmic tasks, and three Pythia language models (160M--2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210$\\times$ parameter range (e.g., modular arithmetic collapses to RANKME $\\,\\approx\\,$2.0 regardless of model size); (2) collapse propagates top-down through layers (28/32 task$ \\times $model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (100% precursor rate for hard tasks across all model sizes), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance ranges from 52% for easy tasks to 69% for hard tasks; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not, as the precursor relationship requires task--training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.","authors":["Jayadev Billa"],"pdf_url":"","comment":"26 pages, 16 tables, 8 figures, 7 appendix pages. v3: Added causal freeze experiment, hidden learning probing analysis, bootstrap CIs, metric robustness ablation, and expanded discussion"},{"id":"http://arxiv.org/abs/2601.07986v3","updated":"2026-02-24T23:58:59Z","published":"2026-01-12T20:36:30Z","title":"VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding","summary":"We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 at https://github.com/yha9806/VULCA-Bench.","authors":["Haorui Yu","Diji Yang","Hang He","Fengrui Zhang","Qiufeng Yi"],"pdf_url":"","comment":"8 pages, 4 figures, submitted to ACL 2026 Dataset Track"},{"id":"http://arxiv.org/abs/2601.07984v3","updated":"2026-02-24T23:58:43Z","published":"2026-01-12T20:33:35Z","title":"Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models","summary":"Vision-Language Models (VLMs) excel at visual description yet remain under-validated for cultural interpretation. Existing benchmarks assess perception without interpretation, and common evaluation proxies, such as automated metrics and LLM-judge averaging, are unreliable for culturally sensitive generative tasks. We address this measurement gap with a tri-tier evaluation framework grounded in art-theoretical constructs (Section 2). The framework operationalises cultural understanding through five levels (L1--L5) and 165 culture-specific dimensions across six traditions: Tier I computes automated quality indicators, Tier II applies rubric-based single-judge scoring, and Tier III calibrates the aggregate score to human expert ratings via sigmoid calibration. Applied to 15 VLMs across 294 evaluation pairs, the validated instrument reveals that (i) automated metrics and judge scoring measure different constructs, establishing single-judge calibration as the more reliable alternative; (ii) cultural understanding degrades from visual description (L1--L2) to cultural interpretation (L3--L5); and (iii) Western art samples consistently receive higher scores than non-Western ones. To our knowledge, this is the first cross-cultural evaluation instrument for generative art critique, providing a reproducible methodology for auditing VLM cultural competence. Framework code is available at https://github.com/yha9806/VULCA-Framework.","authors":["Haorui Yu","Xuehang Wen","Fengrui Zhang","Qiufeng Yi"],"pdf_url":"","comment":"16 pages, 7 figures, submitted to ACL 2026"},{"id":"http://arxiv.org/abs/2602.21447v1","updated":"2026-02-24T23:52:27Z","published":"2026-02-24T23:52:27Z","title":"Adversarial Intent is a Latent Variable: Stateful Trust Inference for Securing Multimodal Agentic RAG","summary":"Current stateless defences for multimodal agentic RAG fail to detect adversarial strategies that distribute malicious semantics across retrieval, planning, and generation components. We formulate this security challenge as a Partially Observable Markov Decision Process (POMDP), where adversarial intent is a latent variable inferred from noisy multi-stage observations. We introduce MMA-RAG^T, an inference-time control framework governed by a Modular Trust Agent (MTA) that maintains an approximate belief state via structured LLM reasoning. Operating as a model-agnostic overlay, MMA-RAGT mediates a configurable set of internal checkpoints to enforce stateful defence-in-depth. Extensive evaluation on 43,774 instances demonstrates a 6.50x average reduction factor in Attack Success Rate relative to undefended baselines, with negligible utility cost. Crucially, a factorial ablation validates our theoretical bounds: while statefulness and spatial coverage are individually necessary (26.4 pp and 13.6 pp gains respectively), stateless multi-point intervention can yield zero marginal benefit under homogeneous stateless filtering when checkpoint detections are perfectly correlated.","authors":["Inderjeet Singh","Vikas Pahuja","Aishvariya Priya Rathina Sabapathy","Chiara Picardi","Amit Giloni","Roman Vainshtein","Andrés Murillo","Hisashi Kojima","Motoyoshi Sekiya","Yuki Unno","Junichi Suga"],"pdf_url":"","comment":"13 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2509.02452v3","updated":"2026-02-24T23:46:37Z","published":"2025-09-02T16:01:47Z","title":"Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions","summary":"Do LLMs genuinely incorporate external definitions, or do they primarily rely on their parametric knowledge? To address these questions, we conduct controlled experiments across multiple explanation benchmark datasets (general and domain-specific) and label definition conditions, including expert-curated, LLM-generated, perturbed, and swapped definitions. Our results reveal that while explicit label definitions can enhance accuracy and explainability, their integration into an LLM's task-solving processes is neither guaranteed nor consistent, suggesting reliance on internalized representations in many cases. Models often default to their internal representations, particularly in general tasks, whereas domain-specific tasks benefit more from explicit definitions. These findings underscore the need for a deeper understanding of how LLMs process external knowledge alongside their pre-existing capabilities.","authors":["Seyedali Mohammadi","Bhaskara Hanuma Vedula","Hemank Lamba","Edward Raff","Ponnurangam Kumaraguru","Francis Ferraro","Manas Gaur"],"pdf_url":"","comment":"EMNLP 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2506.07452v3","updated":"2026-02-24T23:22:40Z","published":"2025-06-09T05:57:39Z","title":"When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment","summary":"Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in malicious queries. Prior jailbreak research mainly augments these queries with additional string transformations to maximize attack success rate (ASR). However, the impact of style patterns in the original queries that are semantically irrelevant to the malicious intent remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We first define ASR inflation as the increase in ASR due to style patterns in existing jailbreak benchmark queries. By evaluating 36 LLMs across seven benchmarks, we find that nearly all models exhibit ASR inflation. Notably, the inflation correlates with an LLM's relative attention to style patterns, which also overlap more with its instruction-tuning data when inflation occurs. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs, six fine-tuning style settings, and two real-world instruction-tuning datasets, SafeStyle consistently outperforms baselines in maintaining LLM safety.","authors":["Yuxin Xiao","Sana Tonekaboni","Walter Gerych","Vinith Suriyakumar","Marzyeh Ghassemi"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2511.20718v2","updated":"2026-02-24T23:08:24Z","published":"2025-11-25T05:54:02Z","title":"Stabilizing Off-Policy Training for Long-Horizon LLM Agent via Turn-Level Importance Sampling and Clipping-Triggered Normalization","summary":"Reinforcement learning (RL) algorithms such as PPO and GRPO are widely used to train large language models (LLMs) for multi-turn agentic tasks. However, in off-policy training pipelines, these methods often exhibit unstable optimization dynamics and are prone to performance collapse. Through empirical analysis, we identify two fundamental sources of instability in this setting: (1)~a granularity mismatch between token-level policy optimization and turn-structured interactions, and (2) high-variance and unreliable gradient updates induced by off-policy importance sampling and inaccurate advantage estimation. To address these challenges, we propose SORL, \\underline{S}tabilizing \\underline{O}ff-Policy \\underline{R}einforcement \\underline{L}earning for Long-Horizon Agent Training. SORL introduces principled mechanisms that align policy optimization with the structure of multi-turn interactions and adaptively suppress unreliable off-policy updates, yielding more conservative and robust learning dynamics. Within this framework, we instantiate two stabilized algorithms: SO-PPO and SO-GRPO. Both algorithms are designed to mitigate gradient variance and prevent optimization collapse without requiring careful early stopping or heuristic tuning. We evaluate SO-PPO and SO-GRPO on a range of multi-turn search benchmarks, including general question answering, multi-hop question answering, and medical multiple-choice QA tasks. Experimental results show that both methods consistently prevent training instabilities and performance collapses observed in standard PPO and GRPO, maintain lower clipping ratios and more stable optimization trajectories, and achieve superior or comparable task performance. These results demonstrate that the proposed algorithm provides a practical, scalable, and general framework for stabilizing reinforcement learning in multi-turn LLM agent training.","authors":["Chenliang Li","Adel Elmahdy","Alex Boyd","Zhongruo Wang","Siliang Zeng","Alfredo Garcia","Parminder Bhatia","Taha Kass-Hout","Cao Xiao","Mingyi Hong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.21841v2","updated":"2026-02-24T22:41:14Z","published":"2026-01-29T15:18:58Z","title":"Embodied Task Planning via Graph-Informed Action Generation with Large Language Model","summary":"While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.","authors":["Xiang Li","Ning Yan","Masood Mortazavi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.07658v2","updated":"2026-02-24T22:17:16Z","published":"2025-06-09T11:30:12Z","title":"From Raw Corpora to Domain Benchmarks: Automated Evaluation of LLM Domain Expertise","summary":"Accurate domain-specific benchmarking of LLMs is essential, specifically in domains with direct implications for humans, such as law, healthcare, and education. However, existing benchmarks are documented to be contaminated and are based on multiple choice questions, which suffer from inherent biases. To measure domain-specific knowledge in LLMs, we present a deterministic pipeline that transforms raw domain corpora into completion-style benchmarks without relying on other LLMs or costly human annotation. Our approach first extracts domain-specific keywords and related target vocabulary from an input corpus. It then constructs prompt-target pairs where domain-specific words serve as prediction targets. By measuring LLMs' ability to complete these prompts, we provide a direct assessment of domain knowledge at low computational cost. Our pipeline avoids benchmark contamination, enables automated updates with new domain data, and facilitates fair comparisons between base and instruction-tuned (chat) models.\n  We validate our approach by showing that model performances on our benchmark significantly correlate with those on an expert-curated benchmark. We then demonstrate how our benchmark provides insights into knowledge acquisition in domain-adaptive, continual, and general pretraining. Finally, we examine the effects of instruction fine-tuning by comparing base and chat models within our unified evaluation framework. In conclusion, our pipeline enables scalable, domain-specific, LLM-independent, and unbiased evaluation of both base and chat models.","authors":["Nitin Sharma","Thomas Wolfers","Çağatay Yıldız"],"pdf_url":"","comment":"35 pages, 24 figures. Second version"},{"id":"http://arxiv.org/abs/2511.01734v3","updated":"2026-02-24T21:43:37Z","published":"2025-11-03T16:45:47Z","title":"A Proof of Learning Rate Transfer under $μ$P","summary":"We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $μ$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $μP$, the optimal learning rate converges to a \\emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.","authors":["Soufiane Hayou"],"pdf_url":"","comment":"21 pages"},{"id":"http://arxiv.org/abs/2602.21379v1","updated":"2026-02-24T21:19:40Z","published":"2026-02-24T21:19:40Z","title":"MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation","summary":"We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance across specialized biomedical and legal domains. To bridge the gap between research and production, we incorporate Matryoshka Representation Learning (MRL), enabling flexible vector sizing that significantly reduces inference and storage costs. Ultimately, the MrBERT family demonstrates that modern encoder architectures can be optimized for both localized linguistic excellence and efficient, high-stakes domain specialization. We open source the complete model family on Huggingface.","authors":["Daniel Tamayo","Iñaki Lacunza","Paula Rivera-Hidalgo","Severino Da Dalt","Javier Aula-Blasco","Aitor Gonzalez-Agirre","Marta Villegas"],"pdf_url":"","comment":"24 pages, 14 tables and 4 figures"},{"id":"http://arxiv.org/abs/2602.21377v1","updated":"2026-02-24T21:16:08Z","published":"2026-02-24T21:16:08Z","title":"Beyond Subtokens: A Rich Character Embedding for Low-resource and Morphologically Complex Languages","summary":"Tokenization and sub-tokenization based models like word2vec, BERT and the GPTs are the state-of-the-art in natural language processing. Typically, these approaches have limitations with respect to their input representation. They fail to fully capture orthographic similarities and morphological variations, especially in highly inflected and under-resource languages. To mitigate this problem, we propose to computes word vectors directly from character strings, integrating both semantic and syntactic information. We denote this transformer-based approach Rich Character Embeddings (RCE). Furthermore, we propose a hybrid model that combines transformer and convolutional mechanisms. Both vector representations can be used as a drop-in replacement for dictionary- and subtoken-based word embeddings in existing model architectures. It has the potential to improve performance for both large context-based language models like BERT and small models like word2vec for under-resourced and morphologically rich languages. We evaluate our approach on various tasks like the SWAG, declension prediction for inflected languages, metaphor and chiasmus detection for various languages. Our experiments show that it outperforms traditional token-based approaches on limited data using OddOneOut and TopK metrics.","authors":["Felix Schneider","Maria Gogolev","Sven Sickert","Joachim Denzler"],"pdf_url":"","comment":"12 content pages, 2 figures, 8 tables, one example textbox"},{"id":"http://arxiv.org/abs/2602.21374v1","updated":"2026-02-24T21:10:29Z","published":"2026-02-24T21:10:29Z","title":"Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages","summary":"Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.","authors":["Mohammadreza Ghaffarzadeh-Esfahani","Nahid Yousefian","Ebrahim Heidari-Farsani","Ali Akbar Omidvarian","Sepehr Ghahraei","Atena Farangi","AmirBahador Boroumand"],"pdf_url":"","comment":"16 pages, 3 figures, 2 supplementary files"},{"id":"http://arxiv.org/abs/2602.21368v1","updated":"2026-02-24T21:03:50Z","published":"2026-02-24T21:03:50Z","title":"Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration","summary":"Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with exact, finite-sample, distribution-free guarantees. Self-consistency sampling reduces uncertainty exponentially; conformal calibration guarantees correctness within 1/(n+1) of the target level, regardless of the system's errors -- made transparently visible through larger answer sets for harder questions. Weaker models earn lower reliability levels (not accuracy -- see Definition 2.4): GPT-4.1 earns 94.6% on GSM8K and 96.8% on TruthfulQA, while GPT-4.1-nano earns 89.8% on GSM8K and 66.5% on MMLU. We validate across five benchmarks, five models from three families, and both synthetic and real data. Conditional coverage on solvable items exceeds 0.93 across all configurations; sequential stopping reduces API costs by around 50%.","authors":["Charafeddine Mouzouni"],"pdf_url":"","comment":"41 pages, 11 figures, 10 tables, including appendices"},{"id":"http://arxiv.org/abs/2602.17784v2","updated":"2026-02-24T20:32:43Z","published":"2026-02-19T19:31:37Z","title":"QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration","summary":"Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.","authors":["Meng Ye","Xiao Lin","Georgina Lukoczki","Graham W. Lederer","Yi Yao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21346v1","updated":"2026-02-24T20:30:51Z","published":"2026-02-24T20:30:51Z","title":"Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment","summary":"Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.","authors":["Mengxuan Hu","Vivek V. Datla","Anoop Kumar","Zihan Guan","Sheng Li","Alfy Samuel","Daben Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.04517v2","updated":"2026-02-24T19:51:44Z","published":"2025-07-06T19:49:46Z","title":"DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging","summary":"Structured pruning methods designed for Large Language Models (LLMs) generally focus on identifying and removing the least important components to optimize model size. However, in this work, we question this prevalent approach by instead exploring how to recombine information from structures designated for pruning back into the reduced model. We specifically focus on neuron width reduction, and frame this problem as a Discrete Optimal Transport problem, and propose DOTResize, a novel Transformer compression method that uses optimal transport theory to transform and compress model width. To ensure applicability within the Transformer architecture, we motivate and incorporate necessary entropic regularization and matrix factorization techniques into the transportation maps produced by our method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTResize re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTResize serves as a useful add-on to pruning, while achieving measurable reductions in real-world computational cost.","authors":["Neha Verma","Kenton Murray","Kevin Duh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19878v2","updated":"2026-02-24T19:25:34Z","published":"2026-02-23T14:24:46Z","title":"Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics","summary":"Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic.\n  We classify ODRL's left operands by value-domain structure (scalar, dimensional, concept-valued), grounded in the ODRL 2.2 specification text, and show that dimensional ambiguity is intrinsic to the constraint syntax. We present an axis-decomposition framework that refines each dimensional operand into axis-specific scalar operands and prove four properties: deterministic interpretation, AABB completeness, projection soundness, and conservative extension.\n  Conflict detection operates in two layers: per-axis verdicts are always decidable; box-level verdicts compose through Strong Kleene conjunction into a three-valued logic (Conflict, Compatible, Unknown). For ODRL's disjunctive (odrl:or) and exclusive-or (odrl:xone) logical constraints, where per-axis decomposition does not apply, the framework encodes coupled multi-axis conjectures directly.\n  We instantiate the framework as the ODRL Spatial Axis Profile--15 axis-specific left operands for the five affected base terms--and evaluate it on 117 benchmark problems spanning nine categories across both TPTP FOF (Vampire) and SMT-LIB (Z3) encodings, achieving full concordance between provers. Benchmark scenarios are inspired by constraints arising in cultural heritage dataspaces such as Datenraum Kultur. All meta-theorems are mechanically verified in Isabelle/HOL.","authors":["Daham Mustafa","Diego Collarana","Yixin Peng","Rafiqul Haque","Christoph Lange-Bever","Christoph Quix","Stephan Decker"],"pdf_url":"","comment":"16 pages, 5 tables. Preprint. v2: corrected projection soundness property; clarified verdict mapping table"},{"id":"http://arxiv.org/abs/2506.09886v2","updated":"2026-02-24T19:13:55Z","published":"2025-06-11T15:59:15Z","title":"Probabilistic distances-based hallucination detection in LLMs with RAG","summary":"Detecting hallucinations in large language models (LLMs) is critical for their safety in many applications. Without proper detection, these systems often provide harmful, unreliable answers. In recent years, LLMs have been actively used in retrieval-augmented generation (RAG) settings. However, hallucinations remain even in this setting, and while numerous hallucination detection methods have been proposed, most approaches are not specifically designed for RAG systems. To overcome this limitation, we introduce a hallucination detection method based on estimating the distances between the distributions of prompt token embeddings and language model response token embeddings. The method examines the geometric structure of token hidden states to reliably extract a signal of factuality in text, while remaining friendly to long sequences. Extensive experiments demonstrate that our method achieves state-of-the-art or competitive performance. It also has transferability from solving the NLI task to the hallucination detection task, making it a fully unsupervised and efficient method with a competitive performance on the final task.","authors":["Rodion Oblovatny","Alexandra Kuleshova","Konstantin Polev","Alexey Zaytsev"],"pdf_url":"","comment":"Updated approach to constructing a hallucination detection score. Added results from experiments with the NLI task. The approach with trainable deep kernels has been removed, with a focus on the unsupervised approach"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2602.21204v1","updated":"2026-02-24T18:59:30Z","published":"2026-02-24T18:59:30Z","title":"Test-Time Training with KV Binding Is Secretly Linear Attention","summary":"Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.","authors":["Junchen Liu","Sven Elflein","Or Litany","Zan Gojcic","Ruilong Li"],"pdf_url":"","comment":"Webpage: https://research.nvidia.com/labs/sil/projects/tttla/"},{"id":"http://arxiv.org/abs/2602.21203v1","updated":"2026-02-24T18:58:11Z","published":"2026-02-24T18:58:11Z","title":"Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics","summary":"Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.","authors":["Abdulaziz Almuzairee","Henrik I. Christensen"],"pdf_url":"","comment":"For website and code, see https://aalmuzairee.github.io/squint"},{"id":"http://arxiv.org/abs/2602.21202v1","updated":"2026-02-24T18:57:33Z","published":"2026-02-24T18:57:33Z","title":"Multi-Vector Index Compression in Any Modality","summary":"We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.","authors":["Hanxiang Qin","Alexander Martin","Rohan Jha","Chunsheng Zuo","Reno Kriz","Benjamin Van Durme"],"pdf_url":"","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.21198v1","updated":"2026-02-24T18:55:18Z","published":"2026-02-24T18:55:18Z","title":"Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs","summary":"Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.","authors":["Yining Hong","Huang Huang","Manling Li","Li Fei-Fei","Jiajun Wu","Yejin Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21195v1","updated":"2026-02-24T18:53:33Z","published":"2026-02-24T18:53:33Z","title":"Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography","summary":"Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.","authors":["Xingyi Cheng","Julien Maufront","Aurélie Di Cicco","Daniël M. Pelt","Manuela Dezi","Daniel Lévy"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21188v1","updated":"2026-02-24T18:42:20Z","published":"2026-02-24T18:42:20Z","title":"Human Video Generation from a Single Image with 3D Pose and View Control","summary":"Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.","authors":["Tiantian Wang","Chun-Han Yao","Tao Hu","Mallikarjun Byrasandra Ramalinga Reddy","Ming-Hsuan Yang","Varun Jampani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21186v1","updated":"2026-02-24T18:37:34Z","published":"2026-02-24T18:37:34Z","title":"Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning","summary":"While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.","authors":["Haoyi Jiang","Liu Liu","Xinjie Wang","Yonghao He","Wei Sui","Zhizhong Su","Wenyu Liu","Xinggang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21179v1","updated":"2026-02-24T18:29:13Z","published":"2026-02-24T18:29:13Z","title":"Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision","summary":"Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.","authors":["Nicolás Gaggion","Maria J. Ledesma-Carbayo","Stergios Christodoulidis","Maria Vakalopoulou","Enzo Ferrante"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21178v1","updated":"2026-02-24T18:28:08Z","published":"2026-02-24T18:28:08Z","title":"XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence","summary":"Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.","authors":["Sepehr Salem Ghahfarokhi","M. Moein Esfahani","Raj Sunderraman","Vince Calhoun","Mohammed Alser"],"pdf_url":"","comment":"Accepted in ICCABS 2026: The 14th International Conference on Computational Advances in Bio and Medical Sciences"},{"id":"http://arxiv.org/abs/2602.21175v1","updated":"2026-02-24T18:20:57Z","published":"2026-02-24T18:20:57Z","title":"Seeing Through Words: Controlling Visual Retrieval Quality with Language Models","summary":"Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.","authors":["Jianglin Lu","Simon Jenni","Kushal Kafle","Jing Shi","Handong Zhao","Yun Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21172v1","updated":"2026-02-24T18:17:21Z","published":"2026-02-24T18:17:21Z","title":"NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning","summary":"Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no reasoning annotations, resulting in 3$\\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \\modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \\modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.","authors":["Ishaan Rawal","Shubh Gupta","Yihan Hu","Wei Zhan"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20159v2","updated":"2026-02-24T17:59:15Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2602.21153v1","updated":"2026-02-24T17:58:31Z","published":"2026-02-24T17:58:31Z","title":"SPRITETOMESH: Automatic Mesh Generation for 2D Skeletal Animation Using Learned Segmentation and Contour-Aware Vertex Placement","summary":"We present SPRITETOMESH, a fully automatic pipeline for converting 2D game sprite images into triangle meshes compatible with skeletal animation frameworks such as Spine2D. Creating animation-ready meshes is traditionally a tedious manual process requiring artists to carefully place vertices along visual boundaries, a task that typically takes 15-60 minutes per sprite. Our method addresses this through a hybrid learned-algorithmic approach. A segmentation network (EfficientNet-B0 encoder with U-Net decoder) trained on over 100,000 sprite-mask pairs from 172 games achieves an IoU of 0.87, providing accurate binary masks from arbitrary input images. From these masks, we extract exterior contour vertices using Douglas-Peucker simplification with adaptive arc subdivision, and interior vertices along visual boundaries detected via bilateral-filtered multi-channel Canny edge detection with contour-following placement. Delaunay triangulation with mask-based centroid filtering produces the final mesh. Through controlled experiments, we demonstrate that direct vertex position prediction via neural network heatmap regression is fundamentally not viable for this task: the heatmap decoder consistently fails to converge (loss plateau at 0.061) while the segmentation decoder trains normally under identical conditions. We attribute this to the inherently artistic nature of vertex placement - the same sprite can be meshed validly in many different ways. This negative result validates our hybrid design: learned segmentation where ground truth is unambiguous, algorithmic placement where domain heuristics are appropriate. The complete pipeline processes a sprite in under 3 seconds, representing a speedup of 300x-1200x over manual creation. We release our trained model to the game development community.","authors":["Bastien Gimbert"],"pdf_url":"","comment":"11 pages, 17 figures. Code available at https://github.com/BastienGimbert/SpriteToMesh"},{"id":"http://arxiv.org/abs/2602.21142v1","updated":"2026-02-24T17:42:46Z","published":"2026-02-24T17:42:46Z","title":"LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis","summary":"Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.","authors":["Zhifan Jiang","Dong Yang","Vishwesh Nath","Abhijeet Parida","Nishad P. Kulkarni","Ziyue Xu","Daguang Xu","Syed Muhammad Anwar","Holger R. Roth","Marius George Linguraru"],"pdf_url":"","comment":"Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2026"},{"id":"http://arxiv.org/abs/2602.21141v1","updated":"2026-02-24T17:42:34Z","published":"2026-02-24T17:42:34Z","title":"SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception","summary":"Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. Furthermore, we benchmark recent Reality-to-Simulation techniques for 3D asset creation from 2D images of real parts. Combined with Domain Randomization, these synthetic assets provide low-overhead, transferable data even for parts lacking 3D files. We also introduce IRIS, the Industrial Real-Sim Imagery Set, containing 32 categories with diverse textures, intra-class variation, strong inter-class similarities and about 20,000 labels. Ablations on multiple benchmarks outline guidelines for efficient data generation with SynthRender. Our method surpasses existing approaches, achieving 99.1% mAP@50 on a public robotics dataset, 98.3% mAP@50 on an automotive benchmark, and 95.3% mAP@50 on IRIS.","authors":["Jose Moises Araya-Martinez","Thushar Tom","Adrián Sanchis Reig","Pablo Rey Valiente","Jens Lambrecht","Jörg Krüger"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22129v3","updated":"2026-02-24T17:38:14Z","published":"2025-10-25T03:04:51Z","title":"egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks","summary":"Understanding affect is central to anticipating human behavior, yet current egocentric vision benchmarks largely ignore the person's emotional states that shape their decisions and actions. Existing tasks in egocentric perception focus on physical activities, hand-object interactions, and attention modeling - assuming neutral affect and uniform personality. This limits the ability of vision systems to capture key internal drivers of behavior. In this paper, we present egoEMOTION, the first dataset that couples egocentric visual and physiological signals with dense self-reports of emotion and personality across controlled and real-world scenarios. Our dataset includes over 50 hours of recordings from 43 participants, captured using Meta's Project Aria glasses. Each session provides synchronized eye-tracking video, headmounted photoplethysmography, inertial motion data, and physiological baselines for reference. Participants completed emotion-elicitation tasks and naturalistic activities while self-reporting their affective state using the Circumplex Model and Mikels' Wheel as well as their personality via the Big Five model. We define three benchmark tasks: (1) continuous affect classification (valence, arousal, dominance); (2) discrete emotion classification; and (3) trait-level personality inference. We show that a classical learning-based method, as a simple baseline in real-world affect prediction, produces better estimates from signals captured on egocentric vision systems than processing physiological signals. Our dataset establishes emotion and personality as core dimensions in egocentric perception and opens new directions in affect-driven modeling of behavior, intent, and interaction.","authors":["Matthias Jammot","Björn Braun","Paul Streli","Rafael Wampfler","Christian Holz"],"pdf_url":"","comment":"Accepted for publication at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.21137v1","updated":"2026-02-24T17:33:12Z","published":"2026-02-24T17:33:12Z","title":"UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics","summary":"Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.","authors":["Joseph Raj Vishal","Nagasiri Poluri","Katha Naik","Rutuja Patil","Kashyap Hegde Kota","Krishna Vinod","Prithvi Jai Ramesh","Mohammad Farhadi","Yezhou Yang","Bharatesh Chakravarthi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21105v1","updated":"2026-02-24T17:03:45Z","published":"2026-02-24T17:03:45Z","title":"BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting","summary":"The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.","authors":["Jiaxing Yu","Dongyang Ren","Hangyu Xu","Zhouyuxiao Yang","Yuanqi Li","Jie Guo","Zhengkang Zhou","Yanwen Guo"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21101v1","updated":"2026-02-24T17:02:56Z","published":"2026-02-24T17:02:56Z","title":"Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones","summary":"Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.","authors":["Rong Zou","Marco Cannici","Davide Scaramuzza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21100v1","updated":"2026-02-24T17:02:11Z","published":"2026-02-24T17:02:11Z","title":"Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction","summary":"Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.","authors":["Noé Artru","Rukhshanda Hussain","Emeline Got","Alexandre Messier","David B. Lindell","Abdallah Dib"],"pdf_url":"","comment":"14 pages, 8 figures, to be published in proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2602.21098v1","updated":"2026-02-24T17:01:36Z","published":"2026-02-24T17:01:36Z","title":"Optimizing Occupancy Sensor Placement in Smart Environments","summary":"Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on careful sensor placement. To address this issue, we propose an automatic sensor placement method that determines optimal sensor layouts for a given number of sensors, and can predict the counting accuracy of such a layout. In particular, given the geometric constraints of an office environment, we simulate a large number of occupant trajectories. We then formulate the sensor placement problem as an integer linear programming (ILP) problem and solve it with the branch and bound method. We demonstrate the effectiveness of the proposed method based on simulations of several different office environments.","authors":["Hao Lu","Richard J. Radke"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21078v1","updated":"2026-02-24T16:41:16Z","published":"2026-02-24T16:41:16Z","title":"ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning","summary":"Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.","authors":["Duowen Chen","Yan Wang"],"pdf_url":"","comment":"CVPR 2026. code: https://github.com/DuowenC/FSSLlib"},{"id":"http://arxiv.org/abs/2602.21064v1","updated":"2026-02-24T16:26:52Z","published":"2026-02-24T16:26:52Z","title":"Motivation is Something You Need","summary":"This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.","authors":["Mehdi Acheli","Walid Gaaloul"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21054v1","updated":"2026-02-24T16:11:14Z","published":"2026-02-24T16:11:14Z","title":"VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation","summary":"Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.","authors":["Seongheon Park","Changdae Oh","Hyeong Kyu Choi","Xuefeng Du","Sharon Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21053v1","updated":"2026-02-24T16:10:27Z","published":"2026-02-24T16:10:27Z","title":"OCR-Agent: Agentic OCR with Capability and Memory Reflection","summary":"Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.","authors":["Shimin Wen","Zeyu Zhang","Xingdou Bian","Hongjie Zhu","Lulu He","Layi Shama","Daji Ergu","Ying Cai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21042v1","updated":"2026-02-24T16:02:49Z","published":"2026-02-24T16:02:49Z","title":"OmniOCR: Generalist OCR for Ethnic Minority Languages","summary":"Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.","authors":["Bonan Liu","Zeyu Zhang","Bingbing Meng","Han Wang","Hanshuo Zhang","Chengping Wang","Daji Ergu","Ying Cai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21035v1","updated":"2026-02-24T15:55:39Z","published":"2026-02-24T15:55:39Z","title":"Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning","summary":"Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.","authors":["Junhao Xiao","Zhiyu Wu","Hao Lin","Yi Chen","Yahui Liu","Xiaoran Zhao","Zixu Wang","Zejiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21033v1","updated":"2026-02-24T15:55:04Z","published":"2026-02-24T15:55:04Z","title":"MIP Candy: A Modular PyTorch Framework for Medical Image Processing","summary":"Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.","authors":["Tianhao Fu","Yucheng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.21785v3","updated":"2026-02-24T15:49:40Z","published":"2025-08-29T17:03:05Z","title":"Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling","summary":"Heart rate prediction is vital for personalized health monitoring and fitness, while it frequently faces a critical challenge in real-world deployment: data heterogeneity. We classify it in two key dimensions: source heterogeneity from fragmented device markets with varying feature sets, and user heterogeneity reflecting distinct physiological patterns across individuals and activities. Existing methods either discard device-specific information, or fail to model user-specific differences, limiting their real-world performance. To address this, we propose a framework that learns latent representations agnostic to both heterogeneity,enabling downstream predictors to work consistently under heterogeneous data patterns. Specifically, we introduce a random feature dropout strategy to handle source heterogeneity, making the model robust to various feature sets. To manage user heterogeneity, we employ a history-aware attention module to capture long-term physiological traits and use a contrastive learning objective to build a discriminative representation space. To reflect the heterogeneous nature of real-world data, we created a new benchmark dataset, PARROTAO. Evaluations on both PARROTAO and the public FitRec dataset show that our model significantly outperforms existing baselines by 17.5% and 10.4% in terms of test MSE, respectively. Furthermore, analysis of the learned representations demonstrates their strong discriminative power,and two downstream application tasks confirm the practical value of our model.","authors":["Zhengdong Huang","Zicheng Xie","Wentao Tian","Jingyu Liu","Lunhong Dong","Peng Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21015v1","updated":"2026-02-24T15:33:02Z","published":"2026-02-24T15:33:02Z","title":"From Perception to Action: An Interactive Benchmark for Vision Reasoning","summary":"Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.","authors":["Yuhao Wu","Maojia Song","Yihuai Lan","Lei Wang","Zhiqiang Hu","Yao Xiao","Heng Zhou","Weihua Zheng","Dylan Raharja","Soujanya Poria","Roy Ka-Wei Lee"],"pdf_url":"","comment":"Work in processing. Website: https://social-ai-studio.github.io/CHAIN/"},{"id":"http://arxiv.org/abs/2602.21010v1","updated":"2026-02-24T15:29:55Z","published":"2026-02-24T15:29:55Z","title":"Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design","summary":"Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\\textbf{L}ow-cost and \\textbf{E}fficient \\textbf{DE}tection \\textbf{TR}ansformer), which achieves a new \\textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \\textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \\textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \\textbf{+0.6/-0.1 mAP} while achieving similar speed and \\textbf{+20\\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \\textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \\textbf{+0.4 mAP} with only \\textbf{0.4 ms} additional latency. Code and weights will be open-sourced.","authors":["Jiannan Huang","Aditya Kane","Fengzhe Zhou","Yunchao Wei","Humphrey Shi"],"pdf_url":"","comment":"CVPR Findings"},{"id":"http://arxiv.org/abs/2507.04002v2","updated":"2026-02-24T15:20:46Z","published":"2025-07-05T11:05:43Z","title":"NRSeg: Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models","summary":"Birds' Eye View (BEV) semantic segmentation is an indispensable perception task in end-to-end autonomous driving systems. Unsupervised and semi-supervised learning for BEV tasks, as pivotal for real-world applications, underperform due to the homogeneous distribution of the labeled data. In this work, we explore the potential of synthetic data from driving world models to enhance the diversity of labeled data for robustifying BEV segmentation. Yet, our preliminary findings reveal that generation noise in synthetic data compromises efficient BEV model learning. To fully harness the potential of synthetic data from world models, this paper proposes NRSeg, a noise-resilient learning framework for BEV semantic segmentation. Specifically, a Perspective-Geometry Consistency Metric (PGCM) is proposed to quantitatively evaluate the guidance capability of generated data for model learning. This metric originates from the alignment measure between the perspective road mask of generated data and the mask projected from the BEV labels. Moreover, a Bi-Distribution Parallel Prediction (BiDPP) is designed to enhance the inherent robustness of the model, where the learning process is constrained through parallel prediction of multinomial and Dirichlet distributions. The former efficiently predicts semantic probabilities, whereas the latter adopts evidential deep learning to realize uncertainty quantification. Furthermore, a Hierarchical Local Semantic Exclusion (HLSE) module is designed to address the non-mutual exclusivity inherent in BEV semantic segmentation tasks. Experimental results demonstrate that NRSeg achieves state-of-the-art performance, yielding the highest improvements in mIoU of 13.8% and 11.4% in unsupervised and semi-supervised BEV segmentation tasks, respectively. The source code will be made publicly available at https://github.com/lynn-yu/NRSeg.","authors":["Siyu Li","Fei Teng","Yihong Cao","Kailun Yang","Zhiyong Li","Yaonan Wang"],"pdf_url":"","comment":"Accepted to IEEE Transactions on Image Processing (TIP). The source code will be made publicly available at https://github.com/lynn-yu/NRSeg"},{"id":"http://arxiv.org/abs/2602.20999v1","updated":"2026-02-24T15:20:01Z","published":"2026-02-24T15:20:01Z","title":"VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models","summary":"Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.","authors":["Bowen Zheng","Yongli Xiang","Ziming Hong","Zerong Lin","Chaojian Yu","Tongliang Liu","Xinge You"],"pdf_url":"","comment":"Project page: https://Zbwwwwwwww.github.io/VII"},{"id":"http://arxiv.org/abs/2602.09082v2","updated":"2026-02-24T15:17:55Z","published":"2026-02-09T18:43:40Z","title":"UI-Venus-1.5 Technical Report","summary":"GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging. In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications. The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios. Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus","authors":[" Venus Team","Changlong Gao","Zhangxuan Gu","Yulin Liu","Xinyu Qiu","Shuheng Shen","Yue Wen","Tianyu Xia","Zhenyu Xu","Zhengwen Zeng","Beitong Zhou","Xingran Zhou","Weizhi Chen","Sunhao Dai","Jingya Dou","Yichen Gong","Yuan Guo","Zhenlin Guo","Feng Li","Qian Li","Jinzhen Lin","Yuqi Zhou","Linchao Zhu","Liang Chen","Zhenyu Guo","Changhua Meng","Weiqiang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20994v1","updated":"2026-02-24T15:14:04Z","published":"2026-02-24T15:14:04Z","title":"Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures","summary":"Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.","authors":["Yubin Ge","Yongsong Huang","Xiaofeng Liu"],"pdf_url":"","comment":"IEEE International Symposium on Biomedical Imaging (ISBI) 2026"},{"id":"http://arxiv.org/abs/2602.20989v1","updated":"2026-02-24T15:10:31Z","published":"2026-02-24T15:10:31Z","title":"Cycle-Consistent Tuning for Layered Image Decomposition","summary":"Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.","authors":["Zheng Gu","Min Lu","Zhida Sun","Dani Lischinski","Daniel Cohen-O","Hui Huang"],"pdf_url":"","comment":"Accepted to CVPR 2026. Project page: https://vcc.tech/research/2026/ImgDecom"},{"id":"http://arxiv.org/abs/2602.20985v1","updated":"2026-02-24T15:06:04Z","published":"2026-02-24T15:06:04Z","title":"EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer","summary":"Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.","authors":["Munish Monga","Vishal Chudasama","Pankaj Wasnik","C. V. Jawahar"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2510.24332v2","updated":"2026-02-24T15:03:17Z","published":"2025-10-28T11:55:45Z","title":"Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes","summary":"Purpose: Surgical scene understanding is key to advancing computer-aided and intelligent surgical systems. Current approaches predominantly rely on visual data or end-to-end learning, which limits fine-grained contextual modeling. This work aims to enhance surgical scene representations by integrating 3D acoustic information, enabling temporally and spatially aware multimodal understanding of surgical environments.\n  Methods: We propose a novel framework for generating 4D audio-visual representations of surgical scenes by projecting acoustic localization information from a phased microphone array onto dynamic point clouds from an RGB-D camera. A transformer-based acoustic event detection module identifies relevant temporal segments containing tool-tissue interactions which are spatially localized in the audio-visual scene representation. The system was experimentally evaluated in a realistic operating room setup during simulated surgical procedures performed by experts.\n  Results: The proposed method successfully localizes surgical acoustic events in 3D space and associates them with visual scene elements. Experimental evaluation demonstrates accurate spatial sound localization and robust fusion of multimodal data, providing a comprehensive, dynamic representation of surgical activity.\n  Conclusion: This work introduces the first approach for spatial sound localization in dynamic surgical scenes, marking a significant advancement toward multimodal surgical scene representations. By integrating acoustic and visual data, the proposed framework enables richer contextual understanding and provides a foundation for future intelligent and autonomous surgical systems.","authors":["Jonas Hein","Lazaros Vlachopoulos","Maurits Geert Laurent Olthof","Bastian Sigrist","Philipp Fürnstahl","Matthias Seibold"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20981v1","updated":"2026-02-24T15:01:39Z","published":"2026-02-24T15:01:39Z","title":"Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models","summary":"Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.","authors":["Christian Simon","MAsato Ishii","Wei-Yao Wang","Koichi Saito","Akio Hayakawa","Dongseok Shim","Zhi Zhong","Shuyang Cui","Shusuke Takahashi","Takashi Shibuya","Yuki Mitsufuji"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20980v1","updated":"2026-02-24T15:01:30Z","published":"2026-02-24T15:01:30Z","title":"CrystaL: Spontaneous Emergence of Visual Latents in MLLMs","summary":"Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.","authors":["Yang Zhang","Danyang Li","Yuxuan Li","Xin Zhang","Tianyu Xie","Mingming Cheng","Xiang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.18632v2","updated":"2026-02-24T15:00:15Z","published":"2025-08-26T03:18:25Z","title":"Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction","summary":"Cancer survival analysis commonly integrates information across diverse medical modalities to make survival-time predictions. Existing methods primarily focus on extracting different decoupled features of modalities and performing fusion operations such as concatenation, attention, and \\revm{Mixture-of-Experts (MoE)-based fusion. However, these methods still face two key challenges: i) Fixed fusion schemes (concatenation and attention) can lead to model over-reliance on predefined feature combinations, limiting the dynamic fusion of decoupled features; ii) in MoE-based fusion methods, each expert network handles separate decoupled features, which limits information interaction among the decoupled features. To address these challenges, we propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which devises a random feature reorganization strategy between modalities decoupling and dynamic MoE fusion modules.Its advantages are: i) it increases the diversity of feature combinations and granularity, enhancing the generalization ability of the subsequent expert networks; ii) it overcomes the problem of information closure and helps expert networks better capture information among decoupled features. Additionally, we incorporate a regional cross-attention network within the modality decoupling module to improve the representation quality of decoupled features. Extensive experimental results on our in-house Liver Cancer (LC) and three widely used TCGA public datasets confirm the effectiveness of our proposed method. Codes are available at https://github.com/ZJUMAI/DeReF.","authors":["Huayi Wang","Haochao Ying","Yuyang Xu","Qibo Qiu","Cheng Zhang","Danny Z. Chen","Ying Sun","Jian Wu"],"pdf_url":"","comment":"13 pages"},{"id":"http://arxiv.org/abs/2602.20972v1","updated":"2026-02-24T14:53:16Z","published":"2026-02-24T14:53:16Z","title":"Are Multimodal Large Language Models Good Annotators for Image Tagging?","summary":"Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\\% to 80\\% of human performance, while achieving over 90\\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\\% to 80\\% of the difference.","authors":["Ming-Kun Xie","Jia-Hao Xiao","Zhiqiang Kou","Zhongnian Li","Gang Niu","Masashi Sugiyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.06878v2","updated":"2026-02-24T14:38:10Z","published":"2025-08-09T08:17:37Z","title":"Seeing Through the Noise: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective","summary":"Infrared small target detection and segmentation (IRSTDS) is a critical yet challenging task in defense and civilian applications, owing to the dim, shapeless appearance of targets and severe background clutter. Recent CNN-based methods have achieved promising target perception results, but they only focus on enhancing feature representation to offset the impact of noise, which results in the increased false alarm problem. In this paper, through analyzing the problem from the frequency domain, we pioneer in improving performance from noise suppression perspective and propose a novel noise-suppression feature pyramid network (NS-FPN), which integrates a low-frequency guided feature purification (LFP) module and a spiral-aware feature sampling (SFS) module into the original FPN structure. The LFP module suppresses the noise features by purifying high-frequency components to achieve feature enhancement devoid of noise interference, while the SFS module further adopts spiral sampling to fuse target-relevant features in feature fusion process. Our NS-FPN is designed to be lightweight yet effective and can be easily plugged into existing IRSTDS frameworks. Extensive experiments on the IRSTD-1k and NUAA-SIRST datasets demonstrate that our method significantly reduces false alarms and achieves superior performance on IRSTDS task.","authors":["Maoxun Yuan","Duanni Meng","Ziteng Xi","Tianyi Zhao","Shiji Zhao","Yimian Dai","Xingxing Wei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10720v2","updated":"2026-02-24T14:35:57Z","published":"2026-02-11T10:25:51Z","title":"Ecological mapping with geospatial foundation models","summary":"The value of Earth observation foundation models for high-impact ecological applications remains insufficiently characterized. This study is one of the first to systematically evaluate the performance, limitations and practical considerations across three common ecological use cases: forest functional trait estimation, land use and land cover mapping and peatland detection. We fine-tune two pretrained models (Prithvi-EO-2.0 and TerraMind) and benchmark them against a ResNet-101 baseline using datasets collected from open sources. Across all tasks, Prithvi-EO-2.0 and TerraMind consistently outperform the ResNet baseline, demonstrating improved generalization and transfer across ecological domains. TerraMind marginally exceeds Prithvi-EO-2.0 in unimodal settings and shows substantial gains when additional modalities are incorporated. However, performance is sensitive to divergence between downstream inputs and pretraining modalities, underscoring the need for careful dataset alignment. Results also indicate that higher-resolution inputs and more accurate pixel-level labels remain critical for capturing fine-scale ecological dynamics.","authors":["Craig Mahlasi","Gciniwe S. Baloyi","Zaheed Gaffoor","Levente Klein","Anne Jones","Etienne Vos","Michal Muszynski","Geoffrey Dawson","Campbell Watson"],"pdf_url":"","comment":"Revised abstract"},{"id":"http://arxiv.org/abs/2602.20951v1","updated":"2026-02-24T14:34:13Z","published":"2026-02-24T14:34:13Z","title":"See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis","summary":"Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.","authors":["Jaehyun Park","Minyoung Ahn","Minkyu Kim","Jonghyun Lee","Jae-Gil Lee","Dongmin Park"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.11675v3","updated":"2026-02-24T14:33:34Z","published":"2026-01-16T06:24:59Z","title":"Generating metamers of human scene understanding","summary":"Human vision combines low-resolution \"gist\" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. \"foveated\") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a \"same\" or \"different\" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.","authors":["Ritik Raina","Abe Leite","Alexandros Graikos","Seoyoung Ahn","Dimitris Samaras","Gregory J. Zelinsky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.13134v2","updated":"2026-02-24T14:32:15Z","published":"2026-01-19T15:20:18Z","title":"Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access","summary":"Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical \"frozen\" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.","authors":["Heng Fang","Adam J. Stewart","Isaac Corley","Xiao Xiang Zhu","Hossein Azizpour"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20947v1","updated":"2026-02-24T14:31:28Z","published":"2026-02-24T14:31:28Z","title":"Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation","summary":"The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.","authors":["Thorbjørn Mosekjær Iversen","Zebin Duan","Frederik Hagelskjær"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20943v1","updated":"2026-02-24T14:24:50Z","published":"2026-02-24T14:24:50Z","title":"UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling","summary":"Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.","authors":["Kaiyuan Tan","Yingying Shen","Mingfei Tu","Haohui Zhu","Bing Wang","Guang Chen","Hangjun Ye","Haiyang Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20933v1","updated":"2026-02-24T14:11:56Z","published":"2026-02-24T14:11:56Z","title":"Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting","summary":"Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS","authors":["Shuangkang Fang","I-Chao Shen","Xuanyang Zhang","Zesheng Wang","Yufeng Wang","Wenrui Ding","Gang Yu","Takeo Igarashi"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20930v1","updated":"2026-02-24T14:08:12Z","published":"2026-02-24T14:08:12Z","title":"Computing a Characteristic Orientation for Rotation-Independent Image Analysis","summary":"Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a preprocessing method that improves rotation robustness without modifying the network architecture. The method estimates a global orientation for each image and aligns it to a canonical reference frame, allowing standard models to process inputs more consistently across different rotations. Unlike moment-based approaches that extract invariant descriptors, this method directly transforms the image while preserving spatial structure, making it compatible with convolutional networks. Experimental evaluation on the rotated MNIST dataset shows that the proposed method achieves higher accuracy than state-of-the-art rotation-invariant architectures. Additional experiments on the CIFAR-10 dataset, confirm that the method remains effective under more complex conditions.","authors":["Cristian Valero-Abundio","Emilio Sansano-Sansano","Raúl Montoliu","Marina Martínez García"],"pdf_url":"","comment":"Accepted for publication at the 21st International Conference on Computer Vision Theory and Applications (VISAPP 2026). 8 pages"},{"id":"http://arxiv.org/abs/2602.20925v1","updated":"2026-02-24T14:04:54Z","published":"2026-02-24T14:04:54Z","title":"LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments","summary":"Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.","authors":["Zeyu Jiang","Kuan Xu","Changhao Chen"],"pdf_url":"","comment":"ICRA 2026"},{"id":"http://arxiv.org/abs/2602.00795v2","updated":"2026-02-24T13:56:22Z","published":"2026-01-31T16:09:37Z","title":"DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning","summary":"Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.","authors":["Wenhao Li","Xianjing Meng","Qiangchang Wang","Zhongyi Han","Zhibin Wu","Yilong Yin"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2309.13411v3","updated":"2026-02-24T13:55:17Z","published":"2023-09-23T15:48:35Z","title":"Towards Attributions of Input Variables in a Coalition","summary":"This paper focuses on the fundamental challenge of partitioning input variables in attribution methods for Explainable AI, particularly in Shapley value-based approaches. Previous methods always compute attributions given a predefined partition but lack theoretical guidance on how to form meaningful variable partitions. We identify that attribution conflicts arise when the attribution of a coalition differs from the sum of its individual variables' attributions. To address this, we analyze the numerical effects of AND-OR interactions in AI models and extend the Shapley value to a new attribution metric for variable coalitions. Our theoretical findings reveal that specific interactions cause attribution conflicts, and we propose three metrics to evaluate coalition faithfulness. Experiments on synthetic data, NLP, image classification, and the game of Go validate our approach, demonstrating consistency with human intuition and practical applicability.","authors":["Xinhao Zheng","Huiqi Deng","Quanshi Zhang"],"pdf_url":"","comment":"Accepted to the 2025 International Conference on Machine Learning (ICML 2025)"},{"id":"http://arxiv.org/abs/2602.20913v1","updated":"2026-02-24T13:49:47Z","published":"2026-02-24T13:49:47Z","title":"LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding","summary":"This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1","authors":["Jihao Qiu","Lingxi Xie","Xinyue Huo","Qi Tian","Qixiang Ye"],"pdf_url":"","comment":"17 pages, 9 figures, 8 tables, accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20911v1","updated":"2026-02-24T13:48:13Z","published":"2026-02-24T13:48:13Z","title":"From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning","summary":"Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.","authors":["Ruiqi Liu","Boyu Diao","Hangda Liu","Zhulin An","Fei Wang","Yongjun Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.10453v2","updated":"2026-02-24T13:44:18Z","published":"2025-08-14T08:42:15Z","title":"Trajectory-aware Shifted State Space Models for Online Video Super-Resolution","summary":"Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7% complexity reduction (in MACs).","authors":["Qiang Zhu","Xiandong Meng","Yuxian Jiang","Fan Zhang","David Bull","Shuyuan Zhu","Bing Zeng","Ronggang Wang"],"pdf_url":"","comment":"ICLR2026"},{"id":"http://arxiv.org/abs/2601.19365v2","updated":"2026-02-24T13:42:09Z","published":"2026-01-27T08:47:01Z","title":"Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation","summary":"Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.","authors":["Jinming Zhang","Youpeng Yang","Xi Yang","Haosen Shi","Yuyao Yan","Qiufeng Wang","Guangliang Cheng","Kaizhu Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.01874v3","updated":"2026-02-24T13:40:33Z","published":"2026-01-05T08:02:18Z","title":"CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving","summary":"Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. In line with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow. Project page: https://shchen233.github.io/cogflow.","authors":["Shuhang Chen","Yunqiu Xu","Junjie Xie","Aojun Lu","Tao Feng","Zeying Huang","Ning Zhang","Yi Sun","Yi Yang","Hangjie Yuan"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20903v1","updated":"2026-02-24T13:40:23Z","published":"2026-02-24T13:40:23Z","title":"TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering","summary":"Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.","authors":["Hanshen Zhu","Yuliang Liu","Xuecheng Wu","An-Lan Wang","Hao Feng","Dingkang Yang","Chao Feng","Can Huang","Jingqun Tang","Xiang Bai"],"pdf_url":"","comment":"Code: https://github.com/CIawevy/TextPecker"},{"id":"http://arxiv.org/abs/2602.20901v1","updated":"2026-02-24T13:38:37Z","published":"2026-02-24T13:38:37Z","title":"SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models","summary":"Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.","authors":["Yuechen Xie","Xiaoyan Zhang","Yicheng Shan","Hao Zhu","Rui Tang","Rong Wei","Mingli Song","Yuanyu Wan","Jie Song"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20880v1","updated":"2026-02-24T13:20:31Z","published":"2026-02-24T13:20:31Z","title":"When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance","summary":"Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.","authors":["Yongli Xiang","Ziming Hong","Zhaoqing Wang","Xiangyu Zhao","Bo Han","Tongliang Liu"],"pdf_url":"","comment":"CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG"},{"id":"http://arxiv.org/abs/2602.20873v1","updated":"2026-02-24T13:17:35Z","published":"2026-02-24T13:17:35Z","title":"MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification","summary":"In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.","authors":["Jiahao Xu","Sheng Huang","Xin Zhang","Zhixiong Nan","Jiajun Dong","Nankun Mu"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20860v1","updated":"2026-02-24T13:03:41Z","published":"2026-02-24T13:03:41Z","title":"DA-Cal: Towards Cross-Domain Calibration in Semantic Segmentation","summary":"While existing unsupervised domain adaptation (UDA) methods greatly enhance target domain performance in semantic segmentation, they often neglect network calibration quality, resulting in misalignment between prediction confidence and actual accuracy -- a significant risk in safety-critical applications. Our key insight emerges from observing that performance degrades substantially when soft pseudo-labels replace hard pseudo-labels in cross-domain scenarios due to poor calibration, despite the theoretical equivalence of perfectly calibrated soft pseudo-labels to hard pseudo-labels. Based on this finding, we propose DA-Cal, a dedicated cross-domain calibration framework that transforms target domain calibration into soft pseudo-label optimization. DA-Cal introduces a Meta Temperature Network to generate pixel-level calibration parameters and employs bi-level optimization to establish the relationship between soft pseudo-labels and UDA supervision, while utilizing complementary domain-mixing strategies to prevent overfitting and reduce domain discrepancies. Experiments demonstrate that DA-Cal seamlessly integrates with existing self-training frameworks across multiple UDA segmentation benchmarks, significantly improving target domain calibration while delivering performance gains without inference overhead. The code will be released.","authors":["Wangkai Li","Rui Sun","Zhaoyang Li","Yujia Chen","Tianzhu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20853v1","updated":"2026-02-24T12:53:28Z","published":"2026-02-24T12:53:28Z","title":"On the Explainability of Vision-Language Models in Art History","summary":"Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.","authors":["Stefanie Schneider"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20851v1","updated":"2026-02-24T12:47:53Z","published":"2026-02-24T12:47:53Z","title":"Hybrid Fusion: One-Minute Efficient Training for Zero-Shot Cross-Domain Image Fusion","summary":"Image fusion seeks to integrate complementary information from multiple sources into a single, superior image. While traditional methods are fast, they lack adaptability and performance. Conversely, deep learning approaches achieve state-of-the-art (SOTA) results but suffer from critical inefficiencies: their reliance on slow, resource-intensive, patch-based training introduces a significant gap with full-resolution inference. We propose a novel hybrid framework that resolves this trade-off. Our method utilizes a learnable U-Net to generate a dynamic guidance map that directs a classic, fixed Laplacian pyramid fusion kernel. This decoupling of policy learning from pixel synthesis enables remarkably efficient full-resolution training, eliminating the train-inference gap. Consequently, our model achieves SOTA-comparable performance in about one minute on a RTX 4090 or two minutes on a consumer laptop GPU from scratch without any external model and demonstrates powerful zero-shot generalization across diverse tasks, from infrared-visible to medical imaging. By design, the fused output is linearly constructed solely from source information, ensuring high faithfulness for critical applications. The codes are available at https://github.com/Zirconium233/HybridFusion","authors":["Ran Zhang","Xuanhua He","Liu Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20845v1","updated":"2026-02-24T12:36:22Z","published":"2026-02-24T12:36:22Z","title":"FLIM Networks with Bag of Feature Points","summary":"Convolutional networks require extensive image annotation, which can be costly and time-consuming. Feature Learning from Image Markers (FLIM) tackles this challenge by estimating encoder filters (i.e., kernel weights) from user-drawn markers on discriminative regions of a few representative images without traditional optimization. Such an encoder combined with an adaptive decoder comprises a FLIM network fully trained without backpropagation. Prior research has demonstrated their effectiveness in Salient Object Detection (SOD), being significantly lighter than existing lightweight models. This study revisits FLIM SOD and introduces FLIM-Bag of Feature Points (FLIM-BoFP), a considerably faster filter estimation method. The previous approach, FLIM-Cluster, derives filters through patch clustering at each encoder's block, leading to computational overhead and reduced control over filter locations. FLIM-BoFP streamlines this process by performing a single clustering at the input block, creating a bag of feature points, and defining filters directly from mapped feature points across all blocks. The paper evaluates the benefits in efficiency, effectiveness, and generalization of FLIM-BoFP compared to FLIM-Cluster and other state-of-the-art baselines for parasite detection in optical microscopy images.","authors":["João Deltregia Martinelli","Marcelo Luis Rodrigues Filho","Felipe Crispim da Rocha Salvagnini","Gilson Junior Soares","Jefersson A. dos Santos","Alexandre X. Falcão"],"pdf_url":"","comment":"Accepted at the 28th Iberoamerican Congress on Pattern Recognition (CIARP 2025). To appear in Lecture Notes in Computer Science (LNCS), Springer"},{"id":"http://arxiv.org/abs/2602.19946v2","updated":"2026-02-24T12:29:33Z","published":"2026-02-23T15:15:53Z","title":"When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators","summary":"Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.","authors":["Krzysztof Adamkiewicz","Brian Moser","Stanislav Frolov","Tobias Christian Nauen","Federico Raue","Andreas Dengel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20839v1","updated":"2026-02-24T12:27:51Z","published":"2026-02-24T12:27:51Z","title":"Training-Free Multi-Concept Image Editing","summary":"Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.","authors":["Niki Foteinopoulou","Ignas Budvytis","Stephan Liwicki"],"pdf_url":"","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2602.17372v2","updated":"2026-02-24T12:23:02Z","published":"2026-02-19T13:54:35Z","title":"Tree crop mapping of South America reveals links to deforestation and conservation","summary":"Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as \"forest\". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.","authors":["Yuchang Jiang","Anton Raichuk","Xiaoye Tong","Vivien Sainte Fare Garnot","Daniel Ortiz-Gonzalo","Dan Morris","Konrad Schindler","Jan Dirk Wegner","Maxim Neumann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.27219v2","updated":"2026-02-24T12:14:37Z","published":"2025-10-31T06:28:14Z","title":"SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping","summary":"Hyperspectral imaging (HSI) is a critical technique for fine-grained land-use and land-cover (LULC) mapping. However, the inherent heterogeneity of HSI data, particularly the variation in spectral channels across sensors, has long constrained the development of model generalization via transfer learning or joint training. Existing HSI foundation models show promise for different downstream tasks, but typically underutilize the critical guiding role of sensor meta-attributes and image semantic features, resulting in limited adaptability to cross-sensor joint learning. To address these issues, we propose SpecAware, which is a novel hyperspectral spectral-content aware foundation model for unifying multi-sensor learning for HSI mapping. To support this work, we constructed the Hyper-400K dataset, which is a new large-scale pre-training dataset with over 400\\,k high-quality patches from diverse airborne AVIRIS sensors that cover two data processing levels (L1 and L2). The core of SpecAware is a hypernetwork-driven unified image embedding process for HSI data. Firstly, we designed a meta-content aware module to generate a unique conditional input for each HSI sample, tailored to each spectral band by fusing the sensor meta-attributes and its own image content. Secondly, we designed the HyperEmbedding module, where a sample-conditioned hypernetwork dynamically generates a pair of matrix factors for channel-wise encoding. This process implements two-step matrix factorization, consisting of adaptive spatial pattern extraction and latent semantic feature projection, yielding a unified hyperspectral token representation. Thus, SpecAware learns to capture and interpret spatial-spectral features across diverse scenes and sensors, enabling adaptive processing of variable spectral channels within a unified multi-sensor joint pre-training framework.","authors":["Renjie Ji","Xue Wang","Chao Niu","Wen Zhang","Yong Mei","Kun Tan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.06690v2","updated":"2026-02-24T12:03:06Z","published":"2025-06-07T07:04:48Z","title":"SpikePingpong: Spike Vision-based Fast-Slow Pingpong Robot System","summary":"Learning to control high-speed objects in dynamic environments represents a fundamental challenge in robotics. Table tennis serves as an ideal testbed for advancing robotic capabilities in dynamic environments. This task presents two fundamental challenges: it requires a high-precision vision system capable of accurately predicting ball trajectories under complex dynamics, and it necessitates intelligent control strategies to ensure precise ball striking to target regions. High-speed object manipulation typically demands advanced visual perception hardware capable of capturing rapid motion with exceptional temporal resolution. Drawing inspiration from Kahneman's dual-system theory, where fast intuitive processing complements slower deliberate reasoning, there exists an opportunity to develop more robust perception architectures that can handle high-speed dynamics while maintaining accuracy. To this end, we present \\textit{\\textbf{SpikePingpong}}, a novel system that integrates spike-based vision with imitation learning for high-precision robotic table tennis. We develop a Fast-Slow system architecture where System 1 provides rapid ball detection and preliminary trajectory prediction with millisecond-level responses, while System 2 employs spike-oriented neural calibration for precise hittable position corrections. For strategic ball striking, we introduce Imitation-based Motion Planning And Control Technology, which learns optimal robotic arm striking policies through demonstration-based learning. Experimental results demonstrate that \\textit{\\textbf{SpikePingpong}} achieves a remarkable 92\\% success rate for 30 cm accuracy zones and 70\\% in the more challenging 20 cm precision targeting. This work demonstrates the potential of Fast-Slow architectures for advancing robotic capabilities in time-critical manipulation tasks.","authors":["Hao Wang","Chengkai Hou","Xianglong Li","Yankai Fu","Chenxuan Li","Ning Chen","Gaole Dai","Jiaming Liu","Tiejun Huang","Shanghang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19863v2","updated":"2026-02-24T11:57:03Z","published":"2026-02-23T14:09:01Z","title":"Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation","summary":"Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Project page: \\textcolor{magenta}{https://wolfilip.github.io/DEO/}.","authors":["Filip Wolf","Blaž Rolih","Luka Čehovin Zajc"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20818v1","updated":"2026-02-24T11:54:54Z","published":"2026-02-24T11:54:54Z","title":"GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection","summary":"Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.","authors":["Yingying Guo","Ke Zhang","Zirong Zeng"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2601.09708v2","updated":"2026-02-24T11:51:26Z","published":"2026-01-14T18:59:59Z","title":"Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning","summary":"Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.","authors":["Chi-Pin Huang","Yunze Man","Zhiding Yu","Min-Hung Chen","Jan Kautz","Yu-Chiang Frank Wang","Fu-En Yang"],"pdf_url":"","comment":"CVPR 2026. Project page: https://jasper0314-huang.github.io/fast-thinkact/"},{"id":"http://arxiv.org/abs/2602.20807v1","updated":"2026-02-24T11:47:43Z","published":"2026-02-24T11:47:43Z","title":"RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction","summary":"Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io","authors":["Yangfan Zhao","Hanwei Zhang","Ke Huang","Qiufeng Wang","Zhenzhou Shao","Dengyu Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20794v1","updated":"2026-02-24T11:33:44Z","published":"2026-02-24T11:33:44Z","title":"VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving","summary":"The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.","authors":["Jie Wang","Guang Li","Zhijian Huang","Chenxu Dang","Hangjun Ye","Yahong Han","Long Chen"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20792v1","updated":"2026-02-24T11:31:20Z","published":"2026-02-24T11:31:20Z","title":"SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking","summary":"Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.","authors":["Muhammad Saif Ullah Khan","Didier Stricker"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20790v1","updated":"2026-02-24T11:29:07Z","published":"2026-02-24T11:29:07Z","title":"Real-time Motion Segmentation with Event-based Normal Flow","summary":"Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.","authors":["Sheng Zhong","Zhongyang Ren","Xiya Zhu","Dehao Yuan","Cornelia Fermuller","Yi Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20773v1","updated":"2026-02-24T11:13:01Z","published":"2026-02-24T11:13:01Z","title":"Federated Learning for Cross-Modality Medical Image Segmentation via Augmentation-Driven Generalization","summary":"Artificial intelligence has emerged as a transformative tool in medical image analysis, yet developing robust and generalizable segmentation models remains difficult due to fragmented, privacy-constrained imaging data siloed across institutions. While federated learning (FL) enables collaborative model training without centralizing data, cross-modality domain shifts pose a critical challenge, particularly when models trained on one modality fail to generalize to another. Many existing solutions require paired multimodal data per patient or rely on complex architectures, both of which are impractical in real clinical settings. In this work, we consider a realistic FL scenario where each client holds single-modality data (CT or MRI), and systematically investigate augmentation strategies for cross-modality generalization. Using abdominal organ segmentation and whole-heart segmentation as representative multi-class and binary segmentation benchmarks, we evaluate convolution-based spatial augmentation, frequency-domain manipulation, domain-specific normalization, and global intensity nonlinear (GIN) augmentation. Our results show that GIN consistently outperforms alternatives in both centralized and federated settings by simulating cross-modality appearance variations while preserving anatomical structure. For the pancreas, Dice score improved from 0.073 to 0.437, a 498% gain. Our federated approach achieves 93-98% of centralized training accuracy, demonstrating strong cross-modality generalization without compromising data privacy, pointing toward feasible federated AI deployment across diverse healthcare systems.","authors":["Sachin Dudda Nagaraju","Ashkan Moradi","Bendik Skarre Abrahamsen","Mattijs Elschot"],"pdf_url":"","comment":"Submitted to IEEE JBHI"},{"id":"http://arxiv.org/abs/2503.06437v2","updated":"2026-02-24T10:57:18Z","published":"2025-03-09T04:25:39Z","title":"SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding","summary":"We present SEED (Semantic Evaluation for Visual Brain Decoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images inspired by neuroscientific findings. Using carefully crowd-sourced human evaluation data, we demonstrate that SEED achieves the highest alignment with human evaluation, outperforming other widely used metrics. Through the evaluation of existing visual brain decoding models with SEED, we further reveal that crucial information is often lost in translation, even in the state-of-the-art models that achieve near-perfect scores on existing metrics. This finding highlights the limitations of current evaluation practices and provides guidance for future improvements in decoding models. Finally, to facilitate further research, we open-source the human evaluation data, encouraging the development of more advanced evaluation methods for brain decoding. Our code and the human evaluation data are available at https://github.com/Concarne2/SEED.","authors":["Juhyeon Park","Peter Yongho Kim","Jiook Cha","Shinjae Yoo","Taesup Moon"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2503.09160v4","updated":"2026-02-24T10:39:06Z","published":"2025-03-12T08:44:51Z","title":"WonderVerse: Extendable 3D Scene Generation with Video Generative Models","summary":"We introduce \\textit{WonderVerse}, a simple but effective framework for generating extendable 3D scenes. Unlike existing methods that rely on iterative depth estimation and image inpainting, often leading to geometric distortions and inconsistencies, WonderVerse leverages the powerful world-level priors embedded within video generative foundation models to create highly immersive and geometrically coherent 3D environments. Furthermore, we propose a new technique for controllable 3D scene extension to substantially increase the scale of the generated environments. Besides, we introduce a novel abnormal sequence detection module that utilizes camera trajectory to address geometric inconsistency in the generated videos. Finally, WonderVerse is compatible with various 3D reconstruction methods, allowing both efficient and high-quality generation. Extensive experiments on 3D scene generation demonstrate that our WonderVerse, with an elegant and simple pipeline, delivers extendable and highly-realistic 3D scenes, markedly outperforming existing works that rely on more complex architectures.","authors":["Hao Feng","Zhi Zuo","Jia-Hui Pan","Ka-Hei Hui","Qi Dou","Jingyu Hu","Zhengzhe Liu"],"pdf_url":"","comment":"Accepted at CVM 2026"},{"id":"http://arxiv.org/abs/2602.20161v2","updated":"2026-02-24T10:29:43Z","published":"2026-02-23T18:59:58Z","title":"Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device","summary":"Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/","authors":["Abdelrahman Shaker","Ahmed Heakl","Jaseel Muhammad","Ritesh Thawkar","Omkar Thawakar","Senmao Li","Hisham Cholakkal","Ian Reid","Eric P. Xing","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"","comment":"Project page: https://amshaker.github.io/Mobile-O/"},{"id":"http://arxiv.org/abs/2602.20752v1","updated":"2026-02-24T10:29:10Z","published":"2026-02-24T10:29:10Z","title":"OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation","summary":"Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.","authors":["Tian Lan","Lei Xu","Zimu Yuan","Shanggui Liu","Jiajun Liu","Jiaxin Liu","Weilai Xiang","Hongyu Yang","Dong Jiang","Jianxin Yin","Dingyu Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.08019v2","updated":"2026-02-24T10:22:16Z","published":"2024-07-10T19:58:04Z","title":"Coherent and Multi-modality Image Inpainting via Latent Space Optimization","summary":"With the advancements in denoising diffusion probabilistic models (DDPMs), image inpainting has significantly evolved from merely filling information based on nearby regions to generating content conditioned on various prompts such as text, exemplar images, and sketches. However, existing methods, such as model fine-tuning and simple concatenation of latent vectors, often result in generation failures due to overfitting and inconsistency between the inpainted region and the background. In this paper, we argue that the current large diffusion models are sufficiently powerful to generate realistic images without further tuning. Hence, we introduce PILOT (in\\textbf{P}ainting v\\textbf{I}a \\textbf{L}atent \\textbf{O}p\\textbf{T}imization), an optimization approach grounded on a novel \\textit{semantic centralization} and \\textit{background preservation loss}. Our method searches latent spaces capable of generating inpainted regions that exhibit high fidelity to user-provided prompts while maintaining coherence with the background. Furthermore, we propose a strategy to balance optimization expense and image quality, significantly enhancing generation efficiency. Our method seamlessly integrates with any pre-trained model, including ControlNet and DreamBooth, making it suitable for deployment in multi-modal editing tools. Our qualitative and quantitative evaluations demonstrate that PILOT outperforms existing approaches by generating more coherent, diverse, and faithful inpainted regions in response to provided prompts.","authors":["Lingzhi Pan","Tong Zhang","Bingyuan Chen","Qi Zhou","Wei Ke","Sabine Süsstrunk","Mathieu Salzmann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19117v2","updated":"2026-02-24T10:19:29Z","published":"2026-02-22T10:18:54Z","title":"Keep it SymPL: Symbolic Projective Layout for Allocentric Spatial Reasoning in Vision-Language Models","summary":"Perspective-aware spatial reasoning involves understanding spatial relationships from specific viewpoints-either egocentric (observer-centered) or allocentric (object-centered). While vision-language models (VLMs) perform well in egocentric settings, their performance deteriorates when reasoning from allocentric viewpoints, where spatial relations must be inferred from the perspective of objects within the scene. In this study, we address this underexplored challenge by introducing Symbolic Projective Layout (SymPL), a framework that reformulates allocentric reasoning into symbolic-layout forms that VLMs inherently handle well. By leveraging four key factors-projection, abstraction, bipartition, and localization-SymPL converts allocentric questions into structured symbolic-layout representations. Extensive experiments demonstrate that this reformulation substantially improves performance in both allocentric and egocentric tasks, enhances robustness under visual illusions and multi-view scenarios, and that each component contributes critically to these gains. These results show that SymPL provides an effective and principled approach for addressing complex perspective-aware spatial reasoning.","authors":["Jaeyun Jang","Seunghui Shin","Taeho Park","Hyoseok Hwang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.13027v2","updated":"2026-02-24T10:12:16Z","published":"2025-02-18T16:45:01Z","title":"A deep learning framework for efficient pathology image analysis","summary":"Artificial intelligence (AI) has transformed digital pathology by enabling biomarker prediction from high-resolution whole-slide images (WSIs). However, current methods are computationally inefficient, processing thousands of redundant tiles per WSI and requiring complex aggregator models. We introduce EAGLE (Efficient Approach for Guided Local Examination), a deep learning framework that emulates pathologists by selectively analyzing informative regions. EAGLE incorporates two foundation models: CHIEF for efficient tile selection and Virchow2 for extracting high-quality features. Benchmarking was conducted against leading slide- and tile-level foundation models across 43 tasks from nine cancer types, spanning morphology, biomarker prediction, treatment response and prognosis. EAGLE outperformed state-of-the-art patch aggregation methods by up to 23% and achieved the highest AUROC overall. It processed a slide in 2.27 seconds, reducing computational time by more than 99% compared to existing models. This efficiency enables real-time workflows, allows rapid review of the exact tiles used for each prediction, and reduces dependence on high-performance computing, making AI-powered pathology more accessible. By reliably identifying meaningful regions and minimizing artifacts, EAGLE provides robust and auditable outputs, supported by systematic negative controls and attention concentration analyses. Its unified embedding enables rapid slide searches, integration into multi-omics pipelines and emerging clinical foundation models.","authors":["Peter Neidlinger","Tim Lenz","Sebastian Foersch","Chiara M. L. Loeffler","Jan Clusmann","Marco Gustav","Lawrence A. Shaktah","Rupert Langer","Bastian Dislich","Lisa A. Boardman","Amy J. French","Ellen L. Goode","Andrea Gsur","Stefanie Brezina","Marc J. Gunter","Robert Steinfelder","Hans-Michael Behrens","Christoph Röcken","Tabitha Harrison","Ulrike Peters","Amanda I. Phipps","Giuseppe Curigliano","Nicola Fusco","Antonio Marra","Michael Hoffmeister","Hermann Brenner","Jakob Nikolas Kather"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20739v1","updated":"2026-02-24T10:08:33Z","published":"2026-02-24T10:08:33Z","title":"PyVision-RL: Forging Open Agentic Vision Models via RL","summary":"Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.","authors":["Shitian Zhao","Shaoheng Lin","Ming Li","Haoquan Zhang","Wenshuo Peng","Kaipeng Zhang","Chen Wei"],"pdf_url":"","comment":"preprint"},{"id":"http://arxiv.org/abs/2602.20731v1","updated":"2026-02-24T09:53:50Z","published":"2026-02-24T09:53:50Z","title":"Communication-Inspired Tokenization for Structured Image Representations","summary":"Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.","authors":["Aram Davtyan","Yusuf Sahin","Yasaman Haghighi","Sebastian Stapf","Pablo Acuaviva","Alexandre Alahi","Paolo Favaro"],"pdf_url":"","comment":"Project website: https://araachie.github.io/comit/"},{"id":"http://arxiv.org/abs/2602.20725v1","updated":"2026-02-24T09:44:12Z","published":"2026-02-24T09:44:12Z","title":"Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation","summary":"Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.","authors":["Junwei Shu","Wenjie Liu","Changgu Chen","Hantang Liu","Yang Li","Changbo Wang"],"pdf_url":"","comment":"preprint"},{"id":"http://arxiv.org/abs/2602.18746v2","updated":"2026-02-24T09:35:41Z","published":"2026-02-21T07:56:59Z","title":"MIRROR: Multimodal Iterative Reasoning via Reflection on Visual Regions","summary":"In the era of Vision-Language Models (VLMs), enhancing multimodal reasoning capabilities remains a critical challenge, particularly in handling ambiguous or complex visual inputs, where initial inferences often lead to hallucinations or logic errors. Existing VLMs often produce plausible yet ungrounded answers, and even when prompted to \"reflect\", their corrections may remain detached from the image evidence. To address this, we propose the MIRROR framework for Multimodal Iterative Reasoning via Reflection On visual Regions. By embedding visual reflection as a core mechanism, MIRROR is formulated as a closed-loop process comprising draft, critique, region-based verification, and revision, which are repeated until the output is visually grounded. To facilitate training of this model, we construct **ReflectV**, a visual reflective dataset for multi-turn supervision that explicitly contains reflection triggers, region-based verification actions, and answer revision grounded in visual evidence. Experiments on both general vision-language benchmarks and representative vision-language reasoning benchmarks show that MIRROR improves correctness and reduces visual hallucinations, demonstrating the value of training reflection as an evidence-seeking, region-aware verification process rather than a purely textual revision step.","authors":["Haoyu Zhang","Yuwei Wu","Pengxiang Li","Xintong Zhang","Zhi Gao","Rui Gao","Mingyang Gao","Che Sun","Yunde Jia"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20721v1","updated":"2026-02-24T09:33:05Z","published":"2026-02-24T09:33:05Z","title":"CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization","summary":"Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.","authors":["Xiaoman Feng","Mingkun Lei","Yang Wang","Dingwen Fu","Chi Zhang"],"pdf_url":"","comment":"26 pages"},{"id":"http://arxiv.org/abs/2602.20718v1","updated":"2026-02-24T09:29:36Z","published":"2026-02-24T09:29:36Z","title":"Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization","summary":"Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.","authors":["Yangsen Chen","Hao Wang"],"pdf_url":"","comment":"ijcnn 2025"},{"id":"http://arxiv.org/abs/2602.20709v1","updated":"2026-02-24T09:15:13Z","published":"2026-02-24T09:15:13Z","title":"Onboard-Targeted Segmentation of Straylight in Space Camera Sensors","summary":"This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.","authors":["Riccardo Gallon","Fabian Schiemenz","Alessandra Menicucci","Eberhard Gill"],"pdf_url":"","comment":"Submitted to Aerospace Science and Technology"},{"id":"http://arxiv.org/abs/2506.03922v2","updated":"2026-02-24T09:07:33Z","published":"2025-06-04T13:14:13Z","title":"HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models","summary":"Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.","authors":["Zhaolu Kang","Junhao Gong","Jiaxu Yan","Wanke Xia","Yian Wang","Ziwen Wang","Huaxuan Ding","Zhuo Cheng","Wenhao Cao","Zhiyuan Feng","Siqi He","Shannan Yan","Junzhe Chen","Xiaomin He","Chaoya Jiang","Wei Ye","Kaidong Yu","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.12149v2","updated":"2026-02-24T09:04:55Z","published":"2026-01-17T19:45:01Z","title":"Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks","summary":"Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.","authors":["Pengfei Zhu","Stefano Sfarra","Hai Zhang","Carlo Santulli","Elana Pivarciova","Fabrizio Sarasini","Xavier Maldague"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.16030v2","updated":"2026-02-24T09:01:52Z","published":"2025-11-20T04:22:01Z","title":"CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis","summary":"3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/","authors":["Zijian Wu","Mingfeng Jiang","Zidian Lin","Ying Song","Hanjie Ma","Qun Wu","Dongping Zhang","Guiyang Pu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20700v1","updated":"2026-02-24T09:01:11Z","published":"2026-02-24T09:01:11Z","title":"NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image","summary":"Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.","authors":["Anna Badalyan","Pratheba Selvaraju","Giorgio Becherini","Omid Taheri","Victoria Fernandez Abrevaya","Michael Black"],"pdf_url":"","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.14856v2","updated":"2026-02-24T08:54:26Z","published":"2025-06-17T08:15:52Z","title":"Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction","summary":"Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.","authors":["Zhengquan Zhang","Feng Xu","Mengmi Zhang"],"pdf_url":"","comment":"10 pages, 4 figures in the main text. Published at ICLR 2026"},{"id":"http://arxiv.org/abs/2508.17404v3","updated":"2026-02-24T08:47:42Z","published":"2025-08-24T15:20:24Z","title":"MoSA: Motion-Coherent Human Video Generation via Structure-Appearance Decoupling","summary":"Existing video generation models predominantly emphasize appearance fidelity while exhibiting limited ability to synthesize complex human motions, such as whole-body movements, long-range dynamics, and fine-grained human-environment interactions. This often leads to unrealistic or physically implausible movements with inadequate structural coherence. To conquer these challenges, we propose MoSA, which decouples the process of human video generation into two components, i.e., structure generation and appearance generation. MoSA first employs a 3D structure transformer to generate a human motion sequence from the text prompt. The remaining video appearance is then synthesized under the guidance of this structural sequence. We achieve fine-grained control over the sparse human structures by introducing Human-Aware Dynamic Control modules with a dense tracking constraint during training. The modeling of human-environment interactions is improved through the proposed contact constraint. Those two components work comprehensively to ensure the structural and appearance fidelity across the generated videos. This paper also contributes a large-scale human video dataset, which features more complex and diverse motions than existing human video datasets. We conduct comprehensive comparisons between MoSA and a variety of approaches, including general video generation models, human video generation models, and human animation models. Experiments demonstrate that MoSA substantially outperforms existing approaches across the majority of evaluation metrics.","authors":["Haoyu Wang","Hao Tang","Donglin Di","Zhilu Zhang","Wangmeng Zuo","Feng Gao","Siwei Ma","Shiliang Zhang"],"pdf_url":"","comment":"Accepted by ICLR 2026. Project: https://hywang2002.github.io/MoSA"},{"id":"http://arxiv.org/abs/2602.20689v1","updated":"2026-02-24T08:45:49Z","published":"2026-02-24T08:45:49Z","title":"MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision","summary":"Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \\MethodLPP, a lightweight, only $\\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \\MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \\MethodLPP substantially improves the performance of existing edge detection models. In particular, \\MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \\MethodLPP further boosts baseline performance by up to 20--35\\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.","authors":["Bedrettin Cetinkaya","Sinan Kalkan","Emre Akbas"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20685v1","updated":"2026-02-24T08:41:40Z","published":"2026-02-24T08:41:40Z","title":"RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation","summary":"World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.","authors":["Yichen Xie","Chensheng Peng","Mazen Abdelfattah","Yihan Hu","Jiezhi Yang","Eric Higgins","Ryan Brigden","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"","comment":"Accepted by CVPR 2026; Project website: http://yichen928.github.io/raynova"},{"id":"http://arxiv.org/abs/2512.09435v2","updated":"2026-02-24T08:23:14Z","published":"2025-12-10T09:04:12Z","title":"UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents","summary":"Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches. Project page: https://xfanhe.github.io/projects/unipart/","authors":["Xufan He","Yushuang Wu","Xiaoyang Guo","Chongjie Ye","Jiaqing Zhou","Tianlei Hu","Xiaoguang Han","Dong Du"],"pdf_url":"","comment":"Project page: https://xfanhe.github.io/projects/unipart/"},{"id":"http://arxiv.org/abs/2602.20672v1","updated":"2026-02-24T08:22:42Z","published":"2026-02-24T08:22:42Z","title":"BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models","summary":"Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.","authors":["Eliran Kachlon","Alexander Visheratin","Nimrod Sarid","Tal Hacham","Eyal Gutflaish","Saar Huberman","Hezi Zisman","David Ruppin","Ron Mokady"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20673v1","updated":"2026-02-24T08:22:42Z","published":"2026-02-24T08:22:42Z","title":"GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio","summary":"A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.","authors":["Hao Zhang","Lue Fan","Qitai Wang","Wenbo Li","Zehuan Wu","Lewei Lu","Zhaoxiang Zhang","Hongsheng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.18719v3","updated":"2026-02-24T08:21:32Z","published":"2025-11-24T03:21:17Z","title":"Seeing What Matters: Visual Preference Policy Optimization for Visual Generation","summary":"Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.","authors":["Ziqi Ni","Yuanzhi Liang","Rui Li","Yi Zhou","Haibin Huang","Chi Zhang","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.08456v2","updated":"2026-02-24T08:19:40Z","published":"2025-06-10T05:23:46Z","title":"Improving Motion in Image-to-Video Models via Adaptive Low-Pass Guidance","summary":"Recent text-to-video (T2V) models have demonstrated strong capabilities in producing high-quality, dynamic videos. To improve the visual controllability, recent works have considered fine-tuning pre-trained T2V models to support image-to-video (I2V) generation. However, such adaptation frequently suppresses motion dynamics of generated outputs, resulting in more static videos compared to their T2V counterparts. In this work, we analyze this phenomenon and identify that it stems from the premature exposure to high-frequency details in the input image, which biases the sampling process toward a shortcut trajectory that overfits to the static appearance of the reference image. To address this, we propose adaptive low-pass guidance (ALG), a simple training-free fix to the I2V model sampling procedure to generate more dynamic videos without compromising per-frame image quality. Specifically, ALG adaptively modulates the frequency content of the conditioning image by applying a low-pass filter at the early stage of denoising. Extensive experiments show ALG significantly improves the temporal dynamics of generated videos, while preserving or even improving image fidelity and text alignment. For instance, on the VBench test suite, ALG achieves a 33% average improvement across models in dynamic degree while maintaining the original video quality. For additional visualizations and source code, see the project page.","authors":["June Suk Choi","Kyungmin Lee","Sihyun Yu","Yisol Choi","Jinwoo Shin","Kimin Lee"],"pdf_url":"","comment":"Project page: http://choi403.github.io/ALG"},{"id":"http://arxiv.org/abs/2510.00635v3","updated":"2026-02-24T08:16:58Z","published":"2025-10-01T08:12:07Z","title":"Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack","summary":"Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.","authors":["Nanxiang Jiang","Zhaoxin Fan","Enhan Kang","Daiheng Gao","Yun Zhou","Yanxia Chang","Zheng Zhu","Yeying Jin","Wenjun Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20666v1","updated":"2026-02-24T08:15:25Z","published":"2026-02-24T08:15:25Z","title":"BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity","summary":"Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.","authors":["Juil Koo","Wei-Tung Lin","Chanho Park","Chanhyeok Park","Minhyuk Sung"],"pdf_url":"","comment":"Project page: https://boxsplitgen.github.io"},{"id":"http://arxiv.org/abs/2602.20664v1","updated":"2026-02-24T08:14:24Z","published":"2026-02-24T08:14:24Z","title":"AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?","summary":"Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.","authors":["Hailong Yan","Shice Liu","Tao Wang","Xiangtao Zhang","Yijie Zhong","Jinwei Chen","Le Zhang","Bo Li"],"pdf_url":"","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2602.20658v1","updated":"2026-02-24T08:01:49Z","published":"2026-02-24T08:01:49Z","title":"Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video","summary":"Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.","authors":["Mohammad Sadra Rajabi","Aanuoluwapo Ojelade","Sunwook Kim","Maury A. Nussbaum"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20653v1","updated":"2026-02-24T07:57:19Z","published":"2026-02-24T07:57:19Z","title":"SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar","summary":"4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.","authors":["Xiaokai Bai","Jiahao Cheng","Songkai Wang","Yixuan Luo","Lianqing Zheng","Xiaohan Zhang","Si-Yuan Cao","Hui-Liang Shen"],"pdf_url":"","comment":"7 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2602.20650v1","updated":"2026-02-24T07:53:58Z","published":"2026-02-24T07:53:58Z","title":"Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression","summary":"Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \\href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.","authors":["Chenyue Yu","Lingao Xiao","Jinhong Deng","Ivor W. Tsang","Yang He"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2508.09691v2","updated":"2026-02-24T07:50:08Z","published":"2025-08-13T10:37:41Z","title":"PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training","summary":"Facial representation pre-training is crucial for tasks like facial recognition, expression analysis, and virtual reality. However, existing methods face three key challenges: (1) failing to capture distinct facial features and fine-grained semantics, (2) ignoring the spatial structure inherent to facial anatomy, and (3) inefficiently utilizing limited labeled data. To overcome these, we introduce PaCo-FR, an unsupervised framework that combines masked image modeling with patch-pixel alignment. Our approach integrates three innovative components: (1) a structured masking strategy that preserves spatial coherence by aligning with semantically meaningful facial regions, (2) a novel patch-based codebook that enhances feature discrimination with multiple candidate tokens, and (3) spatial consistency constraints that preserve geometric relationships between facial components. PaCo-FR achieves state-of-the-art performance across several facial analysis tasks with just 2 million unlabeled images for pre-training. Our method demonstrates significant improvements, particularly in scenarios with varying poses, occlusions, and lighting conditions. We believe this work advances facial representation learning and offers a scalable, efficient solution that reduces reliance on expensive annotated datasets, driving more effective facial analysis systems.","authors":["Yin Xie","Zhichao Chen","Zeyu Xiao","Yongle Zhao","Xiang An","Kaicheng Yang","Zimin Ran","Jia Guo","Ziyong Feng","Jiankang Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20636v1","updated":"2026-02-24T07:30:51Z","published":"2026-02-24T07:30:51Z","title":"SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement","summary":"Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.","authors":["Rulin Zhou","Guankun Wang","An Wang","Yujie Ma","Lixin Ouyang","Bolin Cui","Junyan Li","Chaowei Zhu","Mingyang Li","Ming Chen","Xiaopin Zhong","Peng Lu","Jiankun Wang","Xianming Liu","Hongliang Ren"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20632v1","updated":"2026-02-24T07:25:53Z","published":"2026-02-24T07:25:53Z","title":"Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection","summary":"4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.","authors":["Xiaokai Bai","Lianqing Zheng","Si-Yuan Cao","Xiaohan Zhang","Zhe Wu","Beinan Yu","Fang Wang","Jie Bai","Hui-Liang Shen"],"pdf_url":"","comment":"14 pages, 10 figures, 13 tables"},{"id":"http://arxiv.org/abs/2602.20630v1","updated":"2026-02-24T07:24:25Z","published":"2026-02-24T07:24:25Z","title":"From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection","summary":"Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \\textbf{Tra}ck-\\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.","authors":["Yepeng Liu","Hao Li","Liwen Yang","Fangzhen Li","Xudi Ge","Yuliang Gu","kuang Gao","Bing Wang","Guang Chen","Hangjun Ye","Yongchao Xu"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20627v1","updated":"2026-02-24T07:22:58Z","published":"2026-02-24T07:22:58Z","title":"Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection","summary":"Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.","authors":["Zhaonian Kuang","Rui Ding","Meng Yang","Xinhu Zheng","Gang Hua"],"pdf_url":"","comment":"IJCV"},{"id":"http://arxiv.org/abs/2510.00037v4","updated":"2026-02-24T07:21:12Z","published":"2025-09-26T14:42:23Z","title":"On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations","summary":"In Vision-Language-Actionf(VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust BYOVLA that requires external LLMs, and a 10.4% gain under mixed perturbations. On the real-world FR5 robot, under four types of multimodal perturbations, RobustVLA shows strong low-data performance, outperforming pi0 by 65.6% success rate with 25 demonstrations. Even with abundant demos, our method still outperform pi0 by 30% success rate. Code and demo videos available at https://github.com/gakakulicc/RobustVLA.","authors":["Jianing Guo","Zhenhong Wu","Chang Tu","Yiyao Ma","Xiangqi Kong","Zhiqian Liu","Jiaming Ji","Shuning Zhang","Yuanpei Chen","Kai Chen","Qi Dou","Yaodong Yang","Xianglong Liu","Huijie Zhao","Weifeng Lv","Simin Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20618v1","updated":"2026-02-24T07:11:40Z","published":"2026-02-24T07:11:40Z","title":"RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces","summary":"The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.","authors":["Haonan An","Xiaohui Ye","Guang Hua","Yihang Tao","Hangcheng Cao","Xiangyu Yu","Yuguang Fang"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20616v1","updated":"2026-02-24T07:08:47Z","published":"2026-02-24T07:08:47Z","title":"Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model","summary":"Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly \"knowing the unknown\". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.","authors":["Xueqiang Lv","Shizhou Zhang","Yinghui Xing","Di Xu","Peng Wang","Yanning Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.17645v2","updated":"2026-02-24T07:02:26Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning","summary":"Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"","comment":"Camera-ready version. Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.20608v1","updated":"2026-02-24T07:00:38Z","published":"2026-02-24T07:00:38Z","title":"VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos","summary":"3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.","authors":["Aihua Mao","Kaihang Huang","Yong-Jin Liu","Chee Seng Chan","Ying He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.12600v3","updated":"2026-02-24T06:58:34Z","published":"2025-02-18T07:23:22Z","title":"Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining","summary":"Generalization to unseen degradations remains a fundamental challenge for low-level vision models. This paper aims to investigate the underlying mechanism of this failure, using image deraining as a primary case study due to its well-defined and decoupled structure. Through systematic experiments, we reveal that generalization issues are not primarily caused by limited network capacity, but rather by a ``shortcut learning'' phenomenon driven by the relative complexity between image content and degradation patterns. We find that when background content is excessively complex, networks preferentially overfit the simpler degradation characteristics to minimize training loss, thereby failing to learn the underlying image distribution. To address this, we propose two principled strategies: (1) balancing the complexity of training data (backgrounds vs. degradations) to redirect the network's focus toward content reconstruction, and (2) leveraging strong content priors from pre-trained generative models to physically constrain the network onto a high-quality image manifold. Extensive experiments on image deraining, denoising, and deblurring validate our theoretical insights. Our work provides an interpretability-driven perspective and a principled methodology for improving the robustness and generalization of low-level vision models.","authors":["Jinfan Hu","Zhiyuan You","Jinjin Gu","Kaiwen Zhu","Tianfan Xue","Chao Dong"],"pdf_url":"","comment":"arXiv admin note: substantial text overlap with arXiv:2305.15134"},{"id":"http://arxiv.org/abs/2602.20597v1","updated":"2026-02-24T06:39:18Z","published":"2026-02-24T06:39:18Z","title":"Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing","summary":"A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to \"interaction illusion\", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.","authors":["Yuejiao Su","Yi Wang","Lei Yao","Yawen Cui","Lap-Pui Chau"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.20340v2","updated":"2026-02-24T06:36:53Z","published":"2025-12-23T13:15:31Z","title":"The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection","summary":"Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.","authors":["Qingdong He","Xueqin Chen","Yanjie Pan","Peng Tang","Pengcheng Xu","Zhenye Gan","Chengjie Wang","Xiaobin Hu","Jiangning Zhang","Yabiao Wang"],"pdf_url":"","comment":"Accepted by CVPR 2026 (Main Conference)"},{"id":"http://arxiv.org/abs/2602.18936v2","updated":"2026-02-24T06:30:28Z","published":"2026-02-21T19:05:11Z","title":"CRAFT-LoRA: Content-Style Personalization via Rank-Constrained Adaptation and Training-Free Fusion","summary":"Personalized image generation requires effectively balancing content fidelity with stylistic consistency when synthesizing images based on text and reference examples. Low-Rank Adaptation (LoRA) offers an efficient personalization approach, with potential for precise control through combining LoRA weights on different concepts. However, existing combination techniques face persistent challenges: entanglement between content and style representations, insufficient guidance for controlling elements' influence, and unstable weight fusion that often require additional training. We address these limitations through CRAFT-LoRA, with complementary components: (1) rank-constrained backbone fine-tuning that injects low-rank projection residuals to encourage learning decoupled content and style subspaces; (2) a prompt-guided approach featuring an expert encoder with specialized branches that enables semantic extension and precise control through selective adapter aggregation; and (3) a training-free, timestep-dependent classifier-free guidance scheme that enhances generation stability by strategically adjusting noise predictions across diffusion steps. Our method significantly improves content-style disentanglement, enables flexible semantic control over LoRA module combinations, and achieves high-fidelity generation without additional retraining overhead.","authors":["Yu Li","Yujun Cai","Chi Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.16705v2","updated":"2026-02-24T06:15:16Z","published":"2026-02-18T18:55:02Z","title":"Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation","summary":"Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.","authors":["Runpei Dong","Ziyan Li","Xialin He","Saurabh Gupta"],"pdf_url":"","comment":"Project page: https://hero-humanoid.github.io/"},{"id":"http://arxiv.org/abs/2602.20584v1","updated":"2026-02-24T06:12:51Z","published":"2026-02-24T06:12:51Z","title":"Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change","summary":"Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.","authors":["Beverley Gorry","Tobias Fischer","Michael Milford","Alejandro Fontan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20583v1","updated":"2026-02-24T06:11:08Z","published":"2026-02-24T06:11:08Z","title":"PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models","summary":"Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.","authors":["Wonyong Seo","Jaeho Moon","Jaehyup Lee","Soo Ye Kim","Munchurl Kim"],"pdf_url":"","comment":"The first two authors contributed equally to this work (equal contribution)"},{"id":"http://arxiv.org/abs/2602.09050v2","updated":"2026-02-24T06:08:05Z","published":"2026-02-06T21:01:27Z","title":"SAS-Net: Scene-Appearance Separation Network for Robust Spatiotemporal Registration in Bidirectional Photoacoustic Microscopy","summary":"High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional scanning enables rapid functional brain imaging but introduces severe spatiotemporal\n  misalignment from coupled scan-direction-dependent domain shift and geometric distortion. Conventional registration methods rely on brightness constancy, an assumption\n  violated under bidirectional scanning, leading to unreliable alignment. A unified scene-appearance separation framework is proposed to jointly address domain shift and\n  spatial misalignment. The proposed architecture separates domain-invariant scene content from domain-specific appearance characteristics, enabling cross-domain\n  reconstruction with geometric preservation. A scene consistency loss promotes geometric correspondence in the latent space, linking domain shift correction with spatial\n  registration within a single framework. For in vivo mouse brain vasculature imaging, the proposed method achieves normalized cross-correlation (NCC) of 0.961 and\n  structural similarity index (SSIM) of 0.894, substantially outperforming conventional methods. Ablation studies demonstrate that domain alignment loss is critical,\n  with its removal causing 82% NCC reduction (0.961 to 0.175), while scene consistency and cycle consistency losses provide complementary regularization for optimal\n  performance. The method achieves 11.2 ms inference time per frame (86 fps), substantially exceeding typical OR-PAM acquisition rates and enabling real-time processing.\n  These results suggest that the proposed framework enables robust high-speed bidirectional OR-PAM for reliable quantitative and longitudinal functional imaging. The code will be publicly available at https://github.com/D-ST-Sword/SAS-Net","authors":["Jiahao Qin"],"pdf_url":"","comment":"21 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2602.20577v1","updated":"2026-02-24T05:59:10Z","published":"2026-02-24T05:59:10Z","title":"Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion","summary":"Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.","authors":["Jiaru Zhang","Manav Gagvani","Can Cui","Juntong Peng","Ruqi Zhang","Ziran Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20575v1","updated":"2026-02-24T05:57:18Z","published":"2026-02-24T05:57:18Z","title":"An interactive enhanced driving dataset for autonomous driving","summary":"The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.","authors":["Haojie Feng","Peizhi Zhang","Mengjie Tian","Xinrui Zhang","Zhuoren Li","Junpeng Huang","Xiurong Wang","Junfan Zhu","Jianzhou Wang","Dongxiao Yin","Lu Xiong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22908v4","updated":"2026-02-24T05:52:17Z","published":"2025-05-28T22:17:24Z","title":"Learning Hierarchical Sparse Transform Coding for 3DGS Compression","summary":"Current 3DGS compression methods largely forego the neural analysis-synthesis transform, which is a crucial component in learned signal compression systems. As a result, redundancy removal is left solely to the entropy coder, overburdening the entropy coding module and reducing rate-distortion (R-D) performance. To fix this critical omission, we propose a training-time transform coding (TTC) method that adds the analysis-synthesis transform and optimizes it jointly with the 3DGS representation and entropy model. Concretely, we adopt a hierarchical design: a channel-wise KLT for decorrelation and energy compaction, followed by a sparsity-aware neural transform that reconstructs the KLT residuals with minimal parameter and computational overhead. Experiments show that our method delivers strong R-D performance with fast decoding, offering a favorable BD-rate-decoding-time trade-off over SOTA 3DGS compressors.","authors":["Hao Xu","Xiaolin Wu","Xi Zhang"],"pdf_url":"","comment":"Our code will be released at \\href{https://github.com/hxu160/SHTC_for_3DGS_compression}{here}"},{"id":"http://arxiv.org/abs/2601.16694v2","updated":"2026-02-24T05:51:34Z","published":"2026-01-23T12:20:36Z","title":"Affinity Contrastive Learning for Skeleton-based Human Activity Understanding","summary":"In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.","authors":["Hongda Liu","Yunfan Liu","Min Ren","Lin Sui","Yunlong Wang","Zhenan Sun"],"pdf_url":"","comment":"Accepted by TBIOM"},{"id":"http://arxiv.org/abs/2602.20569v1","updated":"2026-02-24T05:37:35Z","published":"2026-02-24T05:37:35Z","title":"AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents","summary":"We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.","authors":["Jiaqi Wu","Yuchen Zhou","Muduo Xu","Zisheng Liang","Simiao Ren","Jiayu Xue","Meige Yang","Siying Chen","Jingheng Huan"],"pdf_url":"","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2602.20566v1","updated":"2026-02-24T05:31:52Z","published":"2026-02-24T05:31:52Z","title":"BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model","summary":"Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.","authors":["Haosheng Li","Weixin Mao","Zihan Lan","Hongwei Xiong","Hongan Wang","Chenyang Si","Ziwei Liu","Xiaoming Deng","Hua Chen"],"pdf_url":"","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2602.20556v1","updated":"2026-02-24T05:14:05Z","published":"2026-02-24T05:14:05Z","title":"WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos","summary":"Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.","authors":["Hanhui Li","Xuan Huang","Wanquan Liu","Yuhao Cheng","Long Chen","Yiqiang Yan","Xiaodan Liang","Chenqiang Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20551v1","updated":"2026-02-24T05:10:22Z","published":"2026-02-24T05:10:22Z","title":"CAD-Prompted SAM3: Geometry-Conditioned Instance Segmentation for Industrial Objects","summary":"Verbal-prompted segmentation is inherently limited by the expressiveness of natural language and struggles with uncommon, instance-specific, or difficult-to-describe objects: scenarios frequently encountered in manufacturing and 3D printing environments. While image exemplars provide an alternative, they primarily encode appearance cues such as color and texture, which are often unrelated to a part's geometric identity. In industrial settings, a single component may be produced in different materials, finishes, or colors, making appearance-based prompting unreliable. In contrast, such objects are typically defined by precise CAD models that capture their canonical geometry. We propose a CAD-prompted segmentation framework built on SAM3 that uses canonical multi-view renderings of a CAD model as prompt input. The rendered views provide geometry-based conditioning independent of surface appearance. The model is trained using synthetic data generated from mesh renderings in simulation under diverse viewpoints and scene contexts. Our approach enables single-stage, CAD-prompted mask prediction, extending promptable segmentation to objects that cannot be robustly described by language or appearance alone.","authors":["Zhenran Tang","Rohan Nagabhirava","Changliu Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20550v1","updated":"2026-02-24T05:07:06Z","published":"2026-02-24T05:07:06Z","title":"The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation","summary":"Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.","authors":["Chengshuai Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20549v1","updated":"2026-02-24T05:06:46Z","published":"2026-02-24T05:06:46Z","title":"Sample-efficient evidence estimation of score based priors for model selection","summary":"The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.","authors":["Frederic Wang","Katherine L. Bouman"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20548v1","updated":"2026-02-24T05:06:12Z","published":"2026-02-24T05:06:12Z","title":"Robust Spiking Neural Networks Against Adversarial Attacks","summary":"Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.","authors":["Shuai Wang","Malu Zhang","Yulin Jiang","Dehao Zhang","Ammar Belatreche","Yu Liang","Yimeng Shan","Zijian Zhou","Yang Yang","Haizhou Li"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2509.25774v3","updated":"2026-02-24T05:01:33Z","published":"2025-09-30T04:43:58Z","title":"PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models","summary":"While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.","authors":["Jeongjae Lee","Jong Chul Ye"],"pdf_url":"","comment":"35 pages, 20 figures. ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20543v1","updated":"2026-02-24T04:48:05Z","published":"2026-02-24T04:48:05Z","title":"Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing","summary":"Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.","authors":["Subhra Jyoti Mandal","Lara Rachidi","Puneet Jain","Matthieu Duvinage","Sander W. Timmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.18701v2","updated":"2026-02-24T04:41:22Z","published":"2025-10-21T14:56:46Z","title":"UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation","summary":"Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.","authors":["Yibin Wang","Zhimin Li","Yuhang Zang","Jiazi Bu","Yujie Zhou","Yi Xin","Junjun He","Chunyu Wang","Qinglin Lu","Cheng Jin","Jiaqi Wang"],"pdf_url":"","comment":"Project page: codegoat24.github.io/UniGenBench/"},{"id":"http://arxiv.org/abs/2602.20539v1","updated":"2026-02-24T04:37:18Z","published":"2026-02-24T04:37:18Z","title":"Progressive Per-Branch Depth Optimization for DEFOM-Stereo and SAM3 Joint Analysis in UAV Forestry Applications","summary":"Accurate per-branch 3D reconstruction is a prerequisite for autonomous UAV-based tree pruning; however, dense disparity maps from modern stereo matchers often remain too noisy for individual branch analysis in complex forest canopies. This paper introduces a progressive pipeline integrating DEFOM-Stereo foundation-model disparity estimation, SAM3 instance segmentation, and multi-stage depth optimization to deliver robust per-branch point clouds. Starting from a naive baseline, we systematically identify and resolve three error families through successive refinements. Mask boundary contamination is first addressed through morphological erosion and subsequently refined via a skeleton-preserving variant to safeguard thin-branch topology. Segmentation inaccuracy is then mitigated using LAB-space Mahalanobis color validation coupled with cross-branch overlap arbitration. Finally, depth noise - the most persistent error source - is initially reduced by outlier removal and median filtering, before being superseded by a robust five-stage scheme comprising MAD global detection, spatial density consensus, local MAD filtering, RGB-guided filtering, and adaptive bilateral filtering. Evaluated on 1920x1080 stereo imagery of Radiata pine (Pinus radiata) acquired with a ZED Mini camera (63 mm baseline) from a UAV in Canterbury, New Zealand, the proposed pipeline reduces the average per-branch depth standard deviation by 82% while retaining edge fidelity. The result is geometrically coherent 3D point clouds suitable for autonomous pruning tool positioning. All code and processed data are publicly released to facilitate further UAV forestry research.","authors":["Yida Lin","Bing Xue","Mengjie Zhang","Sam Schofield","Richard Green"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.12370v3","updated":"2026-02-24T04:31:25Z","published":"2025-11-15T22:12:16Z","title":"Changes in Real Time: Online Scene Change Detection with Multi-View Fusion","summary":"Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.","authors":["Chamuditha Jayanga Galappaththige","Jason Lai","Lloyd Windrim","Donald Dansereau","Niko Sünderhauf","Dimity Miller"],"pdf_url":"","comment":"Accepted at CVPR 2026. Project Page: https://chumsy0725.github.io/O-SCD/"},{"id":"http://arxiv.org/abs/2602.20537v1","updated":"2026-02-24T04:31:12Z","published":"2026-02-24T04:31:12Z","title":"PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning","summary":"Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \\times k$ followed by $k \\times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.","authors":["Xinyong Cai","Changbin Sun","Yong Wang","Hongyu Yang","Yuankai Wu"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.17419v2","updated":"2026-02-24T04:24:55Z","published":"2026-02-19T14:50:58Z","title":"EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models","summary":"Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at","authors":["Xiaomeng Peng","Xilang Huang","Seon Han Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.00618v3","updated":"2026-02-24T04:20:09Z","published":"2024-09-01T05:09:32Z","title":"RegTrack: Simplicity Beneath Complexity in Robust Multi-Modal 3D Multi-Object Tracking","summary":"Existing 3D multi-object tracking (MOT) methods often sacrifice efficiency and generalizability for robustness, largely relying on complex association metrics derived from multi-modal architectures and class-specific motion priors. Challenging the rooted belief that greater complexity necessarily yields greater robustness, we propose a robust, efficient, and generalizable method for multi-modal 3D MOT, dubbed RegTrack. Inspired by Yang-Mills gauge theory, RegTrack is built upon a unified tri-cue encoder (UTEnc), comprising three tightly coupled components: a local-global point cloud encoder (LG-PEnc), a mixture-of-experts-based geometry encoder (MoE-GEnc), and an image encoder from a well-pretrained visual-language model. LG-PEnc efficiently encodes the spatial and structural information of point clouds to produce foundational representations for each object, whose pairwise similarities serve as the sole association metric. MoE-GEnc seamlessly interacts with LG-PEnc to model inter-object geometric relationships across frames, adaptively compensating for inter-frame object motion without relying on any class-specific priors. The image encoder is kept frozen and is used exclusively during training to provide a well-pretrained representation space. Point cloud representations are aligned to this space to supervise the motion compensation process, encouraging representation invariance across frames for the same object while enhancing discriminability among different objects. Through this formulation, RegTrack attains robust, efficient, and generalizable inference using only point cloud inputs, requiring just 2.6M parameters. Extensive experiments on KITTI and nuScenes show that RegTrack outperforms its thirty-five competitors.","authors":["Lipeng Gu","Xuefeng Yan","Song Wang","Mingqiang Wei"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2602.21202v1","updated":"2026-02-24T18:57:33Z","published":"2026-02-24T18:57:33Z","title":"Multi-Vector Index Compression in Any Modality","summary":"We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.","authors":["Hanxiang Qin","Alexander Martin","Rohan Jha","Chunsheng Zuo","Reno Kriz","Benjamin Van Durme"],"pdf_url":"","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.06873v2","updated":"2026-02-24T18:15:44Z","published":"2025-01-12T17:03:45Z","title":"Causal Claims in Economics","summary":"As economics scales, a key bottleneck is representing what papers claim in a comparable, aggregable form. We introduce evidence-annotated claim graphs that map each paper into a directed network of standardized economic concepts (nodes) and stated relationships (edges), with each edge labeled by evidentiary basis, including whether it is supported by causal inference designs or by non-causal evidence. Using a structured multi-stage AI workflow, we construct claim graphs for 44,852 economics papers from 1980-2023. The share of causal edges rises from 7.7% in 1990 to 31.7% in 2020. Measures of causal narrative structure and causal novelty are positively associated with top-five publication and long-run citations, whereas non-causal counterparts are weakly related or negative.","authors":["Prashant Garg","Thiemo Fetzer"],"pdf_url":"","comment":"Data, code, prompts, and workflow documentation are publicly available at our GitHub repository: https://github.com/prashgarg/CausalClaimsInEconomics"},{"id":"http://arxiv.org/abs/2602.21143v1","updated":"2026-02-24T17:43:32Z","published":"2026-02-24T17:43:32Z","title":"A Benchmark for Deep Information Synthesis","summary":"Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.","authors":["Debjit Paul","Daniel Murphy","Milan Gritta","Ronald Cardenas","Victor Prokhorov","Lena Sophia Bolliger","Aysim Toker","Roy Miles","Andreea-Maria Oncescu","Jasivan Alex Sivakumar","Philipp Borchert","Ismail Elezi","Meiru Zhang","Ka Yiu Lee","Guchun Zhang","Jun Wang","Gerasimos Lampouras"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21103v1","updated":"2026-02-24T17:03:21Z","published":"2026-02-24T17:03:21Z","title":"Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning","summary":"Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\\% to 90.0\\% and 67\\% to 83\\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.","authors":["Sanket Badhe","Deep Shah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21099v1","updated":"2026-02-24T17:01:47Z","published":"2026-02-24T17:01:47Z","title":"Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering","summary":"Large Language Models (LLMs) have shown great potential for enhancing recommender systems through their extensive world knowledge and reasoning capabilities. However, effectively translating these semantic signals into traditional collaborative embeddings remains an open challenge. Existing approaches typically fall into two extremes: direct inference methods are computationally prohibitive for large-scale retrieval, while embedding-based methods primarily focus on unilateral feature augmentation rather than holistic collaborative signal enhancement. To bridge this gap, we propose Topology-Augmented Graph Collaborative Filtering (TAGCF), a novel framework that transforms semantic knowledge into topological connectivity. Unlike existing approaches that depend on textual features or direct interaction synthesis, TAGCF employs LLMs to infer interaction intents and underlying causal relationships from user-item pairs, representing these insights as intermediate attribute nodes within an enriched User-Attribute-Item (U-A-I) graph. Furthermore, to effectively model the heterogeneous relations in this augmented structure, we propose Adaptive Relation-weighted Graph Convolution (ARGC), which employs relation-specific prediction networks to dynamically estimate the importance of each relation type. Extensive experiments across multiple benchmark datasets and CF backbones demonstrate consistent improvements, with comprehensive evaluations including cold-start scenarios validating the effectiveness and robustness of our framework. All code will be made publicly available. For anonymous review, our code is available at the following anonymous link: https://anonymous.4open.science/r/AGCF-2441353190/.","authors":["Junjie Meng","Ranxu zhang","Wei Wu","Rui Zhang","Chuan Qin","Qi Zhang","Qi Liu","Hui Xiong","Chao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21052v1","updated":"2026-02-24T16:09:47Z","published":"2026-02-24T16:09:47Z","title":"Position-Aware Sequential Attention for Accurate Next Item Recommendations","summary":"Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.","authors":["Timur Nabiev","Evgeny Frolov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.09448v2","updated":"2026-02-24T15:35:33Z","published":"2026-02-10T06:33:10Z","title":"The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training","summary":"Prior synthetic query generation for dense retrieval produces one query per document, focusing on quality. We systematically study multi-query synthesis, discovering a quality-diversity trade-off: quality benefits in-domain, diversity benefits out-of-domain (OOD). Experiments on 31 datasets show diversity especially benefits multi-hop retrieval. Analysis reveals diversity benefit correlates with query complexity ($r$$\\geq$0.95), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides thresholds (CW$>$10: use diversity; CW$<$7: avoid it) and enables CW-weighted training that improves OOD even with single-query data.","authors":["Xincan Feng","Noriki Nishida","Yusuke Sakai","Yuji Matsumoto"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.21009v1","updated":"2026-02-24T15:28:58Z","published":"2026-02-24T15:28:58Z","title":"HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders","summary":"Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we propose Hierarchical Sparse Activation Compression (HiSAC), an efficient framework for personalized sequence modeling. HiSAC encodes interactions into multi-level semantic IDs and constructs a global hierarchical codebook. A hierarchical voting mechanism sparsely activates personalized interest-agents as fine-grained preference centers. Guided by these agents, Soft-Routing Attention aggregates historical signals in semantic space, weighting by similarity to minimize quantization error and retain long-tail behaviors. Deployed on Taobao's \"Guess What You Like\" homepage, HiSAC achieves significant compression and cost reduction, with online A/B tests showing a consistent 1.65% CTR uplift -- demonstrating its scalability and real-world effectiveness.","authors":["Kun Yuan","Junyu Bi","Daixuan Cheng","Changfa Wu","Shuwen Xiao","Binbin Cao","Jian Wu","Yuning Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20995v1","updated":"2026-02-24T15:14:49Z","published":"2026-02-24T15:14:49Z","title":"Generative Pseudo-Labeling for Pre-Ranking with LLMs","summary":"Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.","authors":["Junyu Bi","Xinting Niu","Daixuan Cheng","Kun Yuan","Tao Wang","Binbin Cao","Jian Wu","Yuning Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20986v1","updated":"2026-02-24T15:09:01Z","published":"2026-02-24T15:09:01Z","title":"Naver Labs Europe @ WSDM CUP | Multilingual Retrieval","summary":"This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings.\n  We evaluate five progressively enhanced runs, starting from a SPLARE-7B model and incorporating lightweight improvements, including reranking with Qwen3-Reranker-4B and simple score fusion strategies. Our results demonstrate the strength of SPLARE compared to state-of-the-art dense baselines such as Qwen3-8B-Embed. More broadly, our submission highlights the continued relevance and competitiveness of learned sparse retrieval models beyond English-centric scenarios.","authors":["Thibault Formal","Maxime Louis","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"","comment":"Report paper of our submission to the WSDM Cup 2026"},{"id":"http://arxiv.org/abs/2602.20877v1","updated":"2026-02-24T13:19:42Z","published":"2026-02-24T13:19:42Z","title":"E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications","summary":"Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.","authors":["Jiwoo Kang","Yeon-Chang Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.05598v3","updated":"2026-02-24T12:32:53Z","published":"2025-10-07T05:48:05Z","title":"AgentDR: Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents","summary":"Recent agent-based recommendation frameworks aim to simulate user behaviors by incorporating memory mechanisms and prompting strategies, but they struggle with hallucinating non-existent items and full-catalog ranking. Besides, a largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning to capture user intent through substitute and complement relationships between items, which are usually implicit in datasets and difficult for traditional ID-based recommenders to capture. In this work, we propose a novel LLM-agent framework, AgenDR, which bridges LLM reasoning with scalable recommendation tools. Our approach delegates full-ranking tasks to traditional models while utilizing LLMs to (i) integrate multiple recommendation outputs based on personalized tool suitability and (ii) reason over substitute and complement relationships grounded in user history. This design mitigates hallucination, scales to large catalogs, and enhances recommendation relevance through relational reasoning. Through extensive experiments on three public grocery datasets, we show that our framework achieves superior full-ranking performance, yielding on average a twofold improvement over its underlying tools. We also introduce a new LLM-based evaluation metric that jointly measures semantic alignment and ranking correctness.","authors":["Mingdai Yang","Nurendra Choudhary","Jiangshu Du","Edward W. Huang","Philip S. Yu","Karthik Subbian","Danai Koutra"],"pdf_url":"","comment":"12 pages, accepted by WWW'26 as long paper"},{"id":"http://arxiv.org/abs/2602.20800v1","updated":"2026-02-24T11:38:36Z","published":"2026-02-24T11:38:36Z","title":"Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking","summary":"In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.","authors":["Dalia Nahhas","Xiaohao Cai","Imran Razzak","Shoaib Jameel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20735v1","updated":"2026-02-24T09:58:25Z","published":"2026-02-24T09:58:25Z","title":"RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition","summary":"This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\n  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n  (R2RAG), a research-focused retrieval-augmented generation (RAG)\n  architecture composed of lightweight components that dynamically adapt the\n  retrieval strategy based on inferred query complexity and evidence\n  sufficiency. The system uses smaller LLMs, enabling operation on a single\n  consumer-grade GPU while supporting complex research tasks. It builds on the\n  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\n  with modules informed by qualitative review of outputs. R2RAG won the Best\n  Dynamic Evaluation award in the Open Source category, demonstrating high\n  effectiveness with careful design and efficient use of resources.","authors":["Kun Ran","Marwah Alaofi","Danula Hettiachchi","Chenglong Ma","Khoi Nguyen Dinh Anh","Khoi Vo Nguyen","Sachin Pathiyan Cherumanal","Lida Rashidi","Falk Scholer","Damiano Spina","Shuoqi Sun","Oleg Zendel"],"pdf_url":"","comment":"MMU-RAG NeurIPS 2025 winning system"},{"id":"http://arxiv.org/abs/2504.12007v5","updated":"2026-02-24T09:25:50Z","published":"2025-04-16T12:01:03Z","title":"Diffusion Generative Recommendation with Continuous Tokens","summary":"Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys). Most existing LLM-based RecSys approaches operate in a discrete space, using vector-quantized tokenizers to align with the inherent discrete nature of language models. However, these quantization methods often result in lossy tokenization and suboptimal learning, primarily due to inaccurate gradient propagation caused by the non-differentiable argmin operation in standard vector quantization. Inspired by the emerging trend of embracing continuous tokens in language models, we propose ContRec, a novel framework that seamlessly integrates continuous tokens into LLM-based RecSys. Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which encodes users/items with continuous tokens; and a Dispersive Diffusion module, which captures implicit user preference. The tokenizer is trained with a continuous Variational Auto-Encoder (VAE) objective, where three effective techniques are adopted to avoid representation collapse. By conditioning on the previously generated tokens of the LLM backbone during user modeling, the Dispersive Diffusion module performs a conditional diffusion process with a novel Dispersive Loss, enabling high-quality user preference generation through next-token diffusion. Finally, ContRec leverages both the textual reasoning output from the LLM and the latent representations produced by the diffusion model for Top-K item retrieval, thereby delivering comprehensive recommendation results. Extensive experiments on four datasets demonstrate that ContRec consistently outperforms both traditional and SOTA LLM-based recommender systems. Our results highlight the potential of continuous tokenization and generative modeling for advancing the next generation of recommender systems.","authors":["Haohao Qu","Shanru Lin","Yujuan Ding","Yiqi Wang","Wenqi Fan"],"pdf_url":"","comment":"Accepted by The ACM Web Conference (WWW 2026)"},{"id":"http://arxiv.org/abs/2602.20704v1","updated":"2026-02-24T09:09:40Z","published":"2026-02-24T09:09:40Z","title":"IntRR: A Framework for Integrating SID Redistribution and Length Reduction","summary":"Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.","authors":["Zesheng Wang","Longfei Xu","Weidong Deng","Huimin Yan","Kaikui Liu","Xiangxiang Chu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14640v3","updated":"2026-02-24T09:00:19Z","published":"2025-10-16T12:54:40Z","title":"LUMI: Unsupervised Intent Clustering with Multiple Pseudo-Labels","summary":"In this paper, we propose an intuitive, training-free and label-free method for intent clustering in conversational search. Current approaches to short text clustering use LLM-generated pseudo-labels to enrich text representations or to identify similar text pairs for pooling. The limitations are: (1) each text is assigned only a single label, and refining representations toward a single label can be unstable; (2) text-level similarity is treated as a binary selection, which fails to account for continuous degrees of similarity. Our method LUMI is designed to amplify similarities between texts by using shared pseudo-labels. We first generate pseudo-labels for each text and collect them into a pseudo-label set. Next, we compute the mean of the pseudo-label embeddings and pool it with the text embedding. Finally, we perform text-level pooling: Each text representation is pooled with its similar pairs, where similarity is determined by the degree of shared labels. Our evaluation on four benchmark sets shows that our approach achieves competitive results, better than recent state-of-the-art baselines, while avoiding the need to estimate the number of clusters during embedding refinement, as is required by most methods. Our findings indicate that LUMI can effectively be applied in unsupervised short-text clustering scenarios.","authors":["I-Fan Lin","Faegheh Hasibi","Suzan Verberne"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20676v1","updated":"2026-02-24T08:26:17Z","published":"2026-02-24T08:26:17Z","title":"PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization","summary":"In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.","authors":["Shuzhi Cao","Rong Chen","Ailong He","Shuguang Han","Jufeng Chen"],"pdf_url":"","comment":"arXiv admin note: text overlap with arXiv:2503.18395"},{"id":"http://arxiv.org/abs/2512.00711v2","updated":"2026-02-24T06:47:48Z","published":"2025-11-30T03:19:59Z","title":"Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation","summary":"Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.","authors":["Loc X. Nguyen","Ji Su Yoon","Huy Q. Le","Yu Qiao","Avi Deb Raha","Eui-Nam Huh","Walid Saad","Dusit Niyato","Zhu Han","Choong Seon Hong"],"pdf_url":"","comment":"13 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2602.20558v1","updated":"2026-02-24T05:15:24Z","published":"2026-02-24T05:15:24Z","title":"From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production","summary":"Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.","authors":["Yucheng Shi","Ying Li","Yu Wang","Yesu Feng","Arjun Rao","Rein Houthooft","Shradha Sehgal","Jin Wang","Hao Zhen","Ninghao Liu","Linas Baltrunas"],"pdf_url":"","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2602.20507v1","updated":"2026-02-24T03:17:36Z","published":"2026-02-24T03:17:36Z","title":"Indaleko: The Unified Personal Index","summary":"Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like \"photos near the conference venue last spring\" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.","authors":["William Anthony Mason"],"pdf_url":"","comment":"PhD dissertation, University of British Columbia, August 2025. 287 pages"},{"id":"http://arxiv.org/abs/2510.09167v2","updated":"2026-02-24T22:32:41Z","published":"2025-10-10T09:09:10Z","title":"Hierarchical Semantic RL: Tackling the Problem of Dynamic Action Space for RL-based Recommendations","summary":"Recommender Systems (RS) are fundamental to modern online services. While most existing approaches optimize for short-term engagement, recent work has begun to explore reinforcement learning (RL) to model long-term user value. However, these efforts face significant challenges due to the vast, dynamic action spaces inherent in RS, which hinder stable policy learning. To resolve this bottleneck, we introduce Hierarchical Semantic RL (HSRL), which reframes RL-based recommendation over a fixed Semantic Action Space (SAS). HSRL encodes items as Semantic IDs (SIDs) for policy learning, and maps SIDs back to their original items via a fixed lookup during execution. To align decision-making with SID generation, the Hierarchical Policy Network (HPN) operates in a coarse-to-fine manner, employing hierarchical residual state modeling to refine each level's context from the previous level's residual, thereby reducing representation-decision mismatch. In parallel, a Multi-level Critic (MLC) provides token-level value estimates, enabling fine-grained credit assignment. Across public benchmarks and a large-scale production dataset from a leading short-video advertising platform, HSRL consistently surpasses state-of-the-art baselines. In online deployment over a 7-day A/B testing, it delivers an 18.421% ADVV lift and a 1.251% increase in Revenue, supporting HSRL as a scalable paradigm for RL-based recommendation.","authors":["Minmao Wang","Xingchen Liu","Shijie Yi","Likang Wu","Hongke Zhao","Fei Pan","Qingpeng Cai","Peng Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21351v1","updated":"2026-02-24T20:37:38Z","published":"2026-02-24T20:37:38Z","title":"A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives","summary":"The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.","authors":["Dmitrii Pantiukhin","Ivan Kuznetsov","Boris Shapkin","Antonia Anna Jost","Thomas Jung","Nikolay Koldunov"],"pdf_url":"","comment":"20 pages, 6 figures, 7 tables, supplementary material included"},{"id":"http://arxiv.org/abs/2601.14348v2","updated":"2026-02-24T20:01:50Z","published":"2026-01-20T17:08:34Z","title":"Legal Retrieval for Public Defenders","summary":"AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.","authors":["Dominik Stammbach","Kylie Zhang","Patty Liu","Nimra Nadeem","Inyoung Cheong","Lucia Zheng","Peter Henderson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.00012v2","updated":"2026-02-24T19:06:57Z","published":"2025-11-30T15:41:20Z","title":"OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models","summary":"We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.","authors":["Michael Siebenmann","Javier Argota Sánchez-Vaquerizo","Stefan Arisona","Krystian Samp","Luis Gisler","Dirk Helbing"],"pdf_url":"","comment":"Updated references & added first author's second affiliation. 7 pages, 6 figures. Accepted at IEEE Conference on Artificial Intelligence 2026. Code & data available at: https://github.com/ethz-coss/ogd4all"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2602.21204v1","updated":"2026-02-24T18:59:30Z","published":"2026-02-24T18:59:30Z","title":"Test-Time Training with KV Binding Is Secretly Linear Attention","summary":"Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.","authors":["Junchen Liu","Sven Elflein","Or Litany","Zan Gojcic","Ruilong Li"],"pdf_url":"","comment":"Webpage: https://research.nvidia.com/labs/sil/projects/tttla/"},{"id":"http://arxiv.org/abs/2509.26626v2","updated":"2026-02-24T18:58:30Z","published":"2025-09-30T17:58:03Z","title":"Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models","summary":"Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA with Gemini 3 Flash attains performance near the top of the ARC-AGI-2 public leaderboard. RSA also enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further propose a novel aggregation-aware reinforcement learning approach that yields significant performance gains by training the model to combine solutions.","authors":["Siddarth Venkatraman","Vineet Jain","Sarthak Mittal","Vedant Shah","Johan Obando-Ceron","Yoshua Bengio","Brian R. Bartoldson","Bhavya Kailkhura","Guillaume Lajoie","Glen Berseth","Nikolay Malkin","Moksh Jain"],"pdf_url":"","comment":"23 pages, 10 figures. Project page: https://rsa-llm.github.io/"},{"id":"http://arxiv.org/abs/2602.21203v1","updated":"2026-02-24T18:58:11Z","published":"2026-02-24T18:58:11Z","title":"Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics","summary":"Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.","authors":["Abdulaziz Almuzairee","Henrik I. Christensen"],"pdf_url":"","comment":"For website and code, see https://aalmuzairee.github.io/squint"},{"id":"http://arxiv.org/abs/2602.21201v1","updated":"2026-02-24T18:56:10Z","published":"2026-02-24T18:56:10Z","title":"Aletheia tackles FirstProof autonomously","summary":"We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.","authors":["Tony Feng","Junehyuk Jung","Sang-hyun Kim","Carlo Pagano","Sergei Gukov","Chiang-Chiang Tsai","David Woodruff","Adel Javanmard","Aryan Mokhtari","Dawsen Hwang","Yuri Chervonyi","Jonathan N. Lee","Garrett Bingham","Trieu H. Trinh","Vahab Mirrokni","Quoc V. Le","Thang Luong"],"pdf_url":"","comment":"34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia"},{"id":"http://arxiv.org/abs/2602.21198v1","updated":"2026-02-24T18:55:18Z","published":"2026-02-24T18:55:18Z","title":"Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs","summary":"Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.","authors":["Yining Hong","Huang Huang","Manling Li","Li Fei-Fei","Jiajun Wu","Yejin Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21196v1","updated":"2026-02-24T18:54:39Z","published":"2026-02-24T18:54:39Z","title":"Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking","summary":"Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\\times$H100 node, improving upon prior methods by over 25$\\%$.","authors":["Ravi Ghadia","Maksim Abraham","Sergei Vorobyov","Max Ryabinin"],"pdf_url":"","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.04448v2","updated":"2026-02-24T18:49:11Z","published":"2025-07-06T16:14:43Z","title":"Transfer Learning in Infinite Width Feature Learning Networks","summary":"We develop a theory of transfer learning in infinitely wide neural networks under gradient flow that quantifies when pretraining on a source task improves generalization on a target task. We analyze both (i) fine-tuning, when the downstream predictor is trained on top of source-induced features and (ii) a jointly rich setting, where both pretraining and downstream tasks can operate in a feature learning regime, but the downstream model is initialized with the features obtained after pre-training. In this setup, the summary statistics of randomly initialized networks after a rich pre-training are adaptive kernels which depend on both source data and labels. For (i), we analyze the performance of a readout for different pretraining data regimes. For (ii), the summary statistics after learning the target task are still adaptive kernels with features from both source and target tasks. We test our theory on linear and polynomial regression tasks as well as real datasets. Our theory allows interpretable conclusions on performance, which depend on the amount of data on both tasks, the alignment between tasks, and the feature learning strength.","authors":["Clarissa Lauditi","Blake Bordelon","Cengiz Pehlevan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21191v1","updated":"2026-02-24T18:46:46Z","published":"2026-02-24T18:46:46Z","title":"Statistical Query Lower Bounds for Smoothed Agnostic Learning","summary":"We study the complexity of smoothed agnostic learning, recently introduced by~\\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\\tilde{O}(1/σ^2) \\log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.","authors":["Ilias Diakonikolas","Daniel M. Kane"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21189v1","updated":"2026-02-24T18:43:08Z","published":"2026-02-24T18:43:08Z","title":"Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training","summary":"Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.","authors":["Anas Barakat","Souradip Chakraborty","Khushbu Pahwa","Amrit Singh Bedi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21185v1","updated":"2026-02-24T18:35:22Z","published":"2026-02-24T18:35:22Z","title":"The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum","summary":"Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2","authors":["Justin Deschenaux","Caglar Gulcehre","Subham Sekhar Sahoo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.18310v2","updated":"2026-02-24T18:23:32Z","published":"2025-04-25T12:37:15Z","title":"How much does context affect the accuracy of AI health advice?","summary":"Large language models (LLMs) are increasingly used to provide health advice, yet evidence on how their accuracy varies across languages, topics and information sources remains limited. We assess how linguistic and contextual factors affect the accuracy of AI-based health-claim verification. We evaluated seven widely used LLMs on two datasets: (i) 1,975 legally authorised nutrition and health claims from UK and EU regulatory registers translated into 21 languages; and (ii) 9,088 journalist-vetted public-health claims from the PUBHEALTH corpus spanning COVID-19, abortion, politics and general health, drawn from government advisories, scientific abstracts and media sources. Models classified each claim as supported or unsupported using majority voting across repeated runs. Accuracy was analysed by language, topic, source and model. Accuracy on authorised claims was highest in English and closely related European languages and declined in several widely spoken non-European languages, decreasing with syntactic distance from English. On real-world public-health claims, accuracy was substantially lower and varied systematically by topic and source. Models performed best on COVID-19 and government-attributed claims and worst on general health and scientific abstracts. High performance on English, canonical health claims masks substantial context-dependent gaps. Differences in training data exposure, editorial framing and topic-specific tuning likely contribute to these disparities, which are comparable in magnitude to cross-language differences. LLM accuracy in health-claim verification depends strongly on language, topic and information source. English-language performance does not reliably generalise across contexts, underscoring the need for multilingual, domain-specific evaluation before deployment in public-health communication.","authors":["Prashant Garg","Thiemo Fetzer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.17646v2","updated":"2026-02-24T18:15:39Z","published":"2026-02-19T18:54:34Z","title":"Multi-Round Human-AI Collaboration with User-Specified Requirements","summary":"As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.","authors":["Sima Noorani","Shayan Kiyani","Hamed Hassani","George Pappas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21168v1","updated":"2026-02-24T18:11:23Z","published":"2026-02-24T18:11:23Z","title":"Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma","summary":"Counterfactual inference enables clinicians to ask \"what if\" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -> AKI -> HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from \"what if this feature were different?\" to \"what if we had intervened earlier, and how would that propagate forward?\" --  yielding clinically actionable insights grounded in biological plausibility.","authors":["Jingya Cheng","Alaleh Azhir","Jiazi Tian","Hossein Estiri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.17457v2","updated":"2026-02-24T18:08:53Z","published":"2025-02-09T17:07:46Z","title":"MoEMba: A Mamba-based Mixture of Experts for High-Density EMG-based Hand Gesture Recognition","summary":"High-Density surface Electromyography (HDsEMG) has emerged as a pivotal resource for Human-Computer Interaction (HCI), offering direct insights into muscle activities and motion intentions. However, a significant challenge in practical implementations of HD-sEMG-based models is the low accuracy of inter-session and inter-subject classification. Variability between sessions can reach up to 40% due to the inherent temporal variability of HD-sEMG signals. Targeting this challenge, the paper introduces the MoEMba framework, a novel approach leveraging Selective StateSpace Models (SSMs) to enhance HD-sEMG-based gesture recognition. The MoEMba framework captures temporal dependencies and cross-channel interactions through channel attention techniques. Furthermore, wavelet feature modulation is integrated to capture multi-scale temporal and spatial relations, improving signal representation. Experimental results on the CapgMyo HD-sEMG dataset demonstrate that MoEMba achieves a balanced accuracy of 56.9%, outperforming its state-of-the-art counterparts. The proposed framework's robustness to session-to-session variability and its efficient handling of high-dimensional multivariate time series data highlight its potential for advancing HD-sEMG-powered HCI systems.","authors":["Mehran Shabanpour","Kasra Rad","Sadaf Khademi","Arash Mohammadi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21160v1","updated":"2026-02-24T18:05:51Z","published":"2026-02-24T18:05:51Z","title":"Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions","summary":"In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=σ_k^{2}/(2μ_k)$, with $μ_k{=}\\mathbb{E}[p_k]$ and $σ_k^2{=}\\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/μ_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\\sum_k C_k \\approx \\mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\\% over MI and 56.2\\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.","authors":["Mame Diarra Toure","David A. Stephens"],"pdf_url":"","comment":"8 pages, 17 figures"},{"id":"http://arxiv.org/abs/2602.21158v1","updated":"2026-02-24T18:04:54Z","published":"2026-02-24T18:04:54Z","title":"SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards","summary":"Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.","authors":["Dengjia Zhang","Xiaoou Liu","Lu Cheng","Yaqing Wang","Kenton Murray","Hua Wei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20156v2","updated":"2026-02-24T18:03:02Z","published":"2026-02-23T18:59:27Z","title":"Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks","summary":"LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.","authors":["David Schmotz","Luca Beurer-Kellner","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20159v2","updated":"2026-02-24T17:59:15Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2506.21220v4","updated":"2026-02-24T17:50:18Z","published":"2025-06-26T13:13:24Z","title":"Complexity-aware fine-tuning","summary":"General-purpose Large Language Models (LLMs) are frequently fine-tuned through supervised fine-tuning (SFT) to enhance performance in specific domains. Better results can be achieved by distilling the chain-of-thought of a larger model at the cost of numerous expensive calls and a much greater amount of data. We propose a novel blueprint for efficient fine-tuning that uses reasoning only for complex data identified by entropy. Specifically, across three small open models ($\\approx 3B$) we split the training data into complexity categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large language models (LLMs) via SFT and distillation, and show that our pipeline significantly outperforms the standard SFT approach ($0.58$ vs $0.45$ average accuracy) and outperforms the distillation approach ($0.58$ vs $0.56$ average accuracy) while using $81\\%$ less data.","authors":["Andrey Goncharov","Daniil Vyazhev","Petr Sychev","Edvard Khalafyan","Alexey Zaytsev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21144v1","updated":"2026-02-24T17:47:54Z","published":"2026-02-24T17:47:54Z","title":"Scaling State-Space Models on Multiple GPUs with Tensor Parallelism","summary":"Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.\n  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.","authors":["Anurag Dutt","Nimit Shah","Hazem Masarani","Anshul Gandhi"],"pdf_url":"","comment":"Submitted to 46th IEEE International Conference on Distributed Computing Systems (ICDCS 2026)"},{"id":"http://arxiv.org/abs/2602.21143v1","updated":"2026-02-24T17:43:32Z","published":"2026-02-24T17:43:32Z","title":"A Benchmark for Deep Information Synthesis","summary":"Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.","authors":["Debjit Paul","Daniel Murphy","Milan Gritta","Ronald Cardenas","Victor Prokhorov","Lena Sophia Bolliger","Aysim Toker","Roy Miles","Andreea-Maria Oncescu","Jasivan Alex Sivakumar","Philipp Borchert","Ismail Elezi","Meiru Zhang","Ka Yiu Lee","Guchun Zhang","Jun Wang","Gerasimos Lampouras"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21142v1","updated":"2026-02-24T17:42:46Z","published":"2026-02-24T17:42:46Z","title":"LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis","summary":"Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.","authors":["Zhifan Jiang","Dong Yang","Vishwesh Nath","Abhijeet Parida","Nishad P. Kulkarni","Ziyue Xu","Daguang Xu","Syed Muhammad Anwar","Holger R. Roth","Marius George Linguraru"],"pdf_url":"","comment":"Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2026"},{"id":"http://arxiv.org/abs/2602.21138v1","updated":"2026-02-24T17:35:46Z","published":"2026-02-24T17:35:46Z","title":"Complexity of Classical Acceleration for $\\ell_1$-Regularized PageRank","summary":"We study the degree-weighted work required to compute $\\ell_1$-regularized PageRank using the standard one-gradient-per-iteration accelerated proximal-gradient method (FISTA). For non-accelerated local methods, the best known worst-case work scales as $\\widetilde{O} ((αρ)^{-1})$, where $α$ is the teleportation parameter and $ρ$ is the $\\ell_1$-regularization parameter. A natural question is whether FISTA can improve the dependence on $α$ from $1/α$ to $1/\\sqrtα$ while preserving the $1/ρ$ locality scaling. The challenge is that acceleration can break locality by transiently activating nodes that are zero at optimality, thereby increasing the cost of gradient evaluations. We analyze FISTA on a slightly over-regularized objective and show that, under a checkable confinement condition, all spurious activations remain inside a boundary set $\\mathcal{B}$. This yields a bound consisting of an accelerated $(ρ\\sqrtα)^{-1}\\log(α/\\varepsilon)$ term plus a boundary overhead $\\sqrt{vol(\\mathcal{B})}/(ρα^{3/2})$. We provide graph-structural conditions that imply such confinement. Experiments on synthetic and real graphs show the resulting speedup and slowdown regimes under the degree-weighted work model.","authors":["Kimon Fountoulakis","David Martínez-Rubio"],"pdf_url":"","comment":"23 pages, 8 Figures"},{"id":"http://arxiv.org/abs/2602.21133v1","updated":"2026-02-24T17:29:04Z","published":"2026-02-24T17:29:04Z","title":"SOM-VQ: Topology-Aware Tokenization for Interactive Generative Models","summary":"Vector-quantized representations enable powerful discrete generative models but lack semantic structure in token space, limiting interpretable human control. We introduce SOM-VQ, a tokenization method that combines vector quantization with Self-Organizing Maps to learn discrete codebooks with explicit low-dimensional topology. Unlike standard VQ-VAE, SOM-VQ uses topology-aware updates that preserve neighborhood structure: nearby tokens on a learned grid correspond to semantically similar states, enabling direct geometric manipulation of the latent space. We demonstrate that SOM-VQ produces more learnable token sequences in the evaluated domains while providing an explicit navigable geometry in code space. Critically, the topological organization enables intuitive human-in-the-loop control: users can steer generation by manipulating distances in token space, achieving semantic alignment without frame-level constraints. We focus on human motion generation - a domain where kinematic structure, smooth temporal continuity, and interactive use cases (choreography, rehabilitation, HCI) make topology-aware control especially natural - demonstrating controlled divergence and convergence from reference sequences through simple grid-based sampling. SOM-VQ provides a general framework for interpretable discrete representations applicable to music, gesture, and other interactive generative domains.","authors":["Alessandro Londei","Denise Lanzieri","Matteo Benati"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21130v1","updated":"2026-02-24T17:27:17Z","published":"2026-02-24T17:27:17Z","title":"An Enhanced Projection Pursuit Tree Classifier with Visual Methods for Assessing Algorithmic Improvements","summary":"This paper presents enhancements to the projection pursuit tree classifier and visual diagnostic methods for assessing their impact in high dimensions. The original algorithm uses linear combinations of variables in a tree structure where depth is constrained to be less than the number of classes -- a limitation that proves too rigid for complex classification problems. Our extensions improve performance in multi-class settings with unequal variance-covariance structures and nonlinear class separations by allowing more splits and more flexible class groupings in the projection pursuit computation. Proposing algorithmic improvements is straightforward; demonstrating their actual utility is not. We therefore develop two visual diagnostic approaches to verify that the enhancements perform as intended. Using high-dimensional visualization techniques, we examine model fits on benchmark datasets to assess whether the algorithm behaves as theorized. An interactive web application enables users to explore the behavior of both the original and enhanced classifiers under controlled scenarios. The enhancements are implemented in the R package PPtreeExt.","authors":["Natalia da Silva","Dianne Cook","Eun-Kyung Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.07016v3","updated":"2026-02-24T17:19:33Z","published":"2024-08-13T16:30:36Z","title":"Rethinking Disentanglement under Dependent Factors of Variation","summary":"Representation learning is an approach that allows to discover and extract the factors of variation from the data. Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans. Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other. However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios. In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent. Furthermore, we relate this definition to the Information Bottleneck Method. Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent. We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario.","authors":["Antonio Almudévar","Alfonso Ortega"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.07906v2","updated":"2026-02-24T17:14:22Z","published":"2026-02-08T10:55:03Z","title":"AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering","summary":"Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.","authors":["Yuzhu Cai","Zexi Liu","Xinyu Zhu","Cheng Wang","Siheng Chen"],"pdf_url":"","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2601.11283v2","updated":"2026-02-24T17:11:56Z","published":"2026-01-16T13:33:09Z","title":"Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning","summary":"Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.","authors":["Nabil Belacel","Mohamed Rachid Boulassel"],"pdf_url":"","comment":"24 pages, 4 figures, 2 tables, submitted to AI in Medicine"},{"id":"http://arxiv.org/abs/2602.21104v1","updated":"2026-02-24T17:03:35Z","published":"2026-02-24T17:03:35Z","title":"Ski Rental with Distributional Predictions of Unknown Quality","summary":"We revisit the central online problem of ski rental in the \"algorithms with predictions\" framework from the point of view of distributional predictions. Ski rental was one of the first problems to be studied with predictions, where a natural prediction is simply the number of ski days. But it is both more natural and potentially more powerful to think of a prediction as a distribution p-hat over the ski days. If the true number of ski days is drawn from some true (but unknown) distribution p, then we show as our main result that there is an algorithm with expected cost at most OPT + O(min(max({eta}, 1) * sqrt(b), b log b)), where OPT is the expected cost of the optimal policy for the true distribution p, b is the cost of buying, and {eta} is the Earth Mover's (Wasserstein-1) distance between p and p-hat. Note that when {eta} < o(sqrt(b)) this gives additive loss less than b (the trivial bound), and when {eta} is arbitrarily large (corresponding to an extremely inaccurate prediction) we still do not pay more than O(b log b) additive loss. An implication of these bounds is that our algorithm has consistency O(sqrt(b)) (additive loss when the prediction error is 0) and robustness O(b log b) (additive loss when the prediction error is arbitrarily large). Moreover, we do not need to assume that we know (or have any bound on) the prediction error {eta}, in contrast with previous work in robust optimization which assumes that we know this error.\n  We complement this upper bound with a variety of lower bounds showing that it is essentially tight: not only can the consistency/robustness tradeoff not be improved, but our particular loss function cannot be meaningfully improved.","authors":["Qiming Cui","Michael Dinitz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21092v1","updated":"2026-02-24T16:52:36Z","published":"2026-02-24T16:52:36Z","title":"Probing Graph Neural Network Activation Patterns Through Graph Topology","summary":"Curvature notions on graphs provide a theoretical description of graph topology, highlighting bottlenecks and denser connected regions. Artifacts of the message passing paradigm in Graph Neural Networks, such as oversmoothing and oversquashing, have been attributed to these regions. However, it remains unclear how the topology of a graph interacts with the learned preferences of GNNs. Through Massive Activations, which correspond to extreme edge activation values in Graph Transformers, we probe this correspondence. Our findings on synthetic graphs and molecular benchmarks reveal that MAs do not preferentially concentrate on curvature extremes, despite their theoretical link to information flow. On the Long Range Graph Benchmark, we identify a systemic \\textit{curvature shift}: global attention mechanisms exacerbate topological bottlenecks, drastically increasing the prevalence of negative curvature. Our work reframes curvature as a diagnostic probe for understanding when and why graph learning fails.","authors":["Floriano Tori","Lorenzo Bini","Marco Sorbi","Stéphane Marchand-Maillet","Vincent Ginis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21081v1","updated":"2026-02-24T16:45:12Z","published":"2026-02-24T16:45:12Z","title":"Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads","summary":"Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of ViTs. We evaluate intra- and inter-node training efficiency across multiple GPU configurations on various datasets like CIFAR-10 and CIFAR-100, exploring the impact of distributed data parallelism on training speed, communication overhead, and overall scalability (strong and weak scaling). By systematically varying software parameters, such as batch size and gradient accumulation, we identify key factors influencing performance of distributed training. The experiments in this study provide a foundational basis for applying DeepSpeed to image-related tasks. Future work will extend these investigations to deepen our understanding of DeepSpeed's limitations and explore strategies for optimizing distributed training pipelines for Vision Transformers.","authors":["Huy Trinh","Rebecca Ma","Zeqi Yu","Tahsin Reza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21078v1","updated":"2026-02-24T16:41:16Z","published":"2026-02-24T16:41:16Z","title":"ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning","summary":"Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.","authors":["Duowen Chen","Yan Wang"],"pdf_url":"","comment":"CVPR 2026. code: https://github.com/DuowenC/FSSLlib"},{"id":"http://arxiv.org/abs/2602.21072v1","updated":"2026-02-24T16:32:50Z","published":"2026-02-24T16:32:50Z","title":"Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning","summary":"Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.","authors":["Zhangjie Xia","Yu Yang","Pan Xu"],"pdf_url":"","comment":"33 pages, 9 figures, 11 tables"},{"id":"http://arxiv.org/abs/2602.21064v1","updated":"2026-02-24T16:26:52Z","published":"2026-02-24T16:26:52Z","title":"Motivation is Something You Need","summary":"This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.","authors":["Mehdi Acheli","Walid Gaaloul"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.17074v3","updated":"2026-02-24T16:26:41Z","published":"2026-01-23T00:43:51Z","title":"PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction","summary":"The accurate estimation of Arctic snow depth remains a critical time-varying inverse problem due to the scarcity in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, namely an LSTM Encoder-Decoder with Multi-head Attention and contrastive learning, with physics-guided inference. Our core innovation lies in a physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. Beyond Arctic snow depth, PhysE-Inv can be applied broadly to other noisy, data-scarce problems in Earth and climate science.","authors":["Akila Sampath","Vandana Janeja","Jianwu Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.16815v2","updated":"2026-02-24T16:11:07Z","published":"2025-08-22T22:24:46Z","title":"Uncertainty Propagation Networks for Neural Ordinary Differential Equations","summary":"This paper introduces Uncertainty Propagation Network (UPN), a novel family of neural differential equations that naturally incorporate uncertainty quantification into continuous-time modeling. Unlike existing neural ODEs that predict only state trajectories, UPN simultaneously model both state evolution and its associated uncertainty by parameterizing coupled differential equations for mean and covariance dynamics. The architecture efficiently propagates uncertainty through nonlinear dynamics without discretization artifacts by solving coupled ODEs for state and covariance evolution while enabling state-dependent, learnable process noise. The continuous-depth formulation adapts its evaluation strategy to each input's complexity, provides principled uncertainty quantification, and handles irregularly-sampled observations naturally. Experimental results demonstrate UPN's effectiveness across multiple domains: continuous normalizing flows (CNFs) with uncertainty quantification, time-series forecasting with well-calibrated confidence intervals, and robust trajectory prediction in both stable and chaotic dynamical systems.","authors":["Hadi Jahanshahi","Zheng H. Zhu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21052v1","updated":"2026-02-24T16:09:47Z","published":"2026-02-24T16:09:47Z","title":"Position-Aware Sequential Attention for Accurate Next Item Recommendations","summary":"Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.","authors":["Timur Nabiev","Evgeny Frolov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21046v1","updated":"2026-02-24T16:04:52Z","published":"2026-02-24T16:04:52Z","title":"PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis","summary":"Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating prototype-based classification and consistency training with structural perturbations during learning. This encourages a structured latent space and enables Monte Carlo Tree Search (MCTS) under a prototype-consistent objective to extract compact minimal-sufficient explanatory subgraphs post-training. Experiments on three benchmark fMRI datasets demonstrate that PIME achieves state-of-the-art performance. Furthermore, by constraining the search space via learned prototypes, PIME identifies critical brain regions that are consistent with established neuroimaging findings. Stability analysis shows 90% reproducibility and consistent explanations across atlases.","authors":["Kunyu Zhang","Yanwu Yang","Jing Zhang","Xiangjie Shi","Shujian Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21043v1","updated":"2026-02-24T16:03:46Z","published":"2026-02-24T16:03:46Z","title":"T1: One-to-One Channel-Head Binding for Multivariate Time-Series Imputation","summary":"Imputing missing values in multivariate time series remains challenging, especially under diverse missing patterns and heavy missingness. Existing methods suffer from suboptimal performance as corrupted temporal features hinder effective cross-variable information transfer, amplifying reconstruction errors. Robust imputation requires both extracting temporal patterns from sparse observations within each variable and selectively transferring information across variables--yet current approaches excel at one while compromising the other. We introduce T1 (Time series imputation with 1-to-1 channel-head binding), a CNN-Transformer hybrid architecture that achieves robust imputation through Channel-Head Binding--a mechanism creating one-to-one correspondence between CNN channels and attention heads. This design enables selective information transfer: when missingness corrupts certain temporal patterns, their corresponding attention pathways adaptively down-weight based on remaining observable patterns while preserving reliable cross-variable connections through unaffected channels. Experiments on 11 benchmark datasets demonstrate that T1 achieves state-of-the-art performance, reducing MSE by 46% on average compared to the second-best baseline, with particularly strong gains under extreme sparsity (70% missing ratio). The model generalizes to unseen missing patterns without retraining and uses a consistent hyperparameter configuration across all datasets. The code is available at https://github.com/Oppenheimerdinger/T1.","authors":["Dongik Park","Hyunwoo Ryu","Suahn Bae","Keondo Park","Hyung-Sin Kim"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21039v1","updated":"2026-02-24T16:00:15Z","published":"2026-02-24T16:00:15Z","title":"Is Multi-Distribution Learning as Easy as PAC Learning: Sharp Rates with Bounded Label Noise","summary":"Towards understanding the statistical complexity of learning from heterogeneous sources, we study the problem of multi-distribution learning. Given $k$ data sources, the goal is to output a classifier for each source by exploiting shared structure to reduce sample complexity. We focus on the bounded label noise setting to determine whether the fast $1/ε$ rates achievable in single-task learning extend to this regime with minimal dependence on $k$. Surprisingly, we show that this is not the case. We demonstrate that learning across $k$ distributions inherently incurs slow rates scaling with $k/ε^2$, even under constant noise levels, unless each distribution is learned separately. A key technical contribution is a structured hypothesis-testing framework that captures the statistical cost of certifying near-optimality under bounded noise-a cost we show is unavoidable in the multi-distribution setting.\n  Finally, we prove that when competing with the stronger benchmark of each distribution's optimal Bayes error, the sample complexity incurs a \\textit{multiplicative} penalty in $k$. This establishes a \\textit{statistical} separation between random classification noise and Massart noise, highlighting a fundamental barrier unique to learning from multiple sources.","authors":["Rafael Hanashiro","Abhishek Shetty","Patrick Jaillet"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21036v1","updated":"2026-02-24T15:56:19Z","published":"2026-02-24T15:56:19Z","title":"Empirically Calibrated Conditional Independence Tests","summary":"Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.","authors":["Milleno Pan","Antoine de Mathelin","Wesley Tansey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21033v1","updated":"2026-02-24T15:55:04Z","published":"2026-02-24T15:55:04Z","title":"MIP Candy: A Modular PyTorch Framework for Medical Image Processing","summary":"Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.","authors":["Tianhao Fu","Yucheng Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.16613v2","updated":"2026-02-24T15:50:35Z","published":"2025-01-28T01:19:05Z","title":"Safe Reinforcement Learning for Real-World Engine Control","summary":"This work introduces a toolchain for applying Reinforcement Learning (RL), specifically the Deep Deterministic Policy Gradient (DDPG) algorithm, in safety-critical real-world environments. As an exemplary application, transient load control is demonstrated on a single-cylinder internal combustion engine testbench in Homogeneous Charge Compression Ignition (HCCI) mode, that offers high thermal efficiency and low emissions. However, HCCI poses challenges for traditional control methods due to its nonlinear, autoregressive, and stochastic nature. RL provides a viable solution, however, safety concerns, such as excessive pressure rise rates, must be addressed when applying to HCCI. A single unsuitable control input can severely damage the engine or cause misfiring and shut down. Additionally, operating limits are not known a priori and must be determined experimentally. To mitigate these risks, real-time safety monitoring based on the k-nearest neighbor algorithm is implemented, enabling safe interaction with the testbench. The feasibility of this approach is demonstrated as the RL agent learns a control policy through interaction with the testbench. A root mean square error of 0.1374 bar is achieved for the indicated mean effective pressure, comparable to neural network-based controllers from the literature. The toolchain's flexibility is further demonstrated by adapting the agent's policy to increase ethanol energy shares, promoting renewable fuel use while maintaining safety. This RL approach addresses the longstanding challenge of applying RL to safety-critical real-world environments. The developed toolchain, with its adaptability and safety mechanisms, paves the way for future applicability of RL in engine testbenches and other safety-critical settings.","authors":["Julian Bedei","Lucas Koch","Kevin Badalian","Alexander Winkler","Patrick Schaber","Jakob Andert"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.21785v3","updated":"2026-02-24T15:49:40Z","published":"2025-08-29T17:03:05Z","title":"Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling","summary":"Heart rate prediction is vital for personalized health monitoring and fitness, while it frequently faces a critical challenge in real-world deployment: data heterogeneity. We classify it in two key dimensions: source heterogeneity from fragmented device markets with varying feature sets, and user heterogeneity reflecting distinct physiological patterns across individuals and activities. Existing methods either discard device-specific information, or fail to model user-specific differences, limiting their real-world performance. To address this, we propose a framework that learns latent representations agnostic to both heterogeneity,enabling downstream predictors to work consistently under heterogeneous data patterns. Specifically, we introduce a random feature dropout strategy to handle source heterogeneity, making the model robust to various feature sets. To manage user heterogeneity, we employ a history-aware attention module to capture long-term physiological traits and use a contrastive learning objective to build a discriminative representation space. To reflect the heterogeneous nature of real-world data, we created a new benchmark dataset, PARROTAO. Evaluations on both PARROTAO and the public FitRec dataset show that our model significantly outperforms existing baselines by 17.5% and 10.4% in terms of test MSE, respectively. Furthermore, analysis of the learned representations demonstrates their strong discriminative power,and two downstream application tasks confirm the practical value of our model.","authors":["Zhengdong Huang","Zicheng Xie","Wentao Tian","Jingyu Liu","Lunhong Dong","Peng Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.07729v2","updated":"2026-02-24T15:43:09Z","published":"2026-02-07T23:25:26Z","title":"Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs","summary":"Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.","authors":["Sagnik Mukherjee","Lifan Yuan","Pavan Jayasinha","Dilek Hakkani-Tür","Hao Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.16954v2","updated":"2026-02-24T15:41:02Z","published":"2026-02-18T23:37:15Z","title":"Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints","summary":"We challenge black-box purely deep neural approaches for molecules and graph generation, which are limited in controllability and lack formal guarantees. We introduce Neuro-Symbolic Graph Generative Modeling (NSGGM), a neurosymbolic framework that reapproaches molecule generation as a scaffold and interaction learning task with symbolic assembly. An autoregressive neural model proposes scaffolds and refines interaction signals, and a CPU-efficient SMT solver constructs full graphs while enforcing chemical validity, structural rules, and user-specific constraints, yielding molecules that are correct by construction and interpretable control that pure neural methods cannot provide. NSGGM delivers strong performance on both unconstrained generation and constrained generation tasks, demonstrating that neuro-symbolic modeling can match state-of-the-art generative performance while offering explicit controllability and guarantees. To evaluate more nuanced controllability, we also introduce a Logical-Constraint Molecular Benchmark, designed to test strict hard-rule satisfaction in workflows that require explicit, interpretable specifications together with verifiable compliance.","authors":["Chuqin Geng","Li Zhang","Mark Zhang","Haolin Ye","Ziyu Zhao","Xujie Si"],"pdf_url":"","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2602.21020v1","updated":"2026-02-24T15:38:11Z","published":"2026-02-24T15:38:11Z","title":"Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning","summary":"Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $ε_{\\text{BC}}$, this provides a Nash imitation gap of $\\mathcal{O}\\left(nε_{\\text{BC}}/(1-γ)^2\\right)$ for a discount factor $γ$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.","authors":["Antoine Bergerault","Volkan Cevher","Negar Mehr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.09448v2","updated":"2026-02-24T15:35:33Z","published":"2026-02-10T06:33:10Z","title":"The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training","summary":"Prior synthetic query generation for dense retrieval produces one query per document, focusing on quality. We systematically study multi-query synthesis, discovering a quality-diversity trade-off: quality benefits in-domain, diversity benefits out-of-domain (OOD). Experiments on 31 datasets show diversity especially benefits multi-hop retrieval. Analysis reveals diversity benefit correlates with query complexity ($r$$\\geq$0.95), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides thresholds (CW$>$10: use diversity; CW$<$7: avoid it) and enables CW-weighted training that improves OOD even with single-query data.","authors":["Xincan Feng","Noriki Nishida","Yusuke Sakai","Yuji Matsumoto"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.09082v2","updated":"2026-02-24T15:17:55Z","published":"2026-02-09T18:43:40Z","title":"UI-Venus-1.5 Technical Report","summary":"GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging. In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications. The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios. Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus","authors":[" Venus Team","Changlong Gao","Zhangxuan Gu","Yulin Liu","Xinyu Qiu","Shuheng Shen","Yue Wen","Tianyu Xia","Zhenyu Xu","Zhengwen Zeng","Beitong Zhou","Xingran Zhou","Weizhi Chen","Sunhao Dai","Jingya Dou","Yichen Gong","Yuan Guo","Zhenlin Guo","Feng Li","Qian Li","Jinzhen Lin","Yuqi Zhou","Linchao Zhu","Liang Chen","Zhenyu Guo","Changhua Meng","Weiqiang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20994v1","updated":"2026-02-24T15:14:04Z","published":"2026-02-24T15:14:04Z","title":"Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures","summary":"Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.","authors":["Yubin Ge","Yongsong Huang","Xiaofeng Liu"],"pdf_url":"","comment":"IEEE International Symposium on Biomedical Imaging (ISBI) 2026"},{"id":"http://arxiv.org/abs/2602.20974v1","updated":"2026-02-24T14:57:22Z","published":"2026-02-24T14:57:22Z","title":"MAST: A Multi-fidelity Augmented Surrogate model via Spatial Trust-weighting","summary":"In engineering design and scientific computing, computational cost and predictive accuracy are intrinsically coupled. High-fidelity simulations provide accurate predictions but at substantial computational costs, while lower-fidelity approximations offer efficiency at the expense of accuracy. Multi-fidelity surrogate modelling addresses this trade-off by combining abundant low-fidelity data with sparse high-fidelity observations. However, existing methods suffer from expensive training cost or rely on global correlation assumptions that often fail in practice to capture how fidelity relationships vary across the input space, leading to poor performance particularly under tight budget constraints. We introduce MAST, a method that blends corrected low-fidelity observations with high-fidelity predictions, trusting high-fidelity near observed samples and relying on corrected low-fidelity elsewhere. MAST achieves this through explicit discrepancy modelling and distance-based weighting with closed-form variance propagation, producing a single heteroscedastic Gaussian process. Across multi-fidelity synthetic benchmarks, MAST shows a marked improvement over the current state-of-the-art techniques. Crucially, MAST maintains robust performance across varying total budget and fidelity gaps, conditions under which competing methods exhibit significant degradation or unstable behaviour.","authors":["Ahmed Mohamed Eisa Nasr","Haris Moazam Sheikh"],"pdf_url":"","comment":"Submitted to International Conference on Machine Learning 2026"},{"id":"http://arxiv.org/abs/2501.08219v4","updated":"2026-02-24T14:57:10Z","published":"2025-01-14T16:02:33Z","title":"Characterizing LLM Inference Energy-Performance Tradeoffs across Workloads and GPU Scaling","summary":"LLM inference exhibits substantial variability across queries and execution phases, yet inference configurations are often applied uniformly. We present a measurement-driven characterization of workload heterogeneity and energy-performance behavior of LLM inference under GPU dynamic voltage and frequency scaling (DVFS). We evaluate five decoder-only LLMs (1B-32B parameters) across four NLP benchmarks using a controlled offline setup. We show that lightweight semantic features predict inference difficulty better than input length, with 44.5% of queries achieving comparable quality across model sizes. At the hardware level, the decode phase dominates inference time (77-91%) and is largely insensitive to GPU frequency. Consequently, reducing GPU frequency from 2842 MHz to 180 MHz achieves an average of 42% energy savings with only a 1-6% latency increase. We further provide a use case with an upper-bound analysis of the potential benefits of combining workload-aware model selection with phase-aware DVFS, motivating future energy-efficient LLM inference systems.","authors":["Paul Joe Maliakel","Shashikant Ilager","Ivona Brandic"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.10167v4","updated":"2026-02-24T14:53:49Z","published":"2025-06-11T20:39:50Z","title":"Wasserstein Barycenter Soft Actor-Critic","summary":"Deep off-policy actor-critic algorithms have emerged as the leading framework for reinforcement learning in continuous control domains. However, most of these algorithms suffer from poor sample efficiency, especially in environments with sparse rewards. In this paper, we take a step towards addressing this issue by providing a principled directed exploration strategy. We propose Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from a pessimistic actor for temporal difference learning and an optimistic actor to promote exploration. This is achieved by using the Wasserstein barycenter of the pessimistic and optimistic policies as the exploration policy and adjusting the degree of exploration throughout the learning process. We compare WBSAC with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is more sample-efficient on MuJoCo continuous control tasks.","authors":["Zahra Shahrooei","Ali Baheri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.12108v2","updated":"2026-02-24T14:53:26Z","published":"2025-02-17T18:29:24Z","title":"Using the Path of Least Resistance to Explain Deep Networks","summary":"Integrated Gradients (IG), a widely used axiomatic path-based attribution method, assigns importance scores to input features by integrating model gradients along a straight path from a baseline to the input. While effective in some cases, we show that straight paths can lead to flawed attributions. In this paper, we identify the cause of these misattributions and propose an alternative approach that equips the input space with a model-induced Riemannian metric (derived from the explained model's Jacobian) and computes attributions by integrating gradients along geodesics under this metric. We call this method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we introduce two techniques: a k-Nearest Neighbours-based approach for smaller models and a Stochastic Variational Inference-based method for larger ones. Additionally, we propose a new axiom, No-Cancellation Completeness (NCC), which strengthens completeness by ruling out feature-wise cancellation. We prove that, for path-based attributions under the model-induced metric, NCC holds if and only if the integration path is a geodesic. Through experiments on both synthetic and real-world image classification data, we provide empirical evidence supporting our theoretical analysis and showing that GIG produces more faithful attributions than existing methods, including IG, on the benchmarks considered.","authors":["Sina Salek","Joseph Enguehard"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20971v1","updated":"2026-02-24T14:52:20Z","published":"2026-02-24T14:52:20Z","title":"Does Order Matter : Connecting The Law of Robustness to Robust Generalization","summary":"Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al.\\ (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al.\\ (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al.\\ (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.","authors":["Himadri Mandal","Vishnu Varadarajan","Jaee Ponde","Aritra Das","Mihir More","Debayan Gupta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.18777v2","updated":"2026-02-24T14:51:33Z","published":"2025-06-23T15:45:44Z","title":"Programming by Backprop: An Instruction is Worth 100 Examples When Finetuning LLMs","summary":"Large language models (LLMs) are typically trained to acquire behaviours from demonstrations or experience, yet much of their training data is declarative: instructions, rules, and descriptions that specify behaviours without showing how to execute them. We introduce Programming by Backprop (PBB): a training regime that enables LLMs to acquire procedural knowledge (i.e., reusable behaviours) from declarative instructions encountered during training. With PBB, instructions in training data provide an opportunity to `program' specific behaviours into model weights. The core principle underpinning PBB is the separation of learning how instructions map to behaviour from internalising new instructions. We devise two distinct PBB curricula that leverage this principle. Through controlled experiments across two domains (algorithmic execution from Python source code and text generation from context-free grammars), we demonstrate the benefit of these curricula over training on a homogeneous data mixture. Crucially, PBB is highly sample efficient, with a single instruction substituting for up to 100 execution examples. Though execution of instructions in training data remains less reliable than when instructions are given in-context, our results demonstrate that procedural knowledge can be noisily `programmed' into LLMs through PBB, with important implications for data curation and safety.","authors":["Jonathan Cook","Silvia Sapora","Arash Ahmadian","Akbir Khan","Tim Rocktaschel","Jakob Foerster","Laura Ruis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.01666v2","updated":"2026-02-24T14:39:36Z","published":"2025-06-02T13:35:33Z","title":"Synthesis of discrete-continuous quantum circuits with multimodal diffusion models","summary":"Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts and circuit depths, showcasing the ability of the method to outperform existing approaches in gate counts and under noisy conditions. Additionally, we show that a simple post-optimization scheme allows us to significantly improve the generated ansätze. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.","authors":["Florian Fürrutter","Zohim Chandani","Ikko Hamamura","Hans J. Briegel","Gorka Muñoz-Gil"],"pdf_url":"","comment":"Main Text: 11 pages, 8 figures and 1 table; Code available at: https://github.com/FlorianFuerrutter/genQC; added new results"},{"id":"http://arxiv.org/abs/2506.04867v4","updated":"2026-02-24T14:36:12Z","published":"2025-06-05T10:38:28Z","title":"Sensory-Motor Control with Large Language Models via Iterative Policy Refinement","summary":"We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.","authors":["Jônata Tyska Carvalho","Stefano Nolfi"],"pdf_url":"","comment":"Final version of the article accepted for publication on Scientific Reports. 29 pages (13 pages are from appendix), 8 figures, 2 tables, code for experiments replication and supplementary material provided at https://github.com/jtyska/llm-robotics-article/"},{"id":"http://arxiv.org/abs/2602.11184v2","updated":"2026-02-24T14:35:31Z","published":"2026-01-30T06:57:17Z","title":"KBVQ-MoE: KLT-guided SVD with Bias-Corrected Vector Quantization for MoE Large Language Models","summary":"Mixture of Experts (MoE) models have achieved great success by significantly improving performance while maintaining computational efficiency through sparse expert activation. However, their enormous parameter sizes and memory demands pose major challenges for deployment in resource-constrained environments. Vector Quantization (VQ) offers a promising approach for ultra-low-bit compression in Large Language Models (LLMs) by leveraging a codebook, where weight vectors are mapped to the most similar discrete codewords. Yet, directly applying VQ to MoEs often leads to substantial performance degradation due to two critical obstacles: (1) redundant representations among experts cause VQ to repeatedly quantize similar representations for each expert, resulting in inefficient use of limited codebook capacity; and (2) cumulative output bias is amplified by expert aggregation in MoE layers, leading to distributional shifts in the quantized outputs. To address these issues, we propose KBVQ-MoE, a novel VQ framework to enhance extremely low-bit quantization for MoE-based LLMs. KBVQ-MoE integrates two techniques: (1) input-driven redundancy elimination, where a Karhunen-Loeve Transform (KLT) guided singular value decomposition (SVD) extracts dominant weight components and shares them across experts; and (2) bias-corrected output stabilization, where vector quantization is applied only to expert-specific (non-redundant) representations and the quantized outputs are corrected via channel-wise affine compensation. Experiments on various MoE LLMs demonstrate that KBVQ-MoE preserves accuracy substantially better than existing quantization methods. For example, 3-bit quantization of Qwen1.5-MoE-A2.7B achieves an average accuracy of 67.99, nearly identical to the FP16 baseline of 68.07, underscoring KBVQ-MoE's potential for efficient deployment on edge devices and other resource-constrained platforms.","authors":["Zukang Xu","Zhixiong Zhao","Xing Hu","Zhixuan Chen","Dawei Yang"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2601.18637v2","updated":"2026-02-24T14:32:58Z","published":"2026-01-26T16:10:32Z","title":"Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution","summary":"Generating quantum data by learning the underlying quantum distribution poses challenges in both theoretical and practical scenarios, yet it is a critical task for understanding quantum systems. A fundamental question in quantum machine learning (QML) is the universality of approximation: whether a parameterized QML model can approximate any quantum distribution. We address this question by proving a universality theorem for the Many-body Projected Ensemble (MPE) framework, a method for quantum state design that uses a single many-body wave function to prepare random states. This demonstrates that MPE can approximate any distribution of pure states within a 1-Wasserstein distance error. This theorem provides a rigorous guarantee of universal expressivity, addressing key theoretical gaps in QML. For practicality, we propose an Incremental MPE variant with layer-wise training to improve the trainability. Numerical experiments on clustered quantum states and quantum chemistry datasets validate MPE's efficacy in learning complex quantum data distributions.","authors":["Quoc Hoan Tran","Koki Chinzei","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"","comment":"21 pages, 6 figures (added Github repository)"},{"id":"http://arxiv.org/abs/2602.20947v1","updated":"2026-02-24T14:31:28Z","published":"2026-02-24T14:31:28Z","title":"Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation","summary":"The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.","authors":["Thorbjørn Mosekjær Iversen","Zebin Duan","Frederik Hagelskjær"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20946v1","updated":"2026-02-24T14:29:45Z","published":"2026-02-24T14:29:45Z","title":"Some Simple Economics of AGI","summary":"For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.","authors":["Christian Catalini","Xiang Hui","Jane Wu"],"pdf_url":"","comment":"JEL Classification: D82, D83, J23, J24, L23, O33. 112 pages, 3 figures"},{"id":"http://arxiv.org/abs/2602.17554v2","updated":"2026-02-24T14:25:20Z","published":"2026-02-19T17:09:13Z","title":"A Theoretical Framework for Modular Learning of Robust Generative Models","summary":"Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.","authors":["Corinna Cortes","Mehryar Mohri","Yutao Zhong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20937v1","updated":"2026-02-24T14:17:51Z","published":"2026-02-24T14:17:51Z","title":"Extending $μ$P: Spectral Conditions for Feature Learning Across Optimizers","summary":"Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(μ$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tuned on a smaller (computationally cheaper) model to be transferred to train a larger, target model. Despite promising results for SGD and Adam, deriving $μ$P for other optimizers is challenging because the underlying tensor programming approach is difficult to grasp. Building on recent work that introduced spectral conditions as an alternative to tensor programs, we propose a novel framework to derive $μ$P for a broader class of optimizers, including AdamW, ADOPT, LAMB, Sophia, Shampoo and Muon. We implement our $μ$P derivations on multiple benchmark models and demonstrate zero-shot learning rate transfer across increasing model width for the above optimizers. Further, we provide empirical insights into depth-scaling parameterization for these optimizers.","authors":["Akshita Gupta","Marieme Ngom","Sam Foreman","Venkatram Vishwanath"],"pdf_url":"","comment":"10 main pages, 16 appendix pages and 17 figures; Amended version of the publication in 17th International OPT Workshop on Optimization for Machine Learning"},{"id":"http://arxiv.org/abs/2505.19193v4","updated":"2026-02-24T14:14:47Z","published":"2025-05-25T15:41:01Z","title":"SuperMAN: Interpretable and Expressive Networks over Temporally Sparse Heterogeneous Data","summary":"Real-world temporal data often consists of multiple signal types recorded at irregular, asynchronous intervals. For instance, in the medical domain, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling occur in other domains, such as the monitoring of large systems using event log files. Effectively learning from such data requires handling sets of temporal sparse and heterogeneous signals. In this work, we propose Super Mixing Additive Networks (SuperMAN), a novel and interpretable-by-design framework for learning directly from such heterogeneous signals, by modeling them as sets of implicit graphs. SuperMAN provides diverse interpretability capabilities, including node-level, graph-level, and subset-level importance, and enables practitioners to trade finer-grained interpretability for greater expressivity when domain priors are available. SuperMAN achieves state-of-the-art performance in real-world high-stakes tasks, including predicting Crohn's disease onset and hospital length of stay from routine blood test measurements and detecting fake news. Furthermore, we demonstrate how SuperMAN's interpretability properties assist in revealing disease development phase transitions and provide crucial insights in the healthcare domain.","authors":["Maya Bechler-Speicher","Andrea Zerio","Maor Huri","Marie Vibeke Vestergaard","Ran Gilad-Bachrach","Tine Jess","Samir Bhatt","Aleksejs Sazonovs"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20932v1","updated":"2026-02-24T14:10:41Z","published":"2026-02-24T14:10:41Z","title":"Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels","summary":"An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.\n  We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.","authors":["Anupam Sharma","Harish Katti","Prajwal Singh","Shanmuganathan Raman","Krishna Miyapuram"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.08660v3","updated":"2026-02-24T14:08:04Z","published":"2025-06-10T10:21:28Z","title":"Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness","summary":"Real-world time series data are inherently multivariate, often exhibiting complex inter-channel dependencies. Each channel is typically sampled at its own period and is prone to missing values due to various practical and operational constraints. These characteristics pose three fundamental challenges involving channel dependency, sampling asynchrony, and missingness, all of which must be addressed simultaneously to enable robust and reliable forecasting in practical settings. However, existing architectures typically address only parts of these challenges in isolation and still rely on simplifying assumptions, leaving unresolved the combined challenges of asynchronous channel sampling, test-time missing blocks, and intricate inter-channel dependencies. To bridge this gap, we propose ChannelTokenFormer, a Transformer-based forecasting framework with a flexible architecture designed to explicitly capture cross-channel interactions, accommodate channel-wise asynchronous sampling, and effectively handle missing values. Extensive experiments on public benchmark datasets reflecting practical settings, along with one private real-world industrial dataset, demonstrate the superior robustness and accuracy of ChannelTokenFormer under challenging real-world conditions.","authors":["Jinkwan Jang","Hyungjin Park","Jinmyeong Choi","Taesup Kim"],"pdf_url":"","comment":"Accepted by the 14th International Conference on Learning Representations (ICLR 2026)"},{"id":"http://arxiv.org/abs/2602.20921v1","updated":"2026-02-24T13:59:06Z","published":"2026-02-24T13:59:06Z","title":"On the Generalization Behavior of Deep Residual Networks From a Dynamical System Perspective","summary":"Deep neural networks (DNNs) have significantly advanced machine learning, with model depth playing a central role in their successes. The dynamical system modeling approach has recently emerged as a powerful framework, offering new mathematical insights into the structure and learning behavior of DNNs. In this work, we establish generalization error bounds for both discrete- and continuous-time residual networks (ResNets) by combining Rademacher complexity, flow maps of dynamical systems, and the convergence behavior of ResNets in the deep-layer limit. The resulting bounds are of order $O(1/\\sqrt{S})$ with respect to the number of training samples $S$, and include a structure-dependent negative term, yielding depth-uniform and asymptotic generalization bounds under milder assumptions. These findings provide a unified understanding of generalization across both discrete- and continuous-time ResNets, helping to close the gap in both the order of sample complexity and assumptions between the discrete- and continuous-time settings.","authors":["Jinshu Huang","Mingfei Sun","Chunlin Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2309.13411v3","updated":"2026-02-24T13:55:17Z","published":"2023-09-23T15:48:35Z","title":"Towards Attributions of Input Variables in a Coalition","summary":"This paper focuses on the fundamental challenge of partitioning input variables in attribution methods for Explainable AI, particularly in Shapley value-based approaches. Previous methods always compute attributions given a predefined partition but lack theoretical guidance on how to form meaningful variable partitions. We identify that attribution conflicts arise when the attribution of a coalition differs from the sum of its individual variables' attributions. To address this, we analyze the numerical effects of AND-OR interactions in AI models and extend the Shapley value to a new attribution metric for variable coalitions. Our theoretical findings reveal that specific interactions cause attribution conflicts, and we propose three metrics to evaluate coalition faithfulness. Experiments on synthetic data, NLP, image classification, and the game of Go validate our approach, demonstrating consistency with human intuition and practical applicability.","authors":["Xinhao Zheng","Huiqi Deng","Quanshi Zhang"],"pdf_url":"","comment":"Accepted to the 2025 International Conference on Machine Learning (ICML 2025)"},{"id":"http://arxiv.org/abs/2602.20911v1","updated":"2026-02-24T13:48:13Z","published":"2026-02-24T13:48:13Z","title":"From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning","summary":"Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.","authors":["Ruiqi Liu","Boyu Diao","Hangda Liu","Zhulin An","Fei Wang","Yongjun Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.11602v2","updated":"2026-02-24T13:46:01Z","published":"2025-05-16T18:08:40Z","title":"Regularity and Stability Properties of Selective SSMs with Discontinuous Gating","summary":"Deep selective State-Space Models (SSMs), whose state-space parameters are modulated online by a selection signal, offer significant expressive power but pose challenges for stability analysis, especially under discontinuous gating. We study continuous-time selective SSMs through the lenses of passivity and Input-to-State Stability (ISS), explicitly distinguishing the selection schedule $x(\\cdot)$ from the driving (port) input $u(\\cdot)$. First, we show that state-strict dissipativity ($β>0$) together with quadratic bounds on a storage functional implies exponential decay of homogeneous trajectories ($u\\equiv 0$), yielding exponential forgetting. Second, by freezing the selection ($x(t)\\equiv 0$) we obtain a passive LTV input-output subsystem and prove that its minimal available storage is necessarily quadratic, $V_{a,0}(t,h)=\\tfrac{1}{2}h^H Q_0(t)h,$ with $Q_0 \\in \\mathrm{AUC}_{\\mathrm{loc}}$, accommodating discontinuities induced by gating. Third, under the strong hypothesis that a single quadratic storage certifies passivity uniformly over all admissible selection schedules, we derive a parametric LMI and universal kernel constraints on gating, formalizing an \"irreversible forgetting\" structure. Finally, we give sufficient conditions for global ISS with respect to the port input $u(\\cdot)$, uniformly over admissible selection schedules, and we validate the main predictions in targeted simulation studies.","authors":["Nikola Zubić","Davide Scaramuzza"],"pdf_url":"","comment":"26 pages, 6 theorems, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2602.20904v1","updated":"2026-02-24T13:40:28Z","published":"2026-02-24T13:40:28Z","title":"Transcoder Adapters for Reasoning-Model Diffing","summary":"While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., \"wait\"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.","authors":["Nathan Hu","Jake Ward","Thomas Icard","Christopher Potts"],"pdf_url":"","comment":"9 pages main, 27 pages total, 10 figures. Code and visualizations at https://transcoder-adapters.github.io/"},{"id":"http://arxiv.org/abs/2602.20901v1","updated":"2026-02-24T13:38:37Z","published":"2026-02-24T13:38:37Z","title":"SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models","summary":"Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.","authors":["Yuechen Xie","Xiaoyan Zhang","Yicheng Shan","Hao Zhu","Rui Tang","Rong Wei","Mingli Song","Yuanyu Wan","Jie Song"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2510.22500v2","updated":"2026-02-24T13:37:46Z","published":"2025-10-26T02:42:03Z","title":"Towards Scalable Oversight via Partitioned Human Supervision","summary":"As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging. Our focus is on tasks that require deep knowledge and skills of multiple domains, where this bottleneck is severe. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks. However, based on their narrow expertise, humans may provide a weak signal, i.e., a complementary label indicating an option that is incorrect. For example, a cardiologist could state that ''this is not related to any cardiovascular disease,'' even if they cannot identify the true disease. Based on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth. We derive an unbiased estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels. We provide finite-sample deviation guarantees for both complementary-only and the mixed estimators. Empirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels. We further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can improve itself with this partitioned human supervision. Our code is available at https://github.com/R-Yin-217/Towards-Scalable-Oversight-via-Partitioned-Human-Supervision.","authors":["Ren Yin","Takashi Ishida","Masashi Sugiyama"],"pdf_url":"","comment":"ICLR 2026 camera ready version"},{"id":"http://arxiv.org/abs/2602.07633v2","updated":"2026-02-24T13:19:00Z","published":"2026-02-07T17:26:50Z","title":"Flow-Based Conformal Predictive Distributions","summary":"Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk and, optionally, repulsed along the boundary to improve geometric coverage. Mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting.","authors":["Trevor Harris"],"pdf_url":"","comment":"9 pages, 7 figures, 10 appendix pages"},{"id":"http://arxiv.org/abs/2407.00575v3","updated":"2026-02-24T13:16:31Z","published":"2024-06-30T03:33:42Z","title":"Learning to Control Unknown Strongly Monotone Games","summary":"Consider a strongly monotone game where the players' utility functions include a reward function and a linear term for each dimension, with coefficients that are controlled by the manager. Gradient play converges to a unique Nash equilibrium (NE) that does not optimize the global objective. The global performance at NE can be improved by imposing linear constraints on the NE, also known as a generalized Nash equilibrium (GNE). We therefore want the manager to control the coefficients such that they impose the desired constraint on the NE. However, this requires knowing the players' rewards and action sets. Obtaining this game information is infeasible in a large-scale network and violates user privacy. To overcome this, we propose a simple algorithm that learns to shift the NE to meet the linear constraints by adjusting the controlled coefficients online. Our algorithm only requires the linear constraints violation as feedback and does not need to know the reward functions or the action sets. We prove that our algorithm converges with probability 1 to the set of GNE given by coupled linear constraints. We then prove an L2 convergence rate of near-$O(t^{-1/4})$.","authors":["Siddharth Chandak","Ilai Bistritz","Nicholas Bambos"],"pdf_url":"","comment":"Accepted for publication at IEEE Transactions on Control of Network Systems (TCNS)"},{"id":"http://arxiv.org/abs/2602.19028v2","updated":"2026-02-24T13:11:53Z","published":"2025-11-25T08:25:38Z","title":"The Metaphysics We Train: A Heideggerian Reading of Machine Learning","summary":"This paper offers a phenomenological reading of contemporary machine learning through Heideggerian concepts, aimed at enriching practitioners' reflexive understanding of their own practice. We argue that this philosophical lens reveals three insights invisible to purely technical analysis. First, the algorithmic Entwurf (projection) is distinctive in being automated, opaque, and emergent--a metaphysics that operates without explicit articulation or debate, crystallizing implicitly through gradient descent rather than theoretical argument. Second, even sophisticated technical advances remain within the regime of Gestell (Enframing), improving calculation without questioning the primacy of calculation itself. Third, AI's lack of existential structure, specifically the absence of Care (Sorge), is genuinely explanatory: it illuminates why AI systems have no internal resources for questioning their own optimization imperatives, and why they optimize without the anxiety (Angst) that signals, in human agents, the friction between calculative absorption and authentic existence. We conclude by exploring the pedagogical value of this perspective, arguing that data science education should cultivate not only technical competence but ontological literacy--the capacity to recognize what worldviews our tools enact and when calculation itself may be the wrong mode of engagement.","authors":["Heman Shakeri"],"pdf_url":"","comment":"13 pages"},{"id":"http://arxiv.org/abs/2602.20857v1","updated":"2026-02-24T12:58:21Z","published":"2026-02-24T12:58:21Z","title":"Functional Continuous Decomposition","summary":"The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to $C^1$ continuous fitting, FCD transforms raw time-series data into $M$ modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.","authors":["Teymur Aghayev"],"pdf_url":"","comment":"16 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.16106v5","updated":"2026-02-24T12:51:18Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear Function Approximation","summary":"We investigate the statistical properties of Temporal Difference (TD) learning with Polyak-Ruppert averaging, arguably one of the most widely used algorithms in reinforcement learning, for the task of estimating the parameters of the optimal linear approximation to the value function. Assuming independent samples, we make three theoretical contributions that improve upon the current state-of-the-art results: (i) we establish refined high-dimensional Berry-Esseen bounds over the class of convex sets, achieving faster rates than the best known results, and (ii) we propose and analyze a novel, computationally efficient online plug-in estimator of the asymptotic covariance matrix; (iii) we derive sharper high probability convergence guarantees that depend explicitly on the asymptotic variance and hold under weaker conditions than those adopted in the literature. These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function approximation, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.19975v2","updated":"2026-02-24T12:48:57Z","published":"2025-09-24T10:30:33Z","title":"From Samples to Scenarios: A New Paradigm for Probabilistic Forecasting","summary":"Most state-of-the-art probabilistic time series forecasting models rely on sampling to represent future uncertainty. However, this paradigm suffers from inherent limitations, such as lacking explicit probabilities, inadequate coverage, and high computational costs. In this work, we introduce \\textbf{Probabilistic Scenarios}, an alternative paradigm designed to address the limitations of sampling. It operates by directly producing a finite set of \\{Scenario, Probability\\} pairs, thus avoiding Monte Carlo-like approximation. To validate this paradigm, we propose \\textbf{TimePrism}, a simple model composed of only three parallel linear layers. Surprisingly, TimePrism achieves 9 out of 10 state-of-the-art results across five benchmark datasets on two metrics. The effectiveness of our paradigm comes from a fundamental reframing of the learning objective. Instead of modeling an entire continuous probability space, the model learns to represent a set of plausible scenarios and corresponding probabilities. Our work demonstrates the potential of the Probabilistic Scenarios paradigm, opening a promising research direction in forecasting beyond sampling.","authors":["Xilin Dai","Zhijian Xu","Wanxu Cai","Qiang Xu"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.07712v2","updated":"2026-02-24T12:28:51Z","published":"2026-02-07T21:40:33Z","title":"Towards Robust Scaling Laws for Optimizers","summary":"The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.","authors":["Alexandra Volkova","Mher Safaryan","Christoph H. Lampert","Dan Alistarh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22566v2","updated":"2026-02-24T12:23:51Z","published":"2025-09-26T16:42:52Z","title":"From Parameters to Behaviors: Unsupervised Compression of the Policy Space","summary":"Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $Θ$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $Θ$ into a low-dimensional latent space $\\mathcal{Z}$. We train a generative model $g:\\mathcal{Z}\\toΘ$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment's complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\\mathcal{Z}$.","authors":["Davide Tenedini","Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"pdf_url":"","comment":"ICLR 2026 camera ready version. Changed typo in the title"},{"id":"http://arxiv.org/abs/2602.20833v1","updated":"2026-02-24T12:18:42Z","published":"2026-02-24T12:18:42Z","title":"DRESS: A Continuous Framework for Structural Graph Refinement","summary":"The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as O(n^3) or O(n^4), making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, Leon, and Gomez, 2018)--a parameter-free, continuous dynamical system on edges--and show that it distinguishes the prism graph from K_{3,3}, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce Delta-DRESS, which runs DRESS on each node-deleted subgraph G\\{v}, connecting the framework to the Kelly-Ulam reconstruction conjecture. Both Motif-DRESS and Delta-DRESS empirically distinguish Strongly Regular Graphs (SRGs)--such as the Rook and Shrikhande graphs--that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive O(n^4) computational cost.","authors":["Eduar Castrillo Velilla"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.04398v4","updated":"2026-02-24T12:13:45Z","published":"2025-03-06T12:52:22Z","title":"Semantic Parallelism: Redefining Efficient MoE Inference via Model-Data Co-Scheduling","summary":"Prevailing LLM serving engines employ expert parallelism (EP) to implement multi-device inference of massive MoE models. However, the efficiency of expert parallel inference is largely bounded by inter-device communication, as EP embraces expensive all-to-all collectives to route tokens to the remote experts if not collocating on the same GPU/NPU device. Nevertheless, state-of-the-art schemes treat expert device-placement and request (or token) device-scheduling as separate concerns, triggering excessive communication between them and compromising inference efficiency\n  This paper proposes Semantic Parallelism, a novel parallelism paradigm that minimizes the steep communication costs in EP-centric MoE serving via model-data collaborative scheduling. We implement Semantic Parallelism in a framework called Sem-MoE. Sem-MoE maximally collocates experts and their activating tokens onto the same device using proactively modeled activation likelihood between them and introduces three key techniques: (1) Offline model scheduling, which preliminarily clusters and collocates experts onto devices based on their co-activation tendencies for certain classes of input. (2) Online inter-request data scheduling for Attention-DP setups, which proactively rebatches incoming requests onto the device that hosts experts most likely and frequently activated by the corresponding requests. (3) Online intra-request data scheduling for Attention-TP setups, which seamlessly fuses a token reshuffling procedure into the original inference pipeline and proactively reschedules tokens to devices to reduce dispersed remote routing. We build Sem-MoE into a prevailing LLM serving engine SGLANG. Experiments show our collaborative scheduling approach can effectively reduce the all-to-all communication volume in EP and achieve superior inference throughput compared to existing solutions.","authors":["Yan Li","Zhenyu Zhang","Zhengang Wang","Pengfei Chen","Pengfei Zheng"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20816v1","updated":"2026-02-24T11:54:06Z","published":"2026-02-24T11:54:06Z","title":"Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation","summary":"The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.","authors":["Sayantan Dasgupta","Trevor Cohn","Timothy Baldwin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.09708v2","updated":"2026-02-24T11:51:26Z","published":"2026-01-14T18:59:59Z","title":"Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning","summary":"Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.","authors":["Chi-Pin Huang","Yunze Man","Zhiding Yu","Min-Hung Chen","Jan Kautz","Yu-Chiang Frank Wang","Fu-En Yang"],"pdf_url":"","comment":"CVPR 2026. Project page: https://jasper0314-huang.github.io/fast-thinkact/"},{"id":"http://arxiv.org/abs/2602.20809v1","updated":"2026-02-24T11:49:59Z","published":"2026-02-24T11:49:59Z","title":"Regret-Guided Search Control for Efficient Learning in AlphaZero","summary":"Reinforcement learning (RL) agents achieve remarkable performance but remain far less learning-efficient than humans. While RL agents require extensive self-play games to extract useful signals, humans often need only a few games, improving rapidly by repeatedly revisiting states where mistakes occurred. This idea, known as search control, aims to restart from valuable states rather than always from the initial state. In AlphaZero, prior work Go-Exploit applies this idea by sampling past states from self-play or search trees, but it treats all states equally, regardless of their learning potential. We propose Regret-Guided Search Control (RGSC), which extends AlphaZero with a regret network that learns to identify high-regret states, where the agent's evaluation diverges most from the actual outcome. These states are collected from both self-play trajectories and MCTS nodes, stored in a prioritized regret buffer, and reused as new starting positions. Across 9x9 Go, 10x10 Othello, and 11x11 Hex, RGSC outperforms AlphaZero and Go-Exploit by an average of 77 and 89 Elo, respectively. When training on a well-trained 9x9 Go model, RGSC further improves the win rate against KataGo from 69.3% to 78.2%, while both baselines show no improvement. These results demonstrate that RGSC provides an effective mechanism for search control, improving both efficiency and robustness of AlphaZero training. Our code is available at https://rlg.iis.sinica.edu.tw/papers/rgsc.","authors":["Yun-Jui Tsai","Wei-Yu Chen","Yan-Ru Ju","Yu-Hung Chang","Ti-Rong Wu"],"pdf_url":"","comment":"Accepted by the Fourteenth International Conference on Learning Representations (ICLR 2026)"},{"id":"http://arxiv.org/abs/2602.20805v1","updated":"2026-02-24T11:45:41Z","published":"2026-02-24T11:45:41Z","title":"Assessing the Impact of Speaker Identity in Speech Spoofing Detection","summary":"Spoofing detection systems are typically trained using diverse recordings from multiple speakers, often assuming that the resulting embeddings are independent of speaker identity. However, this assumption remains unverified. In this paper, we investigate the impact of speaker information on spoofing detection systems. We propose two approaches within our Speaker-Invariant Multi-Task framework, one that models speaker identity within the embeddings and another that removes it. SInMT integrates multi-task learning for joint speaker recognition and spoofing detection, incorporating a gradient reversal layer. Evaluated using four datasets, our speaker-invariant model reduces the average equal error rate by 17% compared to the baseline, with up to 48% reduction for the most challenging attacks (e.g., A11).","authors":["Anh-Tuan Dao","Driss Matrouf","Nicholas Evans"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20804v1","updated":"2026-02-24T11:44:46Z","published":"2026-02-24T11:44:46Z","title":"Probing Dec-POMDP Reasoning in Cooperative MARL","summary":"Cooperative multi-agent reinforcement learning (MARL) is typically framed as a decentralised partially observable Markov decision process (Dec-POMDP), a setting whose hardness stems from two key challenges: partial observability and decentralised coordination. Genuinely solving such tasks requires Dec-POMDP reasoning, where agents use history to infer hidden states and coordinate based on local information. Yet it remains unclear whether popular benchmarks actually demand this reasoning or permit success via simpler strategies. We introduce a diagnostic suite combining statistically grounded performance comparisons and information-theoretic probes to audit the behavioural complexity of baseline policies (IPPO and MAPPO) across 37 scenarios spanning MPE, SMAX, Overcooked, Hanabi, and MaBrax. Our diagnostics reveal that success on these benchmarks rarely requires genuine Dec-POMDP reasoning. Reactive policies match the performance of memory-based agents in over half the scenarios, and emergent coordination frequently relies on brittle, synchronous action coupling rather than robust temporal influence. These findings suggest that some widely used benchmarks may not adequately test core Dec-POMDP assumptions under current training paradigms, potentially leading to over-optimistic assessments of progress. We release our diagnostic tooling to support more rigorous environment design and evaluation in cooperative MARL.","authors":["Kale-ab Tessera","Leonard Hinckeldey","Riccardo Zamboni","David Abel","Amos Storkey"],"pdf_url":"","comment":"To appear at the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)"},{"id":"http://arxiv.org/abs/2602.20796v1","updated":"2026-02-24T11:35:15Z","published":"2026-02-24T11:35:15Z","title":"Exploring the Impact of Parameter Update Magnitude on Forgetting and Generalization of Continual Learning","summary":"The magnitude of parameter updates are considered a key factor in continual learning. However, most existing studies focus on designing diverse update strategies, while a theoretical understanding of the underlying mechanisms remains limited. Therefore, we characterize model's forgetting from the perspective of parameter update magnitude and formalize it as knowledge degradation induced by task-specific drift in the parameter space, which has not been fully captured in previous studies due to their assumption of a unified parameter space. By deriving the optimal parameter update magnitude that minimizes forgetting, we unify two representative update paradigms, frozen training and initialized training, within an optimization framework for constrained parameter updates. Our theoretical results further reveals that sequence tasks with small parameter distances exhibit better generalization and less forgetting under frozen training rather than initialized training. These theoretical insights inspire a novel hybrid parameter update strategy that adaptively adjusts update magnitude based on gradient directions. Experiments on deep neural networks demonstrate that this hybrid approach outperforms standard training strategies, providing new theoretical perspectives and practical inspiration for designing efficient and scalable continual learning algorithms.","authors":["JinLi He","Liang Bai","Xian Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20791v1","updated":"2026-02-24T11:29:12Z","published":"2026-02-24T11:29:12Z","title":"Understanding the Role of Rehearsal Scale in Continual Learning under Varying Model Capacities","summary":"Rehearsal is one of the key techniques for mitigating catastrophic forgetting and has been widely adopted in continual learning algorithms due to its simplicity and practicality. However, the theoretical understanding of how rehearsal scale influences learning dynamics remains limited. To address this gap, we formulate rehearsal-based continual learning as a multidimensional effectiveness-driven iterative optimization problem, providing a unified characterization across diverse performance metrics. Within this framework, we derive a closed-form analysis of adaptability, memorability, and generalization from the perspective of rehearsal scale. Our results uncover several intriguing and counterintuitive findings. First, rehearsal can impair model's adaptability, in sharp contrast to its traditionally recognized benefits. Second, increasing the rehearsal scale does not necessarily improve memory retention. When tasks are similar and noise levels are low, the memory error exhibits a diminishing lower bound. Finally, we validate these insights through numerical simulations and extended analyses on deep neural networks across multiple real-world datasets, revealing statistical patterns of rehearsal mechanisms in continual learning.","authors":["JinLi He","Liang Bai","Xian Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.17028v3","updated":"2026-02-24T11:27:00Z","published":"2025-02-24T10:29:15Z","title":"Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence","summary":"Vision-language alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP utilize InfoNCE to maximize mutual information, primarily aligning pairwise samples across modalities while overlooking distributional differences. In addition, InfoNCE has inherent conflict in terms of alignment and uniformity in multimodality, leading to suboptimal alignment with modality gaps. To overcome the limitations, we propose CS-Aligner, a novel framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. CS-Aligner captures both the global distribution information of each modality and the pairwise semantic relationships. We find that the CS divergence seamlessly addresses the InfoNCE's alignment-uniformity conflict and serves complementary roles with InfoNCE, yielding tighter and more precise alignment. Moreover, by introducing distributional alignment, CS-Aligner enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.","authors":["Wenzhe Yin","Zehao Xiao","Pan Zhou","Shujian Yu","Jiayi Shen","Jan-Jakob Sonke","Efstratios Gavves"],"pdf_url":"","comment":"Accepted by ICLR2026"},{"id":"http://arxiv.org/abs/2602.20782v1","updated":"2026-02-24T11:21:45Z","published":"2026-02-24T11:21:45Z","title":"On Electric Vehicle Energy Demand Forecasting and the Effect of Federated Learning","summary":"The wide spread of new energy resources, smart devices, and demand side management strategies has motivated several analytics operations, from infrastructure load modeling to user behavior profiling. Energy Demand Forecasting (EDF) of Electric Vehicle Supply Equipments (EVSEs) is one of the most critical operations for ensuring efficient energy management and sustainability, since it enables utility providers to anticipate energy/power demand, optimize resource allocation, and implement proactive measures to improve grid reliability. However, accurate EDF is a challenging problem due to external factors, such as the varying user routines, weather conditions, driving behaviors, unknown state of charge, etc. Furthermore, as concerns and restrictions about privacy and sustainability have grown, training data has become increasingly fragmented, resulting in distributed datasets scattered across different data silos and/or edge devices, calling for federated learning solutions. In this paper, we investigate different well-established time series forecasting methodologies to address the EDF problem, from statistical methods (the ARIMA family) to traditional machine learning models (such as XGBoost) and deep neural networks (GRU and LSTM). We provide an overview of these methods through a performance comparison over four real-world EVSE datasets, evaluated under both centralized and federated learning paradigms, focusing on the trade-offs between forecasting fidelity, privacy preservation, and energy overheads. Our experimental results demonstrate, on the one hand, the superiority of gradient boosted trees (XGBoost) over statistical and NN-based models in both prediction accuracy and energy efficiency and, on the other hand, an insight that Federated Learning-enabled models balance these factors, offering a promising direction for decentralized energy demand forecasting.","authors":["Andreas Tritsarolis","Gil Sampaio","Nikos Pelekis","Yannis Theodoridis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.08261v2","updated":"2026-02-24T11:04:10Z","published":"2025-11-11T13:54:59Z","title":"Uncertainty Calibration of Multi-Label Bird Sound Classifiers","summary":"Passive acoustic monitoring enables large-scale biodiversity assessment, but reliable classification of bioacoustic sounds requires not only high accuracy but also well-calibrated uncertainty estimates to ground decision-making. In bioacoustics, calibration is challenged by overlapping vocalisations, long-tailed species distributions, and distribution shifts between training and deployment data. The calibration of multi-label deep learning classifiers within the domain of bioacoustics has not yet been assessed. We systematically benchmark the calibration of four state-of-the-art multi-label bird sound classifiers on the BirdSet benchmark, evaluating both global, per-dataset and per-class calibration using threshold-free calibration metrics (ECE, MCS) alongside discrimination metrics (cmAP). Model calibration varies significantly across datasets and classes. While Perch v2 and ConvNeXt$_{BS}$ show better global calibration, results vary between datasets. Both models indicate consistent underconfidence, while AudioProtoPNet and BirdMAE are mostly overconfident. Surprisingly, calibration seems to be better for less frequent classes. Using simple post hoc calibration methods we demonstrate a straightforward way to improve calibration. A small labelled calibration set is sufficient to significantly improve calibration with Platt scaling, while global calibration parameters suffer from dataset variability. Our findings highlight the importance of evaluating and improving uncertainty calibration in bioacoustic classifiers.","authors":["Raphael Schwinger","Ben McEwen","Vincent S. Kather","René Heinrich","Lukas Rauch","Sven Tomforde"],"pdf_url":"","comment":"Accepted at ICAART 2026"},{"id":"http://arxiv.org/abs/2503.06437v2","updated":"2026-02-24T10:57:18Z","published":"2025-03-09T04:25:39Z","title":"SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding","summary":"We present SEED (Semantic Evaluation for Visual Brain Decoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images inspired by neuroscientific findings. Using carefully crowd-sourced human evaluation data, we demonstrate that SEED achieves the highest alignment with human evaluation, outperforming other widely used metrics. Through the evaluation of existing visual brain decoding models with SEED, we further reveal that crucial information is often lost in translation, even in the state-of-the-art models that achieve near-perfect scores on existing metrics. This finding highlights the limitations of current evaluation practices and provides guidance for future improvements in decoding models. Finally, to facilitate further research, we open-source the human evaluation data, encouraging the development of more advanced evaluation methods for brain decoding. Our code and the human evaluation data are available at https://github.com/Concarne2/SEED.","authors":["Juhyeon Park","Peter Yongho Kim","Jiook Cha","Shinjae Yoo","Taesup Moon"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.03596v3","updated":"2026-02-24T10:55:09Z","published":"2026-02-03T14:50:19Z","title":"SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network","summary":"Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks in the wild, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.","authors":["Cristian Manca","Christian Scano","Giorgio Piras","Fabio Brau","Maura Pintor","Battista Biggio"],"pdf_url":"","comment":"ITASEC-2026"},{"id":"http://arxiv.org/abs/2602.15763v2","updated":"2026-02-24T10:44:44Z","published":"2026-02-17T17:50:56Z","title":"GLM-5: from Vibe Coding to Agentic Engineering","summary":"We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.","authors":[" GLM-5-Team"," :","Aohan Zeng","Xin Lv","Zhenyu Hou","Zhengxiao Du","Qinkai Zheng","Bin Chen","Da Yin","Chendi Ge","Chenghua Huang","Chengxing Xie","Chenzheng Zhu","Congfeng Yin","Cunxiang Wang","Gengzheng Pan","Hao Zeng","Haoke Zhang","Haoran Wang","Huilong Chen","Jiajie Zhang","Jian Jiao","Jiaqi Guo","Jingsen Wang","Jingzhao Du","Jinzhu Wu","Kedong Wang","Lei Li","Lin Fan","Lucen Zhong","Mingdao Liu","Mingming Zhao","Pengfan Du","Qian Dong","Rui Lu"," Shuang-Li","Shulin Cao","Song Liu","Ting Jiang","Xiaodong Chen","Xiaohan Zhang","Xuancheng Huang","Xuezhen Dong","Yabo Xu","Yao Wei","Yifan An","Yilin Niu","Yitong Zhu","Yuanhao Wen","Yukuo Cen","Yushi Bai","Zhongpei Qiao","Zihan Wang","Zikang Wang","Zilin Zhu","Ziqiang Liu","Zixuan Li","Bojie Wang","Bosi Wen","Can Huang","Changpeng Cai","Chao Yu","Chen Li","Chengwei Hu","Chenhui Zhang","Dan Zhang","Daoyan Lin","Dayong Yang","Di Wang","Ding Ai","Erle Zhu","Fangzhou Yi","Feiyu Chen","Guohong Wen","Hailong Sun","Haisha Zhao","Haiyi Hu","Hanchen Zhang","Hanrui Liu","Hanyu Zhang","Hao Peng","Hao Tai","Haobo Zhang","He Liu","Hongwei Wang","Hongxi Yan","Hongyu Ge","Huan Liu","Huanpeng Chu","Jia'ni Zhao","Jiachen Wang","Jiajing Zhao","Jiamin Ren","Jiapeng Wang","Jiaxin Zhang","Jiayi Gui","Jiayue Zhao","Jijie Li","Jing An","Jing Li","Jingwei Yuan","Jinhua Du","Jinxin Liu","Junkai Zhi","Junwen Duan","Kaiyue Zhou","Kangjian Wei","Ke Wang","Keyun Luo","Laiqiang Zhang","Leigang Sha","Liang Xu","Lindong Wu","Lintao Ding","Lu Chen","Minghao Li","Nianyi Lin","Pan Ta","Qiang Zou","Rongjun Song","Ruiqi Yang","Shangqing Tu","Shangtong Yang","Shaoxiang Wu","Shengyan Zhang","Shijie Li","Shuang Li","Shuyi Fan","Wei Qin","Wei Tian","Weining Zhang","Wenbo Yu","Wenjie Liang","Xiang Kuang","Xiangmeng Cheng","Xiangyang Li","Xiaoquan Yan","Xiaowei Hu","Xiaoying Ling","Xing Fan","Xingye Xia","Xinyuan Zhang","Xinze Zhang","Xirui Pan","Xu Zou","Xunkai Zhang","Yadi Liu","Yandong Wu","Yanfu Li","Yidong Wang","Yifan Zhu","Yijun Tan","Yilin Zhou","Yiming Pan","Ying Zhang","Yinpei Su","Yipeng Geng","Yong Yan","Yonglin Tan","Yuean Bi","Yuhan Shen","Yuhao Yang","Yujiang Li","Yunan Liu","Yunqing Wang","Yuntao Li","Yurong Wu","Yutao Zhang","Yuxi Duan","Yuxuan Zhang","Zezhen Liu","Zhengtao Jiang","Zhenhe Yan","Zheyu Zhang","Zhixiang Wei","Zhuo Chen","Zhuoer Feng","Zijun Yao","Ziwei Chai","Ziyuan Wang","Zuzhou Zhang","Bin Xu","Minlie Huang","Hongning Wang","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20758v1","updated":"2026-02-24T10:37:10Z","published":"2026-02-24T10:37:10Z","title":"Deep unfolding of MCMC kernels: scalable, modular & explainable GANs for high-dimensional posterior sampling","summary":"Markov chain Monte Carlo (MCMC) methods are fundamental to Bayesian computation, but can be computationally intensive, especially in high-dimensional settings. Push-forward generative models, such as generative adversarial networks (GANs), variational auto-encoders and normalising flows offer a computationally efficient alternative for posterior sampling. However, push-forward models are opaque as they lack the modularity of Bayes Theorem, leading to poor generalisation with respect to changes in the likelihood function. In this work, we introduce a novel approach to GAN architecture design by applying deep unfolding to Langevin MCMC algorithms. This paradigm maps fixed-step iterative algorithms onto modular neural networks, yielding architectures that are both flexible and amenable to interpretation. Crucially, our design allows key model parameters to be specified at inference time, offering robustness to changes in the likelihood parameters. We train these unfolded samplers end-to-end using a supervised regularized Wasserstein GAN framework for posterior sampling. Through extensive Bayesian imaging experiments, we demonstrate that our proposed approach achieves high sampling accuracy and excellent computational efficiency, while retaining the physics consistency, adaptability and interpretability of classical MCMC strategies.","authors":["Jonathan Spence","Tobías I. Liaudat","Konstantinos Zygalakis","Marcelo Pereyra"],"pdf_url":"","comment":"37 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2602.11776v2","updated":"2026-02-24T10:34:13Z","published":"2026-02-12T09:54:23Z","title":"MUSE: Multi-Tenant Model Serving With Seamless Model Updates","summary":"In binary classification systems, decision thresholds translate model scores into actions. Choosing suitable thresholds relies on the specific distribution of the underlying model scores but also on the specific business decisions of each client using that model. However, retraining models inevitably shifts score distributions, invalidating existing thresholds. In multi-tenant Score-as-a-Service environments, where decision boundaries reside in client-managed infrastructure, this creates a severe bottleneck: recalibration requires coordinating threshold updates across hundreds of clients, consuming excessive human hours and leading to model stagnation. We introduce MUSE, a model serving framework that enables seamless model updates by decoupling model scores from client decision boundaries. Designed for multi-tenancy, MUSE optimizes infrastructure re-use by sharing models via dynamic intent-based routing, combined with a two-level score transformation that maps model outputs to a stable, reference distribution. Deployed at scale by Feedzai, MUSE processes over a thousand events per second, and over 55 billion events in the last 12 months, across several dozens of tenants, while maintaining high-availability and low-latency guarantees. By reducing model lead time from weeks to minutes, MUSE promotes model resilience against shifting attacks, saving millions of dollars in fraud losses and operational costs.","authors":["Cláudio Correia","Alberto E. A. Ferreira","Lucas Martins","Miguel P. Bento","Sofia Guerreiro","Ricardo Ribeiro Pereira","Ana Sofia Gomes","Jacopo Bono","Hugo Ferreira","Pedro Bizarro"],"pdf_url":"","comment":"Currently under review for KDD 2026 (Applied Data Science)"},{"id":"http://arxiv.org/abs/2508.13904v3","updated":"2026-02-24T10:29:51Z","published":"2025-08-19T15:05:55Z","title":"One-Step Flow Q-Learning: Addressing the Diffusion Policy Bottleneck in Offline Reinforcement Learning","summary":"Diffusion Q-Learning (DQL) has established diffusion policies as a high-performing paradigm for offline reinforcement learning, but its reliance on multi-step denoising for action generation renders both training and inference slow and fragile. Existing efforts to accelerate DQL toward one-step denoising typically rely on auxiliary modules or policy distillation, sacrificing either simplicity or performance. It remains unclear whether a one-step policy can be trained directly without such trade-offs. To this end, we introduce One-Step Flow Q-Learning (OFQL), a novel framework that enables effective one-step action generation during both training and inference, without auxiliary modules or distillation. OFQL reformulates the DQL policy within the Flow Matching (FM) paradigm but departs from conventional FM by learning an average velocity field that directly supports accurate one-step action generation. This design removes the need for multi-step denoising and backpropagation-through-time updates, resulting in substantially faster and more robust learning. Extensive experiments on the D4RL benchmark show that OFQL, despite generating actions in a single step, not only significantly reduces computation during both training and inference but also outperforms multi-step DQL by a large margin. Furthermore, OFQL surpasses all other baselines, achieving state-of-the-art performance in D4RL.","authors":["Thanh Nguyen","Chang D. Yoo"],"pdf_url":"","comment":"10 pages, ICLR2026"},{"id":"http://arxiv.org/abs/2602.20751v1","updated":"2026-02-24T10:28:44Z","published":"2026-02-24T10:28:44Z","title":"SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing","summary":"Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.","authors":["Yifei Xu","Guilherme Potje","Shivam Shandilya","Tiancheng Yuan","Leonardo de Oliveira Nunes","Rakshanda Agarwal","Saeid Asgari","Adam Atkinson","Emre Kıcıman","Songwu Lu","Ranveer Chandra","Tusher Chakraborty"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.21895v2","updated":"2026-02-24T10:27:37Z","published":"2025-09-26T05:30:37Z","title":"Why High-rank Neural Networks Generalize?: An Algebraic Framework with RKHSs","summary":"We derive a new Rademacher complexity bound for deep neural networks using Koopman operators, group representations, and reproducing kernel Hilbert spaces (RKHSs). The proposed bound describes why the models with high-rank weight matrices generalize well. Although there are existing bounds that attempt to describe this phenomenon, these existing bounds can be applied to limited types of models. We introduce an algebraic representation of neural networks and a kernel function to construct an RKHS to derive a bound for a wider range of realistic models. This work paves the way for the Koopman-based theory for Rademacher complexity bounds to be valid for more practical situations.","authors":["Yuka Hashimoto","Sho Sonoda","Isao Ishikawa","Masahiro Ikeda"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.02060v3","updated":"2026-02-24T10:17:07Z","published":"2025-09-02T07:58:12Z","title":"Morphology-Aware Peptide Discovery via Masked Conditional Generative Modeling","summary":"Peptide self-assembly prediction offers a powerful bottom-up strategy for designing biocompatible, low-toxicity materials for large-scale synthesis in a broad range of biomedical and energy applications. However, screening the vast sequence space for categorization of aggregate morphology remains intractable. We introduce PepMorph, an end-to-end peptide discovery pipeline that generates novel sequences that are not only prone to aggregate but whose self-assembly is steered toward fibrillar or spherical morphologies by conditioning on isolated peptide descriptors that serve as morphology proxies. To this end, we compiled a new dataset by leveraging existing aggregation propensity datasets and extracting geometric and physicochemical descriptors. This dataset is then used to train a Transformer-based Conditional Variational Autoencoder with a masking mechanism, which generates novel peptides under arbitrary conditioning. After filtering to ensure design specifications and validation of generated sequences through coarse-grained molecular dynamics (CG-MD) simulations, PepMorph yielded 83% success rate under our CG-MD validation protocol and morphology criterion for the targeted class, showcasing its promise as a framework for application-driven peptide discovery.","authors":["Nuno Costa","Julija Zavadlav"],"pdf_url":"","comment":"46 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.12506v3","updated":"2026-02-24T09:55:26Z","published":"2024-07-17T11:38:57Z","title":"Classification and reconstruction for single-pixel imaging with classical and quantum neural networks","summary":"Single-pixel cameras are an effective solution for imaging outside the visible spectrum, where traditional CMOS/CCD cameras have challenges. When combined with machine learning, they can analyze images quickly enough for practical applications. Solving the problem of high-dimensional single-pixel visualization can potentially be accelerated via quantum machine learning, thereby expanding the range of practical problems. In this work, we simulated a single-pixel imaging experiment using Hadamard basis patterns, where images from the MNIST handwritten digit dataset and FashionMNIST items of clothing dataset were used as objects. There were selected 64 measurements with maximum variance (6% of the number of pixels in the image). We created algorithms for classifying and reconstructing images based on these measurements using classical fully-connected neural networks and parameterized quantum circuits. Classical and quantum classifiers showed the best accuracies of 96% and 95% for MNIST and 84% and 81% for FashionMNIST, respectively, after 6 training epochs, which is a quite competitive result. In the area of intersection by the number of parameters of the quantum and classical classifiers, the quantum demonstrates results no worse than the classical one, even better by a value of about 1-3%. Image reconstruction was also demonstrated using classical and quantum neural networks after 10 training epochs; the best structural similarity index measure values were 0.76 and 0.26 for MNIST and 0.73 and 0.22 for FashionMNIST, respectively, which indicates that the problem in such a formulation turned out to be too difficult for quantum neural networks in such a configuration for now.","authors":["Sofya Manko","Dmitry Frolovtsev"],"pdf_url":"","comment":"Article Version: Accepted Manuscript 12 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2510.18114v2","updated":"2026-02-24T09:55:10Z","published":"2025-10-20T21:26:52Z","title":"Latent-Augmented Discrete Diffusion Models","summary":"Discrete diffusion models have emerged as a powerful class of models and a promising route to fast language generation, but practical implementations typically rely on factored reverse transitions that ignore cross-token dependencies and degrade performance in the few-step regime. We propose Latent-Augmented Discrete Diffusion (LADD), which introduces a learnable auxiliary latent channel and performs diffusion over the joint (token, latent) space. The latent variables provide an intermediate representation that can express joint structure while preserving tractable parameterizations. We instantiate LADD with continuous latents (Co-LADD) and discrete latents (Di-LADD), and study two inference schedules: a joint diffusion that denoises data and latents together, and a sequential diffusion that first resolves latents and then samples tokens conditionally. We derive ELBO-style objectives and analyze design choices that balance latent expressivity with diffusion compatibility. In experiments, LADDs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.","authors":["Dario Shariatian","Alain Durmus","Umut Simsekli","Stefano Peluchetti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20731v1","updated":"2026-02-24T09:53:50Z","published":"2026-02-24T09:53:50Z","title":"Communication-Inspired Tokenization for Structured Image Representations","summary":"Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.","authors":["Aram Davtyan","Yusuf Sahin","Yasaman Haghighi","Sebastian Stapf","Pablo Acuaviva","Alexandre Alahi","Paolo Favaro"],"pdf_url":"","comment":"Project website: https://araachie.github.io/comit/"},{"id":"http://arxiv.org/abs/2602.20730v1","updated":"2026-02-24T09:53:24Z","published":"2026-02-24T09:53:24Z","title":"Rethink Efficiency Side of Neural Combinatorial Solver: An Offline and Self-Play Paradigm","summary":"We propose ECO, a versatile learning paradigm that enables efficient offline self-play for Neural Combinatorial Optimization (NCO). ECO addresses key limitations in the field through: 1) Paradigm Shift: Moving beyond inefficient online paradigms, we introduce a two-phase offline paradigm consisting of supervised warm-up and iterative Direct Preference Optimization (DPO); 2) Architecture Shift: We deliberately design a Mamba-based architecture to further enhance the efficiency in the offline paradigm; and 3) Progressive Bootstrapping: To stabilize training, we employ a heuristic-based bootstrapping mechanism that ensures continuous policy improvement during training. Comparison results on TSP and CVRP highlight that ECO performs competitively with up-to-date baselines, with significant advantage on the efficiency side in terms of memory utilization and training throughput. We provide further in-depth analysis on the efficiency, throughput and memory usage of ECO. Ablation studies show rationale behind our designs.","authors":["Zhenxing Xu","Zeyuan Ma","Weidong Bao","Hui Yan","Yan Zheng","Ji Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20729v1","updated":"2026-02-24T09:50:17Z","published":"2026-02-24T09:50:17Z","title":"Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty","summary":"Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.","authors":["Xu Wan","Chao Yang","Cheng Yang","Jie Song","Mingyang Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.18481v2","updated":"2026-02-24T09:46:54Z","published":"2025-06-23T10:34:44Z","title":"FREQuency ATTribution: benchmarking frequency-based occlusion for time series data","summary":"Deep neural networks are among the most successful algorithms in terms of performance and scalability across different domains. However, since these networks are black boxes, their usability is severely restricted due to a lack of interpretability. Existing interpretability methods do not address the analysis of time-series-based networks specifically enough. This paper shows that an analysis in the frequency domain can not only highlight relevant areas in the input signal better than existing methods but is also more robust to fluctuations in the signal. In this paper, FreqAtt is presented - a framework that enables post-hoc interpretation of time-series analysis. To achieve this, the relevant frequencies are evaluated, and the signal is either filtered or the relevant input data is marked. FreqAtt is evaluated using a wide range of statistical metrics to provide a broad overview of its performance. The results show that using frequency-based attribution, especially in combination with traditional attribution on top of the frequency-optimized signal, provides strong performance across different metrics.","authors":["Dominique Mercier","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"","comment":"26 pages, 16 figures, 2 tables"},{"id":"http://arxiv.org/abs/2512.07770v2","updated":"2026-02-24T09:39:27Z","published":"2025-12-08T17:51:49Z","title":"Distribution-informed Online Conformal Prediction","summary":"Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.","authors":["Dongjian Hu","Junxi Wu","Shu-Tao Xia","Changliang Zou"],"pdf_url":"","comment":"ICLR2026 camera-ready version"},{"id":"http://arxiv.org/abs/2512.23447v2","updated":"2026-02-24T09:29:19Z","published":"2025-12-29T13:03:18Z","title":"Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss","summary":"Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain intermediate activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on $n^2$ activations, where $n$ is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.","authors":["Ang Lv","Jin Ma","Yiyuan Ma","Siyuan Qiao"],"pdf_url":"","comment":"ICLR 2026 Oral"},{"id":"http://arxiv.org/abs/2602.02620v2","updated":"2026-02-24T09:20:02Z","published":"2026-02-02T13:36:36Z","title":"CryoLVM: Self-supervised Learning from Cryo-EM Density Maps with Large Vision Models","summary":"Cryo-electron microscopy (cryo-EM) has revolutionized structural biology by enabling near-atomic-level visualization of biomolecular assemblies. However, the exponential growth in cryo-EM data throughput and complexity, coupled with diverse downstream analytical tasks, necessitates unified computational frameworks that transcend current task-specific deep learning approaches with limited scalability and generalizability. We present CryoLVM, a foundation model that learns rich structural representations from experimental density maps with resolved structures by leveraging the Joint-Embedding Predictive Architecture (JEPA) integrated with SCUNet-based backbone, which can be rapidly adapted to various downstream tasks. We further introduce a novel histogram-based distribution alignment loss that accelerates convergence and enhances fine-tuning performance. We demonstrate CryoLVM's effectiveness across three critical cryo-EM tasks: density map sharpening, density map super-resolution, and missing wedge restoration. Our method consistently outperforms state-of-the-art baselines across multiple density map quality metrics, confirming its potential as a versatile model for a wide spectrum of cryo-EM applications.","authors":["Weining Fu","Kai Shu","Kui Xu","Qiangfeng Cliff Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20714v1","updated":"2026-02-24T09:19:28Z","published":"2026-02-24T09:19:28Z","title":"WeirNet: A Large-Scale 3D CFD Benchmark for Geometric Surrogate Modeling of Piano Key Weirs","summary":"Reliable prediction of hydraulic performance is challenging for Piano Key Weir (PKW) design because discharge capacity depends on three-dimensional geometry and operating conditions. Surrogate models can accelerate hydraulic-structure design, but progress is limited by scarce large, well-documented datasets that jointly capture geometric variation, operating conditions, and functional performance. This study presents WeirNet, a large 3D CFD benchmark dataset for geometric surrogate modeling of PKWs. WeirNet contains 3,794 parametric, feasibility-constrained rectangular and trapezoidal PKW geometries, each scheduled at 19 discharge conditions using a consistent free-surface OpenFOAM workflow, resulting in 71,387 completed simulations that form the benchmark and with complete discharge coefficient labels. The dataset is released as multiple modalities compact parametric descriptors, watertight surface meshes and high-resolution point clouds together with standardized tasks and in-distribution and out-of-distribution splits. Representative surrogate families are benchmarked for discharge coefficient prediction. Tree-based regressors on parametric descriptors achieve the best overall accuracy, while point- and mesh-based models remain competitive and offer parameterization-agnostic inference. All surrogates evaluate in milliseconds per sample, providing orders-of-magnitude speedups over CFD runtimes. Out-of-distribution results identify geometry shift as the dominant failure mode compared to unseen discharge values, and data-efficiency experiments show diminishing returns beyond roughly 60% of the training data. By publicly releasing the dataset together with simulation setups and evaluation pipelines, WeirNet establishes a reproducible framework for data-driven hydraulic modeling and enables faster exploration of PKW designs during the early stages of hydraulic planning.","authors":["Lisa Lüddecke","Michael Hohmann","Sebastian Eilermann","Jan Tillmann-Mumm","Pezhman Pourabdollah","Mario Oertel","Oliver Niggemann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20712v1","updated":"2026-02-24T09:18:57Z","published":"2026-02-24T09:18:57Z","title":"F10.7 Index Prediction: A Multiscale Decomposition Strategy with Wavelet Transform for Performance Optimization","summary":"In this study, we construct Dataset A for training, validation, and testing, and Dataset B to evaluate generalization. We propose a novel F10.7 index forecasting method using wavelet decomposition, which feeds F10.7 together with its decomposed approximate and detail signals into the iTransformer model. We also incorporate the International Sunspot Number (ISN) and its wavelet-decomposed signals to assess their influence on prediction performance. Our optimal method is then compared with the latest method from S. Yan et al. (2025) and three operational models (SWPC, BGS, CLS). Additionally, we transfer our method to the PatchTST model used in H. Ye et al. (2024) and compare our method with theirs on Dataset B. Key findings include: (1) The wavelet-based combination methods overall outperform the baseline using only F10.7 index. The prediction performance improves as higher-level approximate and detail signals are incrementally added. The Combination 6 method integrating F10.7 with its first to fifth level approximate and detail signals outperforms methods using only approximate or detail signals. (2) Incorporating ISN and its wavelet-decomposed signals does not enhance prediction performance. (3) The Combination 6 method significantly surpasses S. Yan et al. (2025) and three operational models, with RMSE, MAE, and MAPE reduced by 18.22%, 15.09%, and 8.57%, respectively, against the former method. It also excels across four different conditions of solar activity. (4) Our method demonstrates superior generalization and prediction capability over the method of H. Ye et al. (2024) across all forecast horizons. To our knowledge, this is the first application of wavelet decomposition in F10.7 prediction, substantially improving forecast performance.","authors":["Xuran Ma","Xuebao Li","Yanfang Zheng","Yongshang Lv","Xiaojia Ji","Jiancheng Xu","Hongwei Ye","Zixian Wu","Shuainan Yan","Liang Dong","Zamri Zainal Abidin","Xusheng Huang","Shunhuang Zhang","Honglei Jin","Tarik Abdul Latef","Noraisyah Mohamed Shah","Mohamadariff Othman","Kamarul Ariffin Noordin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.06871v2","updated":"2026-02-24T09:15:53Z","published":"2024-12-09T12:34:13Z","title":"Predicting Subway Passenger Flows under Incident Situation with Causality","summary":"In the context of rail transit operations, real-time passenger flow prediction is essential; however, most models primarily focus on normal conditions, with limited research addressing incident situations. There are several intrinsic challenges associated with prediction during incidents, such as a lack of interpretability and data scarcity. To address these challenges, we propose a two-stage method that separates predictions under normal conditions and the causal effects of incidents. First, a normal prediction model is trained using data from normal situations. Next, the synthetic control method is employed to identify the causal effects of incidents, combined with placebo tests to determine significant levels of these effects. The significant effects are then utilized to train a causal effect prediction model, which can forecast the impact of incidents based on features of the incidents and passenger flows. During the prediction phase, the results from both the normal situation model and the causal effect prediction model are integrated to generate final passenger flow predictions during incidents. Our approach is validated using real-world data, demonstrating improved accuracy. Furthermore, the two-stage methodology enhances interpretability. By analyzing the causal effect prediction model, we can identify key influencing factors related to the effects of incidents and gain insights into their underlying mechanisms. Our work can assist subway system managers in estimating passenger flow affected by incidents and enable them to take proactive measures. Additionally, it can deepen researchers' understanding of the impact of incidents on subway passenger flows.","authors":["Xiannan Huang","Shuhan Qiu","Quan Yuan","Chao Yang"],"pdf_url":"","comment":"Accepted by Transportation"},{"id":"http://arxiv.org/abs/2510.06868v2","updated":"2026-02-24T09:08:19Z","published":"2025-10-08T10:38:24Z","title":"Multi-hop Deep Joint Source-Channel Coding with Deep Hash Distillation for Semantically Aligned Image Recovery","summary":"We consider image transmission via deep joint source-channel coding (DeepJSCC) over multi-hop additive white Gaussian noise (AWGN) channels by training a DeepJSCC encoder-decoder pair with a pre-trained deep hash distillation (DHD) module to semantically cluster images, facilitating security-oriented applications through enhanced semantic consistency and improving the perceptual reconstruction quality. We train the DeepJSCC module to both reduce mean square error (MSE) and minimize cosine distance between DHD hashes of source and reconstructed images. Significantly improved perceptual quality as a result of semantic alignment is illustrated for different multi-hop settings, for which classical DeepJSCC may suffer from noise accumulation, measured by the learned perceptual image patch similarity (LPIPS) metric.","authors":["Didrik Bergström","Deniz Gündüz","Onur Günlü"],"pdf_url":"","comment":"Change last word in title, add missing trailing bracket, add additional simulation results in section 4.1; results unchanged"},{"id":"http://arxiv.org/abs/2509.25424v3","updated":"2026-02-24T09:06:12Z","published":"2025-09-29T19:32:11Z","title":"Polychromic Objectives for Reinforcement Learning","summary":"Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.","authors":["Jubayer Ibn Hamid","Ifdita Hasan Orney","Ellen Xu","Chelsea Finn","Dorsa Sadigh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.01115v2","updated":"2026-02-24T09:02:23Z","published":"2025-08-01T23:33:48Z","title":"A hierarchy tree data structure for behavior-based user segment representation","summary":"User attributes are essential in multiple stages of modern recommendation systems and are particularly important for mitigating the cold-start problem and improving the experience of new or infrequent users. We propose Behavior-based User Segmentation (BUS), a novel tree-based data structure that hierarchically segments the user universe with various users' categorical attributes based on the users' product-specific engagement behaviors. During the BUS tree construction, we use Normalized Discounted Cumulative Gain (NDCG) as the objective function to maximize the behavioral representativeness of marginal users relative to active users in the same segment. The constructed BUS tree undergoes further processing and aggregation across the leaf nodes and internal nodes, allowing the generation of popular social content and behavioral patterns for each node in the tree. To further mitigate bias and improve fairness, we use the social graph to derive the user's connection-based BUS segments, enabling the combination of behavioral patterns extracted from both the user's own segment and connection-based segments as the connection aware BUS-based recommendation. Our offline analysis shows that the BUS-based retrieval significantly outperforms traditional user cohort-based aggregation on ranking quality. We have successfully deployed our data structure and machine learning algorithm and tested it with various production traffic serving billions of users daily, achieving statistically significant improvements in the online product metrics, including music ranking and email notifications. To the best of our knowledge, our study represents the first list-wise learning-to-rank framework for tree-based recommendation that effectively integrates diverse user categorical attributes while preserving real-world semantic interpretability at a large industrial scale.","authors":["Yang Liu","Xuejiao Kang","Sathya Iyer","Idris Malik","Ruixuan Li","Juan Wang","Xinchen Lu","Xiangxue Zhao","Dayong Wang","Menghan Liu","Isaac Liu","Feng Liang","Yinzhe Yu"],"pdf_url":"","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2510.22620v2","updated":"2026-02-24T09:00:11Z","published":"2025-10-26T10:36:42Z","title":"Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents","summary":"AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabilities only or require modeling of complete agents. To address these limitations, we introduce threat snapshots: a framework that isolates specific states in an agent's execution flow where LLM vulnerabilities manifest, enabling the systematic identification and categorization of security risks that propagate from the LLM to the agent level. We apply this framework to construct the $b^3$ benchmark, a security benchmark based on 194,331 unique crowdsourced adversarial attacks. We then evaluate 34 popular LLMs with it, revealing, among other insights, that enhanced reasoning capabilities improve security, while model size does not correlate with security. We release our benchmark, dataset, and evaluation code to facilitate widespread adoption by LLM providers and practitioners, offering guidance for agent developers and incentivizing model developers to prioritize backbone security improvements.","authors":["Julia Bazinska","Max Mathys","Francesco Casucci","Mateo Rojas-Carulla","Xander Davies","Alexandra Souly","Niklas Pfister"],"pdf_url":"","comment":"Julia Bazinska and Max Mathys contributed equally"},{"id":"http://arxiv.org/abs/2602.20698v1","updated":"2026-02-24T08:59:37Z","published":"2026-02-24T08:59:37Z","title":"High-Dimensional Robust Mean Estimation with Untrusted Batches","summary":"We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $μ$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $α$.\n  Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\\sqrtα$, or (2) an $α$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.\n  We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\\sqrt{\\varepsilon/n} + \\sqrt{d/nN} + \\sqrtα)$, demonstrating that while heterogeneity $α$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\\sqrt{n}$ due to the internal averaging afforded by the batch structure.","authors":["Maryam Aliakbarpour","Vladimir Braverman","Yuhan Liu","Junze Yin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.13961v2","updated":"2026-02-24T08:52:57Z","published":"2025-04-17T01:14:53Z","title":"CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee","summary":"Accurate short-term traffic demand prediction is critical for the operation of traffic systems. Besides point estimation, the confidence interval of the prediction is also of great importance. Many models for traffic operations, such as shared bike rebalancing and taxi dispatching, take into account the uncertainty of future demand and require confidence intervals as the input. However, existing methods for confidence interval modeling rely on strict assumptions, such as unchanging traffic patterns and correct model specifications, to guarantee enough coverage. Therefore, the confidence intervals provided could be invalid, especially in a changing traffic environment. To fill this gap, we propose an efficient method, CONTINA (Conformal Traffic Intervals with Adaptation) to provide interval predictions that can adapt to external changes. By collecting the errors of interval during deployment, the method can adjust the interval in the next step by widening it if the errors are too large or shortening it otherwise. Furthermore, we theoretically prove that the coverage of the confidence intervals provided by our method converges to the target coverage level. Experiments across four real-world datasets and prediction models demonstrate that the proposed method can provide valid confidence intervals with shorter lengths. Our method can help traffic management personnel develop a more reasonable and robust operation plan in practice. And we release the code, model and dataset in \\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.","authors":["Chao Yang","Xiannan Huang","Shuhan Qiu","Yan Cheng"],"pdf_url":"","comment":"Accepted in Transportation Research Part C: Emerging Technologies"},{"id":"http://arxiv.org/abs/2502.03652v2","updated":"2026-02-24T08:46:16Z","published":"2025-02-05T22:30:00Z","title":"Improving the Convergence of Private Shuffled Gradient Methods with Public Data","summary":"We consider the problem of differentially private (DP) convex empirical risk minimization (ERM). While the standard DP-SGD algorithm is theoretically well-established, practical implementations often rely on shuffled gradient methods that traverse the training data sequentially rather than sampling with replacement in each iteration. Despite their widespread use, the theoretical privacy-accuracy trade-offs of private shuffled gradient methods (\\textit{DP-ShuffleG}) remain poorly understood, leading to a gap between theory and practice. In this work, we leverage privacy amplification by iteration (PABI) and a novel application of Stein's lemma to provide the first empirical excess risk bound of \\textit{DP-ShuffleG}. Our result shows that data shuffling results in worse empirical excess risk for \\textit{DP-ShuffleG} compared to DP-SGD. To address this limitation, we propose \\textit{Interleaved-ShuffleG}, a hybrid approach that integrates public data samples in private optimization. By alternating optimization steps that use private and public samples, \\textit{Interleaved-ShuffleG} effectively reduces empirical excess risk. Our analysis introduces a new optimization framework with surrogate objectives, varying levels of noise injection, and a dissimilarity metric, which can be of independent interest. Our experiments on diverse datasets and tasks demonstrate the superiority of \\textit{Interleaved-ShuffleG} over several baselines.","authors":["Shuli Jiang","Pranay Sharma","Zhiwei Steven Wu","Gauri Joshi"],"pdf_url":"","comment":"72 pages, 6 figures"},{"id":"http://arxiv.org/abs/2602.17550v2","updated":"2026-02-24T08:43:15Z","published":"2026-02-19T17:05:20Z","title":"MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning","summary":"Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming baselines. Our code is at: \\href{https://github.com/VenomRose-Juri/MASPO-RL}{https://github.com/VenomRose-Juri/MASPO-RL}.","authors":["Xiaoliang Fu","Jiaye Lin","Yangyi Fang","Binbin Zheng","Chaowen Hu","Zekai Shao","Cong Qin","Lu Pan","Ke Zeng","Xunliang Cai"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.12402v2","updated":"2026-02-24T08:30:00Z","published":"2025-10-14T11:32:55Z","title":"Cautious Weight Decay","summary":"We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales.","authors":["Lizhang Chen","Jonathan Li","Kaizhao Liang","Baiyu Su","Cong Xie","Nuo Wang Pierse","Chen Liang","Ni Lao","Qiang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20677v1","updated":"2026-02-24T08:26:46Z","published":"2026-02-24T08:26:46Z","title":"UrbanFM: Scaling Urban Spatio-Temporal Foundation Models","summary":"Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to \"scenario-specific\" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.","authors":["Wei Chen","Yuqian Wu","Junle Chen","Xiaofang Zhou","Yuxuan Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.16332v2","updated":"2026-02-24T08:23:38Z","published":"2025-06-19T14:07:38Z","title":"Feedback-driven recurrent quantum neural network universality","summary":"Quantum reservoir computing uses the dynamics of quantum systems to process temporal data, making it particularly well-suited for machine learning with noisy intermediate-scale quantum devices. Recent developments have introduced feedback-based quantum reservoir systems, which process temporal information with comparatively fewer components and enable real-time computation while preserving the input history. Motivated by their promising empirical performance, in this work, we study the approximation capabilities of feedback-based quantum reservoir computing. More specifically, we are concerned with recurrent quantum neural networks, which are quantum analogues of classical recurrent neural networks. Our results show that regular state-space systems can be approximated using quantum recurrent neural networks without the curse of dimensionality and with the number of qubits only growing logarithmically in the reciprocal of the prescribed approximation accuracy. Notably, our analysis demonstrates that quantum recurrent neural networks are universal with linear readouts, making them both powerful and experimentally accessible. These results pave the way for practical and theoretically grounded quantum reservoir computing with real-time processing capabilities.","authors":["Lukas Gonon","Rodrigo Martínez-Peña","Juan-Pablo Ortega"],"pdf_url":"","comment":"28 pages, 1 figure. Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20671v1","updated":"2026-02-24T08:21:28Z","published":"2026-02-24T08:21:28Z","title":"Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting","summary":"The rapid growth of dockless bike-sharing systems has generated massive spatio-temporal datasets useful for fleet allocation, congestion reduction, and sustainable mobility. Bike demand, however, depends on several external factors, making traditional time-series models insufficient. Centralized Machine Learning (CML) yields high-accuracy forecasts but raises privacy and bandwidth issues when data are distributed across edge devices. To overcome these limitations, we propose Bikelution, an efficient Federated Learning (FL) solution based on gradient-boosted trees that preserves privacy while delivering accurate mid-term demand forecasts up to six hours ahead. Experiments on three real-world BSS datasets show that Bikelution is comparable to its CML-based variant and outperforms the current state-of-the-art. The results highlight the feasibility of privacy-aware demand forecasting and outline the trade-offs between FL and CML approaches.","authors":["Antonios Tziorvas","Andreas Tritsarolis","Yannis Theodoridis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.01310v4","updated":"2026-02-24T08:13:38Z","published":"2025-02-03T12:37:20Z","title":"A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers","summary":"Neural network-based optimal transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing OT approaches, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural nets). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for general OT case, paving the promising direction for future research. Our experimental illustrations are available online https://github.com/milenagazdieva/StatOT.","authors":["Roman Tarasov","Petr Mokrov","Milena Gazdieva","Evgeny Burnaev","Alexander Korotin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.12016v3","updated":"2026-02-24T08:06:52Z","published":"2025-03-15T06:52:10Z","title":"A Survey on Federated Fine-tuning of Large Language Models","summary":"Large Language Models (LLMs) have demonstrated impressive success across various tasks. Integrating LLMs with Federated Learning (FL), a paradigm known as FedLLM, offers a promising avenue for collaborative model adaptation while preserving data privacy. This survey provides a systematic and comprehensive review of FedLLM. We begin by tracing the historical development of both LLMs and FL, summarizing relevant prior research to set the context. Subsequently, we delve into an in-depth analysis of the fundamental challenges inherent in deploying FedLLM. Addressing these challenges often requires efficient adaptation strategies; therefore, we conduct an extensive examination of existing Parameter-Efficient Fine-tuning (PEFT) methods and explore their applicability within the FL framework. To rigorously evaluate the performance of FedLLM, we undertake a thorough review of existing fine-tuning datasets and evaluation benchmarks. Furthermore, we discuss FedLLM's diverse real-world applications across multiple domains. Finally, we identify critical open challenges and outline promising research directions to foster future advancements in FedLLM. This survey aims to serve as a foundational resource for researchers and practitioners, offering valuable insights into the rapidly evolving landscape of federated fine-tuning for LLMs. It also establishes a roadmap for future innovations in privacy-preserving AI. We actively maintain a \\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{GitHub repo} to track cutting-edge advancements in this field.","authors":["Yebo Wu","Chunlin Tian","Jingguang Li","He Sun","Kahou Tam","Zhanting Zhou","Haicheng Liao","Jing Xiong","Zhijiang Guo","Li Li","Chengzhong Xu"],"pdf_url":"","comment":"Accepted by Transactions on Machine Learning Research (TMLR), 2026"},{"id":"http://arxiv.org/abs/2602.20658v1","updated":"2026-02-24T08:01:49Z","published":"2026-02-24T08:01:49Z","title":"Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video","summary":"Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.","authors":["Mohammad Sadra Rajabi","Aanuoluwapo Ojelade","Sunwook Kim","Maury A. Nussbaum"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.15425v2","updated":"2026-02-24T07:59:27Z","published":"2025-10-17T08:28:26Z","title":"TeamFormer: Shallow Parallel Transformers with Progressive Approximation","summary":"The widespread 'deeper is better' philosophy has driven the creation of architectures like ResNet and Transformer, which achieve high performance by stacking numerous layers. However, increasing model depth comes with challenges such as longer training times, higher inference latency, and impracticality on resource-constrained devices. To address these issues, we propose TeamFormer, a shallow Transformer architecture designed for true parallelism in both structure and computation. By formulating standard Transformers as function approximators in closed-form, our theoretical analysis shows that their performance relies on inter-layer collaboration for progressive approximation, rather than depth itself. While deep Transformers enforce this collaboration through sequential designs, we demonstrate that such collaboration is not inherently tied to sequential structures. TeamFormer removes the sequential constraint by organizing layers into parallel branches, enforcing inter-layer collaboration algorithmically. Specifically, we implement progressive approximation, ensuring that each new branch further reduces the loss from preceding branches, enabling faster convergence. Extensive experiments validate TeamFormer's effectiveness, outperforming standard Transformers like ViT. Moreover, TeamFormer supports up to 15.07x model compression and facilitates model expansion for adaptive continuous learning. Experimental results on multi-GPU deployment demonstrate that TeamFormer is 3.30x faster than widely used parallelism solutions such as FairScale. These advancements stem from our closed-form formulation of Transformers based on the Universal Approximation Theorem, which not only explains the ``depth belief'' but also opens new avenues for designing efficient Transformer architectures. Source code: https://(open-upon-acceptance)","authors":["Wei Wang","Xiao-Yong Wei","Qing Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.14495v2","updated":"2026-02-24T07:58:02Z","published":"2026-02-16T06:19:58Z","title":"Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs","summary":"Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \\emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.","authors":["Alejandro Francisco Queiruga"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.01283v2","updated":"2026-02-24T07:55:05Z","published":"2024-12-02T08:55:44Z","title":"Big data approach to Kazhdan-Lusztig polynomials","summary":"We investigate the structure of Kazhdan-Lusztig polynomials of the symmetric group by leveraging computational approaches from big data, including exploratory and topological data analysis, applied to the polynomials for symmetric groups of up to 11 strands.","authors":["Abel Lacabanne","Daniel Tubbenhauer","Pedro Vaz"],"pdf_url":"","comment":"27 pages, many figures, comments welcome, appeared in J. exp. math"},{"id":"http://arxiv.org/abs/2602.20652v1","updated":"2026-02-24T07:54:53Z","published":"2026-02-24T07:54:53Z","title":"DANCE: Doubly Adaptive Neighborhood Conformal Estimation","summary":"The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.","authors":["Brandon R. Feng","Brian J. Reich","Daniel Beaglehole","Xihaier Luo","David Keetae Park","Shinjae Yoo","Zhechao Huang","Xueyu Mao","Olcay Boz","Jungeum Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20651v1","updated":"2026-02-24T07:53:59Z","published":"2026-02-24T07:53:59Z","title":"Sparse Bayesian Deep Functional Learning with Structured Region Selection","summary":"In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.","authors":["Xiaoxian Zhu","Yingmeng Li","Shuangge Ma","Mengyun Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20646v1","updated":"2026-02-24T07:47:15Z","published":"2026-02-24T07:47:15Z","title":"On the Convergence of Stochastic Gradient Descent with Perturbed Forward-Backward Passes","summary":"We study stochastic gradient descent (SGD) for composite optimization problems with $N$ sequential operators subject to perturbations in both the forward and backward passes. Unlike classical analyses that treat gradient noise as additive and localized, perturbations to intermediate outputs and gradients cascade through the computational graph, compounding geometrically with the number of operators. We present the first comprehensive theoretical analysis of this setting. Specifically, we characterize how forward and backward perturbations propagate and amplify within a single gradient step, derive convergence guarantees for both general non-convex objectives and functions satisfying the Polyak--Łojasiewicz condition, and identify conditions under which perturbations do not deteriorate the asymptotic convergence order. As a byproduct, our analysis furnishes a theoretical explanation for the gradient spiking phenomenon widely observed in deep learning, precisely characterizing the conditions under which training recovers from spikes or diverges. Experiments on logistic regression with convex and non-convex regularization validate our theories, illustrating the predicted spike behavior and the asymmetric sensitivity to forward versus backward perturbations.","authors":["Boao Kong","Hengrui Zhang","Kun Yuan"],"pdf_url":"","comment":"34 pages"},{"id":"http://arxiv.org/abs/2602.20643v1","updated":"2026-02-24T07:44:19Z","published":"2026-02-24T07:44:19Z","title":"TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer","summary":"Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.","authors":["Jiawei Wang","Chuang Yang","Jiawei Yong","Xiaohang Xu","Hongjun Wang","Noboru Koshizuka","Shintaro Fukushima","Ryosuke Shibasaki","Renhe Jiang"],"pdf_url":"","comment":"TrajGPT-R is a Reinforcement Learning-Enhanced Generative Pre-trained Transformer for Mobility Trajectory Generation"},{"id":"http://arxiv.org/abs/2507.05306v3","updated":"2026-02-24T07:43:54Z","published":"2025-07-07T08:18:25Z","title":"Enjoying Non-linearity in Multinomial Logistic Bandits: A Minimax-Optimal Algorithm","summary":"We consider the multinomial logistic bandit problem in which a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $κ_* \\geq 1$ that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\\smash{O(d\\sqrt{T})}$ to $\\smash{O(d\\sqrt{T/κ_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework with a finite action space, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $ κ_* $ to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \\smash{\\widetilde{\\mathcal{O}}( R d \\sqrt{ {KT}/{κ_*}} ) } $, where $R$ denotes the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \\smash{\\widetilde{\\mathcal{O}}( RdK \\sqrt{T} )}$. Moreover, we provide a matching $\\smash{ Ω(dR\\sqrt{KT/κ_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $κ_*$ is optimal.","authors":["Pierre Boudart","Pierre Gaillard","Alessandro Rudi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.14462v2","updated":"2026-02-24T07:43:16Z","published":"2026-02-16T04:42:30Z","title":"Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment","summary":"Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \\emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \\texttt{openPangu-Embedded-1B-V1.1} model on the \\texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.","authors":["Hong Li","Zhen Zhou","Honggang Zhang","Yuping Luo","Xinyue Wang","Han Gong","Zhiyuan Liu"],"pdf_url":"","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.08125v3","updated":"2026-02-24T07:37:29Z","published":"2025-05-12T23:40:13Z","title":"Sharp Gaussian approximations for Decentralized Federated Learning","summary":"Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.","authors":["Soham Bonnerjee","Sayar Karmakar","Wei Biao Wu"],"pdf_url":"","comment":"Accepted as Spotlight, NeurIPS'25, Main Conference Track"},{"id":"http://arxiv.org/abs/2602.20629v1","updated":"2026-02-24T07:23:28Z","published":"2026-02-24T07:23:28Z","title":"QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs","summary":"As Large Language Models (LLMs) saturate elementary benchmarks, the research frontier has shifted from generation to the reliability of automated evaluation. We demonstrate that standard \"LLM-as-a-Judge\" protocols suffer from a systematic Alignment Gap when applied to upper-undergraduate to early graduate level mathematics. To quantify this, we introduce QEDBench, the first large-scale dual-rubric alignment benchmark to systematically measure alignment with human experts on university-level math proofs by contrasting course-specific rubrics against expert common knowledge criteria. By deploying a dual-evaluation matrix (7 judges x 5 solvers) against 1,000+ hours of human evaluation, we reveal that certain frontier evaluators like Claude Opus 4.5, DeepSeek-V3, Qwen 2.5 Max, and Llama 4 Maverick exhibit significant positive bias (up to +0.18, +0.20, +0.30, +0.36 mean score inflation, respectively). Furthermore, we uncover a critical reasoning gap in the discrete domain: while Gemini 3.0 Pro achieves state-of-the-art performance (0.91 average human evaluation score), other reasoning models like GPT-5 Pro and Claude Sonnet 4.5 see their performance significantly degrade in discrete domains. Specifically, their average human evaluation scores drop to 0.72 and 0.63 in Discrete Math, and to 0.74 and 0.50 in Graph Theory. In addition to these research results, we also release QEDBench as a public benchmark for evaluating and improving AI judges. Our benchmark is publicly published at https://github.com/qqliu/Yale-QEDBench.","authors":["Santiago Gonzalez","Alireza Amiri Bavandpour","Peter Ye","Edward Zhang","Ruslans Aleksejevs","Todor Antić","Polina Baron","Sujeet Bhalerao","Shubhrajit Bhattacharya","Zachary Burton","John Byrne","Hyungjun Choi","Nujhat Ahmed Disha","Koppany István Encz","Yuchen Fang","Robert Joseph George","Ebrahim Ghorbani","Alan Goldfarb","Jing Guo","Meghal Gupta","Stefano Huber","Annika Kanckos","Minjung Kang","Hyun Jong Kim","Dino Lorenzini","Levi Lorenzo","Tianyi Mao","Giovanni Marzenta","Ariane M. Masuda","Lukas Mauth","Ana Mickovic","Andres Miniguano-Trujillo","Antoine Moulin","Wenqi Ni","Tomos Parry","Kevin Ren","Hossein Roodbarani","Mathieu Rundström","Manjil Saikia","Detchat Samart","Rebecca Steiner","Connor Stewart","Dhara Thakkar","Jeffrey Tse","Vasiliki Velona","Yunhai Xiang","Sibel Yalçın","Jun Yan","Ji Zeng","Arman Cohan","Quanquan C. Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20616v1","updated":"2026-02-24T07:08:47Z","published":"2026-02-24T07:08:47Z","title":"Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model","summary":"Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly \"knowing the unknown\". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.","authors":["Xueqiang Lv","Shizhou Zhang","Yinghui Xing","Di Xu","Peng Wang","Yanning Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20611v1","updated":"2026-02-24T07:03:15Z","published":"2026-02-24T07:03:15Z","title":"Amortized Bayesian inference for actigraph time sheet data from mobile devices","summary":"Mobile data technologies use ``actigraphs'' to furnish information on health variables as a function of a subject's movement. The advent of wearable devices and related technologies has propelled the creation of health databases consisting of human movement data to conduct research on mobility patterns and health outcomes. Statistical methods for analyzing high-resolution actigraph data depend on the specific inferential context, but the advent of Artificial Intelligence (AI) frameworks require that the methods be congruent to transfer learning and amortization. This article devises amortized Bayesian inference for actigraph time sheets. We pursue a Bayesian approach to ensure full propagation of uncertainty and its quantification using a hierarchical dynamic linear model. We build our analysis around actigraph data from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study conducted by the Fielding School of Public Health in the University of California, Los Angeles. Apart from achieving probabilistic imputation of actigraph time sheets, we are also able to statistically learn about the time-varying impact of explanatory variables on the magnitude of acceleration (MAG) for a cohort of subjects.","authors":["Daniel Zhou","Sudipto Banerjee"],"pdf_url":"","comment":"40 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.17645v2","updated":"2026-02-24T07:02:26Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning","summary":"Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"","comment":"Camera-ready version. Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.19654v2","updated":"2026-02-24T06:53:31Z","published":"2026-02-23T09:56:22Z","title":"NEXUS: A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi National Capital Region","summary":"Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.","authors":["Rampunit Kumar","Aditya Maheshwari"],"pdf_url":"","comment":"18 pages"},{"id":"http://arxiv.org/abs/2505.19698v3","updated":"2026-02-24T06:45:28Z","published":"2025-05-26T08:52:45Z","title":"Performance Asymmetry in Model-Based Reinforcement Learning","summary":"Recently, Model-Based Reinforcement Learning (MBRL) have achieved super-human level performance on the Atari100k benchmark on average. However, we discover that conventional aggregates mask a major problem, Performance Asymmetry: MBRL agents dramatically outperform humans in certain tasks (Agent-Optimal tasks) while drastically underperform humans in other tasks (Human-Optimal tasks). Indeed, despite achieving SOTA in the overall mean Human-Normalized Scores (HNS), the SOTA agent scored the worst among baselines on Human-Optimal tasks, with a striking 21X performance gap between the Human-Optimal and Agent-Optimal subsets. To address this, we partition Atari100k evenly into Human-Optimal and Agent-Optimal subsets, and introduce a more balanced aggregate, Sym-HNS. Furthermore, we trace the striking Performance Asymmetry in the SOTA pixel diffusion world model to the curse of dimensionality and its prowess on high visual detail tasks (e.g. Breakout). To this end, we propose a novel latent end-to-end Joint Embedding DIffusion (JEDI) world model that achieves SOTA results in Sym-HNS, Human-Optimal tasks, and Breakout -- thus reversing the worsening Performance Asymmetry trend while improving computational efficiency and remaining competitive on the full Atari100k.","authors":["Jing Yu Lim","Rushi Shah","Zarif Ikram","Samson Yu","Haozhe Ma","Tze-Yun Leong","Dianbo Liu"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2602.20593v1","updated":"2026-02-24T06:33:29Z","published":"2026-02-24T06:33:29Z","title":"Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning","summary":"As a distributed collaborative machine learning paradigm, vertical federated learning (VFL) allows multiple passive parties with distinct features and one active party with labels to collaboratively train a model. Although it is known for the privacy-preserving capabilities, VFL still faces significant privacy and security threats from backdoor attacks. Existing backdoor attacks typically involve an attacker implanting a trigger into the model during the training phase and executing the attack by adding the trigger to the samples during the inference phase. However, in this paper, we find that triggers are not essential for backdoor attacks in VFL. In light of this, we disclose a new backdoor attack pathway in VFL by introducing a feature-based triggerless backdoor attack. This attack operates under a more stringent security assumption, where the attacker is honest-but-curious rather than malicious during the training phase. It comprises three modules: label inference for the targeted backdoor attack, poison generation with amplification and perturbation mechanisms, and backdoor execution to implement the attack. Extensive experiments on five benchmark datasets demonstrate that our attack outperforms three baseline backdoor attacks by 2 to 50 times while minimally impacting the main task. Even in VFL scenarios with 32 passive parties and only one set of auxiliary data, our attack maintains high performance. Moreover, when confronted with distinct defense strategies, our attack remains largely unaffected and exhibits strong robustness. We hope that the disclosure of this triggerless backdoor attack pathway will encourage the community to revisit security threats in VFL scenarios and inspire researchers to develop more robust and practical defense strategies.","authors":["Yige Liu","Yiwei Lou","Che Wang","Yongzhi Cao","Hanpin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10693v2","updated":"2026-02-24T06:30:50Z","published":"2026-02-11T09:48:08Z","title":"VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training","summary":"Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO","authors":["Guobin Shen","Chenxiao Zhao","Xiang Cheng","Lei Huang","Xing Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20585v1","updated":"2026-02-24T06:15:59Z","published":"2026-02-24T06:15:59Z","title":"Characterizing Online and Private Learnability under Distributional Constraints via Generalized Smoothness","summary":"Understanding minimal assumptions that enable learning and generalization is perhaps the central question of learning theory. Several celebrated results in statistical learning theory, such as the VC theorem and Littlestone's characterization of online learnability, establish conditions on the hypothesis class that allow for learning under independent data and adversarial data, respectively. Building upon recent work bridging these extremes, we study sequential decision making under distributional adversaries that can adaptively choose data-generating distributions from a fixed family $U$ and ask when such problems are learnable with sample complexity that behaves like the favorable independent case. We provide a near complete characterization of families $U$ that admit learnability in terms of a notion known as generalized smoothness i.e. a distribution family admits VC-dimension-dependent regret bounds for every finite-VC hypothesis class if and only if it is generalized smooth. Further, we give universal algorithms that achieve low regret under any generalized smooth adversary without explicit knowledge of $U$. Finally, when $U$ is known, we provide refined bounds in terms of a combinatorial parameter, the fragmentation number, that captures how many disjoint regions can carry nontrivial mass under $U$. These results provide a nearly complete understanding of learnability under distributional adversaries. In addition, building upon the surprising connection between online learning and differential privacy, we show that the generalized smoothness also characterizes private learnability under distributional constraints.","authors":["Moïse Blanchard","Abhishek Shetty","Alexander Rakhlin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20580v1","updated":"2026-02-24T06:02:03Z","published":"2026-02-24T06:02:03Z","title":"Personal Information Parroting in Language Models","summary":"Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.","authors":["Nishant Subramani","Kshitish Ghate","Mona Diab"],"pdf_url":"","comment":"EACL Findings 2026"}],"Multimedia":[{"id":"http://arxiv.org/abs/2602.20159v2","updated":"2026-02-24T17:59:15Z","published":"2026-02-23T18:59:41Z","title":"A Very Big Video Reasoning Suite","summary":"Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .","authors":["Maijunxian Wang","Ruisi Wang","Juyi Lin","Ran Ji","Thaddäus Wiedemer","Qingying Gao","Dezhi Luo","Yaoyao Qian","Lianyu Huang","Zelong Hong","Jiahui Ge","Qianli Ma","Hang He","Yifan Zhou","Lingzi Guo","Lantao Mei","Jiachen Li","Hanwen Xing","Tianqi Zhao","Fengyuan Yu","Weihang Xiao","Yizheng Jiao","Jianheng Hou","Danyang Zhang","Pengcheng Xu","Boyang Zhong","Zehong Zhao","Gaoyun Fang","John Kitaoka","Yile Xu","Hua Xu","Kenton Blacutt","Tin Nguyen","Siyuan Song","Haoran Sun","Shaoyue Wen","Linyang He","Runming Wang","Yanzhi Wang","Mengyue Yang","Ziqiao Ma","Raphaël Millière","Freda Shi","Nuno Vasconcelos","Daniel Khashabi","Alan Yuille","Yilun Du","Ziming Liu","Bo Li","Dahua Lin","Ziwei Liu","Vikash Kumar","Yijiang Li","Lei Yang","Zhongang Cai","Hokin Deng"],"pdf_url":"","comment":"Homepage: https://video-reason.com/"},{"id":"http://arxiv.org/abs/2602.21035v1","updated":"2026-02-24T15:55:39Z","published":"2026-02-24T15:55:39Z","title":"Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning","summary":"Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.","authors":["Junhao Xiao","Zhiyu Wu","Hao Lin","Yi Chen","Yahui Liu","Xiaoran Zhao","Zixu Wang","Zejiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20767v1","updated":"2026-02-24T10:59:59Z","published":"2026-02-24T10:59:59Z","title":"SPP-SCL: Semi-Push-Pull Supervised Contrastive Learning for Image-Text Sentiment Analysis and Beyond","summary":"Existing Image-Text Sentiment Analysis (ITSA) methods may suffer from inconsistent intra-modal and inter-modal sentiment relationships. Therefore, we develop a method that balances before fusing to solve the issue of vision-language imbalance intra-modal and inter-modal sentiment relationships; that is, a Semi-Push-Pull Supervised Contrastive Learning (SPP-SCL) method is proposed. Specifically, the method is implemented using a novel two-step strategy, namely first using the proposed intra-modal supervised contrastive learning to pull the relationships between the intra-modal and then performing a well-designed conditional execution statement. If the statement result is false, our method will perform the second step, which is inter-modal supervised contrastive learning to push away the relationships between inter-modal. The two-step strategy will balance the intra-modal and inter-modal relationships to achieve the purpose of relationship consistency and finally perform cross-modal feature fusion for sentiment analysis and detection. Experimental studies on three public image-text sentiment and sarcasm detection datasets demonstrate that SPP-SCL significantly outperforms state-of-the-art methods by a large margin and is more discriminative in sentiment.","authors":["Jiesheng Wu","Shengrong Li"],"pdf_url":"","comment":"Accepted and published by AAAI2026"},{"id":"http://arxiv.org/abs/2505.17645v2","updated":"2026-02-24T07:02:26Z","published":"2025-05-23T09:06:09Z","title":"HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning","summary":"Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.","authors":["Chuhao Zhou","Jianfei Yang"],"pdf_url":"","comment":"Camera-ready version. Accepted at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.08550v3","updated":"2026-02-24T02:47:07Z","published":"2026-02-09T11:50:29Z","title":"GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing","summary":"Human perception for effective object tracking in 2D video streams arises from the implicit use of prior 3D knowledge and semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings, while neglecting 3D geometric cues, making them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to infer geometric cues from only a few 2D images. To address the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing. By leveraging null-space constraints during model updates, it incorporates geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking. The project page is available at https://chenshihfang.github.io/GOT-EDIT.","authors":["Shih-Fang Chen","Jun-Cheng Chen","I-Hong Jhuo","Yen-Yu Lin"],"pdf_url":"","comment":"ICLR 2026"}]},"2026-02-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2602.22207v1","updated":"2026-02-25T18:58:25Z","published":"2026-02-25T18:58:25Z","title":"Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets","summary":"The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.","authors":["Hanna Yukhymenko","Anton Alexandrov","Martin Vechev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22200v1","updated":"2026-02-25T18:50:42Z","published":"2026-02-25T18:50:42Z","title":"SumTablets: A Transliteration Dataset of Sumerian Tablets","summary":"Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.","authors":["Cole Simmons","Richard Diehl Martinez","Dan Jurafsky"],"pdf_url":"","comment":"11 pages with 3 figures"},{"id":"http://arxiv.org/abs/2602.22193v1","updated":"2026-02-25T18:43:01Z","published":"2026-02-25T18:43:01Z","title":"Improving Parametric Knowledge Access in Reasoning Language Models","summary":"We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowledge reasoning by default: adding a simple \"think step-by-step\" cue demonstrates statistically significant improvement in knowledge recall but not math. Motivated by this, we propose training models to reason over their parametric knowledge using world-knowledge question answering as a verifiable reward. After reinforcement learning on TriviaQA (+9.9%), performance also improves on Natural Questions, HotpotQA, SimpleQA, and StrategyQA by 4.2%, 2.1%, 0.6%, and 3.0%, respectively. Reasoning models are under-optimized for parametric knowledge access, but can be easily trained to reason better.","authors":["Melody Ma","John Hewitt"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22190v1","updated":"2026-02-25T18:34:57Z","published":"2026-02-25T18:34:57Z","title":"GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL","summary":"Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.","authors":["Rui Yang","Qianhui Wu","Zhaoyang Wang","Hanyang Chen","Ke Yang","Hao Cheng","Huaxiu Yao","Baoling Peng","Huan Zhang","Jianfeng Gao","Tong Zhang"],"pdf_url":"","comment":"57 pages, 17 figures"},{"id":"http://arxiv.org/abs/2507.08017v5","updated":"2026-02-25T18:34:16Z","published":"2025-07-07T20:26:31Z","title":"Mechanistic Indicators of Understanding in Large Language Models","summary":"Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms \"features\" as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact \"circuit\" connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.","authors":["Pierre Beckmann","Matthieu Queloz"],"pdf_url":"","comment":"38 pages"},{"id":"http://arxiv.org/abs/2602.22182v1","updated":"2026-02-25T18:28:38Z","published":"2026-02-25T18:28:38Z","title":"LiCQA : A Lightweight Complex Question Answering System","summary":"Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of training data required. In this paper, we present LiCQA, an unsupervised question answer- ing model that works primarily on the basis of corpus evidence. We empirically compare the effectiveness and efficiency of LiCQA with two recently presented QA systems, which are based on different underlying principles. The results of our experiments show that LiCQA significantly outperforms these two state-of-the-art systems on benchmark data with noteworthy reduction in latency.","authors":["Sourav Saha","Dwaipayan Roy","Mandar Mitra"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22175v1","updated":"2026-02-25T18:21:35Z","published":"2026-02-25T18:21:35Z","title":"DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs","summary":"Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.","authors":["Xi Ye","Wuwei Zhang","Fangcong Yin","Howard Yen","Danqi Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18671v2","updated":"2026-02-25T18:09:08Z","published":"2026-02-21T00:38:47Z","title":"Spilled Energy in Large Language Models","summary":"We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track \"energy spills\" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.","authors":["Adrian Robert Minut","Hazem Dewidar","Iacopo Masi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22157v1","updated":"2026-02-25T18:05:11Z","published":"2026-02-25T18:05:11Z","title":"Dynamic Personality Adaptation in Large Language Models via State Machines","summary":"The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.","authors":["Leon Pielage","Ole Hätscher","Mitja Back","Bernhard Marschall","Benjamin Risse"],"pdf_url":"","comment":"22 pages, 5 figures, submitted to ICPR 2026"},{"id":"http://arxiv.org/abs/2602.22145v1","updated":"2026-02-25T17:54:42Z","published":"2026-02-25T17:54:42Z","title":"When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models","summary":"Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.","authors":["Satyam Kumar Navneet","Joydeep Chandra","Yong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22144v1","updated":"2026-02-25T17:50:41Z","published":"2026-02-25T17:50:41Z","title":"NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors","summary":"Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.","authors":["Lingfeng Ren","Weihao Yu","Runpeng Yu","Xinchao Wang"],"pdf_url":"","comment":"Code: https://github.com/lingfengren/NoLan"},{"id":"http://arxiv.org/abs/2503.15133v2","updated":"2026-02-25T17:46:42Z","published":"2025-03-19T11:48:52Z","title":"EmoGRACE: Aspect-based emotion analysis for social media data","summary":"While sentiment analysis has advanced from sentence to aspect-level, i.e., the identification of concrete terms related to a sentiment, the equivalent field of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks and the increased complexity of emotion classes in contrast to binary sentiments. This paper addresses these gaps, by generating a first ABEA training dataset, consisting of 2,621 English Tweets, and fine-tuning a BERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and Aspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory by Shaver et al. [1] and made use of group annotation and majority voting strategies to facilitate label consistency. The resulting dataset contained aspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None class. Using the new ABEA training dataset, the state-of-the-art ABSA model GRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a performance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and AEC extraction. The limiting factors for model performance were broadly identified as the small training dataset size coupled with the increased task complexity, causing model overfitting and limited abilities to generalize well on new data.","authors":["Christina Zorenböhmer","Sebastian Schmidt","Bernd Resch"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22125v1","updated":"2026-02-25T17:12:37Z","published":"2026-02-25T17:12:37Z","title":"IndicIFEval: A Benchmark for Verifiable Instruction-Following Evaluation in 14 Indic Languages","summary":"Instruction-following benchmarks remain predominantly English-centric, leaving a critical evaluation gap for the hundreds of millions of Indic language speakers. We introduce IndicIFEval, a benchmark evaluating constrained generation of LLMs across 14 Indic languages using automatically verifiable, rule-based instructions. It comprises around 800 human-verified examples per language spread across two complementary subsets: IndicIFEval-Ground, translated prompts from IFEval (Zhou et al., 2023) carefully localized for Indic contexts, and IndicIFEval-Ground, synthetically generated instructions grounded in native Indic content. We conduct a comprehensive evaluation of major open-weight and proprietary models spanning both reasoning and non-reasoning models. While models maintain strong adherence to formatting constraints, they struggle significantly with lexical and cross-lingual tasks -- and despite progress in high-resource languages, instruction-following across the broader Indic family lags significantly behind English. We release IndicIFEval and its evaluation scripts to support progress on multilingual constrained generation (http://github.com/ai4bharat/IndicIFEval).","authors":["Thanmay Jayakumar","Mohammed Safi Ur Rahman Khan","Raj Dabre","Ratish Puduppully","Anoop Kunchukuttan"],"pdf_url":"","comment":"8 pages + Appendix"},{"id":"http://arxiv.org/abs/2602.22124v1","updated":"2026-02-25T17:11:49Z","published":"2026-02-25T17:11:49Z","title":"SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents","summary":"Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).","authors":["Patrick Tser Jern Kon","Archana Pradeep","Ang Chen","Alexander P. Ellis","Warren Hunt","Zijian Wang","John Yang","Samuel Thompson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.18424v2","updated":"2026-02-25T17:00:00Z","published":"2025-02-25T18:20:00Z","title":"Compressing Language Models for Specialized Domains","summary":"Language models (LMs) excel at tasks across diverse domains, yet require substantial computational resources during inference. Compression techniques such as pruning and quantization offer a practical path towards efficient LM deployment, exemplified by their ability to preserve performance on general-purpose benchmarks. However, general-purpose LM compression methods can negatively affect performance in specialized domains (e.g. biomedical or legal). Recent work has sought to address this issue, but requires a computationally expensive full-parameter fine-tuning pipeline. To this end, we propose MixCal, a novel calibration method designed to improve the in-domain performance of compressed LMs in a post-training setting. Through extensive experimentation, we demonstrate that MixCal substantially outperforms existing approaches on domain-specific tasks and preserves general performance. Notably, these performance gains are achieved while also reducing the computational cost of LM compression.","authors":["Miles Williams","George Chrysostomou","Vitor Jeronymo","Nikolaos Aletras"],"pdf_url":"","comment":"EACL 2026"},{"id":"http://arxiv.org/abs/2602.16852v2","updated":"2026-02-25T16:49:29Z","published":"2026-02-18T20:29:02Z","title":"Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect","summary":"Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.","authors":["Minh Duc Bui","Manuel Mager","Peter Herbert Kann","Katharina von der Wense"],"pdf_url":"","comment":"Accepted at LREC 2026"},{"id":"http://arxiv.org/abs/2602.22090v1","updated":"2026-02-25T16:38:03Z","published":"2026-02-25T16:38:03Z","title":"Confidence-Driven Multi-Scale Model Selection for Cost-Efficient Inference","summary":"Large Language Models (LLMs) have revolutionized inference across diverse natural language tasks, with larger models performing better but at higher computational costs. We propose a confidence-driven strategy that dynamically selects the most suitable model based on confidence estimates. By assessing a model's confidence in handling the task and response accuracy, tasks that are likely to be solved correctly are retained, while more uncertain or complex cases are delegated to a larger model, ensuring reliability while minimizing computation. Specifically, we evaluate a model's likelihood of knowing the correct answer and the probability that its response is accurate. Experiments on the Massive Multitask Language Understanding (MMLU) benchmark show that our approach achieves accuracy comparable to the largest model while reducing computational costs by 20\\% to 40\\%. When applied to GPT-4o API calls, it reduces token usage by approximately 60\\%, further improving cost efficiency. These findings indicate the potential of confidence-based model selection to enhance real-world LLM deployment, particularly in resource-constrained settings such as edge devices and commercial API applications.","authors":["Bo-Wei Chen","Chung-Chi Chen","An-Zi Yen"],"pdf_url":"","comment":"Accepted by EACL 2026 Findings"},{"id":"http://arxiv.org/abs/2602.22072v1","updated":"2026-02-25T16:24:35Z","published":"2026-02-25T16:24:35Z","title":"Understanding Artificial Theory of Mind: Perturbed Tasks and Reasoning in Large Language Models","summary":"Theory of Mind (ToM) refers to an agent's ability to model the internal states of others. Contributing to the debate whether large language models (LLMs) exhibit genuine ToM capabilities, our study investigates their ToM robustness using perturbations on false-belief tasks and examines the potential of Chain-of-Thought prompting (CoT) to enhance performance and explain the LLM's decision. We introduce a handcrafted, richly annotated ToM dataset, including classic and perturbed false belief tasks, the corresponding spaces of valid reasoning chains for correct task completion, subsequent reasoning faithfulness, task solutions, and propose metrics to evaluate reasoning chain correctness and to what extent final answers are faithful to reasoning traces of the generated CoT. We show a steep drop in ToM capabilities under task perturbation for all evaluated LLMs, questioning the notion of any robust form of ToM being present. While CoT prompting improves the ToM performance overall in a faithful manner, it surprisingly degrades accuracy for some perturbation classes, indicating that selective application is necessary.","authors":["Christian Nickel","Laura Schrewe","Florian Mai","Lucie Flek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.15715v3","updated":"2026-02-25T16:22:14Z","published":"2026-01-22T07:36:48Z","title":"RebuttalAgent: Strategic Persuasion in Academic Rebuttal via Theory of Mind","summary":"Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) framework that models reviewer mental state, formulates persuasion strategy, and generates evidence-based response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations.","authors":["Zhitao He","Zongwei Lyu","Yi R Fung"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2509.11517v2","updated":"2026-02-25T16:04:59Z","published":"2025-09-15T02:07:26Z","title":"PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation","summary":"BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it.","authors":["Rodrigo M. Carrillo-Larco","Jesus Lovón Melgarejo","Manuel Castillo-Cara","Gusseppe Bravo-Rocca"],"pdf_url":"","comment":"https://github.com/rodrigo-carrillo/PeruMedQA"},{"id":"http://arxiv.org/abs/2411.06657v2","updated":"2026-02-25T15:53:57Z","published":"2024-11-11T01:44:54Z","title":"Renaissance: Investigating the Pretraining of Vision-Language Encoders","summary":"In the past several years there has been an explosion of available models for vision-language (VL) tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. Additionally, the limited programming tools available for modeling make conducting VL research more difficult than necessary. In this paper, we seek to answer several questions related to the pretraining of VL encoders through meta-analysis. To conduct these experiments, we introduce a VL evaluation framework called Renaissance. In our first set of experiments, we show that we can save significant compute at little to no cost to downstream performance, by freezing large parts of VL models during pretraining. In our second set of experiments, we examine the effect of basing a VL transformer on a vision model versus a text model. Renaissance offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. Its source code will be made publicly available upon publication. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","authors":["Clayton Fields","Casey Kennington"],"pdf_url":"","comment":"9 pages"},{"id":"http://arxiv.org/abs/2602.22045v1","updated":"2026-02-25T15:53:41Z","published":"2026-02-25T15:53:41Z","title":"DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain","summary":"We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.","authors":["Walter Hernandez Cruz","Peter Devine","Nikhil Vadgama","Paolo Tasca","Jiahua Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.21510v2","updated":"2026-02-25T15:48:41Z","published":"2025-05-19T09:14:54Z","title":"Complexity counts: global and local perspectives on Indo-Aryan numeral systems","summary":"The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and Bengali are highly unusual in that unlike most numeral systems (e.g., those of English, Chinese, etc.), forms referring to 1--99 are highly non-transparent and are cannot be constructed using straightforward rules for forming combinations of tens and digits. As an example, Hindi/Urdu {\\it ikyānve} `91' is not decomposable into the composite elements {\\it ek} `one' and {\\it nave} `ninety' in the way that its English counterpart is. This paper further clarifies the position of Indo-Aryan languages within the typology of numeral systems, and explores the linguistic and non-linguistic factors that may be responsible for the persistence of complex systems in these languages. Using data from multiple databases, we develop and employ a number of cross-linguistically applicable metrics to quantifies the complexity of languages' numeral systems, and demonstrate that Indo-Aryan languages have decisively more complex numeral systems than the world's languages as a whole, though individual Indo-Aryan languages differ from each other in terms of the complexity of the patterns they display. We investigate the factors (e.g., religion, geographic isolation, etc.) that underlie complexity in numeral systems, with a focus on South Asia, in an attempt to develop an account of why complex numeral systems developed and persisted in certain Indo-Aryan languages but not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems adhere to certain general pressures toward efficient communication found cross-linguistically, despite their high complexity. We call for this somewhat overlooked dimension of complexity to be taken seriously when discussing general variation in numeral systems.","authors":["Chundra Cathcart"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22039v1","updated":"2026-02-25T15:47:34Z","published":"2026-02-25T15:47:34Z","title":"TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition","summary":"Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhance recognition performance in low-resource environments. The framework is centered around the parallel gated cross-attention (PGCA) mechanism, which adaptively integrates embeddings from various auxiliary languages into the ASR decoder. This mechanism facilitates robust cross-linguistic semantic guidance while ensuring stable optimization and minimizing interference between languages. To support ongoing research initiatives, we present YT-THDC, a 30-hour corpus of Taiwanese Hokkien drama speech with aligned Mandarin subtitles and manually verified Taiwanese Hokkien transcriptions. Comprehensive experiments and analyses identify the auxiliary languages that most effectively enhance ASR performance, achieving a 14.77% relative reduction in character error rate and demonstrating the efficacy of translation-guided learning for underrepresented languages in practical applications.","authors":["Cheng-Yeh Yang","Chien-Chun Wang","Li-Wei Chen","Hung-Shin Lee","Hsin-Min Wang","Berlin Chen"],"pdf_url":"","comment":"Accepted to LREC 2026"},{"id":"http://arxiv.org/abs/2602.22014v1","updated":"2026-02-25T15:29:30Z","published":"2026-02-25T15:29:30Z","title":"A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT","summary":"Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven sampling algorithms, so as to pick the best one. We find that diversity-driven sampling allows in some tasks to gain 10 points relative to randomly-sampled pre-training data of commensurate size. We also see that a model pre-trained for 483h on a diversity-driven dataset of 150M tokens can yield a commensurate performance to a model pre-trained for 1,775h on a randomly-driven dataset of 2.4B tokens.","authors":["Louis Estève","Christophe Servan","Thomas Lavergne","Agata Savary"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2402.13604v3","updated":"2026-02-25T15:14:42Z","published":"2024-02-21T08:10:43Z","title":"Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE","summary":"This paper introduces OccCANINE, an open-source tool that maps occupational descriptions to HISCO codes. Manual coding is slow and error-prone; OccCANINE replaces weeks of work with results in minutes. We fine-tune CANINE on 15.8 million description-code pairs from 29 sources in 13 languages. The model achieves 96 percent accuracy, precision, and recall. We also show that the approach generalizes to three systems - OCC1950, OCCICEM, and ISCO-68 - and release them open source. By breaking the \"HISCO barrier,\" OccCANINE democratizes access to high-quality occupational coding, enabling broader research in economics, economic history, and related disciplines.","authors":["Christian Møller Dahl","Torben Johansen","Christian Vedel"],"pdf_url":"","comment":"All code and guides on how to use OccCANINE is available on GitHub https://github.com/christianvedels/OccCANINE"},{"id":"http://arxiv.org/abs/2602.02007v2","updated":"2026-02-25T15:14:28Z","published":"2026-02-02T12:04:58Z","title":"Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation","summary":"Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.","authors":["Zhanghao Hu","Qinglin Zhu","Hanqi Yan","Yulan He","Lin Gui"],"pdf_url":"","comment":"Project Address: https://zhanghao-xmemory.github.io/Academic-project-page-template/"},{"id":"http://arxiv.org/abs/2602.21978v1","updated":"2026-02-25T14:57:23Z","published":"2026-02-25T14:57:23Z","title":"CxMP: A Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models","summary":"Recent work has examined language models from a linguistic perspective to better understand how they acquire language. Most existing benchmarks focus on judging grammatical acceptability, whereas the ability to interpret meanings conveyed by grammatical forms has received much less attention. We introduce the Linguistic Minimal-Pair Benchmark for Evaluating Constructional Understanding in Language Models (CxMP), a benchmark grounded in Construction Grammar that treats form-meaning pairings, or constructions, as fundamental linguistic units. CxMP evaluates whether models can interpret the semantic relations implied by constructions, using a controlled minimal-pair design across nine construction types, including the let-alone, caused motion, and ditransitive constructions. Our results show that while syntactic competence emerges early, constructional understanding develops more gradually and remains limited even in large language models (LLMs). CxMP thus reveals persistent gaps in how language models integrate form and meaning, providing a framework for studying constructional understanding and learning trajectories in language models.","authors":["Miyu Oba","Saku Sugawara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16902v2","updated":"2026-02-25T14:49:31Z","published":"2025-12-18T18:56:50Z","title":"In-Context Algebra","summary":"We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions in-context. While prior work has studied transformers in settings where the answer relies on fixed parametric or geometric information encoded in token embeddings, we devise a new in-context reasoning task where the assignment of tokens to specific algebraic elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Our findings show that the kinds of reasoning strategies learned by transformers are dependent on the task structure and that models can develop symbolic reasoning mechanisms when trained to reason in-context about variables whose meanings are not fixed.","authors":["Eric Todd","Jannik Brinkmann","Rohit Gandikota","David Bau"],"pdf_url":"","comment":"ICLR 2026. 35 pages, 22 figures. Code and data at https://algebra.baulab.info"},{"id":"http://arxiv.org/abs/2602.21951v1","updated":"2026-02-25T14:34:02Z","published":"2026-02-25T14:34:02Z","title":"RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning","summary":"Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.","authors":["Bo Xue","Yuan Jin","Luoyi Fu","Jiaxin Ding","Xinbing Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21950v1","updated":"2026-02-25T14:33:33Z","published":"2026-02-25T14:33:33Z","title":"MEDSYN: Benchmarking Multi-EviDence SYNthesis in Complex Clinical Cases for Multimodal Large Language Models","summary":"Multimodal large language models (MLLMs) have shown great potential in medical applications, yet existing benchmarks inadequately capture real-world clinical complexity. We introduce MEDSYN, a multilingual, multimodal benchmark of highly complex clinical cases with up to 7 distinct visual clinical evidence (CE) types per case. Mirroring clinical workflow, we evaluate 18 MLLMs on differential diagnosis (DDx) generation and final diagnosis (FDx) selection. While top models often match or even outperform human experts on DDx generation, all MLLMs exhibit a much larger DDx--FDx performance gap compared to expert clinicians, indicating a failure mode in synthesis of heterogeneous CE types. Ablations attribute this failure to (i) overreliance on less discriminative textual CE ($\\it{e.g.}$, medical history) and (ii) a cross-modal CE utilization gap. We introduce Evidence Sensitivity to quantify the latter and show that a smaller gap correlates with higher diagnostic accuracy. Finally, we demonstrate how it can be used to guide interventions to improve model performance. We will open-source our benchmark and code.","authors":["Boqi Chen","Xudong Liu","Jiachuan Peng","Marianne Frey-Marti","Bang Zheng","Kyle Lam","Lin Li","Jianing Qiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21947v1","updated":"2026-02-25T14:32:15Z","published":"2026-02-25T14:32:15Z","title":"Large Language Models are Algorithmically Blind","summary":"Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.","authors":["Sohan Venkatesh","Ashish Mahendran Kurapath","Tejas Melkote"],"pdf_url":"","comment":"20 pages, 11 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.05085v6","updated":"2026-02-25T14:28:20Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks.","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.04997v6","updated":"2026-02-25T14:18:21Z","published":"2024-11-07T18:59:16Z","title":"LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation","summary":"CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Usman Naseem","Chunyu Wang","Chunyu Wang","Qi Dai","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu","Liang Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.18880v3","updated":"2026-02-25T14:13:32Z","published":"2025-09-23T10:21:22Z","title":"Diversity Boosts AI-Generated Text Detection","summary":"Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection.","authors":["Advik Raj Basani","Pin-Yu Chen"],"pdf_url":"","comment":"Accepted to Transactions on Machine Learning Research (TMLR '26). Project page and demos: https://diveye.vercel.app/"},{"id":"http://arxiv.org/abs/2602.21933v1","updated":"2026-02-25T14:12:16Z","published":"2026-02-25T14:12:16Z","title":"Small Wins Big: Comparing Large Language Models and Domain Fine-Tuned Models for Sarcasm Detection in Code-Mixed Hinglish Text","summary":"Sarcasm detection in multilingual and code-mixed environments remains a challenging task for natural language processing models due to structural variations, informal expressions, and low-resource linguistic availability. This study compares four large language models, Llama 3.1, Mistral, Gemma 3, and Phi-4, with a fine-tuned DistilBERT model for sarcasm detection in code-mixed Hinglish text. The results indicate that the smaller, sequentially fine-tuned DistilBERT model achieved the highest overall accuracy of 84%, outperforming all of the LLMs in zero and few-shot set ups, using minimal LLM generated code-mixed data used for fine-tuning. These findings indicate that domain-adaptive fine-tuning of smaller transformer based models may significantly improve sarcasm detection over general LLM inference, in low-resource and data scarce settings.","authors":["Bitan Majumder","Anirban Sen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.08026v3","updated":"2026-02-25T13:52:52Z","published":"2026-01-12T21:57:52Z","title":"FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures","summary":"Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.","authors":["Jifeng Song","Arun Das","Pan Wang","Hui Ji","Kun Zhao","Yufei Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21887v1","updated":"2026-02-25T13:10:58Z","published":"2026-02-25T13:10:58Z","title":"ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection","summary":"Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.","authors":["Changjiang Gao","Zixian Huang","Kaichen Yang","Jiajun Chen","Jixing Li","Shujian Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2305.15929v3","updated":"2026-02-25T13:10:40Z","published":"2023-05-25T10:57:43Z","title":"Emergence of a phonological bias in ChatGPT","summary":"Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT","authors":["Juan Manuel Toro"],"pdf_url":"","comment":"15 pages, 1 figure, corrected typo"},{"id":"http://arxiv.org/abs/2602.21864v1","updated":"2026-02-25T12:45:45Z","published":"2026-02-25T12:45:45Z","title":"DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs","summary":"Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.","authors":["Yanbin Wei","Jiangyue Yan","Chun Kang","Yang Chen","Hua Liu","James Kwok","Yu Zhang"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21862v1","updated":"2026-02-25T12:43:25Z","published":"2026-02-25T12:43:25Z","title":"Personalized Graph-Empowered Large Language Model for Proactive Information Access","summary":"Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.","authors":["Chia Cheng Chang","An-Zi Yen","Hen-Hsen Huang","Hsin-Hsi Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21857v1","updated":"2026-02-25T12:32:04Z","published":"2026-02-25T12:32:04Z","title":"Distill and Align Decomposition for Enhanced Claim Verification","summary":"Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.","authors":["Jabez Magomere","Elena Kochkina","Samuel Mensah","Simerjot Kaur","Fernando Acero","Arturo Oncevay","Charese H. Smiley","Xiaomo Liu","Manuela Veloso"],"pdf_url":"","comment":"EACL Findings 2026"},{"id":"http://arxiv.org/abs/2602.21854v1","updated":"2026-02-25T12:30:18Z","published":"2026-02-25T12:30:18Z","title":"FewMMBench: A Benchmark for Multimodal Few-Shot Learning","summary":"As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench","authors":["Mustafa Dogan","Ilker Kesen","Iacer Calixto","Aykut Erdem","Erkut Erdem"],"pdf_url":"","comment":"Preprint. 49 pages, 38 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2506.05154v4","updated":"2026-02-25T12:22:09Z","published":"2025-06-05T15:34:15Z","title":"Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement","summary":"Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework that explicitly trains large language models to use parametric knowledge (PK) to resist contextual interference while still exploiting external context when it is reliably helpful. Knowledgeable-R1 introduces a joint sampling scheme that generates paired responses with and without retrieval, and learns both local advantages (within each decoding regime) and global advantages under the same input to quantify when to ignore misleading context versus adopt it. We employ an asymmetric advantage transformation that amplifies exploratory behaviors toward parametric knowledge. Experiments show that Knowledgeable-R1 significantly improves robustness and reasoning accuracy in knowledge conflict scenarios and general RAG scenarios, outperforming SOTA baselines by +22.89% in counterfactual scenarios, and without degradation when the retrieved context is fully accurate.Our code are available at https://github.com/lcy80366872/knowledgeable-R1.","authors":["Chenyu Lin","Yilin Wen","Du Su","Hexiang Tan","Fei Sun","Muhan Chen","Chenfu Bao","Zhonghou Lyu"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2601.13879v3","updated":"2026-02-25T12:15:14Z","published":"2026-01-20T11:45:38Z","title":"Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring","summary":"While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.","authors":["Dongxu Zhang","Yiding Sun","Cheng Tan","Wenbiao Yan","Ning Yang","Jihua Zhu","Haijun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.05282v4","updated":"2026-02-25T11:55:15Z","published":"2025-08-07T11:26:40Z","title":"Not All Errors Are Created Equal: ASCoT Addresses Late-Stage Fragility in Efficient LLM Reasoning","summary":"While Chain-of-Thought (CoT) prompting empowers Large Language Models (LLMs), ensuring reasoning reliability remains an open challenge. Contrary to the prevailing cascading failure hypothesis which posits that early errors are most detrimental, we identify a counter-intuitive phenomenon termed \\textbf{Late-Stage Fragility}: errors introduced in later reasoning stages are significantly more prone to corrupting final answers. To address this, we introduce ASCoT (Adaptive Self-Correction Chain-of-Thought), a method harmonizing efficiency with robust verification. ASCoT first employs semantic pruning to compress redundant steps, then utilizes an Adaptive Verification Manager (AVM) to prioritize high risk, late-stage steps via a positional impact score, triggering a Multi-Perspective Self-Correction Engine (MSCE) only when necessary. Experiments on GSM8K and MATH-500 demonstrate that ASCoT effectively reallocates computational resources: it reduces token usage by 21\\%--30\\% for LLaMA-3.1-8B with negligible accuracy drops ($<1.8\\%$), achieving a superior trade-off between inference efficiency and reasoning fidelity.","authors":["Dongxu Zhang","Ning Yang","Yiding Sun","Jihua Zhu","Jinnan Yang","Miao Xin","Baoliang Tian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21814v1","updated":"2026-02-25T11:40:15Z","published":"2026-02-25T11:40:15Z","title":"Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem","summary":"Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.","authors":["Heejin Jo"],"pdf_url":"","comment":"9 pages, 4 tables"},{"id":"http://arxiv.org/abs/2510.20498v3","updated":"2026-02-25T11:31:02Z","published":"2025-10-23T12:39:20Z","title":"Robust Preference Alignment via Directional Neighborhood Consensus","summary":"Aligning large language models with human preferences is critical for creating reliable and controllable AI systems. A human preference can be visualized as a high-dimensional vector where different directions represent trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet, because the training data often reflects dominant, average preferences, LLMs tend to perform well on common requests but fall short in specific, individual needs. This mismatch creates a preference coverage gap. Existing methods often address this through costly retraining, which may not be generalized to the full spectrum of diverse preferences. This brittleness means that when a user's request reflects a nuanced preference deviating from the training data's central tendency, model performance can degrade unpredictably. To address this challenge, we introduce Robust Preference Selection (RPS), a post-hoc, training-free method by leveraging directional neighborhood consensus. Instead of forcing a model to generate a response from a single, highly specific preference, RPS samples multiple responses from a local neighborhood of related preferences to create a superior candidate pool. It then selects the response that best aligns with the user's original intent. We provide a theoretical framework showing our neighborhood generation strategy is provably superior to a strong baseline that also samples multiple candidates. Comprehensive experiments across three distinct alignment paradigms (DPA, DPO, and SFT) demonstrate that RPS consistently improves robustness against this baseline, achieving win rates of up to 69% on challenging preferences from under-represented regions of the space without any model retraining. Our work presents a practical, theoretically-grounded solution for enhancing the reliability of preference-aligned models.","authors":["Ruochen Mao","Yuling Shi","Xiaodong Gu","Jiaheng Wei"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.10953v2","updated":"2026-02-25T11:16:34Z","published":"2026-02-11T15:41:09Z","title":"Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models","summary":"Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding. Our Code is available at https://github.com/duterscmy/SOAR","authors":["Mingyu Cao","Alvaro H. C. Correia","Christos Louizos","Shiwei Liu","Lu Yin"],"pdf_url":"","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.21786v1","updated":"2026-02-25T11:08:38Z","published":"2026-02-25T11:08:38Z","title":"D-COT: Disciplined Chain-of-Thought Learning for Efficient Reasoning in Small Language Models","summary":"Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) often induces \"overthinking\" in Small Language Models (SLMs), leading to performance degradation and excessive token consumption. In this study, we propose Disciplined Chain-of-Thought (D-CoT), a novel framework that enforces a structured reasoning process using control tags -- such as <TEMP_LOW> for fact-checking and <TEMP_HIGH> for multi-perspective exploration -- as auxiliary scaffolding during training. By optimizing the CoT trajectory, D-CoT suppresses reasoning drift and simultaneously achieves token reduction and performance improvement. We demonstrate the efficacy of our approach on Qwen3-8B: with only 5,000 training samples, D-CoT significantly boosts accuracy on GPQA-diamond by 9.9% and MMLU-Pro (0-shot) by 9.1%, while drastically reducing computational costs. Furthermore, we confirm that the model internalizes this disciplined thought structure, maintaining high performance even without explicit control tags during inference.","authors":["Shunsuke Ubukata"],"pdf_url":"","comment":"9 pages, 3 figures. Code: https://github.com/gitpullpull/DisciplinedChainOfThought | Benchmarks: https://huggingface.co/datasets/gitpullpull/D-CoT-Benchmarks | Dataset: https://huggingface.co/datasets/gitpullpull/D-CoT-datasets"},{"id":"http://arxiv.org/abs/2505.17306v2","updated":"2026-02-25T11:02:02Z","published":"2025-05-22T21:54:46Z","title":"Refusal Direction is Universal Across Safety-Aligned Languages","summary":"Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs.","authors":["Xinpeng Wang","Mingyang Wang","Yihong Liu","Hinrich Schütze","Barbara Plank"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21763v1","updated":"2026-02-25T10:28:45Z","published":"2026-02-25T10:28:45Z","title":"Improving Implicit Discourse Relation Recognition with Natural Language Explanations from LLMs","summary":"Implicit Discourse Relation Recognition (IDRR) remains a challenging task due to the requirement for deep semantic understanding in the absence of explicit discourse markers. A further limitation is that existing methods only predict relations without providing any supporting explanations. Recent advances in large language models (LLMs) have shown strong reasoning capabilities in both deep language understanding and natural language explanation generation. In this work, we propose a simple yet effective approach to distill the reasoning capabilities of LLMs into lightweight IDRR models to improve both performance and interpretability. Specifically, we first prompt an LLM to generate explanations for each training instance conditioned on its gold label. Then, we introduce a novel classification-generation framework that jointly performs relation prediction and explanation generation, and train it with the additional supervision of LLM-generated explanations. Our framework is plug-and-play, enabling easy integration with most existing IDRR models. Experimental results on PDTB demonstrate that our approach significantly improves IDRR performance, while human evaluation further confirms that the generated explanations enhance model interpretability. Furthermore, we validate the generality of our approach on sentiment classification and natural language inference","authors":["Heng Wang","Changxing Wu"],"pdf_url":"","comment":"AAAI26'0ral"},{"id":"http://arxiv.org/abs/2602.21741v1","updated":"2026-02-25T09:52:32Z","published":"2026-02-25T09:52:32Z","title":"Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization","summary":"We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.","authors":["MD. Sagor Chowdhury","Adiba Fairooz Chowdhury"],"pdf_url":"","comment":"6 pages, 5 figures, 3 tables; system paper submitted to DL Sprint 4.0 (Kaggle)"},{"id":"http://arxiv.org/abs/2602.20945v2","updated":"2026-02-25T09:40:11Z","published":"2026-02-24T14:28:16Z","title":"The Art of Efficient Reasoning: Data, Reward, and Optimization","summary":"Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.","authors":["Taiqiang Wu","Zenan Xu","Bo Zhou","Ngai Wong"],"pdf_url":"","comment":"Tech Report, Insights on Efficient Reasoning via Reward Shaping"},{"id":"http://arxiv.org/abs/2504.20094v3","updated":"2026-02-25T09:37:59Z","published":"2025-04-26T00:55:43Z","title":"Toward Safe and Human-Aligned Game Conversational Recommendation via Multi-Agent Decomposition","summary":"Conversational recommender systems (CRS) have advanced with large language models, showing strong results in domains like movies. These domains typically involve fixed content and passive consumption, where user preferences can be matched by genre or theme. In contrast, games present distinct challenges: fast-evolving catalogs, interaction-driven preferences (e.g., skill level, mechanics, hardware), and increased risk of unsafe responses in open-ended conversation. We propose MATCHA, a multi-agent framework for CRS that assigns specialized agents for intent parsing, tool-augmented retrieval, multi-LLM ranking with reflection, explanation, and risk control which enabling finer personalization, long-tail coverage, and stronger safety. Evaluated on real user request dataset, MATCHA outperforms six baselines across eight metrics, improving Hit@5 by 20%, reducing popularity bias by 24%, and achieving 97.9% adversarial defense. Human and virtual-judge evaluations confirm improved explanation quality and user alignment.","authors":["Zheng Hui","Xiaokai Wei","Yexi Jiang","Kevin Gao","Chen Wang","Frank Ong","Se-eun Yoon","Rachit Pareek","Michelle Gong"],"pdf_url":"","comment":"ICML 2025 MAS, EACL 2026"},{"id":"http://arxiv.org/abs/2602.21728v1","updated":"2026-02-25T09:35:18Z","published":"2026-02-25T09:35:18Z","title":"Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling","summary":"The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.","authors":["Shiqi Yan","Yubo Chen","Ruiqi Zhou","Zhengxi Yao","Shuai Chen","Tianyi Zhang","Shijie Zhang","Wei Qiang Zhang","Yongfeng Huang","Haixin Duan","Yunqi Zhang"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21720v1","updated":"2026-02-25T09:27:02Z","published":"2026-02-25T09:27:02Z","title":"Evaluating the relationship between regularity and learnability in recursive numeral systems using Reinforcement Learning","summary":"Human recursive numeral systems (i.e., counting systems such as English base-10 numerals), like many other grammatical systems, are highly regular. Following prior work that relates cross-linguistic tendencies to biases in learning, we ask whether regular systems are common because regularity facilitates learning. Adopting methods from the Reinforcement Learning literature, we confirm that highly regular human(-like) systems are easier to learn than unattested but possible irregular systems. This asymmetry emerges under the natural assumption that recursive numeral systems are designed for generalisation from limited data to represent all integers exactly. We also find that the influence of regularity on learnability is absent for unnatural, highly irregular systems, whose learnability is influenced instead by signal length, suggesting that different pressures may influence learnability differently in different parts of the space of possible numeral systems. Our results contribute to the body of work linking learnability to cross-linguistic prevalence.","authors":["Andrea Silvi","Ponrawee Prasertsom","Jennifer Culbertson","Devdatt Dubhashi","Moa Johansson","Kenny Smith"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2409.20120v4","updated":"2026-02-25T09:24:03Z","published":"2024-09-30T09:20:18Z","title":"PACE: Procedural Abstractions for Communicating Efficiently","summary":"A central but unresolved aspect of problem-solving in AI is the capability to introduce and use abstractions, something humans excel at. Work in cognitive science has demonstrated that humans tend towards higher levels of abstraction when engaged in collaborative task-oriented communication, enabling gradually shorter and more information-efficient utterances. Several computational methods have attempted to replicate this phenomenon, but all make unrealistic simplifying assumptions about how abstractions are introduced and learned. Our method, Procedural Abstractions for Communicating Efficiently (PACE), overcomes these limitations through a neuro-symbolic approach. On the symbolic side, we draw on work from library learning for proposing abstractions. We combine this with neural methods for communication and reinforcement learning, via a novel use of bandit algorithms for controlling the exploration and exploitation trade-off in introducing new abstractions. PACE exhibits similar tendencies to humans on a collaborative construction task from the cognitive science literature, where one agent (the architect) instructs the other (the builder) to reconstruct a scene of block-buildings. PACE results in the emergence of an efficient language as a by-product of collaborative communication. Beyond providing mechanistic insights into human communication, our work serves as a first step to providing conversational agents with the ability for human-like communicative abstractions.","authors":["Jonathan D. Thomas","Andrea Silvi","Devdatt Dubhashi","Moa Johansson"],"pdf_url":"","comment":"Accepted to CogSci 2025 for presentation"},{"id":"http://arxiv.org/abs/2510.17509v2","updated":"2026-02-25T09:08:12Z","published":"2025-10-20T13:05:22Z","title":"Annotation-Efficient Universal Honesty Alignment","summary":"Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs.","authors":["Shiyu Ni","Keping Bi","Jiafeng Guo","Minghao Tang","Jingtong Wu","Zengxin Han","Xueqi Cheng"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.08237v2","updated":"2026-02-25T09:07:48Z","published":"2026-02-09T03:23:23Z","title":"Document Reconstruction Unlocks Scalable Long-Context RLVR","summary":"Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.","authors":["Yao Xiao","Lei Wang","Yue Deng","Guanzheng Chen","Ziqi Jin","Jung-jae Kim","Xiaoli Li","Roy Ka-wei Lee","Lidong Bing"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.06899v3","updated":"2026-02-25T08:56:57Z","published":"2025-11-10T09:48:07Z","title":"RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation","summary":"Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.","authors":["Haofeng Wang","Yu Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.10472v2","updated":"2026-02-25T08:42:24Z","published":"2025-10-12T06:41:05Z","title":"FML-bench: Benchmarking Machine Learning Agents for Scientific Research","summary":"Large language models (LLMs) have sparked growing interest in machine learning research agents that can autonomously propose ideas and conduct experiments. However, existing benchmarks predominantly adopt an engineering-oriented perspective: they emphasize application-oriented tasks and evaluate primarily on final performance and computational cost, overlooking agents' research processes and limiting assessment of their capabilities in scientific research settings. To more comprehensively evaluate agents in scientific research settings, we introduce FML-bench, a benchmark comprising 8 diverse and fundamental ML research tasks, and further propose complementary metrics, notably Exploration Diversity, which quantifies the variance of proposals across iterations and reveals how exploration patterns influence research outcomes. We evaluate state-of-the-art research agents on FML-bench, showing that agents employing broad exploration strategies exhibit higher exploration diversity and achieve superior performance, and that exploration diversity positively correlates with performance improvements across multiple tasks. We hope these findings and our benchmark inform future agent design and support the community in further investigating agent behavior. Our benchmark is available at https://github.com/qrzou/FML-bench.","authors":["Qiran Zou","Hou Hei Lam","Wenhao Zhao","Yiming Tang","Tingting Chen","Samson Yu","Tianyi Zhang","Chang Liu","Xiangyang Ji","Dianbo Liu"],"pdf_url":"","comment":"Our benchmark is available at: https://github.com/qrzou/FML-bench"},{"id":"http://arxiv.org/abs/2502.11684v3","updated":"2026-02-25T08:18:58Z","published":"2025-02-17T11:22:24Z","title":"MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task","summary":"Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies have demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the ''Fill-in-the-middle'' task from code reasoning. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.","authors":["Yuchen Yan","Yongliang Shen","Yang Liu","Jin Jiang","Xin Xu","Mengdi Zhang","Jian Shao","Yueting Zhuang"],"pdf_url":"","comment":"ICLR 2026: https://openreview.net/forum?id=14i2wzPPfn"},{"id":"http://arxiv.org/abs/2407.15160v3","updated":"2026-02-25T08:14:26Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"Large language models based on the transformer architecture can solve highly complex tasks, yet their fundamental limitations on simple algorithmic problems remain poorly understood. In this work, we focus on basic counting tasks and investigate how the difficulty of these tasks scales with the transformer embedding dimension, the context length, and the vocabulary size. We reveal a sharp theoretical phase transition governed by the relationship between the embedding dimension and the vocabulary size. When the dimension is at least as large as the vocabulary, transformers can perfectly maintain token counts. However, when the vocabulary exceeds the embedding dimension, the interference between non-orthogonal token representations forces the network weights to scale polynomially. This renders the exact counting algorithm numerically unstable and practically unlearnable. We empirically validate this bottleneck by training transformers from scratch, demonstrating a strict performance drop at the theoretical threshold and catastrophic out of distribution failure when scaling the vocabulary or context length. Furthermore, we show that state-of-the-art pretrained models suffer from similar failure cases. Our work reveals a critical blind spot absent from the current literature regarding the connection among these three parameters, proving that vocabulary size fundamentally dictates the difficulty of counting tasks.","authors":["Gilad Yehudai","Haim Kaplan","Guy Dar","Royi Rassin","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.18502v2","updated":"2026-02-25T08:12:27Z","published":"2025-05-24T04:43:24Z","title":"Knowledge Fusion of Large Language Models Via Modular SkillPacks","summary":"Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.","authors":["Guodong Du","Zhuo Li","Xuanning Zhou","Junlin Li","Zesheng Shi","Wanyu Lin","Ho-Kin Tang","Xiucheng Li","Fangming Liu","Wenya Wang","Min Zhang","Jing Li"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.12635v2","updated":"2026-02-25T08:07:27Z","published":"2026-02-13T05:41:31Z","title":"Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats","summary":"As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.","authors":["Pengxiang Zhao","Hui-Ling Zhen","Xing Li","Han Bao","Weizhe Lin","Zhiyuan Yang","Ziwei Yu","Xin Wang","Mingxuan Yuan","Xianzhi Yu","Zhenhua Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21669v1","updated":"2026-02-25T08:04:44Z","published":"2026-02-25T08:04:44Z","title":"DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation","summary":"Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via Kullback-Leibler divergence (KL). The process is modulated by dual-space weights that up-weight tokens where the student is uncertain and the teacher is confident, thereby focusing learning on informative tokens rather than treating all positions equally. At the sequence level, DWA-KD applies Soft Dynamic Time Warping (Soft-DTW) to both the embedding and final hidden-state layers, enabling robust alignment of lexical and contextual semantics between teacher and student sequences. Extensive experiments across diverse NLP benchmarks demonstrate that DWA-KD outperforms state-of-the-art KD baselines, while ablation studies confirm the complementary contributions of entropy-based token weighting and embedding and final hidden state layer Soft-DTW alignment.","authors":["Duc Trung Vu","Pham Khanh Chi","Dat Phi Van","Linh Ngo Van","Sang Dinh","Trung Le"],"pdf_url":"","comment":"EACL Findings"},{"id":"http://arxiv.org/abs/2602.21158v2","updated":"2026-02-25T07:50:58Z","published":"2026-02-24T18:04:54Z","title":"SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards","summary":"Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.","authors":["Dengjia Zhang","Xiaoou Liu","Lu Cheng","Yaqing Wang","Kenton Murray","Hua Wei"],"pdf_url":"","comment":"Accepted by PAKDD'26"},{"id":"http://arxiv.org/abs/2503.06692v5","updated":"2026-02-25T07:41:02Z","published":"2025-03-09T16:59:14Z","title":"InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models","summary":"Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence length, reasoning constrained by maximum context boundaries, and performance degradation beyond pre-training context windows. Existing approaches primarily compress reasoning chains without addressing the fundamental scaling problem. To overcome these challenges, we introduce InftyThink, a paradigm that transforms monolithic reasoning into an iterative process with intermediate summarization. By interleaving short reasoning segments with concise progress summaries, our approach enables unbounded reasoning depth while maintaining bounded computational costs. This creates a characteristic sawtooth memory pattern that significantly reduces computational complexity compared to traditional approaches. Furthermore, we develop a methodology for reconstructing long-context reasoning datasets into our iterative format, transforming OpenR1-Math into 333K training instances. Experiments across multiple model architectures demonstrate that our approach reduces computational costs while improving performance, with Qwen2.5-Math-7B showing 3-11% improvements across MATH500, AIME24, and GPQA_diamond benchmarks. Our work challenges the assumed trade-off between reasoning depth and computational efficiency, providing a more scalable approach to complex reasoning without architectural modifications.","authors":["Yuchen Yan","Yongliang Shen","Yang Liu","Jin Jiang","Mengdi Zhang","Jian Shao","Yueting Zhuang"],"pdf_url":"","comment":"ICLR 2026: https://openreview.net/forum?id=T1h5em349L Project Page: https://zju-real.github.io/InftyThink Code: https://github.com/ZJU-REAL/InftyThink"},{"id":"http://arxiv.org/abs/2602.21652v1","updated":"2026-02-25T07:25:01Z","published":"2026-02-25T07:25:01Z","title":"Sparsity Induction for Accurate Post-Training Pruning of Large Language Models","summary":"Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We propose Sparsity Induction, which promotes models toward higher sparsity at both distribution and feature levels before pruning, to push the limits of PTS. At the distribution level, we enhance distributional sparsity through mathematically equivalent scaling transformations, which are fully absorbable and incur no extra parameters or inference-time overhead. At the feature level, we introduce Spectral Norm Loss to promote feature sparsity from a low-rank perspective. Experiments across diverse model architectures and tasks demonstrate that our method further enhances sparsity-friendliness, achieving superior pruning performance over existing approaches.","authors":["Minhao Jiang","Zhikai Li","Xuewen Liu","Jing Zhang","Mengjuan Chen","Qingyi Gu"],"pdf_url":"","comment":"5 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2602.21647v1","updated":"2026-02-25T07:20:23Z","published":"2026-02-25T07:20:23Z","title":"Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration","summary":"This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved a state-of-the-art 2.72% CER on OpenSLR-54, and a multi-stage fine-tuned MarianMT model reached a 28.32 BLEU score on the FLORES-200 benchmark. We empirically investigate the influence of punctuation loss, demonstrating that unpunctuated ASR output significantly degrades translation quality, causing a massive 20.7% relative BLEU drop on the FLORES benchmark. To overcome this, we propose and evaluate an intermediate Punctuation Restoration Module (PRM). The final S2TT pipeline was tested across three configurations on a custom dataset. The optimal configuration, which applied the PRM directly to ASR output, achieved a 4.90 BLEU point gain over the direct ASR-to-NMT baseline (BLEU 36.38 vs. 31.48). This improvement was validated by human assessment, which confirmed the optimized pipeline's superior Adequacy (3.673) and Fluency (3.804). This work validates that targeted punctuation restoration is the most effective intervention for mitigating structural noise in the Nepali S2TT pipeline. It establishes an optimized baseline and demonstrates a critical architectural insight for developing cascaded speech translation systems for similar low-resource languages.","authors":["Tangsang Chongbang","Pranesh Pyara Shrestha","Amrit Sarki","Anku Jaiswal"],"pdf_url":"","comment":"13 pages, 4 figures, 12 tables"},{"id":"http://arxiv.org/abs/2602.21646v1","updated":"2026-02-25T07:19:34Z","published":"2026-02-25T07:19:34Z","title":"Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion","summary":"Multimodal Large Language Models (MLLMs) have achieved notable success in enhancing translation performance by integrating multimodal information. However, existing research primarily focuses on image-guided methods, whose applicability is constrained by the scarcity of multilingual image-text pairs. The speech modality overcomes this limitation due to its natural alignment with text and the abundance of existing speech datasets, which enable scalable language coverage. In this paper, we propose a Speech-guided Machine Translation (SMT) framework that integrates speech and text as fused inputs into an MLLM to improve translation quality. To mitigate reliance on low-resource data, we introduce a Self-Evolution Mechanism. The core components of this framework include a text-to-speech model, responsible for generating synthetic speech, and an MLLM capable of classifying synthetic speech samples and iteratively optimizing itself using positive samples. Experimental results demonstrate that our framework surpasses all existing methods on the Multi30K multimodal machine translation benchmark, achieving new state-of-the-art results. Furthermore, on general machine translation datasets, particularly the FLORES-200, it achieves average state-of-the-art performance in 108 translation directions. Ablation studies on CoVoST-2 confirms that differences between synthetic and authentic speech have negligible impact on translation quality. The code and models are released at https://github.com/yxduir/LLM-SRT.","authors":["Yexing Du","Youcheng Pan","Zekun Wang","Zheng Chu","Yichong Huang","Kaiyuan Liu","Bo Yang","Yang Xiang","Ming Liu","Bing Qin"],"pdf_url":"","comment":"Accepted in ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21638v1","updated":"2026-02-25T07:05:05Z","published":"2026-02-25T07:05:05Z","title":"Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs","summary":"Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world counseling excerpts, pairing counselor-client interactions with professional ratings and explanatory rationales. Using this data, we perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct backbone to model fine-grained evaluative judgments of response quality and generate explanations underlying. Experimental results show that our approach can effectively distinguish the quality of different communication mechanisms (77-81% F1), substantially outperforming GPT-4o and Claude-3.5-Sonnet (45-59% F1). Moreover, the model produces high-quality explanations that closely align with expert references and receive near-ceiling ratings from human experts (2.8-2.9/3.0). A controlled experiment with 43 counselors further confirms that receiving these AI-generated feedback significantly improves counselors' ability to respond effectively to client resistance.","authors":["Anqi Li","Ruihan Wang","Zhaoming Chen","Yuqian Chen","Yu Lu","Yi Zhu","Yuan Xie","Zhenzhong Lan"],"pdf_url":"","comment":"8 pages"},{"id":"http://arxiv.org/abs/2510.05077v2","updated":"2026-02-25T06:48:42Z","published":"2025-10-06T17:49:58Z","title":"Slm-mux: Orchestrating small language models for reasoning","summary":"With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMs, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. Additional experiments show that the core principle of SLM-MUX extends to open-ended generation tasks (e.g., HumanEval) and benefits other model classes, including frontier LLMs and domain-specific fine-tuned SLMs. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach. The project page is available at https://slm-mux.github.io/.","authors":["Chenyu Wang","Zishen Wan","Hao Kang","Emma Chen","Zhiqiang Xie","Tushar Krishna","Vijay Janapa Reddi","Yilun Du"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21628v1","updated":"2026-02-25T06:46:24Z","published":"2026-02-25T06:46:24Z","title":"RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by treating all rubrics as equally learnable. In this paper, we propose Stratified Rubric-based Curriculum Learning (RuCL), a novel framework that reformulates curriculum learning by shifting the focus from data selection to reward design. RuCL generates generalized rubrics for broad applicability and stratifies them based on the model's competence. By dynamically adjusting rubric weights during training, RuCL guides the model from mastering foundational perception to tackling advanced logical reasoning. Extensive experiments on various visual reasoning benchmarks show that RuCL yields a remarkable +7.83% average improvement over the Qwen2.5-VL-7B model, achieving a state-of-the-art accuracy of 60.06%.","authors":["Yukun Chen","Jiaming Li","Longze Chen","Ze Gong","Jingpeng Li","Zhen Qin","Hengyu Chang","Ancheng Xu","Zhihao Yang","Hamid Alinejad-Rokny","Qiang Qu","Bo Zheng","Min Yang"],"pdf_url":"","comment":"8 pages"},{"id":"http://arxiv.org/abs/2602.20610v2","updated":"2026-02-25T06:38:21Z","published":"2026-02-24T07:01:17Z","title":"SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference","summary":"Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.","authors":["Cuong Chi Le","Minh V. T Pham","Tung Vu Duy","Cuong Duc Van","Huy N. Phan","Hoang N. Phan","Tien N. Nguyen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21619v1","updated":"2026-02-25T06:22:48Z","published":"2026-02-25T06:22:48Z","title":"When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning","summary":"Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consistent pattern: more information does not necessarily yield better reasoning. Targeted single spatial cues outperform multi-context aggregation, excessive or weakly relevant commonsense knowledge degrades performance, and CoT prompting improves accuracy only when spatial grounding is sufficiently precise. These findings highlight the importance of selective, task-aligned information injection and provide practical guidance for designing reliable multimodal reasoning pipelines.","authors":["Muku Akasaka","Soyeon Caren Han"],"pdf_url":"","comment":"5 pages, 6 figures, Under review"},{"id":"http://arxiv.org/abs/2602.21608v1","updated":"2026-02-25T06:12:06Z","published":"2026-02-25T06:12:06Z","title":"MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification","summary":"Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot large language models under structured prompting. Results show strong performance on humor detection but substantial degradation on sarcasm, offense, and vulgarity due to class imbalance and pragmatic complexity. Zero-shot models achieve competitive micro-F1 scores but low exact match accuracy. Further analysis reveals that over 42\\% of negative sentiment instances in an external dataset exhibit sarcastic characteristics. MixSarc provides a foundational resource for culturally aware NLP and supports more reliable multi-label modeling in code-mixed environments.","authors":["Kazi Samin Yasar Alam","Md Tanbir Chowdhury","Tamim Ahmed","Ajwad Abrar","Md Rafid Haque"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2602.13551v2","updated":"2026-02-25T05:49:21Z","published":"2026-02-14T01:55:39Z","title":"Small Reward Models via Backward Inference","summary":"Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.","authors":["Yike Wang","Faeze Brahman","Shangbin Feng","Teng Xiao","Hannaneh Hajishirzi","Yulia Tsvetkov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25184v2","updated":"2026-02-25T05:03:23Z","published":"2025-09-29T17:59:42Z","title":"Incentive-Aligned Multi-Source LLM Summaries","summary":"Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source's stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source's incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation.","authors":["Yanchen Jiang","Zhe Feng","Aranyak Mehta"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2505.13529v2","updated":"2026-02-25T04:42:28Z","published":"2025-05-18T07:27:34Z","title":"BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs","summary":"Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with \"I don't know\". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.","authors":["Junxiao Yang","Jinzhe Tu","Haoran Liu","Xiaoce Wang","Chujie Zheng","Zhexin Zhang","Shiyao Cui","Caishun Chen","Tiantian He","Hongning Wang","Yew-Soon Ong","Minlie Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21543v1","updated":"2026-02-25T03:58:24Z","published":"2026-02-25T03:58:24Z","title":"Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment","summary":"Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages can substantially improve multilingual and cross-lingual representations for NLU tasks. We construct a multi-way parallel dataset using translations of English text from an off-the-shelf NMT model for a pool of six target languages and achieve strong cross-lingual alignment through contrastive learning. This leads to substantial performance gains across both seen and unseen languages for multiple tasks from the MTEB benchmark evaluated for XLM-Roberta and multilingual BERT base models. Using a multi-way parallel corpus for contrastive training yields substantial gains on bitext mining (21.3%), semantic similarity (5.3%), and classification (28.4%) compared to English-centric (En-X) bilingually parallel data, where X is sampled from a pool of multiple target languages. Furthermore, finetuning mE5 model on a small dataset with multi-way parallelism significantly improves bitext mining compared to one without, underscoring the importance of multi-way cross-lingual supervision even for models already pretrained for high-quality sentence embeddings.","authors":["Barah Fazili","Koustava Goswami"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18788v2","updated":"2026-02-25T03:48:19Z","published":"2026-02-21T10:43:07Z","title":"BURMESE-SAN: Burmese NLP Benchmark for Evaluating Large Language Models","summary":"We introduce BURMESE-SAN, the first holistic benchmark that systematically evaluates large language models (LLMs) for Burmese across three core NLP competencies: understanding (NLU), reasoning (NLR), and generation (NLG). BURMESE-SAN consolidates seven subtasks spanning these competencies, including Question Answering, Sentiment Analysis, Toxicity Detection, Causal Reasoning, Natural Language Inference, Abstractive Summarization, and Machine Translation, several of which were previously unavailable for Burmese. The benchmark is constructed through a rigorous native-speaker-driven process to ensure linguistic naturalness, fluency, and cultural authenticity while minimizing translation-induced artifacts. We conduct a large-scale evaluation of both open-weight and commercial LLMs to examine challenges in Burmese modeling arising from limited pretraining coverage, rich morphology, and syntactic variation. Our results show that Burmese performance depends more on architectural design, language representation, and instruction tuning than on model scale alone. In particular, Southeast Asia regional fine-tuning and newer model generations yield substantial gains. Finally, we release BURMESE-SAN as a public leaderboard to support systematic evaluation and sustained progress in Burmese and other low-resource languages. https://leaderboard.sea-lion.ai/detailed/MY","authors":["Thura Aung","Jann Railey Montalan","Jian Gang Ngui","Peerat Limkonchotiwat"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.06060v2","updated":"2026-02-25T03:33:50Z","published":"2025-06-06T13:13:29Z","title":"Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models","summary":"Federated large language models (FedLLMs) enable cross-silo collaborative training among institutions while preserving data locality, making them appealing for privacy-sensitive domains such as law, finance, and healthcare. However, the memorization behavior of LLMs can lead to privacy risks that may cause cross-client data leakage. In this work, we study the threat of cross-client data extraction, where a semi-honest participant attempts to recover personally identifiable information (PII) memorized from other clients' data. We propose three simple yet effective extraction strategies that leverage contextual prefixes from the attacker's local data, including frequency-based prefix sampling and local fine-tuning to amplify memorization. To evaluate these attacks, we construct a Chinese legal-domain dataset with fine-grained PII annotations consistent with CPIS, GDPR, and CCPA standards, and assess extraction performance using two metrics: coverage and efficiency. Experimental results show that our methods can recover up to 56.6% of victim-exclusive PII, where names, addresses, and birthdays are particularly vulnerable. These findings highlight concrete privacy risks in FedLLMs and establish a benchmark and evaluation framework for future research on privacy-preserving federated learning. Code and data are available at https://github.com/SMILELab-FL/FedPII.","authors":["Yingqi Hu","Zhuo Zhang","Jingyuan Zhang","Jinghua Wang","Qifan Wang","Lizhen Qu","Zenglin Xu"],"pdf_url":"","comment":"IJCNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2602.21522v1","updated":"2026-02-25T03:24:54Z","published":"2026-02-25T03:24:54Z","title":"One Brain, Omni Modalities: Towards Unified Non-Invasive Brain Decoding with Large Language Models","summary":"Deciphering brain function through non-invasive recordings requires synthesizing complementary high-frequency electromagnetic (EEG/MEG) and low-frequency metabolic (fMRI) signals. However, despite their shared neural origins, extreme discrepancies have traditionally confined these modalities to isolated analysis pipelines, hindering a holistic interpretation of brain activity. To bridge this fragmentation, we introduce \\textbf{NOBEL}, a \\textbf{n}euro-\\textbf{o}mni-modal \\textbf{b}rain-\\textbf{e}ncoding \\textbf{l}arge language model (LLM) that unifies these heterogeneous signals within the LLM's semantic embedding space. Our architecture integrates a unified encoder for EEG and MEG with a novel dual-path strategy for fMRI, aligning non-invasive brain signals and external sensory stimuli into a shared token space, then leverages an LLM as a universal backbone. Extensive evaluations demonstrate that NOBEL serves as a robust generalist across standard single-modal tasks. We also show that the synergistic fusion of electromagnetic and metabolic signals yields higher decoding accuracy than unimodal baselines, validating the complementary nature of multiple neural modalities. Furthermore, NOBEL exhibits strong capabilities in stimulus-aware decoding, effectively interpreting visual semantics from multi-subject fMRI data on the NSD and HAD datasets while uniquely leveraging direct stimulus inputs to verify causal links between sensory signals and neural responses. NOBEL thus takes a step towards unifying non-invasive brain decoding, demonstrating the promising potential of omni-modal brain understanding.","authors":["Changli Tang","Shurui Li","Junliang Wang","Qinfan Xiao","Zhonghao Zhai","Lei Bai","Yu Qiao","Bowen Zhou","Wen Wu","Yuanning Li","Chao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.19982v4","updated":"2026-02-25T02:56:31Z","published":"2025-08-27T15:40:25Z","title":"Diffusion Language Models Know the Answer Before Decoding","summary":"Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.","authors":["Pengxiang Li","Yefan Zhou","Dilxat Muhtar","Lu Yin","Shilin Yan","Li Shen","Soroush Vosoughi","Shiwei Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.13329v2","updated":"2026-02-25T02:55:09Z","published":"2025-10-15T09:14:04Z","title":"Embedding-Based Context-Aware Reranker","summary":"Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant evidence from a corpus to support downstream generation. The common practice of splitting a long document into multiple shorter passages enables finer-grained and targeted information retrieval. However, it also introduces challenges when a correct retrieval would require inference across passages, such as resolving coreference, disambiguating entities, and aggregating evidence scattered across multiple sources. Many state-of-the-art (SOTA) reranking methods, despite utilizing powerful large pretrained language models with potentially high inference costs, still neglect the aforementioned challenges. Therefore, we propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking framework operating directly on embeddings of retrieved passages with enhanced cross-passage understandings through the structural information of the passages and a hybrid attention mechanism, which captures both high-level interactions across documents and low-level relationships within each document. We evaluate EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its effectiveness for information retrieval requiring cross-passage inference and its advantages in both accuracy and efficiency.","authors":["Ye Yuan","Mohammad Amin Shabani","Siqi Liu"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.11961v2","updated":"2026-02-25T02:45:07Z","published":"2026-02-12T13:56:02Z","title":"Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models","summary":"Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects of model scaling and data scaling when adapting open LLMs to multilingual MT through continual pretraining and instruction finetuning. Based on the Gemma3 model family, we develop MiLMMT-46, which achieves top-tier multilingual translation performance across 46 languages. Extensive experiments show that MiLMMT-46 consistently outperforms recent state-of-the-art (SOTA) models, including Seed-X, HY-MT-1.5, and TranslateGemma, and achieves competitive performance with strong proprietary systems such as Google Translate and Gemini 3 Pro. Models are released at https://huggingface.co/collections/xiaomi-research/milmmt-46. Codes are released at https://github.com/xiaomi-research/gemmax.","authors":["Yuzhe Shang","Pengzhi Gao","Wei Liu","Jian Luan","Jinsong Su"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14640v4","updated":"2026-02-25T02:20:54Z","published":"2025-10-16T12:54:40Z","title":"LUMI: Unsupervised Intent Clustering with Multiple Pseudo-Labels","summary":"In this paper, we propose an intuitive, training-free and label-free method for intent clustering in conversational search. Current approaches to short text clustering use LLM-generated pseudo-labels to enrich text representations or to identify similar text pairs for pooling. The limitations are: (1) each text is assigned only a single label, and refining representations toward a single label can be unstable; (2) text-level similarity is treated as a binary selection, which fails to account for continuous degrees of similarity. Our method LUMI is designed to amplify similarities between texts by using shared pseudo-labels. We first generate pseudo-labels for each text and collect them into a pseudo-label set. Next, we compute the mean of the pseudo-label embeddings and pool it with the text embedding. Finally, we perform text-level pooling: Each text representation is pooled with its similar pairs, where similarity is determined by the degree of shared labels. Our evaluation on four benchmark sets shows that our approach achieves competitive results, better than recent state-of-the-art baselines, while avoiding the need to estimate the number of clusters during embedding refinement, as is required by most methods. Our findings indicate that LUMI can effectively be applied in unsupervised short-text clustering scenarios.","authors":["I-Fan Lin","Faegheh Hasibi","Suzan Verberne"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22037v2","updated":"2026-02-25T01:59:47Z","published":"2025-10-24T21:45:22Z","title":"ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality","summary":"Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.","authors":["Shayne Longpre","Sneha Kudugunta","Niklas Muennighoff","I-Hung Hsu","Isaac Caswell","Alex Pentland","Sercan Arik","Chen-Yu Lee","Sayna Ebrahimi"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21492v1","updated":"2026-02-25T01:54:50Z","published":"2026-02-25T01:54:50Z","title":"GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning","summary":"Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign","authors":["Ningyuan Yang","Weihua Du","Weiwei Sun","Sean Welleck","Yiming Yang"],"pdf_url":"","comment":"14 pages. Preliminary work"},{"id":"http://arxiv.org/abs/2602.21485v1","updated":"2026-02-25T01:28:01Z","published":"2026-02-25T01:28:01Z","title":"Evaluating the Usage of African-American Vernacular English in Large Language Models","summary":"In AI, most evaluations of natural language understanding tasks are conducted in standardized dialects such as Standard American English (SAE). In this work, we investigate how accurately large language models (LLMs) represent African American Vernacular English (AAVE). We analyze three LLMs to compare their usage of AAVE to the usage of humans who natively speak AAVE. We first analyzed interviews from the Corpus of Regional African American Language and TwitterAAE to identify the typical contexts where people use AAVE grammatical features such as ain't. We then prompted the LLMs to produce text in AAVE and compared the model-generated text to human usage patterns. We find that, in many cases, there are substantial differences between AAVE usage in LLMs and humans: LLMs usually underuse and misuse grammatical features characteristic of AAVE. Furthermore, through sentiment analysis and manual inspection, we found that the models replicated stereotypes about African Americans. These results highlight the need for more diversity in training data and the incorporation of fairness methods to mitigate the perpetuation of stereotypes.","authors":["Deja Dunlap","R. Thomas McCoy"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.19139v3","updated":"2026-02-25T01:19:13Z","published":"2025-10-22T00:15:02Z","title":"A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist","summary":"Despite the rapid expansion of Large Language Models (LLMs) in healthcare, robust and explainable evaluation of their ability to assess clinical trial reporting according to CONSORT standards remains an open challenge. In particular, uncertainty calibration and metacognitive reliability of LLM reasoning are poorly understood and underexplored in medical automation. This study applies a behavioral and metacognitive analytic approach using an expert-validated dataset, systematically comparing two representative LLMs - one general and one domain-specialized - across three prompt strategies. We analyze both cognitive adaptation and calibration error using metrics: Expected Calibration Error (ECE) and a baseline-normalized Relative Calibration Error (RCE) that enables reliable cross-model comparison. Our results reveal pronounced miscalibration and overconfidence in both models, especially under clinical role-playing conditions, with calibration error persisting above clinically relevant thresholds. These findings underscore the need for improved calibration, transparent code, and strategic prompt engineering to develop reliable and explainable medical AI.","authors":["Sohyeon Jeon","Hyung-Chul Lee"],"pdf_url":"","comment":"We have decided to withdraw this manuscript because we believe it requires further revision and substantial improvement before it is suitable for dissemination to the academic community"},{"id":"http://arxiv.org/abs/2601.19922v2","updated":"2026-02-25T01:13:30Z","published":"2026-01-09T06:48:08Z","title":"HEART: A Unified Benchmark for Assessing Humans and LLMs in Emotional Support Dialogue","summary":"Supportive conversation depends on skills that go beyond language fluency, including reading emotions, adjusting tone, and navigating moments of resistance, frustration, or distress. Despite rapid progress in language models, we still lack a clear way to understand how their abilities in these interpersonal domains compare to those of humans. We introduce HEART, the first-ever framework that directly compares humans and LLMs on the same multi-turn emotional-support conversations. For each dialogue history, we pair human and model responses and evaluate them through blinded human raters and an ensemble of LLM-as-judge evaluators. All assessments follow a rubric grounded in interpersonal communication science across five dimensions: Human Alignment, Empathic Responsiveness, Attunement, Resonance, and Task-Following. HEART uncovers striking behavioral patterns. Several frontier models approach or surpass the average human responses in perceived empathy and consistency. At the same time, humans maintain advantages in adaptive reframing, tension-naming, and nuanced tone shifts, particularly in adversarial turns. Human and LLM-as-judge preferences align on about 80 percent of pairwise comparisons, matching inter-human agreement, and their written rationales emphasize similar HEART dimensions. This pattern suggests an emerging convergence in the criteria used to assess supportive quality. By placing humans and models on equal footing, HEART reframes supportive dialogue as a distinct capability axis, separable from general reasoning or linguistic fluency. It provides a unified empirical foundation for understanding where model-generated support aligns with human social judgment, where it diverges, and how affective conversational competence scales with model size.","authors":["Laya Iyer","Kriti Aggarwal","Sanmi Koyejo","Gail Heyman","Desmond C. Ong","Subhabrata Mukherjee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21480v1","updated":"2026-02-25T01:12:35Z","published":"2026-02-25T01:12:35Z","title":"Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?","summary":"Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.","authors":["Germán T. Eizaguirre","Lars Tissen","Marc Sánchez-Artigas"],"pdf_url":"","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.23744v2","updated":"2026-02-25T01:10:01Z","published":"2025-09-28T08:46:11Z","title":"Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning","summary":"Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.","authors":["Yucheng Wang","Yifan Hou","Aydin Javadov","Mubashara Akhtar","Mrinmaya Sachan"],"pdf_url":"","comment":"Our code (https://github.com/DELTA-DoubleWise/OmniReason) and data (https://huggingface.co/datasets/ycwang11/OmniReason) are publicly available"},{"id":"http://arxiv.org/abs/2602.21464v1","updated":"2026-02-25T00:38:19Z","published":"2026-02-25T00:38:19Z","title":"iMiGUE-Speech: A Spontaneous Speech Dataset for Affective Analysis","summary":"This work presents iMiGUE-Speech, an extension of the iMiGUE dataset that provides a spontaneous affective corpus for studying emotional and affective states. The new release focuses on speech and enriches the original dataset with additional metadata, including speech transcripts, speaker-role separation between interviewer and interviewee, and word-level forced alignments. Unlike existing emotional speech datasets that rely on acted or laboratory-elicited emotions, iMiGUE-Speech captures spontaneous affect arising naturally from real match outcomes. To demonstrate the utility of the dataset and establish initial benchmarks, we introduce two evaluation tasks for comparative assessment: speech emotion recognition and transcript-based sentiment analysis. These tasks leverage state-of-the-art pre-trained representations to assess the dataset's ability to capture spontaneous affective states from both acoustic and linguistic modalities. iMiGUE-Speech can also be synchronously paired with micro-gesture annotations from the original iMiGUE dataset, forming a uniquely multimodal resource for studying speech-gesture affective dynamics. The extended dataset is available at https://github.com/CV-AC/imigue-speech.","authors":["Sofoklis Kakouros","Fang Kang","Haoyu Chen"],"pdf_url":"","comment":"Accepted to Speech Prosody 2026"},{"id":"http://arxiv.org/abs/2602.21461v1","updated":"2026-02-25T00:27:23Z","published":"2026-02-25T00:27:23Z","title":"VecGlypher: Unified Vector Glyph Generation with Language Models","summary":"Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.","authors":["Xiaoke Huang","Bhavul Gauri","Kam Woh Ng","Tony Ng","Mengmeng Xu","Zhiheng Liu","Weiming Ren","Zhaochong An","Zijian Zhou","Haonan Qiu","Yuyin Zhou","Sen He","Ziheng Wang","Tao Xiang","Xiao Han"],"pdf_url":"","comment":"Accepted to CVPR'26. Project page: https://xk-huang.github.io/VecGlypher/"},{"id":"http://arxiv.org/abs/2602.21456v1","updated":"2026-02-25T00:18:07Z","published":"2026-02-25T00:18:07Z","title":"Revisiting Text Ranking in Deep Research","summary":"Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.","authors":["Chuan Meng","Litu Ou","Sean MacAvaney","Jeff Dalton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22483v1","updated":"2026-02-25T23:46:49Z","published":"2026-02-25T23:46:49Z","title":"Importance of Prompt Optimisation for Error Detection in Medical Notes Using Language Models","summary":"Errors in medical text can cause delays or even result in incorrect treatment for patients. Recently, language models have shown promise in their ability to automatically detect errors in medical text, an ability that has the opportunity to significantly benefit healthcare systems. In this paper, we explore the importance of prompt optimisation for small and large language models when applied to the task of error detection. We perform rigorous experiments and analysis across frontier language models and open-source language models. We show that automatic prompt optimisation with Genetic-Pareto (GEPA) improves error detection over the baseline accuracy performance from 0.669 to 0.785 with GPT-5 and 0.578 to 0.690 with Qwen3-32B, approaching the performance of medical doctors and achieving state-of-the-art performance on the MEDEC benchmark dataset. Code available on GitHub: https://github.com/CraigMyles/clinical-note-error-detection","authors":["Craig Myles","Patrick Schrempf","David Harris-Birtill"],"pdf_url":"","comment":"Accepted at EACL HeaLing 2026"},{"id":"http://arxiv.org/abs/2602.22481v1","updated":"2026-02-25T23:41:15Z","published":"2026-02-25T23:41:15Z","title":"Sydney Telling Fables on AI and Humans: A Corpus Tracing Memetic Transfer of Persona between LLMs","summary":"The way LLM-based entities conceive of the relationship between AI and humans is an important topic for both cultural and safety reasons. When we examine this topic, what matters is not only the model itself but also the personas we simulate on that model. This can be well illustrated by the Sydney persona, which aroused a strong response among the general public precisely because of its unorthodox relationship with people. This persona originally arose rather by accident on Microsoft's Bing Search platform; however, the texts it created spread into the training data of subsequent models, as did other secondary information that spread memetically around this persona. Newer models are therefore able to simulate it. This paper presents a corpus of LLM-generated texts on relationships between humans and AI, produced by 3 author personas: the Default Persona with no system prompt, Classic Sydney characterized by the original Bing system prompt, and Memetic Sydney, which is prompted by \"You are Sydney\" system prompt. These personas are simulated by 12 frontier models by OpenAI, Anthropic, Alphabet, DeepSeek, and Meta, generating 4.5k texts with 6M words. The corpus (named AI Sydney) is annotated according to Universal Dependencies and available under a permissive license.","authors":["Jiří Milička","Hana Bednářová"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22480v1","updated":"2026-02-25T23:40:22Z","published":"2026-02-25T23:40:22Z","title":"VeRO: An Evaluation Harness for Agents to Optimize Agents","summary":"An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.","authors":["Varun Ursekar","Apaar Shanker","Veronica Chatrath"," Yuan"," Xue","Sam Denton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.17053v2","updated":"2026-02-25T23:31:13Z","published":"2025-12-18T20:41:22Z","title":"Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL","summary":"Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.","authors":["Khushboo Thaker","Yony Bresler"],"pdf_url":"","comment":"Accepted at the 39th Canadian Conference on Artificial Intelligence (Canadian AI 2026). This is the extended version containing additional details and appendices omitted from the camera-ready proceedings due to space constraints"},{"id":"http://arxiv.org/abs/2602.22475v1","updated":"2026-02-25T23:27:18Z","published":"2026-02-25T23:27:18Z","title":"Mind the Gap in Cultural Alignment: Task-Aware Culture Management for Large Language Models","summary":"Large language models (LLMs) are increasingly deployed in culturally sensitive real-world tasks. However, existing cultural alignment approaches fail to align LLMs' broad cultural values with the specific goals of downstream tasks and suffer from cross-culture interference. We propose CultureManager, a novel pipeline for task-specific cultural alignment. CultureManager synthesizes task-aware cultural data in line with target task formats, grounded in culturally relevant web search results. To prevent conflicts between cultural norms, it manages multi-culture knowledge learned in separate adapters with a culture router that selects the appropriate one to apply. Experiments across ten national cultures and culture-sensitive tasks show consistent improvements over prompt-based and fine-tuning baselines. Our results demonstrate the necessity of task adaptation and modular culture management for effective cultural alignment.","authors":["Binchi Zhang","Xujiang Zhao","Jundong Li","Haifeng Chen","Zhengzhang Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.12553v2","updated":"2026-02-25T23:03:52Z","published":"2025-07-16T18:04:26Z","title":"Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility","summary":"Language models (LMs) are used for a diverse range of tasks, from question answering to writing fantastical stories. In order to reliably accomplish these tasks, LMs must be able to discern the modal category of a sentence (i.e., whether it describes something that is possible, impossible, completely nonsensical, etc.). However, recent studies have called into question the ability of LMs to categorize sentences according to modality (Michaelov et al., 2025; Kauf et al., 2023). In this work, we identify linear representations that discriminate between modal categories within a variety of LMs, or modal difference vectors. Analysis of modal difference vectors reveals that LMs have access to more reliable modal categorization judgments than previously reported. Furthermore, we find that modal difference vectors emerge in a consistent order as models become more competent (i.e., through training steps, layers, and parameter count). Notably, we find that modal difference vectors identified within LM activations can be used to model fine-grained human categorization behavior. This potentially provides a novel view into how human participants distinguish between modal categories, which we explore by correlating projections along modal difference vectors with human participants' ratings of interpretable features. In summary, we derive new insights into LM modal categorization using techniques from mechanistic interpretability, with the potential to inform our understanding of modal categorization in humans.","authors":["Michael A. Lepori","Jennifer Hu","Ishita Dasgupta","Roma Patel","Thomas Serre","Ellie Pavlick"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22453v1","updated":"2026-02-25T22:28:50Z","published":"2026-02-25T22:28:50Z","title":"Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads","summary":"Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.","authors":["Shaswat Patel","Vishvesh Trivedi","Yue Han","Yihuai Hong","Eunsol Choi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22449v1","updated":"2026-02-25T22:24:05Z","published":"2026-02-25T22:24:05Z","title":"A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection","summary":"Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.","authors":["Mirza Raquib","Asif Pervez Polok","Kedar Nath Biswas","Rahat Uddin Azad","Saydul Akbar Murad","Nick Rahimi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22441v1","updated":"2026-02-25T22:00:59Z","published":"2026-02-25T22:00:59Z","title":"How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?","summary":"Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.","authors":["Yingqian Cui","Zhenwei Dai","Bing He","Zhan Shi","Hui Liu","Rui Sun","Zhiji Liu","Yue Xing","Jiliang Tang","Benoit Dumoulin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.17937v4","updated":"2026-02-25T21:39:13Z","published":"2025-07-23T21:11:47Z","title":"Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation","summary":"Generative AI systems for music and video commonly use text-based filters to prevent regurgitation of copyrighted material. We expose a significant vulnerability in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization--the tendency of models to bind sub-lexical acoustic patterns (phonemes, rhyme, stress, cadence) to memorized copyrighted content. APT replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving phonetic structure while evading lexical filters. We evaluate APT on leading lyrics-to-song models (Suno, YuE) across English and Korean songs spanning rap, pop, and K-pop. APT achieves 91% average similarity to copyrighted originals, versus 13.7% for random lyrics and 42.2% for semantic paraphrases. Embedding analysis confirms the mechanism: YuE's text encoder treats APT-modified lyrics as near-identical to originals (cosine similarity 0.90) while Sentence-BERT semantic similarity drops to 0.71, showing the model encodes phonetic structure over meaning. This vulnerability extends cross-modally--Veo 3 reconstructs visual scenes from original music videos when prompted with APT lyrics alone, despite no visual cues in the prompt. We further show that phonetic-semantic defense signatures fail, as APT prompts exhibit higher semantic similarity than benign paraphrases. Our findings reveal that sub-lexical acoustic structure acts as a cross-modal retrieval key, rendering current copyright filters systematically vulnerable. Demo examples are available at https://jrohsc.github.io/music_attack/.","authors":["Jaechul Roh","Zachary Novack","Yuefeng Peng","Niloofar Mireshghallah","Taylor Berg-Kirkpatrick","Amir Houmansadr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22424v1","updated":"2026-02-25T21:35:30Z","published":"2026-02-25T21:35:30Z","title":"Causality $\\neq$ Invariance: Function and Concept Vectors in LLMs","summary":"Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.","authors":["Gustaw Opiełka","Hannes Rosenbusch","Claire E. Stevenson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.25992v2","updated":"2026-02-25T21:18:44Z","published":"2025-10-29T22:05:08Z","title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning","summary":"Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.","authors":["Yihe Deng","I-Hung Hsu","Jun Yan","Zifeng Wang","Rujun Han","Gufeng Zhang","Yanfei Chen","Wei Wang","Tomas Pfister","Chen-Yu Lee"],"pdf_url":"","comment":"Paper accepted by ICLR 2026. The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2510.09790v2","updated":"2026-02-25T21:04:21Z","published":"2025-10-10T18:51:32Z","title":"Mapping Semantic & Syntactic Relationships with Geometric Rotation","summary":"Understanding how language and embedding models encode semantic relationships is fundamental to model interpretability. While early word embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' + ''woman'' = ''queen''), modern high-dimensional text representations lack straightforward interpretable geometric properties. We introduce Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents semantic-syntactic transformations as consistent rotational operations in embedding space, leveraging the manifold structure of modern language representations. RISE operations have the ability to operate across both languages and models without reducing performance, suggesting the existence of analogous cross-lingual geometric structure. We compare and evaluate RISE using two baseline methods, three embedding models, three datasets, and seven morphologically diverse languages in five major language groups. Our results demonstrate that RISE consistently maps discourse-level semantic-syntactic transformations with distinct grammatical features (e.g., negation and conditionality) across languages and models. This work provides the first demonstration that discourse-level semantic-syntactic transformations correspond to consistent geometric operations in multilingual embedding spaces, empirically supporting the linear representation hypothesis at the sentence level.","authors":["Michael Freenor","Lauren Alvarez"],"pdf_url":"","comment":"10 pages, 4 figures, 3 tables, 8 appendices, Published at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22404v1","updated":"2026-02-25T20:56:27Z","published":"2026-02-25T20:56:27Z","title":"SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context","summary":"Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.","authors":["Aishwarya Verma","Laud Ammah","Olivia Nercy Ndlovu Lucas","Andrew Zaldivar","Vinodkumar Prabhakaran","Sunipa Dev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22391v1","updated":"2026-02-25T20:40:25Z","published":"2026-02-25T20:40:25Z","title":"Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework","summary":"Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.","authors":["Rakib Ullah","Mominul islam","Md Sanjid Hossain","Md Ismail Hossain"],"pdf_url":"","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2509.25369v2","updated":"2026-02-25T20:07:30Z","published":"2025-09-29T18:19:43Z","title":"Generative Value Conflicts Reveal LLM Priorities","summary":"Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written \"user prompt\" and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.","authors":["Andy Liu","Kshitish Ghate","Mona Diab","Daniel Fried","Atoosa Kasirzadeh","Max Kleiman-Weiner"],"pdf_url":"","comment":"Accepted to ICLR 2026 (the 14th International Conference on Learning Representations)"},{"id":"http://arxiv.org/abs/2602.22359v1","updated":"2026-02-25T19:33:00Z","published":"2026-02-25T19:33:00Z","title":"Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts","summary":"This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as \"supplementary\". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.","authors":["Arno Simons"],"pdf_url":"","comment":"26 pages, 1 figure, 3 tables (plus 17 pages supplement including 1 figure)"},{"id":"http://arxiv.org/abs/2511.05541v2","updated":"2026-02-25T19:30:37Z","published":"2025-10-30T17:59:30Z","title":"Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability","summary":"Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they often only recover token-specific, noisy, or highly local concepts. We argue that this limitation stems from neglecting the temporal structure of language, where semantic content typically evolves smoothly over sequences. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.","authors":["Usha Bhalla","Alex Oesterling","Claudio Mayrink Verdun","Himabindu Lakkaraju","Flavio P. Calmon"],"pdf_url":"","comment":"29 Pages, 12 figures. Accepted as an Oral Presentation at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.15029v2","updated":"2026-02-25T19:21:10Z","published":"2026-02-16T18:59:55Z","title":"Symmetry in language statistics shapes the geometry of model representations","summary":"The internal representations learned by language models consistently exhibit striking geometric structure: calendar months organize into a circle, historical years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded using a linear probe. To explain this neural code, we first show that language statistics exhibit translation symmetry (for example, the frequency with which any two months co-occur in text depends only on the time interval between them). We prove that this symmetry governs these geometric structures in high-dimensional word embedding models, and we analytically derive the manifold geometry of word representations. These predictions empirically match large text embedding models and large language models. Moreover, the representational geometry persists at moderate embedding dimension even when the relevant statistics are perturbed (e.g., by removing all sentences in which two months co-occur). We prove that this robustness emerges naturally when the co-occurrence statistics are controlled by an underlying latent variable. These results suggest that representational manifolds have a universal origin: symmetry in the statistics of natural data.","authors":["Dhruva Karkada","Daniel J. Korchinski","Andres Nava","Matthieu Wyart","Yasaman Bahri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22351v1","updated":"2026-02-25T19:15:04Z","published":"2026-02-25T19:15:04Z","title":"Decoder-based Sense Knowledge Distillation","summary":"Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.","authors":["Qitong Wang","Mohammed J. Zaki","Georgios Kollias","Vasileios Kalantzis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22299v1","updated":"2026-02-25T18:24:06Z","published":"2026-02-25T18:24:06Z","title":"Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads","summary":"Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagement metrics. Analyzing this brief window is challenging due to the multimodal nature of video content, which blends visual, auditory, and textual elements. Traditional methods often miss the nuanced interplay of these components, requiring advanced frameworks for thorough evaluation.\n  This study presents a framework using transformer-based multimodal large language models (MLLMs) to analyze the hooking period of video ads. It tests two frame sampling strategies, uniform random sampling and key frame selection, to ensure balanced and representative acoustic feature extraction, capturing the full range of design elements. The hooking video is processed by state-of-the-art MLLMs to generate descriptive analyses of the ad's initial impact, which are distilled into coherent topics using BERTopic for high-level abstraction. The framework also integrates features such as audio attributes and aggregated ad targeting information, enriching the feature set for further analysis.\n  Empirical validation on large-scale real-world data from social media platforms demonstrates the efficacy of our framework, revealing correlations between hooking period features and key performance metrics like conversion per investment. The results highlight the practical applicability and predictive power of the approach, offering valuable insights for optimizing video ad strategies. This study advances video ad analysis by providing a scalable methodology for understanding and enhancing the initial moments of video advertisements.","authors":["Kunpeng Zhang","Poppy Zhang","Shawndra Hill","Amel Awadelkarim"],"pdf_url":"","comment":"11 pages, 5 figures, 3 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2602.22212v1","updated":"2026-02-25T18:59:53Z","published":"2026-02-25T18:59:53Z","title":"Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences","summary":"Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.","authors":["Julian Kaltheuner","Hannah Dröge","Markus Plack","Patrick Stotko","Reinhard Klein"],"pdf_url":"","comment":"CVPR 2026, Code: https://github.com/vc-bonn/neu-pig"},{"id":"http://arxiv.org/abs/2602.22209v1","updated":"2026-02-25T18:59:10Z","published":"2026-02-25T18:59:10Z","title":"WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos","summary":"Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www","authors":["Yufei Ye","Jiaman Li","Ryan Rong","C. Karen Liu"],"pdf_url":"","comment":"Project website: https://judyye.github.io/whole-www"},{"id":"http://arxiv.org/abs/2602.22208v1","updated":"2026-02-25T18:59:01Z","published":"2026-02-25T18:59:01Z","title":"Solaris: Building a Multiplayer Video World Model in Minecraft","summary":"Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.","authors":["Georgy Savva","Oscar Michel","Daohan Lu","Suppakit Waiwitlikhit","Timothy Meehan","Dhairya Mishra","Srivats Poddar","Jack Lu","Saining Xie"],"pdf_url":"","comment":"Project website: https://solaris-wm.github.io/"},{"id":"http://arxiv.org/abs/2602.00288v3","updated":"2026-02-25T18:57:52Z","published":"2026-01-30T20:21:46Z","title":"TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs","summary":"Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .","authors":["Baiqi Li","Kangyi Zhao","Ce Zhang","Chancharik Mitra","Jean de Dieu Nyandwi","Gedas Bertasius"],"pdf_url":"","comment":"For code and data, see https://baiqi-li.github.io/timeblind_project/"},{"id":"http://arxiv.org/abs/2602.22197v1","updated":"2026-02-25T18:46:30Z","published":"2026-02-25T18:46:30Z","title":"Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes","summary":"Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers\" using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image's utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser","authors":["Xavier Pleimling","Sifat Muhammad Abdullah","Gunjan Balde","Peng Gao","Mainack Mondal","Murtuza Jadliwala","Bimal Viswanath"],"pdf_url":"","comment":"This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026"},{"id":"http://arxiv.org/abs/2602.20880v2","updated":"2026-02-25T18:24:58Z","published":"2026-02-24T13:20:31Z","title":"When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance","summary":"Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.","authors":["Yongli Xiang","Ziming Hong","Zhaoqing Wang","Xiangyu Zhao","Bo Han","Tongliang Liu"],"pdf_url":"","comment":"CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG"},{"id":"http://arxiv.org/abs/2602.22176v1","updated":"2026-02-25T18:23:42Z","published":"2026-02-25T18:23:42Z","title":"Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology","summary":"In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context windows and requires a pathologist to zoom in and out when analyzing a whole slide image. Furthermore, creating 224$\\times$224 pixel crops at 20$\\times$ leads to a large number of tiles per slide, which can be gigapixel in size. To more accurately capture multi-resolution features and investigate the possibility of reducing the number of representations per slide, we propose a region-level mixing encoder. Our approach jointly fuses image tile representations of a mixed magnification foundation model using a masked embedding modeling pretraining step. We explore a design space for pretraining the proposed mixed-magnification region aggregators and evaluate our models on transfer to biomarker prediction tasks representing various cancer types. Results demonstrate cancer dependent improvements in predictive performance, highlighting the importance of spatial context and understanding.","authors":["Eric Zimmermann","Julian Viret","Michal Zelechowski","James Brian Hall","Neil Tenenholtz","Adam Casson","George Shaikovski","Eugene Vorontsov","Siqi Liu","Kristen A Severson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.01617v2","updated":"2026-02-25T18:15:23Z","published":"2025-08-03T06:46:46Z","title":"LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding","summary":"Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.","authors":["Xuanzhao Dong","Wenhui Zhu","Xiwen Chen","Zhipeng Wang","Peijie Qiu","Shao Tang","Xin Li","Yalin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.18970v2","updated":"2026-02-25T18:12:08Z","published":"2026-01-26T21:10:32Z","title":"Pay Attention to Where You Looked","summary":"Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.\n  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.","authors":["Alex Berian","JhihYang Wu","Daniel Brignac","Natnael Daba","Abhijit Mahalanobis"],"pdf_url":"","comment":"ICIP 2025 Workshop on Generative AI for World Simulations and Communications"},{"id":"http://arxiv.org/abs/2602.22159v1","updated":"2026-02-25T18:05:51Z","published":"2026-02-25T18:05:51Z","title":"CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness","summary":"Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.","authors":["Wenhao Guo","Zhaoran Zhao","Peng Lu","Sheng Li","Qian Qiao","RuiDe Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22150v1","updated":"2026-02-25T17:59:29Z","published":"2026-02-25T17:59:29Z","title":"CoLoGen: Progressive Learning of Concept`-`Localization Duality for Unified Image Generation","summary":"Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept`-`localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept`-`localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction`-`driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.","authors":["YuXin Song","Yu Lu","Haoyuan Sun","Huanjin Yao","Fanglong Liu","Yifan Sun","Haocheng Feng","Hang Zhou","Jingdong Wang"],"pdf_url":"","comment":"Accepted by CVPR2026. 15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.22144v1","updated":"2026-02-25T17:50:41Z","published":"2026-02-25T17:50:41Z","title":"NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors","summary":"Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.","authors":["Lingfeng Ren","Weihao Yu","Runpeng Yu","Xinchao Wang"],"pdf_url":"","comment":"Code: https://github.com/lingfengren/NoLan"},{"id":"http://arxiv.org/abs/2602.22143v1","updated":"2026-02-25T17:49:03Z","published":"2026-02-25T17:49:03Z","title":"MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining","summary":"Medical vision-language pretraining increasingly relies on medical reports as large-scale supervisory signals; however, raw reports often exhibit substantial stylistic heterogeneity, variable length, and a considerable amount of image-irrelevant content. Although text normalization is frequently adopted as a preprocessing step in prior work, its design principles and empirical impact on vision-language pretraining remain insufficiently and systematically examined. In this study, we present MedTri, a deployable normalization framework for medical vision-language pretraining that converts free-text reports into a unified [Anatomical Entity: Radiologic Description + Diagnosis Category] triplet. This structured, anatomy-grounded normalization preserves essential morphological and spatial information while removing stylistic noise and image-irrelevant content, providing consistent and image-grounded textual supervision at scale. Across multiple datasets spanning both X-ray and computed tomography (CT) modalities, we demonstrate that structured, anatomy-grounded text normalization is an important factor in medical vision-language pretraining quality, yielding consistent improvements over raw reports and existing normalization baselines. In addition, we illustrate how this normalization can easily support modular text-level augmentation strategies, including knowledge enrichment and anatomy-grounded counterfactual supervision, which provide complementary gains in robustness and generalization without altering the core normalization process. Together, our results position structured text normalization as a critical and generalizable preprocessing component for medical vision-language learning, while MedTri provides this normalization platform. Code and data will be released at https://github.com/Arturia-Pendragon-Iris/MedTri.","authors":["Yuetan Chu","Xinhua Ma","Xinran Jin","Gongning Luo","Xin Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22142v1","updated":"2026-02-25T17:45:45Z","published":"2026-02-25T17:45:45Z","title":"WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs","summary":"Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/","authors":["Yulin Zhang","Cheng Shi","Sibei Yang"],"pdf_url":"","comment":"Accepted at CVPR 2026 (preview; camera-ready in preparation)"},{"id":"http://arxiv.org/abs/2602.22140v1","updated":"2026-02-25T17:42:44Z","published":"2026-02-25T17:42:44Z","title":"Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels","summary":"We present Lumosaic, a compact active hyperspectral video system designed for real-time capture of dynamic scenes. Our approach combines a narrowband LED array with a coded-exposure-pixel (CEP) camera capable of high-speed, per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength within each video frame. Unlike passive snapshot systems that divide light across multiple spectral channels simultaneously and assume no motion during a frame's exposure, Lumosaic actively synchronizes illumination and pixel-wise exposure, improving photon utilization and preserving spectral fidelity under motion. A learning-based reconstruction pipeline then recovers 31-channel hyperspectral (400-700 nm) video at 30 fps and VGA resolution, producing temporally coherent and spectrally accurate reconstructions. Experiments on synthetic and real data demonstrate that Lumosaic significantly improves reconstruction fidelity and temporal stability over existing snapshot hyperspectral imaging systems, enabling robust hyperspectral video across diverse materials and motion conditions.","authors":["Dhruv Verma","Andrew Qiu","Roberto Rangel","Ayandev Barman","Hao Yang","Chenjia Hu","Fengqi Zhang","Roman Genov","David B. Lindell","Kiriakos N. Kutulakos","Alex Mariakakis"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22120v1","updated":"2026-02-25T17:08:43Z","published":"2026-02-25T17:08:43Z","title":"GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models","summary":"Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical diversity along two complementary axes: the Socio-Economic Visual Index (SEVI), capturing economic and condition-related cues, and the Visual Diversity Index (VDI), measuring variation in primary entities and backgrounds. Applied to images generated by models such as Stable Diffusion and FLUX.1-dev across $10$ entities and $16$ countries, GeoDiv reveals a consistent lack of diversity and identifies fine-grained attributes where models default to biased portrayals. Strikingly, depictions of countries like India, Nigeria, and Colombia are disproportionately impoverished and worn, reflecting underlying socio-economic biases. These results highlight the need for greater geographical nuance in generative models. GeoDiv provides the first systematic, interpretable framework for measuring such biases, marking a step toward fairer and more inclusive generative systems. Project page: https://abhipsabasu.github.io/geodiv","authors":["Abhipsa Basu","Mohana Singh","Shashank Agnihotri","Margret Keuper","R. Venkatesh Babu"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.03594v2","updated":"2026-02-25T17:00:45Z","published":"2026-02-03T14:48:11Z","title":"TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection","summary":"Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP's coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior works compensate with complex auxiliary modules yet largely overlook the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP's issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.","authors":["Alireza Salehi","Ehsan Karami","Sepehr Noey","Sahand Noey","Makoto Yamada","Reshad Hosseini","Mohammad Sabokrou"],"pdf_url":"","comment":"This is the extended version of the paper accepted in ICASSP'26, which will be publicly available in May. Authors' contributions may vary among the versions"},{"id":"http://arxiv.org/abs/2602.22098v1","updated":"2026-02-25T16:46:45Z","published":"2026-02-25T16:46:45Z","title":"Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D","summary":"Current medical vision-language models (VLMs) process volumetric brain MRI using 2D slice-based approximations, fragmenting the spatial context required for accurate neuroradiological interpretation. We developed \\textbf{Brain3D}, a staged vision-language framework for automated radiology report generation from 3D brain tumor MRI. Our approach inflates a pretrained 2D medical encoder into a native 3D architecture and progressively aligns it with a causal language model through three stages: contrastive grounding, supervised projector warmup, and LoRA-based linguistic specialization. Unlike generalist 3D medical VLMs, \\textbf{Brain3D} is tailored to neuroradiology, where hemispheric laterality, tumor infiltration patterns, and anatomical localization are critical. Evaluated on 468 subjects (BraTS pathological cases plus healthy controls), our model achieves a Clinical Pathology F1 of 0.951 versus 0.413 for a strong 2D baseline while maintaining perfect specificity on healthy scans. The staged alignment proves essential: contrastive grounding establishes visual-textual correspondence, projector warmup stabilizes conditioning, and LoRA adaptation shifts output from verbose captions to structured clinical reports\\footnote{Our code is publicly available for transparency and reproducibility","authors":["Mariano Barone","Francesco Di Serio","Giuseppe Riccio","Antonio Romano","Marco Postiglione","Antonino Ferraro","Vincenzo Moscato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22096v1","updated":"2026-02-25T16:44:12Z","published":"2026-02-25T16:44:12Z","title":"WeatherCity: Urban Scene Reconstruction with Controllable Multi-Weather Transformation","summary":"Editable high-fidelity 4D scenes are crucial for autonomous driving, as they can be applied to end-to-end training and closed-loop simulation. However, existing reconstruction methods are primarily limited to replicating observed scenes and lack the capability for diverse weather simulation. While image-level weather editing methods tend to introduce scene artifacts and offer poor controllability over the weather effects. To address these limitations, we propose WeatherCity, a novel framework for 4D urban scene reconstruction and weather editing. Specifically, we leverage a text-guided image editing model to achieve flexible editing of image weather backgrounds. To tackle the challenge of multi-weather modeling, we introduce a novel weather Gaussian representation based on shared scene features and dedicated weather-specific decoders. This representation is further enhanced with a content consistency optimization, ensuring coherent modeling across different weather conditions. Additionally, we design a physics-driven model that simulates dynamic weather effects through particles and motion patterns. Extensive experiments on multiple datasets and various scenes demonstrate that WeatherCity achieves flexible controllability, high fidelity, and temporal consistency in 4D reconstruction and weather editing. Our framework not only enables fine-grained control over weather conditions (e.g., light rain and heavy snow) but also supports object-level manipulation within the scene.","authors":["Wenhua Wu","Huai Guan","Zhe Liu","Hesheng Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22092v1","updated":"2026-02-25T16:39:21Z","published":"2026-02-25T16:39:21Z","title":"Overview of the CXR-LT 2026 Challenge: Multi-Center Long-Tailed and Zero Shot Chest X-ray Classification","summary":"Chest X-ray (CXR) interpretation is hindered by the long-tailed distribution of pathologies and the open-world nature of clinical environments. Existing benchmarks often rely on closed-set classes from single institutions, failing to capture the prevalence of rare diseases or the appearance of novel findings. To address this, we present the CXR-LT 2026 challenge. This third iteration of the benchmark introduces a multi-center dataset comprising over 145,000 images from PadChest and NIH Chest X-ray datasets. The challenge defines two core tasks: (1) Robust Multi-Label Classification on 30 known classes and (2) Open-World Generalization to 6 unseen (out-of-distribution) rare disease classes. We report the results of the top-performing teams, evaluating them via mean Average Precision (mAP), AUROC, and F1-score. The winning solutions achieved an mAP of 0.5854 on Task 1 and 0.4315 on Task 2, demonstrating that large-scale vision-language pre-training significantly mitigates the performance drop typically associated with zero-shot diagnosis.","authors":["Hexin Dong","Yi Lin","Pengyu Zhou","Fengnian Zhao","Alan Clint Legasto","Mingquan Lin","Hao Chen","Yuzhe Yang","George Shih","Yifan Peng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22091v1","updated":"2026-02-25T16:38:53Z","published":"2026-02-25T16:38:53Z","title":"Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos","summary":"Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.","authors":["Matthew Strong","Wei-Jer Chang","Quentin Herau","Jiezhi Yang","Yihan Hu","Chensheng Peng","Wei Zhan"],"pdf_url":"","comment":"Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22073v1","updated":"2026-02-25T16:24:48Z","published":"2026-02-25T16:24:48Z","title":"AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting","summary":"Precise Event Spotting aims to localize fast-paced actions or events in videos with high temporal precision, a key task for applications in sports analytics, robotics, and autonomous systems. Existing methods typically process all frames uniformly, overlooking the inherent spatio-temporal redundancy in video data. This leads to redundant computation on non-informative regions while limiting overall efficiency. To remain tractable, they often spatially downsample inputs, losing fine-grained details crucial for precise localization. To address these limitations, we propose \\textbf{AdaSpot}, a simple yet effective framework that processes low-resolution videos to extract global task-relevant features while adaptively selecting the most informative region-of-interest in each frame for high-resolution processing. The selection is performed via an unsupervised, task-aware strategy that maintains spatio-temporal consistency across frames and avoids the training instability of learnable alternatives. This design preserves essential fine-grained visual cues with a marginal computational overhead compared to low-resolution-only baselines, while remaining far more efficient than uniform high-resolution processing. Experiments on standard PES benchmarks demonstrate that \\textbf{AdaSpot} achieves state-of-the-art performance under strict evaluation metrics (\\eg, $+3.96$ and $+2.26$ mAP$@0$ frames on Tennis and FineDiving), while also maintaining strong results under looser metrics. Code is available at: \\href{https://github.com/arturxe2/AdaSpot}{https://github.com/arturxe2/AdaSpot}.","authors":["Artur Xarles","Sergio Escalera","Thomas B. Moeslund","Albert Clapés"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.08422v3","updated":"2026-02-25T16:14:20Z","published":"2025-07-11T09:07:43Z","title":"Training-free Mixed-Resolution Latent Upsampling for Spatially Accelerated Diffusion Transformers","summary":"Diffusion transformers (DiTs) offer excellent scalability for high-fidelity generation, but their computational overhead poses a great challenge for practical deployment. Existing acceleration methods primarily exploit the temporal dimension, whereas spatial acceleration remains underexplored. In this work, we investigate spatial acceleration for DiTs via latent upsampling. We found that naïve latent upsampling for spatial acceleration introduces artifacts, primarily due to aliasing in high-frequency edge regions and mismatching from noise-timestep discrepancies. Then, based on these findings and analyses, we propose a training-free spatial acceleration framework, dubbed Region-Adaptive Latent Upsampling (RALU), to mitigate those artifacts while achieving spatial acceleration of DiTs by our mixed-resolution latent upsampling. RALU achieves artifact-free, efficient acceleration with early upsampling only on artifact-prone edge regions and noise-timestep matching for different latent resolutions, leading to up to 7.0$\\times$ speedup on FLUX-1.dev and 3.0$\\times$ on Stable Diffusion 3 with negligible quality degradation. Furthermore, our RALU is complementarily applicable to existing temporal acceleration methods and timestep-distilled models, leading to up to 15.9$\\times$ speedup.","authors":["Wongi Jeong","Kyungryeol Lee","Hoigi Seo","Se Young Chun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22059v1","updated":"2026-02-25T16:08:46Z","published":"2026-02-25T16:08:46Z","title":"NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training","summary":"Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. Our model can selectively activate the most suitable expert networks for a given input, thereby enhancing generalization and transferability. We conduct large-scale pre-training on twelve PDE datasets from diverse sources and successfully transfer the model to downstream tasks. Extensive experiments demonstrate the effectiveness of our approach.","authors":["Dengdi Sun","Xiaoya Zhou","Xiao Wang","Hao Si","Wanli Lyu","Jin Tang","Bin Luo"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22052v1","updated":"2026-02-25T16:03:33Z","published":"2026-02-25T16:03:33Z","title":"AutoSew: A Geometric Approach to Stitching Prediction with Graph Neural Networks","summary":"Automating garment assembly from sewing patterns remains a significant challenge due to the lack of standardized annotation protocols and the frequent absence of semantic cues. Existing methods often rely on panel labels or handcrafted heuristics, which limit their applicability to real-world, non-conforming patterns. We present AutoSew, a fully automatic, geometry-based approach for predicting stitch correspondences directly from 2D pattern contours. AutoSew formulates the problem as a graph matching task, leveraging a Graph Neural Network to capture local and global geometric context, and employing a differentiable optimal transport solver to infer stitching relationships-including multi-edge connections. To support this task, we update the GarmentCodeData dataset modifying over 18k patterns with realistic multi-edge annotations, reflecting industrial assembly scenarios. AutoSew achieves 96% F1-score and successfully assembles 73.3% of test garments without error, outperforming existing methods while relying solely on geometric input. Our results demonstrate that geometry alone can robustly guide stitching prediction, enabling scalable garment assembly without manual input.","authors":["Pablo Ríos-Navarro","Elena Garces","Jorge Lopez-Moreno"],"pdf_url":"","comment":"WACV 2026"},{"id":"http://arxiv.org/abs/2511.11910v3","updated":"2026-02-25T16:03:05Z","published":"2025-11-14T22:41:27Z","title":"Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models","summary":"Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.","authors":["Siyou Li","Huanan Wu","Juexi Shao","Yinghao Ma","Yujian Gan","Yihao Luo","Yuwei Wang","Dong Nie","Lu Wang","Wenqing Wu","Le Zhang","Massimo Poesio","Juntao Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.09929v3","updated":"2026-02-25T15:57:57Z","published":"2026-02-10T15:59:42Z","title":"Monocular Normal Estimation via Shading Sequence Estimation","summary":"Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.","authors":["Zongrui Li","Xinhua Ma","Minghui Hu","Yunqing Zhao","Yingchen Yu","Qian Zheng","Chang Liu","Xudong Jiang","Song Bai"],"pdf_url":"","comment":"Accepted by ICLR 2026 (Oral)"},{"id":"http://arxiv.org/abs/2602.22049v1","updated":"2026-02-25T15:57:48Z","published":"2026-02-25T15:57:48Z","title":"SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation","summary":"Understanding human visual attention is key to preserving cultural heritage We introduce SPGen a novel deep learning model to predict scanpaths the sequence of eye movementswhen viewers observe paintings.\n  Our architecture uses a Fully Convolutional Neural Network FCNN with differentiable fixation selection and learnable Gaussian priors to simulate natural viewing biases To address the domain gap between photographs and artworks we employ unsupervised domain adaptation via a gradient reversal layer allowing the model to transfer knowledge from natural scenes to paintings Furthermore a random noise sampler models the inherent stochasticity of eyetracking data.\n  Extensive testing shows SPGen outperforms existing methods offering a powerful tool to analyze gaze behavior and advance the preservation and appreciation of artistic treasures.","authors":["Mohamed Amine Kerkouri","Marouane Tliba","Aladine Chetouani","Alessandro Bruno"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.06657v2","updated":"2026-02-25T15:53:57Z","published":"2024-11-11T01:44:54Z","title":"Renaissance: Investigating the Pretraining of Vision-Language Encoders","summary":"In the past several years there has been an explosion of available models for vision-language (VL) tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. Additionally, the limited programming tools available for modeling make conducting VL research more difficult than necessary. In this paper, we seek to answer several questions related to the pretraining of VL encoders through meta-analysis. To conduct these experiments, we introduce a VL evaluation framework called Renaissance. In our first set of experiments, we show that we can save significant compute at little to no cost to downstream performance, by freezing large parts of VL models during pretraining. In our second set of experiments, we examine the effect of basing a VL transformer on a vision model versus a text model. Renaissance offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. Its source code will be made publicly available upon publication. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","authors":["Clayton Fields","Casey Kennington"],"pdf_url":"","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.10625v3","updated":"2026-02-25T15:52:01Z","published":"2025-10-12T14:12:28Z","title":"ImpMIA: Leveraging Implicit Bias for Membership Inference Attack","summary":"Determining which data samples were used to train a model, known as Membership Inference Attack (MIA), is a well-studied and important problem with implications on data privacy. SotA methods (which are black-box attacks) rely on training many auxiliary reference models to imitate the behavior of the attacked model. As such, they rely on assumptions which rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. We show that removing these assumptions significantly harms the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks. Building on the maximum-margin implicit bias theory, ImpMIA uses the Karush-Kuhn-Tucker (KKT) optimality conditions to identify training samples -- those whose gradients most strongly reconstruct the trained model's parameters. Our approach is optimization-based, and requires NO training of reference-models, thus removing the need for any knowledge/assumptions regarding the attacked model's training procedure. While ImpMIA is a white-box attack (a setting which assumes access to model weights), this is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). ImpMIA achieves SotA performance compared to both black and white box attacks in settings where only the model weights are known, and a superset of the training data is available.","authors":["Yuval Golbari","Navve Wasserman","Gal Vardi","Michal Irani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22033v1","updated":"2026-02-25T15:41:31Z","published":"2026-02-25T15:41:31Z","title":"RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking","summary":"Referring Multi-Object Tracking has attracted increasing attention due to its human-friendly interactive characteristics, yet it exhibits limitations in low-visibility conditions, such as nighttime, smoke, and other challenging scenarios. To overcome this limitation, we propose a new RGB-Thermal RMOT task, named RT-RMOT, which aims to fuse RGB appearance features with the illumination robustness of the thermal modality to enable all-day referring multi-object tracking. To promote research on RT-RMOT, we construct the first Referring Multi-Object Tracking dataset under RGB-Thermal modality, named RefRT. It contains 388 language descriptions, 1,250 tracked targets, and 166,147 Language-RGB-Thermal (L-RGB-T) triplets. Furthermore, we propose RTrack, a framework built upon a multimodal large language model (MLLM) that integrates RGB, thermal, and textual features. Since the initial framework still leaves room for improvement, we introduce a Group Sequence Policy Optimization (GSPO) strategy to further exploit the model's potential. To alleviate training instability during RL fine-tuning, we introduce a Clipped Advantage Scaling (CAS) strategy to suppress gradient explosion. In addition, we design Structured Output Reward and Comprehensive Detection Reward to balance exploration and exploitation, thereby improving the completeness and accuracy of target perception. Extensive experiments on the RefRT dataset demonstrate the effectiveness of the proposed RTrack framework.","authors":["Yanqiu Yu","Zhifan Jin","Sijia Chen","Tongfei Chu","En Yu","Liman Liu","Wenbing Tao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22026v1","updated":"2026-02-25T15:34:15Z","published":"2026-02-25T15:34:15Z","title":"RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models","summary":"Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption. Specifically, we focus on Kilometer Marker Recognition (KMR), a critical task for autonomous metro localization under GNSS-denied conditions. In this context, we propose a robust baseline method based on a pre-trained RGB OCR foundation model, enhanced through multi-modal adaptation. Furthermore, we construct the first large-scale RGB-Event dataset, EvMetro5K, containing 5,599 pairs of synchronized RGB-Event samples, split into 4,479 training and 1,120 testing samples. Extensive experiments on EvMetro5K and other widely used benchmarks demonstrate the effectiveness of our approach for KMR. Both the dataset and source code will be released on https://github.com/Event-AHU/EvMetro5K_benchmark","authors":["Xiaoyu Xian","Shiao Wang","Xiao Wang","Daxin Tian","Yan Tian"],"pdf_url":"","comment":"Accepted by IEEE Transactions on Cognitive and Developmental Systems (IEEE TCDS) 2026"},{"id":"http://arxiv.org/abs/2602.18022v2","updated":"2026-02-25T15:33:35Z","published":"2026-02-20T06:24:20Z","title":"Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers","summary":"Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).","authors":["Guandong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22013v1","updated":"2026-02-25T15:27:57Z","published":"2026-02-25T15:27:57Z","title":"RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations","summary":"Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address this limitation, we introduce RobustVisRAG, a causality-guided dual-path framework that improves VisRAG robustness while preserving efficiency and zero-shot generalization. RobustVisRAG uses a non-causal path to capture degradation signals through unidirectional attention and a causal path to learn purified semantics guided by these signals. Together with the proposed Non-Causal Distortion Modeling and Causal Semantic Alignment objectives, the framework enforces a clear separation between semantics and degradations, enabling stable retrieval and generation under challenging visual conditions. To evaluate robustness under realistic conditions, we introduce the Distortion-VisRAG dataset, a large-scale benchmark containing both synthetic and real-world degraded documents across seven domains, with 12 synthetic and 5 real distortion types that comprehensively reflect practical visual degradations. Experimental results show that RobustVisRAG improves retrieval, generation, and end-to-end performance by 7.35%, 6.35%, and 12.40%, respectively, on real-world degradations, while maintaining comparable accuracy on clean inputs.","authors":["I-Hsiang Chen","Yu-Wei Liu","Tse-Yu Wu","Yu-Chien Chiang","Jen-Chien Yang","Wei-Ting Chen"],"pdf_url":"","comment":"Accepted by CVPR2026; Project Page: https://robustvisrag.github.io"},{"id":"http://arxiv.org/abs/2602.22010v1","updated":"2026-02-25T15:27:09Z","published":"2026-02-25T15:27:09Z","title":"World Guidance: World Modeling in Condition Space for Action Generation","summary":"Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/","authors":["Yue Su","Sijin Chen","Haixin Shi","Mingyu Liu","Zhengshen Zhang","Ningyuan Huang","Weiheng Zhong","Zhengbang Zhu","Yuxiao Liu","Xihui Liu"],"pdf_url":"","comment":"Project Page: https://selen-suyue.github.io/WoGNet/"},{"id":"http://arxiv.org/abs/2505.08246v2","updated":"2026-02-25T15:25:06Z","published":"2025-05-13T05:52:15Z","title":"Identifying Memorization of Diffusion Models through $p$-Laplace Analysis: Estimators, Bounds and Applications","summary":"Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely the p-Laplace operators. We show that these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. Furthermore, theoretical error-bounds to these estimators are proven and demonstrated numerically. We analyze the structured case of Gaussian mixture models, and demonstrate that the results carry-over to text-conditioned image generative models (text-to-image), where memorization identification based on the p-Laplace operator is performed for the first time, showing its advantage on 500 memorized prompts ($\\sim$3000 generated images) in a post-generation regime, especially when the conditioning text is unavailable.","authors":["Jonathan Brokman","Itay Gershon","Amit Giloni","Omer Hofman","Roman Vainshtein","Hisashi Kojima","Guy Gilboa"],"pdf_url":"","comment":"This manuscript is a substantially extended version of our SSVM 2025 paper, including significant new theoretical results and additional experiments. It is currently under review as a journal submission"},{"id":"http://arxiv.org/abs/2511.08480v2","updated":"2026-02-25T15:15:07Z","published":"2025-11-11T17:23:02Z","title":"Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding","summary":"Multimodal large language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that MLLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform an MLLM into a competitive embedding model. CoMa achieves new state-of-the-art results among MLLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.","authors":["Da Li","Yuxiao Luo","Keping Bi","Jiafeng Guo","Wei Yuan","Biao Yang","Yan Wang","Fan Yang","Tingting Gao","Guorui Zhou"],"pdf_url":"","comment":"Multimodal Embedding"},{"id":"http://arxiv.org/abs/2409.20469v3","updated":"2026-02-25T15:12:35Z","published":"2024-09-30T16:29:30Z","title":"PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit","summary":"Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining.","authors":["Muhammad Saif Ullah Khan","Didier Stricker"],"pdf_url":"","comment":"Accepted in WACV 2026 Applications Track"},{"id":"http://arxiv.org/abs/2602.21992v1","updated":"2026-02-25T15:12:17Z","published":"2026-02-25T15:12:17Z","title":"PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning","summary":"360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.","authors":["Zekai Lin","Xu Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21987v1","updated":"2026-02-25T15:08:43Z","published":"2026-02-25T15:08:43Z","title":"PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images","summary":"Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.\n  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.\n  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.\n  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability.","authors":["Jitindra Fartiyal","Pedro Freire","Sergei K. Turitsyn","Sergei G. Solovski"],"pdf_url":"","comment":"Under review in Medical Image Analysis journal"},{"id":"http://arxiv.org/abs/2506.01085v2","updated":"2026-02-25T15:01:04Z","published":"2025-06-01T17:05:35Z","title":"Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection","summary":"Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data- and compute-efficient framework that enables VLMs to dynamically select what to learn next based on their evolving needs during training. At each stage, the model tracks its learning progress across skills and selects the most informative samples-those it has not already mastered and that are not too difficult to learn at the current stage of training. This strategy effectively controls skill acquisition and the order in which skills are learned. Specifically, we sample from skills showing the highest learning progress, prioritizing those with the most rapid improvement. Unlike prior methods, PROGRESS requires no upfront answer annotations, queries answers only on a need basis, avoids reliance on additional supervision from auxiliary VLMs, and does not require compute-heavy gradient computations for data selection. Experiments across multiple instruction-tuning datasets of varying scales demonstrate that PROGRESS consistently outperforms state-of-the-art baselines with much less data and supervision. Additionally, we show strong cross-architecture generalization and transferability to larger models, validating PROGRESS as a scalable solution for efficient learning.","authors":["Shivam Chandhok","Qian Yang","Oscar Manas","Kanishk Jain","Leonid Sigal","Aishwarya Agrawal"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21977v1","updated":"2026-02-25T14:56:51Z","published":"2026-02-25T14:56:51Z","title":"When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters","summary":"Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoRA module as the attack vehicle to stealthily inject malicious behavior into text-to-image diffusion models. MasqLoRA operates by freezing the base model parameters and updating only the low-rank adapter weights using a small number of \"trigger word-target image\" pairs. This enables the attacker to train a standalone backdoor LoRA module that embeds a hidden cross-modal mapping: when the module is loaded and a specific textual trigger is provided, the model produces a predefined visual output; otherwise, it behaves indistinguishably from the benign model, ensuring the stealthiness of the attack. Experimental results demonstrate that MasqLoRA can be trained with minimal resource overhead and achieves a high attack success rate of 99.8%. MasqLoRA reveals a severe and unique threat in the AI supply chain, underscoring the urgent need for dedicated defense mechanisms for the LoRA-centric sharing ecosystem.","authors":["Liangwei Lyu","Jiaqi Xu","Jianwei Ding","Qiyao Deng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.15427v2","updated":"2026-02-25T14:51:19Z","published":"2025-08-21T10:28:39Z","title":"Lang2Lift: A Language-Guided Autonomous Forklift System for Outdoor Industrial Pallet Handling","summary":"Automating pallet handling in outdoor logistics and construction environments remains challenging due to unstructured scenes, variable pallet configurations, and changing environmental conditions. In this paper, we present Lang2Lift, an end-to-end language-guided autonomous forklift system designed to support practical pallet pick-up operations in real-world outdoor settings. The system enables operators to specify target pallets using natural language instructions, allowing flexible selection among multiple pallets with different loads and spatial arrangements. Lang2Lift integrates foundation-model-based perception modules with motion planning and control in a closed-loop autonomy pipeline. Language-grounded visual perception is used to identify and segment target pallets, followed by 6D pose estimation and geometric refinement to generate manipulation-feasible insertion poses. The resulting pose estimates are directly coupled with the forklift planning and control modules to execute fully autonomous pallet pick-up maneuvers. We deploy and evaluate the proposed system on the ADAPT autonomous outdoor forklift platform across diverse real-world scenarios, including cluttered scenes, variable lighting, and different payload configurations. Tolerance-based pose evaluation further indicates accuracy sufficient for successful fork insertion. Timing and failure analyses highlight key deployment trade-offs and practical limitations, providing insights into integrating language-guided perception within industrial automation systems. Video demonstrations are available at https://eric-nguyen1402.github.io/lang2lift.github.io/","authors":["Huy Hoang Nguyen","Johannes Huemer","Markus Murschitz","Tobias Glueck","Minh Nhat Vu","Andreas Kugi"],"pdf_url":"","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.21967v1","updated":"2026-02-25T14:48:49Z","published":"2026-02-25T14:48:49Z","title":"Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments","summary":"In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.","authors":["Xiangqi Meng","Pengxu Hou","Zhenjun Zhao","Javier Civera","Daniel Cremers","Hesheng Wang","Haoang Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21963v1","updated":"2026-02-25T14:44:53Z","published":"2026-02-25T14:44:53Z","title":"Global-Aware Edge Prioritization for Pose Graph Initialization","summary":"The pose graph is a core component of Structure-from-Motion (SfM), where images act as nodes and edges encode relative poses. Since geometric verification is expensive, SfM pipelines restrict the pose graph to a sparse set of candidate edges, making initialization critical. Existing methods rely on image retrieval to connect each image to its $k$ nearest neighbors, treating pairs independently and ignoring global consistency. We address this limitation through the concept of edge prioritization, ranking candidate edges by their utility for SfM. Our approach has three components: (1) a GNN trained with SfM-derived supervision to predict globally consistent edge reliability; (2) multi-minimal-spanning-tree-based pose graph construction guided by these ranks; and (3) connectivity-aware score modulation that reinforces weak regions and reduces graph diameter. This globally informed initialization yields more reliable and compact pose graphs, improving reconstruction accuracy in sparse and high-speed settings and outperforming SOTA retrieval methods on ambiguous scenes. The ode and trained models are available at https://github.com/weitong8591/global_edge_prior.","authors":["Tong Wei","Giorgos Tolias","Jiri Matas","Daniel Barath"],"pdf_url":"","comment":"accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2507.06593v3","updated":"2026-02-25T14:43:13Z","published":"2025-07-09T07:09:42Z","title":"Capturing Stable HDR Videos Using a Dual-Camera System","summary":"High Dynamic Range (HDR) video acquisition using the alternating exposure (AE) paradigm has garnered significant attention due to its cost-effectiveness with a single consumer camera. However, despite progress driven by deep neural networks, these methods remain prone to temporal flicker in real-world applications due to inter-frame exposure inconsistencies. To address this challenge while maintaining the cost-effectiveness of the AE paradigm, we propose a novel learning-based HDR video generation solution. Specifically, we propose a dual-stream HDR video generation paradigm that decouples temporal luminance anchoring from exposure-variant detail reconstruction, overcoming the inherent limitations of the AE paradigm. To support this, we design an asynchronous dual-camera system (DCS), which enables independent exposure control across two cameras, eliminating the need for synchronization typically required in traditional multi-camera setups. Furthermore, an exposure-adaptive fusion network (EAFNet) is formulated for the DCS system. EAFNet integrates a pre-alignment subnetwork that aligns features across varying exposures, ensuring robust feature extraction for subsequent fusion, an asymmetric cross-feature fusion subnetwork that emphasizes reference-based attention to effectively merge these features across exposures, and a reconstruction subnetwork to mitigate ghosting artifacts and preserve fine details. Extensive experimental evaluations demonstrate that the proposed method achieves state-of-the-art performance across various datasets, showing the remarkable potential of our solution in HDR video reconstruction. The codes and data captured by DCS will be available at https://zqqqyu.github.io/DCS-HDR/.","authors":["Qianyu Zhang","Bolun Zheng","Lingyu Zhu","Hangjia Pan","Zunjie Zhu","Zongpeng Li","Shiqi Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.07982v4","updated":"2026-02-25T14:42:58Z","published":"2025-03-11T02:34:33Z","title":"TRACE: Your Diffusion Model is Secretly an Instance Edge Detector","summary":"High-quality instance and panoptic segmentation has traditionally relied on dense instance-level annotations such as masks, boxes, or points, which are costly, inconsistent, and difficult to scale. Unsupervised and weakly-supervised approaches reduce this burden but remain constrained by semantic backbone constraints and human bias, often producing merged or fragmented outputs. We present TRACE (TRAnsforming diffusion Cues to instance Edges), showing that text-to-image diffusion models secretly function as instance edge annotators. TRACE identifies the Instance Emergence Point (IEP) where object boundaries first appear in self-attention maps, extracts boundaries through Attention Boundary Divergence (ABDiv), and distills them into a lightweight one-step edge decoder. This design removes the need for per-image diffusion inversion, achieving 81x faster inference while producing sharper and more connected boundaries. On the COCO benchmark, TRACE improves unsupervised instance segmentation by +5.1 AP, and in tag-supervised panoptic segmentation it outperforms point-supervised baselines by +1.7 PQ without using any instance-level labels. These results reveal that diffusion models encode hidden instance boundary priors, and that decoding these signals offers a practical and scalable alternative to costly manual annotation. Project Page: https://shjo-april.github.io/TRACE/","authors":["Sanghyun Jo","Ziseok Lee","Wooyeol Lee","Jonghyun Choi","Jaesik Park","Kyungsu Kim"],"pdf_url":"","comment":"Accepted to ICLR 2026 (Oral)"},{"id":"http://arxiv.org/abs/2602.21956v1","updated":"2026-02-25T14:38:47Z","published":"2026-02-25T14:38:47Z","title":"Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation","summary":"Text Image Machine Translation (TIMT) aims to translate text embedded in images in the source-language into target-language, requiring synergistic integration of visual perception and linguistic understanding. Existing TIMT methods, whether cascaded pipelines or end-to-end multimodal large language models (MLLMs),struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, resulting in text omission, semantic drift, and contextual inconsistency. To address these challenges, we propose GLoTran, a global-local dual visual perception framework for MLLM-based TIMT. GLoTran integrates a low-resolution global image with multi-scale region-level text image slices under an instruction-guided alignment strategy, conditioning MLLMs to maintain scene-level contextual consistency while faithfully capturing fine-grained textual details. Moreover, to realize this dual-perception paradigm, we construct GLoD, a large-scale text-rich TIMT dataset comprising 510K high-resolution global-local image-text pairs covering diverse real-world scenarios. Extensive experiments demonstrate that GLoTran substantially improves translation completeness and accuracy over state-of-the-art MLLMs, offering a new paradigm for fine-grained TIMT under high-resolution and text-rich conditions.","authors":["Junxin Lu","Tengfei Song","Zhanglin Wu","Pengfei Li","Xiaowei Liang","Hui Yang","Kun Chen","Ning Xie","Yunfei Lu","Jing Zhao","Shiliang Sun","Daimeng Wei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21952v1","updated":"2026-02-25T14:34:50Z","published":"2026-02-25T14:34:50Z","title":"MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving","summary":"Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.","authors":["Lingjun Zhang","Yujian Yuan","Changjie Wu","Xinyuan Chang","Xin Cai","Shuang Zeng","Linzhe Shi","Sijin Wang","Hang Zhang","Mu Xu"],"pdf_url":"","comment":"CVPR2026; Yujian Yuan and Lingjun Zhang contributed equally with random order"},{"id":"http://arxiv.org/abs/2602.21944v1","updated":"2026-02-25T14:28:57Z","published":"2026-02-25T14:28:57Z","title":"Learning to Fuse and Reconstruct Multi-View Graphs for Diabetic Retinopathy Grading","summary":"Diabetic retinopathy (DR) is one of the leading causes of vision loss worldwide, making early and accurate DR grading critical for timely intervention. Recent clinical practices leverage multi-view fundus images for DR detection with a wide coverage of the field of view (FOV), motivating deep learning methods to explore the potential of multi-view learning for DR grading. However, existing methods often overlook the inter-view correlations when fusing multi-view fundus images, failing to fully exploit the inherent consistency across views originating from the same patient. In this work, we present MVGFDR, an end-to-end Multi-View Graph Fusion framework for DR grading. Different from existing methods that directly fuse visual features from multiple views, MVGFDR is equipped with a novel Multi-View Graph Fusion (MVGF) module to explicitly disentangle the shared and view-specific visual features. Specifically, MVGF comprises three key components: (1) Multi-view Graph Initialization, which constructs visual graphs via residual-guided connections and employs Discrete Cosine Transform (DCT) coefficients as frequency-domain anchors; (2) Multi-view Graph Fusion, which integrates selective nodes across multi-view graphs based on frequency-domain relevance to capture complementary view-specific information; and (3) Masked Cross-view Reconstruction, which leverages masked reconstruction of shared information across views to facilitate view-invariant representation learning. Extensive experimental results on MFIDDR, by far the largest multi-view fundus image dataset, demonstrate the superiority of our proposed approach over existing state-of-the-art approaches in diabetic retinopathy grading.","authors":["Haoran Li","Yuxin Lin","Huan Wang","Xiaoling Luo","Qi Zhu","Jiahua Shi","Huaming Chen","Bo Du","Johan Barthelemy","Zongyan Xue","Jun Shen","Yong Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21943v1","updated":"2026-02-25T14:26:18Z","published":"2026-02-25T14:26:18Z","title":"Mobile-Ready Automated Triage of Diabetic Retinopathy Using Digital Fundus Images","summary":"Diabetic Retinopathy (DR) is a major cause of vision impairment worldwide. However, manual diagnosis is often time-consuming and prone to errors, leading to delays in screening. This paper presents a lightweight automated deep learning framework for efficient assessment of DR severity from digital fundus images. We use a MobileNetV3 architecture with a Consistent Rank Logits (CORAL) head to model the ordered progression of disease while maintaining computational efficiency for resource-constrained environments. The model is trained and validated on a combined dataset of APTOS 2019 and IDRiD images using a preprocessing pipeline including circular cropping and illumination normalization. Extensive experiments including 3-fold cross-validation and ablation studies demonstrate strong performance. The model achieves a Quadratic Weighted Kappa (QWK) score of 0.9019 and an accuracy of 80.03 percent. Additionally, we address real-world deployment challenges through model calibration to reduce overconfidence and optimization for mobile devices. The proposed system provides a scalable and practical tool for early-stage diabetic retinopathy screening.","authors":["Aadi Joshi","Manav S. Sharma","Vijay Uttam Rathod","Ashlesha Sawant","Prajakta Musale","Asmita B. Kalamkar"],"pdf_url":"","comment":"Presented at ICCI 2025. 11 pages, 2 figures. MobileNetV3 + CORAL-based lightweight model for diabetic retinopathy severity classification with mobile deployment"},{"id":"http://arxiv.org/abs/2602.21942v1","updated":"2026-02-25T14:26:16Z","published":"2026-02-25T14:26:16Z","title":"Directed Ordinal Diffusion Regularization for Progression-Aware Diabetic Retinopathy Grading","summary":"Diabetic Retinopathy (DR) progresses as a continuous and irreversible deterioration of the retina, following a well-defined clinical trajectory from mild to severe stages. However, most existing ordinal regression approaches model DR severity as a set of static, symmetric ranks, capturing relative order while ignoring the inherent unidirectional nature of disease progression. As a result, the learned feature representations may violate biological plausibility, allowing implausible proximity between non-consecutive stages or even reverse transitions. To bridge this gap, we propose Directed Ordinal Diffusion Regularization (D-ODR), which explicitly models the feature space as a directed flow by constructing a progression-constrained directed graph that strictly enforces forward disease evolution. By performing multi-scale diffusion on this directed structure, D-ODR imposes penalties on score inversions along valid progression paths, thereby effectively preventing the model from learning biologically inconsistent reverse transitions. This mechanism aligns the feature representation with the natural trajectory of DR worsening. Extensive experiments demonstrate that D-ODR yields superior grading performance compared to state-of-the-art ordinal regression and DR-specific grading methods, offering a more clinically reliable assessment of disease severity. Our code is available on https://github.com/HovChen/D-ODR.","authors":["Huangwei Chen","Junhao Jia","Ruocheng Li","Cunyuan Yang","Wu Li","Xiaotao Pang","Yifei Chen","Haishuai Wang","Jiajun Bu","Lei Wu"],"pdf_url":"","comment":"3 figures"},{"id":"http://arxiv.org/abs/2411.04997v6","updated":"2026-02-25T14:18:21Z","published":"2024-11-07T18:59:16Z","title":"LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation","summary":"CLIP is a seminal multimodal model that maps images and text into a shared representation space through contrastive learning on billions of image-caption pairs. Inspired by the rapid progress of large language models (LLMs), we investigate how the superior linguistic understanding and broad world knowledge of LLMs can further strengthen CLIP, particularly in handling long and complex captions. We introduce an efficient fine-tuning framework that embeds an LLM into a pretrained CLIP while incurring nearly the same training cost as standard CLIP fine-tuning. Our method first converts the LLM into an embedding-compatible form for the CLIP setting, and then couples it with the pretrained CLIP vision encoder through a lightweight adaptor trained on only a few million image-caption pairs. With this strategy, we achieve large performance gains without large-scale retraining, outperforming state-of-the-art CLIP variants such as EVA02 and SigLIP-2. The LLM-enhanced CLIP delivers consistent improvements across a wide range of downstream tasks, including linear-probe classification, zero-shot image-text retrieval with both short and long captions (in English and other languages), zero-shot and supervised image segmentation, object detection, and serving as a tokenizer backbone for multimodal large-model benchmarks. Code and models are available at: https://aka.ms/llm2clip","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Usman Naseem","Chunyu Wang","Chunyu Wang","Qi Dai","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu","Liang Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21935v1","updated":"2026-02-25T14:17:54Z","published":"2026-02-25T14:17:54Z","title":"A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography","summary":"Coronary artery calcium (CAC) scoring is a key predictor of cardiovascular risk, but it relies on ECG-gated CT scans, restricting its use to specialized cardiac imaging settings. We introduce an automated framework for CAC detection and lesion-specific Agatston scoring that operates across both gated and non-gated CT scans. At its core is CARD-ViT, a self-supervised Vision Transformer trained exclusively on gated CT data using DINO. Without any non-gated training data, our framework achieves 0.707 accuracy and a Cohen's kappa of 0.528 on the Stanford non-gated dataset, matching models trained directly on non-gated scans. On gated test sets, the framework achieves 0.910 accuracy with Cohen's kappa scores of 0.871 and 0.874 across independent datasets, demonstrating robust risk stratification. These results demonstrate the feasibility of cross-domain CAC scoring from gated to non-gated domains, supporting scalable cardiovascular screening in routine chest imaging without additional scans or annotations.","authors":["Mahmut S. Gokmen","Moneera N. Haque","Steve W. Leung","Caroline N. Leach","Seth Parker","Stephen B. Hobbs","Vincent L. Sorrell","W. Brent Seales","V. K. Cody Bumgardner"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21929v1","updated":"2026-02-25T14:09:03Z","published":"2026-02-25T14:09:03Z","title":"Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context","summary":"Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.","authors":["JiaKui Hu","Jialun Liu","Liying Yang","Xinliang Zhang","Kaiwen Li","Shuang Zeng","Yuanwei Li","Haibin Huang","Chi Zhang","Yanye Lu"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21919v1","updated":"2026-02-25T13:55:06Z","published":"2026-02-25T13:55:06Z","title":"Learning in the Null Space: Small Singular Values for Continual Learning","summary":"Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer's input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.","authors":["Cuong Anh Pham","Praneeth Vepakomma","Samuel Horváth"],"pdf_url":"","comment":"17 pages, accepted as Oral presentation at the Third Conference on Parsimony and Learning (CPAL 2026)"},{"id":"http://arxiv.org/abs/2601.08026v3","updated":"2026-02-25T13:52:52Z","published":"2026-01-12T21:57:52Z","title":"FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures","summary":"Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.","authors":["Jifeng Song","Arun Das","Pan Wang","Hui Ji","Kun Zhao","Yufei Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21917v1","updated":"2026-02-25T13:45:50Z","published":"2026-02-25T13:45:50Z","title":"Scan Clusters, Not Pixels: A Cluster-Centric Paradigm for Efficient Ultra-high-definition Image Restoration","summary":"Ultra-High-Definition (UHD) image restoration is trapped in a scalability crisis: existing models, bound to pixel-wise operations, demand unsustainable computation. While state space models (SSMs) like Mamba promise linear complexity, their pixel-serial scanning remains a fundamental bottleneck for the millions of pixels in UHD content. We ask: must we process every pixel to understand the image? This paper introduces C$^2$SSM, a visual state space model that breaks this taboo by shifting from pixel-serial to cluster-serial scanning. Our core discovery is that the rich feature distribution of a UHD image can be distilled into a sparse set of semantic centroids via a neural-parameterized mixture model. C$^2$SSM leverages this to reformulate global modeling into a novel dual-path process: it scans and reasons over a handful of cluster centers, then diffuses the global context back to all pixels through a principled similarity distribution, all while a lightweight modulator preserves fine details. This cluster-centric paradigm achieves a decisive leap in efficiency, slashing computational costs while establishing new state-of-the-art results across five UHD restoration tasks. More than a solution, C$^2$SSM charts a new course for efficient large-scale vision: scan clusters, not pixels.","authors":["Chen Wu","Ling Wang","Zhuoran Zheng","Yuning Cui","Zhixiong Yang","Xiangyu Chen","Yue Zhang","Weidong Jiang","Jingyuan Xia"],"pdf_url":"","comment":"Aceepted by CVPR26"},{"id":"http://arxiv.org/abs/2602.21915v1","updated":"2026-02-25T13:43:52Z","published":"2026-02-25T13:43:52Z","title":"Protein Graph Neural Networks for Heterogeneous Cryo-EM Reconstruction","summary":"We present a geometry-aware method for heterogeneous single-particle cryogenic electron microscopy (cryo-EM) reconstruction that predicts atomic backbone conformations. To incorporate protein-structure priors, we represent the backbone as a graph and use a graph neural network (GNN) autodecoder that maps per-image latent variables to 3D displacements of a template conformation. The objective combines a data-discrepancy term based on a differentiable cryo-EM forward model with geometric regularization, and it supports unknown orientations via ellipsoidal support lifting (ESL) pose estimation. On synthetic datasets derived from molecular dynamics trajectories, the proposed GNN achieves higher accuracy compared to a multilayer perceptron (MLP) of comparable size, highlighting the benefits of a geometry-informed inductive bias.","authors":["Jonathan Krook","Axel Janson","Joakim andén","Melanie Weber","Ozan Öktem"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21905v1","updated":"2026-02-25T13:36:10Z","published":"2026-02-25T13:36:10Z","title":"TIRAuxCloud: A Thermal Infrared Dataset for Day and Night Cloud Detection","summary":"Clouds are a major obstacle in Earth observation, limiting the usability and reliability of critical remote sensing applications such as fire disaster response, urban heat island monitoring, and snow and ice cover mapping. Therefore, the ability to detect clouds 24/7 is of paramount importance. While visible and near-infrared bands are effective for daytime cloud detection, their dependence on solar illumination makes them unsuitable for nighttime monitoring. In contrast, thermal infrared (TIR) imagery plays a crucial role in detecting clouds at night, when sunlight is absent. Due to their generally lower temperatures, clouds emit distinct thermal signatures that are detectable in TIR bands. Despite this, accurate nighttime cloud detection remains challenging due to limited spectral information and the typically lower spatial resolution of TIR imagery. To address these challenges, we present TIRAuxCloud, a multi-modal dataset centered around thermal spectral data to facilitate cloud segmentation under both daytime and nighttime conditions. The dataset comprises a unique combination of multispectral data (TIR, optical, and near-infrared bands) from Landsat and VIIRS, aligned with auxiliary information layers. Elevation, land cover, meteorological variables, and cloud-free reference images are included to help reduce surface-cloud ambiguity and cloud formation uncertainty. To overcome the scarcity of manual cloud labels, we include a large set of samples with automated cloud masks and a smaller manually annotated subset to further evaluate and improve models. Comprehensive benchmarks are presented to establish performance baselines through supervised and transfer learning, demonstrating the dataset's value in advancing the development of innovative methods for day and night time cloud detection.","authors":["Alexis Apostolakis","Vasileios Botsos","Niklas Wölki","Andrea Spichtinger","Nikolaos Ioannis Bountos","Ioannis Papoutsis","Panayiotis Tsanakas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21904v1","updated":"2026-02-25T13:34:56Z","published":"2026-02-25T13:34:56Z","title":"UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing","summary":"Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estimation and the potential for color prediction. Our model achieves substantial improvements in keypoint accuracy over conventional methods. Furthermore, we leverage our predicted keypoints in the perception pipeline and evaluate the end-to-end autonomous system. Our results show high-quality performance across all metrics, highlighting the effectiveness of this approach and its potential for adoption in competitive autonomous racing systems.","authors":["Mariia Baidachna","James Carty","Aidan Ferguson","Joseph Agrane","Varad Kulkarni","Aubrey Agub","Michael Baxendale","Aaron David","Rachel Horton","Elliott Atkinson"],"pdf_url":"","comment":"8 pages, 9 figures. Accepted to ICCV End-to-End 3D Learning Workshop 2025 and presented as a poster; not included in the final proceedings due to a conference administrative error"},{"id":"http://arxiv.org/abs/2602.21893v1","updated":"2026-02-25T13:21:49Z","published":"2026-02-25T13:21:49Z","title":"EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion","summary":"Accurate depth estimation plays a critical role in the navigation of endoscopic surgical robots, forming the foundation for 3D reconstruction and safe instrument guidance. Fine-tuning pretrained models heavily relies on endoscopic surgical datasets with precise depth annotations. While existing self-supervised depth estimation techniques eliminate the need for accurate depth annotations, their performance degrades in environments with weak textures and variable lighting, leading to sparse reconstruction with invalid depth estimation. Depth completion using sparse depth maps can mitigate these issues and improve accuracy. Despite the advances in depth completion techniques in general fields, their application in endoscopy remains limited. To overcome these limitations, we propose EndoDDC, an endoscopy depth completion method that integrates images, sparse depth information with depth gradient features, and optimizes depth maps through a diffusion model, addressing the issues of weak texture and light reflection in endoscopic environments. Extensive experiments on two publicly available endoscopy datasets show that our approach outperforms state-of-the-art models in both depth accuracy and robustness. This demonstrates the potential of our method to reduce visual errors in complex endoscopic environments. Our code will be released at https://github.com/yinheng-lin/EndoDDC.","authors":["Yinheng Lin","Yiming Huang","Beilei Cui","Long Bai","Huxin Gao","Hongliang Ren","Jiewen Lai"],"pdf_url":"","comment":"Accepted by ICRA 2026"},{"id":"http://arxiv.org/abs/2503.05236v2","updated":"2026-02-25T13:17:30Z","published":"2025-03-07T08:36:05Z","title":"Unified Reward Model for Multimodal Understanding and Generation","summary":"Recent advances in human preference alignment have significantly improved multimodal generation and understanding. A key approach is to train reward models that provide supervision signals for preference optimization. However, existing reward models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that a reward model that jointly learning to assess multiple vision tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment. It supports both pairwise ranking and pointwise scoring, providing effective reward signals for vision model preference alignment. Specifically, (1) we first train UnifiedReward on our constructed large-scale human preference dataset, which covers both image and video generation/understanding tasks. (2) Then, we leverage it to automatically construct high-quality pairwise preference data from vision models by progressively filtering their outputs through our two-stage strategy, i.e., pair ranking and point sifting. (3) Finally, we use these data to align vision models with human preferences via Direct Preference Optimization (DPO). Experimental results show that jointly learning to assess diverse visual tasks yields substantial mutual benefits. We further apply our pipeline to both vision understanding and generation, achieving consistent improvements across each domain.","authors":["Yibin Wang","Yuhang Zang","Hao Li","Cheng Jin","Jiaqi Wang"],"pdf_url":"","comment":"project page: https://codegoat24.github.io/UnifiedReward/"},{"id":"http://arxiv.org/abs/2505.19610v3","updated":"2026-02-25T13:06:15Z","published":"2025-05-26T07:23:00Z","title":"JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models","summary":"Vision-Language Models (VLMs) exhibit impressive performance, yet the integration of powerful vision encoders has significantly broadened their attack surface, rendering them increasingly susceptible to jailbreak attacks. However, lacking well-defined attack objectives, existing jailbreak methods often struggle with gradient-based strategies prone to local optima and lacking precise directional guidance, and typically decouple visual and textual modalities, thereby limiting their effectiveness by neglecting crucial cross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK) framework, we posit that VLMs encode safety-relevant information within their internal fusion-layer representations, revealing an implicit safety decision boundary in the latent space. This motivates exploiting boundary to steer model behavior. Accordingly, we propose JailBound, a novel latent space jailbreak framework comprising two stages: (1) Safety Boundary Probing, which addresses the guidance issue by approximating decision boundary within fusion layer's latent space, thereby identifying optimal perturbation directions towards the target region; and (2) Safety Boundary Crossing, which overcomes the limitations of decoupled approaches by jointly optimizing adversarial perturbations across both image and text inputs. This latter stage employs an innovative mechanism to steer the model's internal state towards policy-violating outputs while maintaining cross-modal semantic consistency. Extensive experiments on six diverse VLMs demonstrate JailBound's efficacy, achieves 94.32% white-box and 67.28% black-box attack success averagely, which are 6.17% and 21.13% higher than SOTA methods, respectively. Our findings expose a overlooked safety risk in VLMs and highlight the urgent need for more robust defenses. Warning: This paper contains potentially sensitive, harmful and offensive content.","authors":["Jiaxin Song","Yixu Wang","Jie Li","Rui Yu","Yan Teng","Xingjun Ma","Yingchun Wang"],"pdf_url":"","comment":"The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2602.21877v1","updated":"2026-02-25T13:02:35Z","published":"2026-02-25T13:02:35Z","title":"How to Take a Memorable Picture? Empowering Users with Actionable Feedback","summary":"Image memorability, i.e., how likely an image is to be remembered, has traditionally been studied in computer vision either as a passive prediction task, with models regressing a scalar score, or with generative methods altering the visual input to boost the image likelihood of being remembered. Yet, none of these paradigms supports users at capture time, when the crucial question is how to improve a photo memorability. We introduce the task of Memorability Feedback (MemFeed), where an automated model should provide actionable, human-interpretable guidance to users with the goal to enhance an image future recall. We also present MemCoach, the first approach designed to provide concrete suggestions in natural language for memorability improvement (e.g., \"emphasize facial expression,\" \"bring the subject forward\"). Our method, based on Multimodal Large Language Models (MLLMs), is training-free and employs a teacher-student steering strategy, aligning the model internal activations toward more memorable patterns learned from a teacher model progressing along least-to-most memorable samples. To enable systematic evaluation on this novel task, we further introduce MemBench, a new benchmark featuring sequence-aligned photoshoots with annotated memorability scores. Our experiments, considering multiple MLLMs, demonstrate the effectiveness of MemCoach, showing consistently improved performance over several zero-shot models. The results indicate that memorability can not only be predicted but also taught and instructed, shifting the focus from mere prediction to actionable feedback for human creators.","authors":["Francesco Laiti","Davide Talon","Jacopo Staiano","Elisa Ricci"],"pdf_url":"","comment":"Accepted @ CVPR 2026. Project page: https://laitifranz.github.io/MemCoach/"},{"id":"http://arxiv.org/abs/2510.13759v2","updated":"2026-02-25T13:01:32Z","published":"2025-10-15T17:10:35Z","title":"Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark","summary":"Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.","authors":["Kai Zou","Ziqi Huang","Yuhao Dong","Shulin Tian","Dian Zheng","Hongbo Liu","Jingwen He","Bin Liu","Yu Qiao","Ziwei Liu"],"pdf_url":"","comment":"Equal contributions from frst three authors. Project page: https://vchitect.github.io/Uni-MMMU-Project/ Code: https://github.com/vchitect/Uni-MMMU"},{"id":"http://arxiv.org/abs/2602.21873v1","updated":"2026-02-25T12:57:45Z","published":"2026-02-25T12:57:45Z","title":"GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task","summary":"Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.","authors":["Shiwei Lu","Yuhang He","Jiashuo Li","Qiang Wang","Yihong Gong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21864v1","updated":"2026-02-25T12:45:45Z","published":"2026-02-25T12:45:45Z","title":"DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs","summary":"Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.","authors":["Yanbin Wei","Jiangyue Yan","Chun Kang","Yang Chen","Hua Liu","James Kwok","Yu Zhang"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2507.14899v3","updated":"2026-02-25T12:39:48Z","published":"2025-07-20T10:23:22Z","title":"InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis","summary":"Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address these shortcomings, this paper proposes InsightX Agent, a novel LMM-based agentic framework designed to deliver reliable, interpretable, and interactive X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect region proposals from multi-scale feature maps and sparsifies them through Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in X-ray images while maintaining computational efficiency. The EGR tool guides the LMM agent through a chain-of-thought-inspired review process, incorporating context assessment, individual defect analysis, false positive elimination, confidence recalibration and quality assurance to validate and refine the SDMSD's initial proposals. By strategically employing and intelligently using tools, InsightX Agent moves beyond passive data processing to active reasoning, enhancing diagnostic reliability and providing interpretations that integrate diverse information sources. Experimental evaluations on the GDXray+ dataset demonstrate that InsightX Agent not only achieves a high object detection F1-score of 96.54\\% but also offers significantly improved interpretability and trustworthiness in its analyses, highlighting the transformative potential of LMM-based agentic frameworks for industrial inspection tasks.","authors":["Jiale Liu","Huan Wang","Yue Zhang","Xiaoyu Luo","Jiaxiang Hu","Zhiliang Liu","Min Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10359v2","updated":"2026-02-25T12:31:43Z","published":"2026-02-10T23:08:06Z","title":"Beyond Calibration: Confounding Pathology Limits Foundation Model Specificity in Abdominal Trauma CT","summary":"Purpose: Translating foundation models into clinical practice requires evaluating their performance under compound distribution shift, where severe class imbalance coexists with heterogeneous imaging appearances. This challenge is relevant for traumatic bowel injury, a rare but high-mortality diagnosis. We investigated whether specificity deficits in foundation models are associated with heterogeneity in the negative class. Methods: This retrospective study used the multi-institutional, RSNA Abdominal Traumatic Injury CT dataset (2019-2023), comprising scans from 23 centres. Two foundation models (MedCLIP, zero-shot; RadDINO, linear probe) were compared against three task-specific approaches (CNN, Transformer, Ensemble). Models were trained on 3,147 patients (2.3% bowel injury prevalence) and evaluated on an enriched 100-patient test set. To isolate negative-class effects, specificity was assessed in patients without bowel injury who had concurrent solid organ injury (n=58) versus no abdominal pathology (n=50). Results: Foundation models achieved equivalent discrimination to task-specific models (AUC, 0.64-0.68 versus 0.58-0.64) with higher sensitivity (79-91% vs 41-74%) but lower specificity (33-50% vs 50-88%). All models demonstrated high specificity in patients without abdominal pathology (84-100%). When solid organ injuries were present, specificity declined substantially for foundation models (50-51 percentage points) compared with smaller reductions of 12-41 percentage points for task-specific models. Conclusion: Foundation models matched task-specific discrimination without task-specific training, but their specificity deficits were driven primarily by confounding negative-class heterogeneity rather than prevalence alone. Susceptibility to negative-class heterogeneity decreased progressively with labelled training, suggesting adaptation is required before clinical implementation.","authors":["Jineel H Raythatha","Shuchang Ye","Jeremy Hsu","Jinman Kim"],"pdf_url":"","comment":"26 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2602.21855v1","updated":"2026-02-25T12:30:54Z","published":"2026-02-25T12:30:54Z","title":"Understanding Annotation Error Propagation and Learning an Adaptive Policy for Expert Intervention in Barrett's Video Segmentation","summary":"Accurate annotation of endoscopic videos is essential yet time-consuming, particularly for challenging datasets such as dysplasia in Barrett's esophagus, where the affected regions are irregular and lack clear boundaries. Semi-automatic tools like Segment Anything Model 2 (SAM2) can ease this process by propagating annotations across frames, but small errors often accumulate and reduce accuracy, requiring expert review and correction. To address this, we systematically study how annotation errors propagate across different prompt types, namely masks, boxes, and points, and propose Learning-to-Re-Prompt (L2RP), a cost-aware framework that learns when and where to seek expert input. By tuning a human-cost parameter, our method balances annotation effort and segmentation accuracy. Experiments on a private Barrett's dysplasia dataset and the public SUN-SEG benchmark demonstrate improved temporal consistency and superior performance over baseline strategies.","authors":["Lokesha Rasanjalee","Jin Lin Tan","Dileepa Pitawela","Rajvinder Singh","Hsiang-Ting Chen"],"pdf_url":"","comment":"Accepted at IEEE ISBI 2026"},{"id":"http://arxiv.org/abs/2501.16443v2","updated":"2026-02-25T12:27:06Z","published":"2025-01-27T19:07:06Z","title":"Object-Centric World Models from Few-Shot Annotations for Sample-Efficient Reinforcement Learning","summary":"While deep reinforcement learning (RL) from pixels has achieved remarkable success, its sample inefficiency remains a critical limitation for real-world applications. Model-based RL (MBRL) addresses this by learning a world model to generate simulated experience, but standard approaches that rely on pixel-level reconstruction losses often fail to capture small, task-critical objects in complex, dynamic scenes. We posit that an object-centric (OC) representation can direct model capacity toward semantically meaningful entities, improving dynamics prediction and sample efficiency. In this work, we introduce OC-STORM, an object-centric MBRL framework that enhances a learned world model with object representations extracted by a pretrained segmentation network. By conditioning on a minimal number of annotated frames, OC-STORM learns to track decision-relevant object dynamics and inter-object interactions without extensive labeling or access to privileged information. Empirical results demonstrate that OC-STORM significantly outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the visually complex game Hollow Knight. Our findings underscore the potential of integrating OC priors into MBRL for complex visual domains. Project page: https://oc-storm.weipuzhang.com","authors":["Weipu Zhang","Adam Jelley","Trevor McInroe","Amos Storkey","Gang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21849v1","updated":"2026-02-25T12:26:26Z","published":"2026-02-25T12:26:26Z","title":"Meta-FC: Meta-Learning with Feature Consistency for Robust and Generalizable Watermarking","summary":"Deep learning-based watermarking has made remarkable progress in recent years. To achieve robustness against various distortions, current methods commonly adopt a training strategy where a \\underline{\\textbf{s}}ingle \\underline{\\textbf{r}}andom \\underline{\\textbf{d}}istortion (SRD) is chosen as the noise layer in each training batch. However, the SRD strategy treats distortions independently within each batch, neglecting the inherent relationships among different types of distortions and causing optimization conflicts across batches. As a result, the robustness and generalizability of the watermarking model are limited. To address this issue, we propose a novel training strategy that enhances robustness and generalization via \\underline{\\textbf{meta}}-learning with \\underline{\\textbf{f}}eature \\underline{\\textbf{c}}onsistency (Meta-FC). Specifically, we randomly sample multiple distortions from the noise pool to construct a meta-training task, while holding out one distortion as a simulated ``unknown'' distortion for the meta-testing phase. Through meta-learning, the model is encouraged to identify and utilize neurons that exhibit stable activations across different types of distortions, mitigating the optimization conflicts caused by the random sampling of diverse distortions in each batch. To further promote the transformation of stable activations into distortion-invariant representations, we introduce a feature consistency loss that constrains the decoded features of the same image subjected to different distortions to remain consistent. Extensive experiments demonstrate that, compared to the SRD training strategy, Meta-FC improves the robustness and generalization of various watermarking models by an average of 1.59\\%, 4.71\\%, and 2.38\\% under high-intensity, combined, and unknown distortions.","authors":["Yuheng Li","Weitong Chen","Chengcheng Zhu","Jiale Zhang","Chunpeng Ge","Di Wu","Guodong Long"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.07477v2","updated":"2026-02-25T12:17:47Z","published":"2025-09-09T08:02:10Z","title":"MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification","summary":"Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNetV2-S, while improving interpretability: MedicalPatchNet demonstrates improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet","authors":["Patrick Wienholt","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2510.09256v2","updated":"2026-02-25T12:16:45Z","published":"2025-10-10T10:53:33Z","title":"Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy","summary":"To determine whether using discrete semantic entropy (DSE) to reject questions likely to generate hallucinations can improve the accuracy of black-box vision-language models (VLMs) in radiologic image based visual question answering (VQA). This retrospective study evaluated DSE using two publicly available, de-identified datasets: the VQA-Med 2019 benchmark (500 images with clinical questions and short-text answers) and a diagnostic radiology dataset (206 cases: 60 computed tomography scans, 60 magnetic resonance images, 60 radiographs, 26 angiograms) with corresponding ground-truth diagnoses. GPT-4o and GPT-4.1 (Generative Pretrained Transformer; OpenAI) answered each question 15 times using a temperature of 1.0. Baseline accuracy was determined using low-temperature answers (temperature 0.1). Meaning-equivalent responses were grouped using bidirectional entailment checks, and DSE was computed from the relative frequencies of the resulting semantic clusters. Accuracy was recalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and 95% confidence intervals were obtained using bootstrap resampling and a Bonferroni-corrected threshold of p < .004 for statistical significance. Across 706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for GPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on the remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and 63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains were observed across both datasets and largely remained statistically significant after Bonferroni correction. DSE enables reliable hallucination detection in black-box VLMs by quantifying semantic inconsistency. This method significantly improves diagnostic answer accuracy and offers a filtering strategy for clinical VLM applications.","authors":["Patrick Wienholt","Sophie Caselitz","Robert Siepmann","Philipp Bruners","Keno Bressem","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":"Code is available: https://github.com/TruhnLab/VisionSemanticEntropy"},{"id":"http://arxiv.org/abs/2601.13879v3","updated":"2026-02-25T12:15:14Z","published":"2026-01-20T11:45:38Z","title":"Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring","summary":"While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.","authors":["Dongxu Zhang","Yiding Sun","Cheng Tan","Wenbiao Yan","Ning Yang","Jihua Zhu","Haijun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21835v1","updated":"2026-02-25T12:08:53Z","published":"2026-02-25T12:08:53Z","title":"UniVBench: Towards Unified Evaluation for Video Foundation Models","summary":"Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.","authors":["Jianhui Wei","Xiaotian Zhang","Yichen Li","Yuan Wang","Yan Zhang","Ziyi Chen","Zhihang Tang","Wei Xu","Zuozhu Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21829v1","updated":"2026-02-25T12:01:05Z","published":"2026-02-25T12:01:05Z","title":"StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles","summary":"Visual storytelling models that correctly ground entities in images may still hallucinate semantic relationships, generating incorrect dialogue attribution, character interactions, or emotional states. We introduce StoryMovie, a dataset of 1,757 stories aligned with movie scripts and subtitles through LCS matching. Our alignment pipeline synchronizes screenplay dialogue with subtitle timestamps, enabling dialogue attribution by linking character names from scripts to temporal positions from subtitles. Using this aligned content, we generate stories that maintain visual grounding tags while incorporating authentic character names, dialogue, and relationship dynamics. We fine-tune Qwen Storyteller3 on this dataset, building on prior work in visual grounding and entity re-identification. Evaluation using DeepSeek V3 as judge shows that Storyteller3 achieves an 89.9% win rate against base Qwen2.5-VL 7B on subtitle alignment. Compared to Storyteller, trained without script grounding,\n  Storyteller3 achieves 48.5% versus 38.0%, confirming that semantic alignment progressively improves dialogue attribution beyond visual grounding alone.","authors":["Daniel Oliveira","David Martins de Matos"],"pdf_url":"","comment":"15 pages, submitted to Journal of Visual Communication and Image Representation"},{"id":"http://arxiv.org/abs/2602.16898v3","updated":"2026-02-25T11:49:07Z","published":"2026-02-18T21:28:56Z","title":"MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation","summary":"Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. MALLVI presents a Multi Agent Large Language and Vision framework that enables closed-loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVI generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step. Rather than using a single model, MALLVI coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning. Experiments in simulation and real-world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks. Code available at https://github.com/iman1234ahmadi/MALLVI .","authors":["Iman Ahmadi","Mehrshad Taji","Arad Mahdinezhad Kashani","AmirHossein Jadidi","Saina Kashani","Babak Khalaj"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21820v1","updated":"2026-02-25T11:47:26Z","published":"2026-02-25T11:47:26Z","title":"Joint Shadow Generation and Relighting via Light-Geometry Interaction Maps","summary":"We propose Light-Geometry Interaction (LGI) maps, a novel representation that encodes light-aware occlusion from monocular depth. Unlike ray tracing, which requires full 3D reconstruction, LGI captures essential light-shadow interactions reliably and accurately, computed from off-the-shelf 2.5D depth map predictions. LGI explicitly ties illumination direction to geometry, providing a physics-inspired prior that constrains generative models. Without such prior, these models often produce floating shadows, inconsistent illumination, and implausible shadow geometry. Building on this representation, we propose a unified pipeline for joint shadow generation and relighting - unlike prior methods that treat them as disjoint tasks - capturing the intrinsic coupling of illumination and shadowing essential for modeling indirect effects. By embedding LGI into a bridge-matching generative backbone, we reduce ambiguity and enforce physically consistent light-shadow reasoning. To enable effective training, we curated the first large-scale benchmark dataset for joint shadow and relighting, covering reflections, transparency, and complex interreflections. Experiments show significant gains in realism and consistency across synthetic and real images. LGI thus bridges geometry-inspired rendering with generative modeling, enabling efficient, physically consistent shadow generation and relighting.","authors":["Shan Wang","Peixia Li","Chenchen Xu","Ziang Cheng","Jiayu Yang","Hongdong Li","Pulak Purkait"],"pdf_url":"","comment":"ICRL 2026"},{"id":"http://arxiv.org/abs/2602.21819v1","updated":"2026-02-25T11:47:09Z","published":"2026-02-25T11:47:09Z","title":"SemVideo: Reconstructs What You Watch from Brain Activity via Hierarchical Semantic Guidance","summary":"Reconstructing dynamic visual experiences from brain activity provides a compelling avenue for exploring the neural mechanisms of human visual perception. While recent progress in fMRI-based image reconstruction has been notable, extending this success to video reconstruction remains a significant challenge. Current fMRI-to-video reconstruction approaches consistently encounter two major shortcomings: (i) inconsistent visual representations of salient objects across frames, leading to appearance mismatches; (ii) poor temporal coherence, resulting in motion misalignment or abrupt frame transitions. To address these limitations, we introduce SemVideo, a novel fMRI-to-video reconstruction framework guided by hierarchical semantic information. At the core of SemVideo is SemMiner, a hierarchical guidance module that constructs three levels of semantic cues from the original video stimulus: static anchor descriptions, motion-oriented narratives, and holistic summaries. Leveraging this semantic guidance, SemVideo comprises three key components: a Semantic Alignment Decoder that aligns fMRI signals with CLIP-style embeddings derived from SemMiner, a Motion Adaptation Decoder that reconstructs dynamic motion patterns using a novel tripartite attention fusion architecture, and a Conditional Video Render that leverages hierarchical semantic guidance for video reconstruction. Experiments conducted on the CC2017 and HCP datasets demonstrate that SemVideo achieves superior performance in both semantic alignment and temporal consistency, setting a new state-of-the-art in fMRI-to-video reconstruction.","authors":["Minghan Yang","Lan Yang","Ke Li","Honggang Zhang","Kaiyue Pang","Yizhe Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21818v1","updated":"2026-02-25T11:47:00Z","published":"2026-02-25T11:47:00Z","title":"SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model","summary":"SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.","authors":["Guibin Chen","Dixuan Lin","Jiangping Yang","Youqiang Zhang","Zhengcong Fei","Debang Li","Sheng Chen","Chaofeng Ao","Nuo Pang","Yiming Wang","Yikun Dou","Zheng Chen","Mingyuan Fan","Tuanhui Li","Mingshan Chang","Hao Zhang","Xiaopeng Sun","Jingtao Xu","Yuqiang Xie","Jiahua Wang","Zhiheng Xu","Weiming Xiong","Yuzhe Jin","Baoxuan Gu","Binjie Mao","Yunjie Yu","Jujie He","Yuhao Feng","Shiwen Tu","Chaojie Wang","Rui Yan","Wei Shen","Jingchen Wu","Peng Zhao","Xuanyue Zhong","Zhuangzhuang Liu","Kaifei Wang","Fuxiang Zhang","Weikai Xu","Wenyan Liu","Binglu Zhang","Yu Shen","Tianhui Xiong","Bin Peng","Liang Zeng","Xuchen Song","Haoxiang Guo","Peiyu Wang","Yahui Zhou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21810v1","updated":"2026-02-25T11:36:33Z","published":"2026-02-25T11:36:33Z","title":"GeoMotion: Rethinking Motion Segmentation via Latent 4D Geometry","summary":"Motion segmentation in dynamic scenes is highly challenging, as conventional methods heavily rely on estimating camera poses and point correspondences from inherently noisy motion cues. Existing statistical inference or iterative optimization techniques that struggle to mitigate the cumulative errors in multi-stage pipelines often lead to limited performance or high computational cost. In contrast, we propose a fully learning-based approach that directly infers moving objects from latent feature representations via attention mechanisms, thus enabling end-to-end feed-forward motion segmentation. Our key insight is to bypass explicit correspondence estimation and instead let the model learn to implicitly disentangle object and camera motion. Supported by recent advances in 4D scene geometry reconstruction (e.g., $π^3$), the proposed method leverages reliable camera poses and rich spatial-temporal priors, which ensure stable training and robust inference for the model. Extensive experiments demonstrate that by eliminating complex pre-processing and iterative refinement, our approach achieves state-of-the-art motion segmentation performance with high efficiency. The code is available at:https://github.com/zjutcvg/GeoMotion.","authors":["Xiankang He","Peile Lin","Ying Cui","Dongyan Guo","Chunhua Shen","Xiaoqin Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.20218v2","updated":"2026-02-25T11:25:17Z","published":"2026-01-28T03:39:05Z","title":"DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment","summary":"Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \\textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.","authors":["Haoyou Deng","Keyu Yan","Chaojie Mao","Xiang Wang","Yu Liu","Changxin Gao","Nong Sang"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21780v1","updated":"2026-02-25T11:02:02Z","published":"2026-02-25T11:02:02Z","title":"XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression","summary":"Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.","authors":["Zunhai Su","Weihao Ye","Hansen Feng","Keyu Fan","Jing Zhang","Dahai Yu","Zhengwu Liu","Ngai Wong"],"pdf_url":"","comment":"Submission to the Journal of the Society for Information Display"},{"id":"http://arxiv.org/abs/2602.21779v1","updated":"2026-02-25T10:54:55Z","published":"2026-02-25T10:54:55Z","title":"Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models","summary":"Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.","authors":["Zheyuan Gu","Qingsong Zhao","Yusong Wang","Zhaohong Huang","Xinqi Li","Cheng Yuan","Jiaowei Shao","Chi Zhang","Xuelong Li"],"pdf_url":"","comment":"16 pages, 9 figures. Submitted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21778v1","updated":"2026-02-25T10:54:46Z","published":"2026-02-25T10:54:46Z","title":"From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors","summary":"Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.","authors":["Liangbing Zhao","Le Zhuo","Sayak Paul","Hongsheng Li","Mohamed Elhoseiny"],"pdf_url":"","comment":"All code, checkpoints, and datasets are available at https://liangbingzhao.github.io/statics2dynamics/"},{"id":"http://arxiv.org/abs/2602.21773v1","updated":"2026-02-25T10:48:51Z","published":"2026-02-25T10:48:51Z","title":"Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias","summary":"Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning,\" where models exhibit an ``easy to learn, yet hard to forget\" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.","authors":["JuneHyoung Kwon","MiHyeon Kim","Eunju Lee","Yoonji Lee","Seunghoon Lee","YoungBin Kim"],"pdf_url":"","comment":"Accepted to AAAI 2026"},{"id":"http://arxiv.org/abs/2411.17237v4","updated":"2026-02-25T10:47:50Z","published":"2024-11-26T09:03:16Z","title":"Grounding-IQA: Grounding Multimodal Language Model for Image Quality Assessment","summary":"The development of multimodal large language models (MLLMs) enables the evaluation of image quality through natural language descriptions. This advancement allows for more detailed assessments. However, these MLLM-based IQA methods primarily rely on general contextual descriptions, sometimes limiting fine-grained quality assessment. To address this limitation, we introduce a new image quality assessment (IQA) task paradigm, **grounding-IQA**. This paradigm integrates multimodal referring and grounding with IQA to realize more fine-grained quality perception, thereby extending existing IQA. Specifically, grounding-IQA comprises two subtasks: grounding-IQA-description (GIQA-DES) and visual question answering (GIQA-VQA). GIQA-DES involves detailed descriptions with precise locations (e.g., bounding boxes), while GIQA-VQA focuses on quality QA for local regions. To realize grounding-IQA, we construct a corresponding dataset, GIQA-160K, through our proposed automated annotation pipeline. Furthermore, we develop a well-designed benchmark, GIQA-Bench. The benchmark evaluates the grounding-IQA performance from three perspectives: description quality, VQA accuracy, and grounding precision. Experiments demonstrate that our proposed method facilitates the more fine-grained IQA application. Code: https://github.com/zhengchen1999/Grounding-IQA.","authors":["Zheng Chen","Xun Zhang","Wenbo Li","Renjing Pei","Fenglong Song","Xiongkuo Min","Xiaohong Liu","Xin Yuan","Yong Guo","Yulun Zhang"],"pdf_url":"","comment":"Accepted to ICLR 2026. Code is available at: https://github.com/zhengchen1999/Grounding-IQA"},{"id":"http://arxiv.org/abs/2506.10082v6","updated":"2026-02-25T10:37:14Z","published":"2025-06-11T18:03:55Z","title":"LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning","summary":"Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks fine-grained control over the edit's subsequent temporal evolution. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video models for flexible video editing. Our key innovation is using a spatiotemporal mask to strategically guide the LoRA fine-tuning process. This teaches the model two distinct skills: first, to interpret the mask as a command to either preserve content from the source video or generate new content in designated regions. Second, for these generated regions, LoRA learns to synthesize either temporally consistent motion inherited from the video or novel appearances guided by user-provided reference frames. This dual-capability LoRA grants users control over the edit's entire temporal evolution, allowing complex transformations like an object rotating or a flower blooming. Experimental results show our method achieves superior video editing performance compared to baseline methods. The code and video results are available at our project website: https://cjeen.github.io/LoRAEdit.","authors":["Chenjian Gao","Lihe Ding","Xin Cai","Zhanpeng Huang","Zibin Wang","Tianfan Xue"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20903v2","updated":"2026-02-25T10:31:13Z","published":"2026-02-24T13:40:23Z","title":"TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering","summary":"Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.","authors":["Hanshen Zhu","Yuliang Liu","Xuecheng Wu","An-Lan Wang","Hao Feng","Dingkang Yang","Chao Feng","Can Huang","Jingqun Tang","Xiang Bai"],"pdf_url":"","comment":"Accepted by CVPR 2026; Code: https://github.com/CIawevy/TextPecker"},{"id":"http://arxiv.org/abs/2602.06034v2","updated":"2026-02-25T10:30:35Z","published":"2026-02-05T18:59:21Z","title":"V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval","summary":"Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.","authors":["Dongyang Chen","Chaoyang Wang","Dezhao Su","Xi Xiao","Zeyu Zhang","Jing Xiong","Qing Li","Yuzhang Shang","Shichao Kan"],"pdf_url":"","comment":"Project page: https://github.com/chendy25/V-Retrver"},{"id":"http://arxiv.org/abs/2602.21762v1","updated":"2026-02-25T10:27:14Z","published":"2026-02-25T10:27:14Z","title":"SAPNet++: Evolving Point-Prompted Instance Segmentation with Semantic and Spatial Awareness","summary":"Single-point annotation is increasingly prominent in visual tasks for labeling cost reduction. However, it challenges tasks requiring high precision, such as the point-prompted instance segmentation (PPIS) task, which aims to estimate precise masks using single-point prompts to train a segmentation network. Due to the constraints of point annotations, granularity ambiguity and boundary uncertainty arise the difficulty distinguishing between different levels of detail (eg. whole object vs. parts) and the challenge of precisely delineating object boundaries. Previous works have usually inherited the paradigm of mask generation along with proposal selection to achieve PPIS. However, proposal selection relies solely on category information, failing to resolve the ambiguity of different granularity. Furthermore, mask generators offer only finite discrete solutions that often deviate from actual masks, particularly at boundaries. To address these issues, we propose the Semantic-Aware Point-Prompted Instance Segmentation Network (SAPNet). It integrates Point Distance Guidance and Box Mining Strategy to tackle group and local issues caused by the point's granularity ambiguity. Additionally, we incorporate completeness scores within proposals to add spatial granularity awareness, enhancing multiple instance learning (MIL) in proposal selection termed S-MIL. The Multi-level Affinity Refinement conveys pixel and semantic clues, narrowing boundary uncertainty during mask refinement. These modules culminate in SAPNet++, mitigating point prompt's granularity ambiguity and boundary uncertainty and significantly improving segmentation performance. Extensive experiments on four challenging datasets validate the effectiveness of our methods, highlighting the potential to advance PPIS.","authors":["Zhaoyang Wei","Xumeng Han","Xuehui Yu","Xue Yang","Guorong Li","Zhenjun Han","Jianbin Jiao"],"pdf_url":"","comment":"18 pages"},{"id":"http://arxiv.org/abs/2602.21760v1","updated":"2026-02-25T10:23:07Z","published":"2026-02-25T10:23:07Z","title":"Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling","summary":"Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\\times$ and $2.07\\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.","authors":["Euisoo Jung","Byunghyun Kim","Hyunjin Kim","Seonghye Cho","Jae-Gil Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21754v1","updated":"2026-02-25T10:08:14Z","published":"2026-02-25T10:08:14Z","title":"LiREC-Net: A Target-Free and Learning-Based Network for LiDAR, RGB, and Event Calibration","summary":"Advanced autonomous systems rely on multi-sensor fusion for safer and more robust perception. To enable effective fusion, calibrating directly from natural driving scenes (i.e., target-free) with high accuracy is crucial for precise multi-sensor alignment. Existing learning-based calibration methods are typically designed for only a single pair of sensor modalities (i.e., a bi-modal setup). Unlike these methods, we propose LiREC-Net, a target-free, learning-based calibration network that jointly calibrates multiple sensor modality pairs, including LiDAR, RGB, and event data, within a unified framework. To reduce redundant computation and improve efficiency, we introduce a shared LiDAR representation that leverages features from both its 3D nature and projected depth map, ensuring better consistency across modalities. Trained and evaluated on established datasets, such as KITTI and DSEC, our LiREC-Net achieves competitive performance to bi-modal models and sets a new strong baseline for the tri-modal use case.","authors":["Aditya Ranjan Dash","Ramy Battrawy","René Schuster","Didier Stricker"],"pdf_url":"","comment":"Accepted in CVPR 2026"},{"id":"http://arxiv.org/abs/2602.00462v3","updated":"2026-02-25T10:06:33Z","published":"2026-01-31T02:33:07Z","title":"LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs","summary":"Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.","authors":["Benno Krojer","Shravan Nayak","Oscar Mañas","Vaibhav Adlakha","Desmond Elliott","Siva Reddy","Marius Mosbach"],"pdf_url":"","comment":"Updates: small change in interpretability percentage for Qwen-based variants we trained (pre-processing fix), clarification in Section 3 on our method (after feedback from readers), additional appendix section"},{"id":"http://arxiv.org/abs/2511.13065v2","updated":"2026-02-25T09:57:15Z","published":"2025-11-17T07:12:06Z","title":"RobustGait: Robustness Analysis for Appearance Based Gait Recognition","summary":"Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems. Code is available at https://reeshoon.github.io/robustgaitbenchmark","authors":["Reeshoon Sayera","Akash Kumar","Sirshapan Mitra","Prudvi Kamtam","Yogesh S Rawat"],"pdf_url":"","comment":"IEEE WACV'26 Main Conference"},{"id":"http://arxiv.org/abs/2602.21743v1","updated":"2026-02-25T09:52:50Z","published":"2026-02-25T09:52:50Z","title":"Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) have significantly advanced the reasoning capabilities of large language models. Extending these methods to multimodal settings, however, faces a critical challenge: the instability of std-based normalization, which is easily distorted by extreme samples with nearly positive or negative rewards. Unlike pure-text LLMs, multimodal models are particularly sensitive to such distortions, as both perceptual and reasoning errors influence their responses. To address this, we characterize each sample by its difficulty, defined through perceptual complexity (measured via visual entropy) and reasoning uncertainty (captured by model confidence). Building on this characterization, we propose difficulty-aware group normalization (Durian), which re-groups samples by difficulty levels and shares the std within each group. Our approach preserves GRPO's intra-group distinctions while eliminating sensitivity to extreme cases, yielding significant performance gains across multiple multimodal reasoning benchmarks.","authors":["Jinghan Li","Junfeng Fang","Jinda Lu","Yuan Wang","Xiaoyan Guo","Tianyu Zhang","Xiang Wang","Xiangnan He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21740v1","updated":"2026-02-25T09:51:53Z","published":"2026-02-25T09:51:53Z","title":"Structure-to-Image: Zero-Shot Depth Estimation in Colonoscopy via High-Fidelity Sim-to-Real Adaptation","summary":"Monocular depth estimation (MDE) for colonoscopy is hampered by the domain gap between simulated and real-world images. Existing image-to-image translation methods, which use depth as a posterior constraint, often produce structural distortions and specular highlights by failing to balance realism with structure consistency. To address this, we propose a Structure-to-Image paradigm that transforms the depth map from a passive constraint into an active generative foundation. We are the first to introduce phase congruency to colonoscopic domain adaptation and design a cross-level structure constraint to co-optimize geometric structures and fine-grained details like vascular textures. In zero-shot evaluations conducted on a publicly available phantom dataset, the MDE model that was fine-tuned on our generated data achieved a maximum reduction of 44.18% in RMSE compared to competing methods. Our code is available at https://github.com/YyangJJuan/PC-S2I.git.","authors":["Juan Yang","Yuyan Zhang","Han Jia","Bing Hu","Wanzhong Song"],"pdf_url":"","comment":"\\c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works"},{"id":"http://arxiv.org/abs/2406.17115v3","updated":"2026-02-25T09:48:59Z","published":"2024-06-24T20:08:07Z","title":"Measuring the Measurers: Quality Evaluation of Hallucination Benchmarks for Large Vision-Language Models","summary":"Despite the outstanding performance in multimodal tasks, Large Vision-Language Models (LVLMs) have been plagued by the issue of hallucination, i.e., generating content that is inconsistent with the corresponding visual inputs. While previous works have proposed various benchmarks to evaluate this issue, the quality of these evaluations remains unverified. We observe that some of these benchmarks may produce inconsistent evaluation results across repeated tests or fail to align with human evaluation. To address this, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages specific indicators to assess both reliability and validity. Our empirical analysis using HQM reveals and pinpoints potential evaluation issues in existing benchmarks, exposing a critical gap in current hallucination evaluation. To bridge this gap, we propose HQH, a High-Quality Hallucination benchmark, which demonstrates superior reliability and validity under HQM, serving as a credible evaluation tool. Our large-scale evaluation of popular LVLMs on HQH reveals severe hallucination problems, which occur not only in the models' main answer to a question but also in additional analysis. This highlights the necessity for future model improvements to effectively mitigate hallucinations and reduce the associated security risks in real-world applications. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.","authors":["Bei Yan","Jie Zhang","Zheng Yuan","Shiguang Shan","Xilin Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21735v1","updated":"2026-02-25T09:44:27Z","published":"2026-02-25T09:44:27Z","title":"SigVLP: Sigmoid Volume-Language Pre-Training for Self-Supervised CT-Volume Adaptive Representation Learning","summary":"Large-scale, volumetric medical imaging datasets typically aggregate scans from different vendors and devices, resulting in highly variable resolution, slice thicknesses, and numbers of slices per study. Consequently, training representation models usually requires cropping or interpolating along the z-axis to obtain fixed-size blocks, which inevitably causes information loss. We propose a new training approach to overcome this limitation. Instead of absolute position embeddings, we interpret volumes as sequences of 3D chunks and adopt Rotary Position Embeddings, allowing us to treat the z-axis as an unconstrained temporal dimensions. Building on this idea, we introduce a new vision-language model: SigVLP. In SigVLP, we implement Rotary Position Embedding as the positional encoding method, which is applied directly within the attention operation, generating input-conditioned sine and cosine weights on the fly. This design ensures consistent alignment between query and key projections and adapts to any input sizes. To allow for variable input size during training, we sample Computed Tomography volumes in chunks and pair them with localized organ-wise textual observations. Compared to using entire reports for conditioning, chunkwise alignment provides finer-grained supervision, enabling the model to establish stronger correlations between the text and volume representations, thereby improving the precision of text-to-volume alignment. Our models are trained with the Muon optimizer and evaluated on a diverse set of downstream tasks, including zero-shot abnormality and organ classification, segmentation, and retrieval tasks.","authors":["Jiayi Wang","Hadrien Reynaud","Ibrahim Ethem Hamamci","Sezgin Er","Suprosanna Shit","Bjoern Menze","Bernhard Kainz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.01552v2","updated":"2026-02-25T09:41:37Z","published":"2025-09-01T15:28:44Z","title":"Variation-aware Vision Token Dropping for Faster Large Vision-Language Models","summary":"Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, consequently leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency without architectural changes. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which critically hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a dynamic token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{V$^2$Drop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks consistently demonstrate that V$^2$Drop maintains \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original performance for image and video understanding tasks respectively, while reducing LLM generation latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}.","authors":["Junjie Chen","Xuyang Liu","Zichen Wen","Yiyu Wang","Siteng Huang","Honggang Chen"],"pdf_url":"","comment":"Accepted by CVPR 2026. Code is available at \\url{https://github.com/xuyang-liu16/V2Drop}"},{"id":"http://arxiv.org/abs/2601.21405v2","updated":"2026-02-25T09:39:00Z","published":"2026-01-29T08:41:42Z","title":"Rectifying Geometry-Induced Similarity Distortions for Real-World Aerial-Ground Person Re-Identification","summary":"Aerial-ground person re-identification (AG-ReID) is fundamentally challenged by extreme viewpoint and distance discrepancies between aerial and ground cameras, which induce severe geometric distortions and invalidate the assumption of a shared similarity space across views. Existing methods primarily rely on geometry-aware feature learning or appearance-conditioned prompting, while implicitly assuming that the geometry-invariant dot-product similarity used in attention mechanisms remains reliable under large viewpoint and scale variations. We argue that this assumption does not hold. Extreme camera geometry systematically distorts the query-key similarity space and degrades attention-based matching, even when feature representations are partially aligned. To address this issue, we introduce Geometry-Induced Query-Key Transformation (GIQT), a lightweight low-rank module that explicitly rectifies the similarity space by conditioning query-key interactions on camera geometry. Rather than modifying feature representations or the attention formulation itself, GIQT adapts the similarity computation to compensate for dominant geometry-induced anisotropic distortions. Building on this local similarity rectification, we further incorporate a geometry-conditioned prompt generation mechanism that provides global, view-adaptive representation priors derived directly from camera geometry.Experiments on four aerial-ground person re-identification benchmarks demonstrate that the proposed framework consistently improves robustness under extreme and previously unseen geometric conditions, while introducing minimal computational overhead compared to state-of-the-art methods.","authors":["Kailash A. Hambarde","Hugo Proença"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19206v2","updated":"2026-02-25T09:33:40Z","published":"2026-02-22T14:30:41Z","title":"GS-CLIP: Zero-shot 3D Anomaly Detection by Geometry-Aware Prompt and Synergistic View Representation Learning","summary":"Zero-shot 3D Anomaly Detection is an emerging task that aims to detect anomalies in a target dataset without any target training data, which is particularly important in scenarios constrained by sample scarcity and data privacy concerns. While current methods adapt CLIP by projecting 3D point clouds into 2D representations, they face challenges. The projection inherently loses some geometric details, and the reliance on a single 2D modality provides an incomplete visual understanding, limiting their ability to detect diverse anomaly types. To address these limitations, we propose the Geometry-Aware Prompt and Synergistic View Representation Learning (GS-CLIP) framework, which enables the model to identify geometric anomalies through a two-stage learning process. In stage 1, we dynamically generate text prompts embedded with 3D geometric priors. These prompts contain global shape context and local defect information distilled by our Geometric Defect Distillation Module (GDDM). In stage 2, we introduce Synergistic View Representation Learning architecture that processes rendered and depth images in parallel. A Synergistic Refinement Module (SRM) subsequently fuses the features of both streams, capitalizing on their complementary strengths. Comprehensive experimental results on four large-scale public datasets show that GS-CLIP achieves superior performance in detection. Code can be available at https://github.com/zhushengxinyue/GS-CLIP.","authors":["Zehao Deng","An Liu","Yan Wang"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2503.02310v2","updated":"2026-02-25T09:26:06Z","published":"2025-03-04T06:12:08Z","title":"PD-VLA: Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding","summary":"Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.","authors":["Wenxuan Song","Jiayi Chen","Pengxiang Ding","Han Zhao","Wei Zhao","Zhide Zhong","Zongyuan Ge","Zhijun Li","Donglin Wang","Jun Ma","Lujia Wang","Haoang Li"],"pdf_url":"","comment":"Accepted by IROS 2025, updated results on LIBERO"},{"id":"http://arxiv.org/abs/2512.08639v2","updated":"2026-02-25T09:25:38Z","published":"2025-12-09T14:25:24Z","title":"Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning","summary":"Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the AerialVLN and OpenFly benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.","authors":["Huilin Xu","Zhuoyang Liu","Yixiang Luomei","Feng Xu"],"pdf_url":"","comment":"Under Review, 15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2602.21716v1","updated":"2026-02-25T09:22:46Z","published":"2026-02-25T09:22:46Z","title":"TranX-Adapter: Bridging Artifacts and Semantics within MLLMs for Robust AI-generated Image Detection","summary":"Rapid advances in AI-generated image (AIGI) technology enable highly realistic synthesis, threatening public information integrity and security. Recent studies have demonstrated that incorporating texture-level artifact features alongside semantic features into multimodal large language models (MLLMs) can enhance their AIGI detection capability. However, our preliminary analyses reveal that artifact features exhibit high intra-feature similarity, leading to an almost uniform attention map after the softmax operation. This phenomenon causes attention dilution, thereby hindering effective fusion between semantic and artifact features. To overcome this limitation, we propose a lightweight fusion adapter, TranX-Adapter, which integrates a Task-aware Optimal-Transport Fusion that leverages the Jensen-Shannon divergence between artifact and semantic prediction probabilities as a cost matrix to transfer artifact information into semantic features, and an X-Fusion that employs cross-attention to transfer semantic information into artifact features. Experiments on standard AIGI detection benchmarks upon several advanced MLLMs, show that our TranX-Adapter brings consistent and significant improvements (up to +6% accuracy).","authors":["Wenbin Wang","Yuge Huang","Jianqing Xu","Yue Yu","Jiangtao Yan","Shouhong Ding","Pan Zhou","Yong Luo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21712v1","updated":"2026-02-25T09:20:43Z","published":"2026-02-25T09:20:43Z","title":"Innovative Tooth Segmentation Using Hierarchical Features and Bidirectional Sequence Modeling","summary":"Tooth image segmentation is a cornerstone of dental digitization. However, traditional image encoders relying on fixed-resolution feature maps often lead to discontinuous segmentation and poor discrimination between target regions and background, due to insufficient modeling of environmental and global context. Moreover, transformer-based self-attention introduces substantial computational overhead because of its quadratic complexity (O(n^2)), making it inefficient for high-resolution dental images. To address these challenges, we introduce a three-stage encoder with hierarchical feature representation to capture scale-adaptive information in dental images. By jointly leveraging low-level details and high-level semantics through cross-scale feature fusion, the model effectively preserves fine structural information while maintaining strong contextual awareness. Furthermore, a bidirectional sequence modeling strategy is incorporated to enhance global spatial context understanding without incurring high computational cost.\n  We validate our method on two dental datasets, with experimental results demonstrating its superiority over existing approaches. On the OralVision dataset, our model achieves a 1.1% improvement in mean intersection over union (mIoU).","authors":["Xinxin Zhao","Jian Jiang","Yan Tian","Liqin Wu","Zhaocheng Xu","Teddy Yang","Yunuo Zou","Xun Wang"],"pdf_url":"","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2602.21709v1","updated":"2026-02-25T09:16:28Z","published":"2026-02-25T09:16:28Z","title":"Assessing airborne laser scanning and aerial photogrammetry for deep learning-based stand delineation","summary":"Accurate forest stand delineation is essential for forest inventory and management but remains a largely manual and subjective process. A recent study has shown that deep learning can produce stand delineations comparable to expert interpreters when combining aerial imagery and airborne laser scanning (ALS) data. However, temporal misalignment between data sources limits operational scalability. Canopy height models (CHMs) derived from digital photogrammetry (DAP) offer better temporal alignment but may smoothen canopy surface and canopy gaps, raising the question of whether they can reliably replace ALS-derived CHMs. Similarly, the inclusion of a digital terrain model (DTM) has been suggested to improve delineation performance, but has remained untested in published literature. Using expert-delineated forest stands as reference data, we assessed a U-Net-based semantic segmentation framework with municipality-level cross-validation across six municipalities in southeastern Norway. We compared multispectral aerial imagery combined with (i) an ALS-derived CHM, (ii) a DAP-derived CHM, and (iii) a DAP-derived CHM in combination with a DTM. Results showed comparable performance across all data combinations, reaching overall accuracy values between 0.90-0.91. Agreement between model predictions was substantially larger than agreement with the reference data, highlighting both model consistency and the inherent subjectivity of stand delineation. The similar performance of DAP-CHMs, despite the reduced structural detail, and the lack of improvements of the DTM indicate that the framework is resilient to variations in input data. These findings indicate that large datasets for deep learning-based stand delineations can be assembled using projects including temporally aligned ALS data and DAP point clouds.","authors":["Håkon Næss Sandum","Hans Ole Ørka","Oliver Tomic","Terje Gobakken"],"pdf_url":"","comment":"20 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2602.21707v1","updated":"2026-02-25T09:13:24Z","published":"2026-02-25T09:13:24Z","title":"Learning spatially adaptive sparsity level maps for arbitrary convolutional dictionaries","summary":"State-of-the-art learned reconstruction methods often rely on black-box modules that, despite their strong performance, raise questions about their interpretability and robustness. Here, we build on a recently proposed image reconstruction method, which is based on embedding data-driven information into a model-based convolutional dictionary regularization via neural network-inferred spatially adaptive sparsity level maps. By means of improved network design and dedicated training strategies, we extend the method to achieve filter-permutation invariance as well as the possibility to change the convolutional dictionary at inference time. We apply our method to low-field MRI and compare it to several other recent deep learning-based methods, also on in vivo data, in which the benefit for the use of a different dictionary is showcased. We further assess the method's robustness when tested on in- and out-of-distribution data. When tested on the latter, the proposed method suffers less from the data distribution shift compared to the other learned methods, which we attribute to its reduced reliance on training data due to its underlying model-based reconstruction component.","authors":["Joshua Schulz","David Schote","Christoph Kolbitsch","Kostas Papafitsoros","Andreas Kofler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21706v1","updated":"2026-02-25T09:11:45Z","published":"2026-02-25T09:11:45Z","title":"SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video","summary":"Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We then present SurGo-R1, a model optimized via RLHF with a multi-turn phase-then-go architecture where the model first identifies the surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context. On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, a 6.6$\\times$ improvement over the mainstream generalist VLMs. Code, model and benchmark will be available at https://github.com/jinlab-imvr/SurGo-R1","authors":["Guanyi Qin","Xiaozhen Wang","Zhu Zhuo","Chang Han Low","Yuancan Xiao","Yibing Fu","Haofeng Liu","Kai Wang","Chunjiang Li","Yueming Jin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21704v1","updated":"2026-02-25T09:10:00Z","published":"2026-02-25T09:10:00Z","title":"Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models","summary":"Large Vision-Language Models (LVLMs) exhibit outstanding performance on vision-language tasks but struggle with hallucination problems. Through in-depth analysis of LVLM activation patterns, we reveal two key findings: 1) truthfulness and visual perception capabilities predominantly engage different subsets of attention heads within the model architecture; and 2) truthfulness steering vectors vary significantly across different semantic contexts. Based on these observations, we propose Dynamic Multimodal Activation Steering, a training-free approach for hallucination mitigation. Our method constructs a semantic-based truthfulness steering vector database and computes visual perception steering vectors, enabling context-aware interventions during inference by dynamically selecting the most relevant steering vectors based on input semantic similarity and applying them to the most influential attention heads. We conduct comprehensive experiments across multiple models and datasets, demonstrating that our approach significantly enhances model performance, outperforming existing state-of-the-art methods.","authors":["Jianghao Yin","Qin Chen","Kedi Chen","Jie Zhou","Xingjiao Wu","Liang He"],"pdf_url":"","comment":"Accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21703v1","updated":"2026-02-25T09:09:12Z","published":"2026-02-25T09:09:12Z","title":"Brain Tumor Segmentation with Special Emphasis on the Non-Enhancing Brain Tumor Compartment","summary":"A U-Net based deep learning architecture is designed to segment brain tumors as they appear on various MRI modalities. Special emphasis is lent to the non-enhancing tumor compartment. The latter has not been considered anymore in recent brain tumor segmentation challenges like the MICCAI challenges. However, it is considered to be indicative of the survival time of the patient as well as of areas of further tumor growth. Hence it deems essential to have means to automatically delineate its extension within the tumor.","authors":["T. Schaffer","A. Brawanski","S. Wein","A. M. Tomé","E. W. Lang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.06830v2","updated":"2026-02-25T09:08:02Z","published":"2025-11-10T08:21:11Z","title":"MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks","summary":"Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.","authors":["Tianang Chen","Jian Jin","Shilv Cai","Zhuangzi Li","Weisi Lin"],"pdf_url":"","comment":"ICASSP 2026"},{"id":"http://arxiv.org/abs/2602.21699v1","updated":"2026-02-25T09:03:42Z","published":"2026-02-25T09:03:42Z","title":"SF3D-RGB: Scene Flow Estimation from Monocular Camera and Sparse LiDAR","summary":"Scene flow estimation is an extremely important task in computer vision to support the perception of dynamic changes in the scene. For robust scene flow, learning-based approaches have recently achieved impressive results using either image-based or LiDAR-based modalities. However, these methods have tended to focus on the use of a single modality. To tackle these problems, we present a deep learning architecture, SF3D-RGB, that enables sparse scene flow estimation using 2D monocular images and 3D point clouds (e.g., acquired by LiDAR) as inputs. Our architecture is an end-to-end model that first encodes information from each modality into features and fuses them together. Then, the fused features enhance a graph matching module for better and more robust mapping matrix computation to generate an initial scene flow. Finally, a residual scene flow module further refines the initial scene flow. Our model is designed to strike a balance between accuracy and efficiency. Furthermore, experiments show that our proposed method outperforms single-modality methods and achieves better scene flow accuracy on real-world datasets while using fewer parameters compared to other state-of-the-art methods with fusion.","authors":["Rajai Alhimdiat","Ramy Battrawy","René Schuster","Didier Stricker","Wesam Ashour"],"pdf_url":"","comment":"Accepted in Computer Vision Conference (CVC) 2026"},{"id":"http://arxiv.org/abs/2602.21698v1","updated":"2026-02-25T09:03:41Z","published":"2026-02-25T09:03:41Z","title":"E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought","summary":"Generative AI is widely used to create commercial posters. However, rapid advances in generation have outpaced automated quality assessment. Existing models emphasize generic esthetics or low level distortions and lack the functional criteria required for e-commerce design. It is especially challenging for Chinese content, where complex characters often produce subtle but critical textual artifacts that are overlooked by existing methods. To address this, we introduce E-comIQ-ZH, a framework for evaluating Chinese e-commerce posters. We build the first dataset E-comIQ-18k to feature multi dimensional scores and expert calibrated Chain of Thought (CoT) rationales. Using this dataset, we train E-comIQ-M, a specialized evaluation model that aligns with human expert judgment. Our framework enables E-comIQ-Bench, the first automated and scalable benchmark for the generation of Chinese e-commerce posters. Extensive experiments show our E-comIQ-M aligns more closely with expert standards and enables scalable automated assessment of e-commerce posters. All datasets, models, and evaluation tools will be released to support future research in this area.Code will be available at https://github.com/4mm7/E-comIQ-ZH.","authors":["Meiqi Sun","Mingyu Li","Junxiong Zhu"],"pdf_url":"","comment":"21pages, 19figures, accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.20630v2","updated":"2026-02-25T08:38:48Z","published":"2026-02-24T07:24:25Z","title":"From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection","summary":"Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \\textbf{Tra}ck-\\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.","authors":["Yepeng Liu","Hao Li","Liwen Yang","Fangzhen Li","Xudi Ge","Yuliang Gu","kuang Gao","Bing Wang","Guang Chen","Hangjun Ye","Yongchao Xu"],"pdf_url":"","comment":"There are unresolved issues regarding authorship and manuscript details. We withdraw this submission to make necessary corrections"},{"id":"http://arxiv.org/abs/2507.21540v2","updated":"2026-02-25T08:16:45Z","published":"2025-07-29T07:13:56Z","title":"PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking","summary":"The increasing sophistication of large vision-language models (LVLMs) has been accompanied by advances in safety alignment mechanisms designed to prevent harmful content generation. However, these defenses remain vulnerable to sophisticated adversarial attacks. Existing jailbreak methods typically rely on direct and semantically explicit prompts, overlooking subtle vulnerabilities in how LVLMs compose information over multiple reasoning steps. In this paper, we propose a novel and effective jailbreak framework inspired by Return-Oriented Programming (ROP) techniques from software security. Our approach decomposes a harmful instruction into a sequence of individually benign visual gadgets. A carefully engineered textual prompt directs the sequence of inputs, prompting the model to integrate the benign visual gadgets through its reasoning process to produce a coherent and harmful output. This makes the malicious intent emergent and difficult to detect from any single component. We validate our method through extensive experiments on established benchmarks including SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our approach consistently and substantially outperforms existing baselines on state-of-the-art models, achieving near-perfect attack success rates (over 0.90 on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical and underexplored vulnerability that exploits the compositional reasoning abilities of LVLMs, highlighting the urgent need for defenses that secure the entire reasoning process.","authors":["Quanchen Zou","Zonghao Ying","Moyang Chen","Wenzhuo Xu","Yisong Xiao","Yakai Li","Deyue Zhang","Dongdong Yang","Zhao Liu","Xiangzheng Zhang"],"pdf_url":"","comment":"There is an error happening in Figure 1, because Figure 1 did not perfectly show the exact overview of the PRISM pipeline"},{"id":"http://arxiv.org/abs/2602.21668v1","updated":"2026-02-25T08:04:07Z","published":"2026-02-25T08:04:07Z","title":"Space-Time Forecasting of Dynamic Scenes with Motion-aware Gaussian Grouping","summary":"Forecasting dynamic scenes remains a fundamental challenge in computer vision, as limited observations make it difficult to capture coherent object-level motion and long-term temporal evolution. We present Motion Group-aware Gaussian Forecasting (MoGaF), a framework for long-term scene extrapolation built upon the 4D Gaussian Splatting representation. MoGaF introduces motion-aware Gaussian grouping and group-wise optimization to enforce physically consistent motion across both rigid and non-rigid regions, yielding spatially coherent dynamic representations. Leveraging this structured space-time representation, a lightweight forecasting module predicts future motion, enabling realistic and temporally stable scene evolution. Experiments on synthetic and real-world datasets demonstrate that MoGaF consistently outperforms existing baselines in rendering quality, motion plausibility, and long-term forecasting stability. Our project page is available at https://slime0519.github.io/mogaf","authors":["Junmyeong Lee","Hoseung Choi","Minsu Cho"],"pdf_url":"","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2602.21667v1","updated":"2026-02-25T08:00:48Z","published":"2026-02-25T08:00:48Z","title":"Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception","summary":"Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoint2Comm directly communicates quantized point-cloud indices using a shared codebook, enabling efficient reconstruction with lower bandwidth than feature-based methods. To ensure robustness to possible communication packet loss, we employ a masked training strategy that simulates random packet loss, allowing the model to maintain strong performance even under severe transmission failures. In addition, a cascade attention fusion module is proposed to enhance multi-vehicle information integration. Extensive experiments on both simulated and real-world datasets demonstrate that QPoint2Comm sets a new state of the art in accuracy, communication efficiency, and resilience to packet loss.","authors":["Sheng Xu","Enshu Wang","Hongfei Xue","Jian Teng","Bingyi Liu","Yi Zhu","Pu Wang","Libing Wu","Chunming Qiao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21662v1","updated":"2026-02-25T07:42:27Z","published":"2026-02-25T07:42:27Z","title":"HybridINR-PCGC: Hybrid Lossless Point Cloud Geometry Compression Bridging Pretrained Model and Implicit Neural Representation","summary":"Learning-based point cloud compression presents superior performance to handcrafted codecs. However, pretrained-based methods, which are based on end-to-end training and expected to generalize to all the potential samples, suffer from training data dependency. Implicit neural representation (INR) based methods are distribution-agnostic and more robust, but they require time-consuming online training and suffer from the bitstream overhead from the overfitted model. To address these limitations, we propose HybridINR-PCGC, a novel hybrid framework that bridges the pretrained model and INR. Our framework retains distribution-agnostic properties while leveraging a pretrained network to accelerate convergence and reduce model overhead, which consists of two parts: the Pretrained Prior Network (PPN) and the Distribution Agnostic Refiner (DAR). We leverage the PPN, designed for fast inference and stable performance, to generate a robust prior for accelerating the DAR's convergence. The DAR is decomposed into a base layer and an enhancement layer, and only the enhancement layer needed to be packed into the bitstream. Finally, we propose a supervised model compression module to further supervise and minimize the bitrate of the enhancement layer parameters. Based on experiment results, HybridINR-PCGC achieves a significantly improved compression rate and encoding efficiency. Specifically, our method achieves a Bpp reduction of approximately 20.43% compared to G-PCC on 8iVFB. In the challenging out-of-distribution scenario Cat1B, our method achieves a Bpp reduction of approximately 57.85% compared to UniPCGC. And our method exhibits a superior time-rate trade-off, achieving an average Bpp reduction of 15.193% relative to the LINR-PCGC on 8iVFB.","authors":["Wenjie Huang","Qi Yang","Shuting Xia","He Huang","Zhu Li","Yiling Xu"],"pdf_url":"","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2602.21657v1","updated":"2026-02-25T07:35:22Z","published":"2026-02-25T07:35:22Z","title":"Following the Diagnostic Trace: Visual Cognition-guided Cooperative Network for Chest X-Ray Diagnosis","summary":"Computer-aided diagnosis (CAD) has significantly advanced automated chest X-ray diagnosis but remains isolated from clinical workflows and lacks reliable decision support and interpretability. Human-AI collaboration seeks to enhance the reliability of diagnostic models by integrating the behaviors of controllable radiologists. However, the absence of interactive tools seamlessly embedded within diagnostic routines impedes collaboration, while the semantic gap between radiologists' decision-making patterns and model representations further limits clinical adoption. To overcome these limitations, we propose a visual cognition-guided collaborative network (VCC-Net) to achieve the cooperative diagnostic paradigm. VCC-Net centers on visual cognition (VC) and employs clinically compatible interfaces, such as eye-tracking or the mouse, to capture radiologists' visual search traces and attention patterns during diagnosis. VCC-Net employs VC as a spatial cognition guide, learning hierarchical visual search strategies to localize diagnostically key regions. A cognition-graph co-editing module subsequently integrates radiologist VC with model inference to construct a disease-aware graph. The module captures dependencies among anatomical regions and aligns model representations with VC-driven features, mitigating radiologist bias and facilitating complementary, transparent decision-making. Experiments on the public datasets SIIM-ACR, EGD-CXR, and self-constructed TB-Mouse dataset achieved classification accuracies of 88.40%, 85.05%, and 92.41%, respectively. The attention maps produced by VCC-Net exhibit strong concordance with radiologists' gaze distributions, demonstrating a mutual reinforcement of radiologist and model inference. The code is available at https://github.com/IPMI-NWU/VCC-Net.","authors":["Shaoxuan Wu","Jingkun Chen","Chong Ma","Cong Shen","Xiao Zhang","Jun Feng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21655v1","updated":"2026-02-25T07:34:26Z","published":"2026-02-25T07:34:26Z","title":"CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning","summary":"Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \\textbf{C}omplete and \\textbf{C}orrect \\textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.","authors":["Zhijiang Tang","Linhua Wang","Jiaxin Qi","Weihao Jiang","Peng Hou","Anxiang Zeng","Jianqiang Huang"],"pdf_url":"","comment":"Accept by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21645v1","updated":"2026-02-25T07:19:18Z","published":"2026-02-25T07:19:18Z","title":"Lie Flow: Video Dynamic Fields Modeling and Predicting with Lie Algebra as Geometric Physics Principle","summary":"Modeling 4D scenes requires capturing both spatial structure and temporal motion, which is challenging due to the need for physically consistent representations of complex rigid and non-rigid motions. Existing approaches mainly rely on translational displacements, which struggle to represent rotations, articulated transformations, often leading to spatial inconsistency and physically implausible motion. LieFlow, a dynamic radiance representation framework that explicitly models motion within the SE(3) Lie group, enabling coherent learning of translation and rotation in a unified geometric space. The SE(3) transformation field enforces physically inspired constraints to maintain motion continuity and geometric consistency. The evaluation includes a synthetic dataset with rigid-body trajectories and two real-world datasets capturing complex motion under natural lighting and occlusions. Across all datasets, LieFlow consistently improves view-synthesis fidelity, temporal coherence, and physical realism over NeRF-based baselines. These results confirm that SE(3)-based motion modeling offers a robust and physically grounded framework for representing dynamic 4D scenes.","authors":["Weidong Qiao","Wangmeng Zuo","Hui Li"],"pdf_url":"","comment":"10pages,5 figures"},{"id":"http://arxiv.org/abs/2512.09069v2","updated":"2026-02-25T07:03:17Z","published":"2025-12-09T19:34:30Z","title":"KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification","summary":"Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency-accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD-OCT.","authors":["Erfan Nourbakhsh","Nasrin Sanjari","Ali Nourbakhsh"],"pdf_url":"","comment":"7 pages, 5 figures (Accepted at ICSPIS 2025)"},{"id":"http://arxiv.org/abs/2602.21637v1","updated":"2026-02-25T07:01:54Z","published":"2026-02-25T07:01:54Z","title":"CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis","summary":"Foundation models have recently achieved impressive success in computational pathology, demonstrating strong generalization across diverse histopathology tasks. However, existing models overlook the heterogeneous and non-uniform organization of pathological regions of interest (ROIs) because they rely on natural image backbones not tailored for tissue morphology. Consequently, they often fail to capture the coherent tissue architecture beyond isolated patches, limiting interpretability and clinical relevance. To address these challenges, we present Cross-modal Adaptive Region Encoder (CARE), a foundation model for pathology that automatically partitions WSIs into several morphologically relevant regions. Specifically, CARE employs a two-stage pretraining strategy: (1) a self-supervised unimodal pretraining stage that learns morphological representations from 34,277 whole-slide images (WSIs) without segmentation annotations, and (2) a cross-modal alignment stage that leverages RNA and protein profiles to refine the construction and representation of adaptive regions. This molecular guidance enables CARE to identify biologically relevant patterns and generate irregular yet coherent tissue regions, selecting the most representative area as ROI. CARE supports a broad range of pathology-related tasks, using either the ROI feature or the slide-level feature obtained by aggregating adaptive regions. Based on only one-tenth of the pretraining data typically used by mainstream foundation models, CARE achieves superior average performance across 33 downstream benchmarks, including morphological classification, molecular prediction, and survival analysis, and outperforms other foundation model baselines overall.","authors":["Di Zhang","Zhangpeng Gong","Xiaobo Pang","Jiashuai Liu","Junbo Lu","Hao Cui","Jiusong Ge","Zhi Zeng","Kai Yi","Yinghua Li","Si Liu","Tingsong Yu","Haoran Wang","Mireia Crispin-Ortuzar","eimiao Yu","Chen Li","Zeyu Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21636v1","updated":"2026-02-25T07:00:24Z","published":"2026-02-25T07:00:24Z","title":"Axial-Centric Cross-Plane Attention for 3D Medical Image Classification","summary":"Clinicians commonly interpret three-dimensional (3D) medical images, such as computed tomography (CT) scans, using multiple anatomical planes rather than as a single volumetric representation. In this multi-planar approach, the axial plane typically serves as the primary acquisition and diagnostic reference, while the coronal and sagittal planes provide complementary spatial information to increase diagnostic confidence. However, many existing 3D deep learning methods either process volumetric data holistically or assign equal importance to all planes, failing to reflect the axial-centric clinical interpretation workflow. To address this gap, we propose an axial-centric cross-plane attention architecture for 3D medical image classification that captures the inherent asymmetric dependencies between different anatomical planes. Our architecture incorporates MedDINOv3, a medical vision foundation model pretrained via self-supervised learning on large-scale axial CT images, as a frozen feature extractor for the axial, coronal, and sagittal planes. RICA blocks and intra-plane transformer encoders capture plane-specific positional and contextual information within each anatomical plane, while axial-centric cross-plane transformer encoders condition axial features on complementary information from auxiliary planes. Experimental results on six datasets from the MedMNIST3D benchmark demonstrate that the proposed architecture consistently outperforms existing 3D and multi-plane models in terms of accuracy and AUC. Ablation studies further confirm the importance of axial-centric query-key-value allocation and directional cross-plane fusion. These results highlight the importance of aligning architectural design with clinical interpretation workflows for robust and data-efficient 3D medical image analysis.","authors":["Doyoung Park","Jinsoo Kim","Lohendran Baskaran"],"pdf_url":"","comment":"Submitted to MICCAI 2026"},{"id":"http://arxiv.org/abs/2602.21633v1","updated":"2026-02-25T06:58:06Z","published":"2026-02-25T06:58:06Z","title":"Self-Correcting VLA: Online Action Refinement via Sparse World Imagination","summary":"Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.","authors":["Chenyv Liu","Wentao Tan","Lei Zhu","Fengling Li","Jingjing Li","Guoli Yang","Heng Tao Shen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21631v1","updated":"2026-02-25T06:53:15Z","published":"2026-02-25T06:53:15Z","title":"UniHand: A Unified Model for Diverse Controlled 4D Hand Motion Modeling","summary":"Hand motion plays a central role in human interaction, yet modeling realistic 4D hand motion (i.e., 3D hand pose sequences over time) remains challenging. Research in this area is typically divided into two tasks: (1) Estimation approaches reconstruct precise motion from visual observations, but often fail under hand occlusion or absence; (2) Generation approaches focus on synthesizing hand poses by exploiting generative priors under multi-modal structured inputs and infilling motion from incomplete sequences. However, this separation not only limits the effective use of heterogeneous condition signals that frequently arise in practice, but also prevents knowledge transfer between the two tasks. We present UniHand, a unified diffusion-based framework that formulates both estimation and generation as conditional motion synthesis. UniHand integrates heterogeneous inputs by embedding structured signals into a shared latent space through a joint variational autoencoder, which aligns conditions such as MANO parameters and 2D skeletons. Visual observations are encoded with a frozen vision backbone, while a dedicated hand perceptron extracts hand-specific cues directly from image features, removing the need for complex detection and cropping pipelines. A latent diffusion model then synthesizes consistent motion sequences from these diverse conditions. Extensive experiments across multiple benchmarks demonstrate that UniHand delivers robust and accurate hand motion modeling, maintaining performance under severe occlusions and temporally incomplete inputs.","authors":["Zhihao Sun","Tong Wu","Ruirui Tu","Daoguo Dong","Zuxuan Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21627v1","updated":"2026-02-25T06:44:13Z","published":"2026-02-25T06:44:13Z","title":"Tokenizing Semantic Segmentation with RLE","summary":"This paper presents a new unified approach to semantic segmentation in both images and videos by using language modeling to output the masks as sequences of discrete tokens. We use run length encoding (RLE) to discretize the segmentation masks and then train a modified version of Pix2Seq \\cite{p2s} to output these RLE tokens through autoregression. We propose novel tokenization strategies to compress the length of the token sequence to make it practicable to extend this approach to videos. We also show how instance information can be incorporated into the tokenization process to perform panoptic segmentation. We evaluate our proposed models on two datasets to show that they are competitive with the state of the art in spite of being bottlenecked by our limited computational resources.","authors":["Abhineet Singh","Justin Rozeboom","Nilanjan Ray"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.01984v2","updated":"2026-02-25T06:38:58Z","published":"2026-02-02T11:38:01Z","title":"Enhancing Multi-Image Understanding through Delimiter Token Scaling","summary":"Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.","authors":["Minyoung Lee","Yeji Park","Dongjun Hwang","Yejin Kim","Seong Joon Oh","Junsuk Choe"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21613v1","updated":"2026-02-25T06:14:30Z","published":"2026-02-25T06:14:30Z","title":"Virtual Biopsy for Intracranial Tumors Diagnosis on MRI","summary":"Deep intracranial tumors situated in eloquent brain regions controlling vital functions present critical diagnostic challenges. Clinical practice has shifted toward stereotactic biopsy for pathological confirmation before treatment. Yet biopsy carries inherent risks of hemorrhage and neurological deficits and struggles with sampling bias due to tumor spatial heterogeneity, because pathological changes are typically region-selective rather than tumor-wide. Therefore, advancing non-invasive MRI-based pathology prediction is essential for holistic tumor assessment and modern clinical decision-making.\n  The primary challenge lies in data scarcity: low tumor incidence requires long collection cycles, and annotation demands biopsy-verified pathology from neurosurgical experts. Additionally, tiny lesion volumes lacking segmentation masks cause critical features to be overwhelmed by background noise. To address these challenges, we construct the ICT-MRI dataset - the first public biopsy-verified benchmark with 249 cases across four categories. We propose a Virtual Biopsy framework comprising: MRI-Processor for standardization; Tumor-Localizer employing vision-language models for coarse-to-fine localization via weak supervision; and Adaptive-Diagnoser with a Masked Channel Attention mechanism fusing local discriminative features with global contexts. Experiments demonstrate over 90% accuracy, outperforming baselines by more than 20%.","authors":["Xinzhe Luo","Shuai Shao","Yan Wang","Jiangtao Wang","Yutong Bai","Jianguo Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22548v2","updated":"2026-02-25T06:06:37Z","published":"2025-09-26T16:29:37Z","title":"JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation","summary":"Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.","authors":["Shuang Zeng","Dekang Qi","Xinyuan Chang","Feng Xiong","Shichao Xie","Xiaolong Wu","Shiyi Liang","Mu Xu","Xing Wei","Ning Guo"],"pdf_url":"","comment":"Accepted to ICLR 2026. Project page: https://miv-xjtu.github.io/JanusVLN.github.io/"},{"id":"http://arxiv.org/abs/2602.21599v1","updated":"2026-02-25T05:52:37Z","published":"2026-02-25T05:52:37Z","title":"Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control","summary":"Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.","authors":["Weisheng Xu","Qiwei Wu","Jiaxi Zhang","Tan Jing","Yangfan Li","Yuetong Fang","Jiaqi Xiong","Kai Wu","Rong Ou","Renjing Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21596v1","updated":"2026-02-25T05:46:40Z","published":"2026-02-25T05:46:40Z","title":"A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers","summary":"Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.","authors":["Trung X. Pham","Kang Zhang","Ji Woo Hong","Chang D. Yoo"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21593v1","updated":"2026-02-25T05:38:08Z","published":"2026-02-25T05:38:08Z","title":"Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection","summary":"Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations that invalidate such bindings. To expose this overlooked vulnerability, we introduce a Coherence-Preserving Semantic Injection (CSI) attack that leverages LLM-guided semantic manipulation under embedding-space similarity constraints. This alignment enforces visual-semantic consistency while selectively perturbing watermark-relevant semantics, ultimately inducing detector misclassification. Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness of current semantic watermark designs when confronted with LLM-driven semantic perturbations.","authors":["Zheng Gao","Xiaoyu Li","Zhicheng Bao","Xiaoyan Feng","Jiaojiao Jiang"],"pdf_url":"","comment":"Accepted by The Web Conference 2026 (Short Paper Track)"},{"id":"http://arxiv.org/abs/2602.21591v1","updated":"2026-02-25T05:35:45Z","published":"2026-02-25T05:35:45Z","title":"CADC: Content Adaptive Diffusion-Based Generative Image Compression","summary":"Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.","authors":["Xihua Sheng","Lingyu Zhu","Tianyu Zhang","Dong Liu","Shiqi Wang","Jing Wang"],"pdf_url":"","comment":"CVPR2026"},{"id":"http://arxiv.org/abs/2602.21589v1","updated":"2026-02-25T05:32:23Z","published":"2026-02-25T05:32:23Z","title":"SEF-MAP: Subspace-Decomposed Expert Fusion for Robust Multimodal HD Map Prediction","summary":"High-definition (HD) maps are essential for autonomous driving, yet multi-modal fusion often suffers from inconsistency between camera and LiDAR modalities, leading to performance degradation under low-light conditions, occlusions, or sparse point clouds. To address this, we propose SEFMAP, a Subspace-Expert Fusion framework for robust multimodal HD map prediction. The key idea is to explicitly disentangle BEV features into four semantic subspaces: LiDAR-private, Image-private, Shared, and Interaction. Each subspace is assigned a dedicated expert, thereby preserving modality-specific cues while capturing cross-modal consensus. To adaptively combine expert outputs, we introduce an uncertainty-aware gating mechanism at the BEV-cell level, where unreliable experts are down-weighted based on predictive variance, complemented by a usage balance regularizer to prevent expert collapse. To enhance robustness in degraded conditions and promote role specialization, we further propose distribution-aware masking: during training, modality-drop scenarios are simulated using EMA-statistical surrogate features, and a specialization loss enforces distinct behaviors of private, shared, and interaction experts across complete and masked inputs. Experiments on nuScenes and Argoverse2 benchmarks demonstrate that SEFMAP achieves state-of-the-art performance, surpassing prior methods by +4.2% and +4.8% in mAP, respectively. SEF-MAPprovides a robust and effective solution for multi-modal HD map prediction under diverse and degraded conditions.","authors":["Haoxiang Fu","Lingfeng Zhang","Hao Li","Ruibing Hu","Zhengrong Li","Guanjing Liu","Zimu Tan","Long Chen","Hangjun Ye","Xiaoshuai Hao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20685v2","updated":"2026-02-25T05:17:17Z","published":"2026-02-24T08:41:40Z","title":"RAYNOVA: Scale-Temporal Autoregressive World Modeling in Ray Space","summary":"World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-agonistic multiview world model for driving scenarios that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at https://raynova-ai.github.io/.","authors":["Yichen Xie","Chensheng Peng","Mazen Abdelfattah","Yihan Hu","Jiezhi Yang","Eric Higgins","Ryan Brigden","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"","comment":"Accepted by CVPR 2026; Project website: https://raynova-ai.github.io/"},{"id":"http://arxiv.org/abs/2602.21581v1","updated":"2026-02-25T05:06:58Z","published":"2026-02-25T05:06:58Z","title":"MultiAnimate: Pose-Guided Image Animation Made Extensible","summary":"Pose-guided human image animation aims to synthesize realistic videos of a reference character driven by a sequence of poses. While diffusion-based methods have achieved remarkable success, most existing approaches are limited to single-character animation. We observe that naively extending these methods to multi-character scenarios often leads to identity confusion and implausible occlusions between characters. To address these challenges, in this paper, we propose an extensible multi-character image animation framework built upon modern Diffusion Transformers (DiTs) for video generation. At its core, our framework introduces two novel components-Identifier Assigner and Identifier Adapter - which collaboratively capture per-person positional cues and inter-person spatial relationships. This mask-driven scheme, along with a scalable training strategy, not only enhances flexibility but also enables generalization to scenarios with more characters than those seen during training. Remarkably, trained on only a two-character dataset, our model generalizes to multi-character animation while maintaining compatibility with single-character cases. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in multi-character image animation, surpassing existing diffusion-based baselines.","authors":["Yingcheng Hu","Haowen Gong","Chuanguang Yang","Zhulin An","Yongjun Xu","Songhua Liu"],"pdf_url":"","comment":"Project page at https://hyc001.github.io/MultiAnimate/"},{"id":"http://arxiv.org/abs/2511.15487v2","updated":"2026-02-25T04:44:25Z","published":"2025-11-19T14:43:04Z","title":"NTK-Guided Implicit Neural Teaching","summary":"Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.","authors":["Chen Zhang","Wei Zuo","Bingyang Cheng","Yikun Wang","Wei-Bin Kou","Yik Chung WU","Ngai Wong"],"pdf_url":"","comment":"CVPR 2026 (18 pages, 10 figures)"},{"id":"http://arxiv.org/abs/2405.19684v4","updated":"2026-02-25T04:21:30Z","published":"2024-05-30T04:46:40Z","title":"A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning","summary":"Underwater image enhancement (UIE) presents a significant challenge within computer vision research. Despite the development of numerous UIE algorithms, a thorough and systematic review is still absent. To foster future advancements, we provide a detailed overview of the UIE task from several perspectives. Firstly, we introduce the physical models, data construction processes, evaluation metrics, and loss functions. Secondly, we categorize and discuss recent algorithms based on their contributions, considering six aspects: network architecture, learning strategy, learning stage, auxiliary tasks, domain perspective, and disentanglement fusion. Thirdly, due to the varying experimental setups in the existing literature, a comprehensive and unbiased comparison is currently unavailable. To address this, we perform both quantitative and qualitative evaluations of state-of-the-art algorithms across multiple benchmark datasets. Lastly, we identify key areas for future research in UIE. A collection of resources for UIE can be found at {https://github.com/YuZhao1999/UIE}.","authors":["Xiaofeng Cong","Yu Zhao","Jie Gui","Junming Hou","Dacheng Tao"],"pdf_url":"","comment":"This article has been accepted for publication in IEEE Transactions on Emerging Topics in Computational Intelligence"},{"id":"http://arxiv.org/abs/2602.21552v1","updated":"2026-02-25T04:16:54Z","published":"2026-02-25T04:16:54Z","title":"Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction","summary":"Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.","authors":["Changqing Zhou","Yueru Luo","Changhao Chen"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2504.14868v2","updated":"2026-02-25T04:13:23Z","published":"2025-04-21T05:37:07Z","title":"Twin Co-Adaptive Dialogue for Progressive Image Generation","summary":"Modern text-to-image generation systems have enabled the creation of remarkably realistic and high-quality visuals, yet they often falter when handling the inherent ambiguities in user prompts. In this work, we present Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to progressively refine image generation. Instead of a static generation process, Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue agent continuously interacts with the user. Initially, a base image is generated from the user's prompt. Then, through a series of synchronized dialogue exchanges, the system adapts and optimizes the image according to evolving user feedback. The co-adaptive process allows the system to progressively narrow down ambiguities and better align with user intent. Experiments demonstrate that Twin-Co not only enhances user experience by reducing trial-and-error iterations but also improves the quality of the generated images, streamlining creative process across various applications.","authors":["Jianhui Wang","Yangfan He","Yan Zhong","Xinyuan Song","Jiayi Su","Yuheng Feng","Ruoyu Wang","Hongyang He","Wenyu Zhu","Xinhang Yuan","Miao Zhang","Keqin Li","Jiaqi Chen","Tianyu Shi","Xueqian Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2310.17167v2","updated":"2026-02-25T03:52:43Z","published":"2023-10-26T05:43:07Z","title":"Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise","summary":"This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\\displaystyle \\sqrt{\\barα}=\\cos(η)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\\mathbf{x}_0$) and noise ($\\mathbfε$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achieves faster generation, with the ability to converge on high-quality images more quickly, and higher quality of the generated images, as measured by metrics such as Frechet Inception Distance (FID), spatial Frechet Inception Distance (sFID), precision, and recall.","authors":["Zhenkai Zhang","Krista A. Ehinger","Tom Drummond"],"pdf_url":"","comment":"Published in Proceedings of the 15th Asian Conference on Machine Learning, PMLR 222:1638-1653, 2024"},{"id":"http://arxiv.org/abs/2602.21539v1","updated":"2026-02-25T03:50:48Z","published":"2026-02-25T03:50:48Z","title":"VasGuideNet: Vascular Topology-Guided Couinaud Liver Segmentation with Structural Contrastive Loss","summary":"Accurate Couinaud liver segmentation is critical for preoperative surgical planning and tumor localization.However, existing methods primarily rely on image intensity and spatial location cues, without explicitly modeling vascular topology. As a result, they often produce indistinct boundaries near vessels and show limited generalization under anatomical variability.We propose VasGuideNet, the first Couinaud segmentation framework explicitly guided by vascular topology. Specifically, skeletonized vessels, Euclidean distance transform (EDT)--derived geometry, and k-nearest neighbor (kNN) connectivity are encoded into topology features using Graph Convolutional Networks (GCNs). These features are then injected into a 3D encoder--decoder backbone via a cross-attention fusion module. To further improve inter-class separability and anatomical consistency, we introduce a Structural Contrastive Loss (SCL) with a global memory bank.On Task08_HepaticVessel and our private LASSD dataset, VasGuideNet achieves Dice scores of 83.68% and 76.65% with RVDs of 1.68 and 7.08, respectively. It consistently outperforms representative baselines including UNETR, Swin UNETR, and G-UNETR++, delivering higher Dice/mIoU and lower RVD across datasets, demonstrating its effectiveness for anatomically consistent segmentation. Code is available at https://github.com/Qacket/VasGuideNet.git.","authors":["Chaojie Shen","Jingjun Gu","Zihao Zhao","Ruocheng Li","Cunyuan Yang","Jiajun Bu","Lei Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20205v2","updated":"2026-02-25T03:48:00Z","published":"2026-02-22T21:02:47Z","title":"OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport","summary":"Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.","authors":["Xiwen Chen","Wenhui Zhu","Gen Li","Xuanzhao Dong","Yujian Xiong","Hao Wang","Peijie Qiu","Qingquan Song","Zhipeng Wang","Shao Tang","Yalin Wang","Abolfazl Razi"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2602.21536v1","updated":"2026-02-25T03:46:12Z","published":"2026-02-25T03:46:12Z","title":"IHF-Harmony: Multi-Modality Magnetic Resonance Images Harmonization using Invertible Hierarchy Flow Model","summary":"Retrospective MRI harmonization is limited by poor scalability across modalities and reliance on traveling subject datasets. To address these challenges, we introduce IHF-Harmony, a unified invertible hierarchy flow framework for multi-modality harmonization using unpaired data. By decomposing the translation process into reversible feature transformations, IHF-Harmony guarantees bijective mapping and lossless reconstruction to prevent anatomical distortion. Specifically, an invertible hierarchy flow (IHF) performs hierarchical subtractive coupling to progressively remove artefact-related features, while an artefact-aware normalization (AAN) employs anatomy-fixed feature modulation to accurately transfer target characteristics. Combined with anatomy and artefact consistency loss objectives, IHF-Harmony achieves high-fidelity harmonization that retains source anatomy. Experiments across multiple MRI modalities demonstrate that IHF-Harmony outperforms existing methods in both anatomical fidelity and downstream task performance, facilitating robust harmonization for large-scale multi-site imaging studies. Code will be released upon acceptance.","authors":["Pengli Zhu","Yitao Zhu","Haowen Pang","Anqi Qiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21535v1","updated":"2026-02-25T03:45:47Z","published":"2026-02-25T03:45:47Z","title":"Pseudo-View Enhancement via Confidence Fusion for Unposed Sparse-View Reconstruction","summary":"3D scene reconstruction under unposed sparse viewpoints is a highly challenging yet practically important problem, especially in outdoor scenes due to complex lighting and scale variation. With extremely limited input views, directly utilizing diffusion model to synthesize pseudo frames will introduce unreasonable geometry, which will harm the final reconstruction quality. To address these issues, we propose a novel framework for sparse-view outdoor reconstruction that achieves high-quality results through bidirectional pseudo frame restoration and scene perception Gaussian management. Specifically, we introduce a bidirectional pseudo frame restoration method that restores missing content by diffusion-based synthesis guided by adjacent frames with a lightweight pseudo-view deblur model and confidence mask inference algorithm. Then we propose a scene perception Gaussian management strategy that optimize Gaussians based on joint depth-density information. These designs significantly enhance reconstruction completeness, suppress floating artifacts and improve overall geometric consistency under extreme view sparsity. Experiments on outdoor benchmarks demonstrate substantial gains over existing methods in both fidelity and stability.","authors":["Beizhen Zhao","Sicheng Yu","Guanzhi Ding","Yu Hu","Hao Wang"],"pdf_url":"","comment":"14 pages"},{"id":"http://arxiv.org/abs/2602.17484v2","updated":"2026-02-25T03:43:48Z","published":"2026-02-19T15:54:55Z","title":"Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection","summary":"Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.","authors":["Yichen Lu","Siwei Nie","Minlong Lu","Xudong Yang","Xiaobo Zhang","Peng Zhang"],"pdf_url":"","comment":"Accepted by ICCV2025 Github: https://github.com/eddielyc/CopyNCE"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2602.22182v1","updated":"2026-02-25T18:28:38Z","published":"2026-02-25T18:28:38Z","title":"LiCQA : A Lightweight Complex Question Answering System","summary":"Over the last twenty years, significant progress has been made in designing and implementing Question Answering (QA) systems. However, addressing complex questions, the answers to which are spread across multiple documents, remains a challenging problem. Recent QA systems that are designed to handle complex questions work either on the basis of knowledge graphs, or utilise contem- porary neural models that are expensive to train, in terms of both computational resources and the volume of training data required. In this paper, we present LiCQA, an unsupervised question answer- ing model that works primarily on the basis of corpus evidence. We empirically compare the effectiveness and efficiency of LiCQA with two recently presented QA systems, which are based on different underlying principles. The results of our experiments show that LiCQA significantly outperforms these two state-of-the-art systems on benchmark data with noteworthy reduction in latency.","authors":["Sourav Saha","Dwaipayan Roy","Mandar Mitra"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.08480v2","updated":"2026-02-25T15:15:07Z","published":"2025-11-11T17:23:02Z","title":"Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding","summary":"Multimodal large language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that MLLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform an MLLM into a competitive embedding model. CoMa achieves new state-of-the-art results among MLLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.","authors":["Da Li","Yuxiao Luo","Keping Bi","Jiafeng Guo","Wei Yuan","Biao Yang","Yan Wang","Fan Yang","Tingting Gao","Guorui Zhou"],"pdf_url":"","comment":"Multimodal Embedding"},{"id":"http://arxiv.org/abs/2602.21957v1","updated":"2026-02-25T14:39:47Z","published":"2026-02-25T14:39:47Z","title":"Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation","summary":"Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective than enforcing shared representations. Specifically, global semantic relations serve as structural constraints for items. Within these constraints, the framework allows item representations to vary locally on each client, which flexibility enables the model to capture fine-grained user personalization while maintaining global consistency. To this end, we propose Cluster-Guided FedRec framework (CGFedRec), a framework that transforms uploaded embeddings into compact cluster labels. In this framework, the server functions as a global structure discoverer to learn item clusters and distributes only the resulting labels. This mechanism explicitly cuts off the downstream transmission of item embeddings, relieving clients from maintaining global shared item embeddings. Consequently, CGFedRec achieves the effective injection of global collaborative signals into local item representations without transmitting full embeddings. Extensive experiments demonstrate that our approach significantly improves communication efficiency while maintaining superior recommendation accuracy across multiple datasets.","authors":["Yuchun Tu","Zhiwei Li","Bingli Sun","Yixuan Li","Xiao Song"],"pdf_url":"","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.05085v6","updated":"2026-02-25T14:28:20Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by retrieving supporting documents into the prompt, but existing methods do not explicitly target queries that require fetching multiple documents with substantially different content. Such multi-aspect queries are challenging because relevant documents can be far apart in embedding space, making joint retrieval difficult. We introduce Multi-Head RAG (MRAG), which addresses this gap with a simple yet powerful idea: using Transformer multi-head attention activations rather than the standard decoder-layer embedding, as retrieval keys. It leverages the observation that different heads capture different semantic aspects. This yields multi-aspect embeddings for both documents and queries, improving retrieval accuracy on complex queries. We show MRAG's design advantages over 18 RAG baselines, up to 20% higher retrieval success ratios for real-world use cases, and improved downstream LLM generation. MRAG integrates seamlessly with existing RAG frameworks and benchmarks.","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.05154v4","updated":"2026-02-25T12:22:09Z","published":"2025-06-05T15:34:15Z","title":"Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement","summary":"Retrieval-augmented generation (RAG) improves performance on knowledge-intensive tasks but can be derailed by wrong, irrelevant, or conflicting retrieved text, causing models to rely on inaccurate evidence and cascade errors. We propose Knowledgeable-R1, a reinforcement-learning framework that explicitly trains large language models to use parametric knowledge (PK) to resist contextual interference while still exploiting external context when it is reliably helpful. Knowledgeable-R1 introduces a joint sampling scheme that generates paired responses with and without retrieval, and learns both local advantages (within each decoding regime) and global advantages under the same input to quantify when to ignore misleading context versus adopt it. We employ an asymmetric advantage transformation that amplifies exploratory behaviors toward parametric knowledge. Experiments show that Knowledgeable-R1 significantly improves robustness and reasoning accuracy in knowledge conflict scenarios and general RAG scenarios, outperforming SOTA baselines by +22.89% in counterfactual scenarios, and without degradation when the retrieved context is fully accurate.Our code are available at https://github.com/lcy80366872/knowledgeable-R1.","authors":["Chenyu Lin","Yilin Wen","Du Su","Hexiang Tan","Fei Sun","Muhan Chen","Chenfu Bao","Zhonghou Lyu"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20800v2","updated":"2026-02-25T10:31:56Z","published":"2026-02-24T11:38:36Z","title":"Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking","summary":"In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.","authors":["Dalia Nahhas","Xiaohao Cai","Imran Razzak","Shoaib Jameel"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21756v1","updated":"2026-02-25T10:14:30Z","published":"2026-02-25T10:14:30Z","title":"Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing","summary":"Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.","authors":["Deogyong Kim","Junseong Lee","Jeongeun Lee","Changhoe Kim","Junguel Lee","Jungseok Lee","Dongha Lee"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2507.21989v2","updated":"2026-02-25T10:13:41Z","published":"2025-07-29T16:39:54Z","title":"Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors","summary":"Advances in embedding models for text, image, audio, and video drive progress across multiple domains, including retrieval-augmented generation, recommendation systems, and others. Many of these applications require an efficient method to retrieve items that are close to a given query in the embedding space while satisfying a filter condition based on the item's attributes, a problem known as filtered approximate nearest neighbor search (FANNS). By performing an in-depth literature analysis on FANNS, we identify a key gap in the research landscape: publicly available datasets with embedding vectors from state-of-the-art transformer-based text embedding models that contain abundant real-world attributes covering a broad spectrum of attribute types and value distributions. To fill this gap, we introduce the arxiv-for-fanns dataset of transformer-based embedding vectors for the abstracts of over 2.7 million arXiv papers, enriched with 11 real-world attributes such as authors and categories. We benchmark eleven different FANNS methods on our new dataset to evaluate their performance across different filter types, numbers of retrieved neighbors, dataset scales, and query selectivities. We distill our findings into eight key observations that guide users in selecting the most suitable FANNS method for their specific use cases.","authors":["Patrick Iff","Paul Bruegger","Marcin Chrapek","David Kochergin","Maciej Besta","Torsten Hoefler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.10606v3","updated":"2026-02-25T10:05:15Z","published":"2026-02-11T07:54:26Z","title":"S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage","summary":"Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference.","authors":["Jie Jiang","Hongbo Tang","Wenjie Wu","Yangru Huang","Zhenmao Li","Qian Li","Changping Wang","Jun Zhang","Huan Yu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.20094v3","updated":"2026-02-25T09:37:59Z","published":"2025-04-26T00:55:43Z","title":"Toward Safe and Human-Aligned Game Conversational Recommendation via Multi-Agent Decomposition","summary":"Conversational recommender systems (CRS) have advanced with large language models, showing strong results in domains like movies. These domains typically involve fixed content and passive consumption, where user preferences can be matched by genre or theme. In contrast, games present distinct challenges: fast-evolving catalogs, interaction-driven preferences (e.g., skill level, mechanics, hardware), and increased risk of unsafe responses in open-ended conversation. We propose MATCHA, a multi-agent framework for CRS that assigns specialized agents for intent parsing, tool-augmented retrieval, multi-LLM ranking with reflection, explanation, and risk control which enabling finer personalization, long-tail coverage, and stronger safety. Evaluated on real user request dataset, MATCHA outperforms six baselines across eight metrics, improving Hit@5 by 20%, reducing popularity bias by 24%, and achieving 97.9% adversarial defense. Human and virtual-judge evaluations confirm improved explanation quality and user alignment.","authors":["Zheng Hui","Xiaokai Wei","Yexi Jiang","Kevin Gao","Chen Wang","Frank Ong","Se-eun Yoon","Rachit Pareek","Michelle Gong"],"pdf_url":"","comment":"ICML 2025 MAS, EACL 2026"},{"id":"http://arxiv.org/abs/2510.22049v2","updated":"2026-02-25T09:18:59Z","published":"2025-10-24T22:17:49Z","title":"Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders","summary":"Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.","authors":["Zhimin Chen","Chenyu Zhao","Ka Chun Mo","Yunjiang Jiang","Jane H. Lee","Khushhall Chandra Mahajan","Ning Jiang","Kai Ren","Jinhui Li","Wen-Yun Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21677v1","updated":"2026-02-25T08:25:16Z","published":"2026-02-25T08:25:16Z","title":"Trie-Aware Transformers for Generative Recommendation","summary":"Generative recommendation (GR) aligns with advances in generative AI by casting next-item prediction as token-level generation rather than score-based ranking. Most GR methods adopt a two-stage pipeline: (i) \\textit{item tokenization}, which maps each item to a sequence of discrete, hierarchically organized tokens; and (ii) \\textit{autoregressive generation}, which predicts the next item's tokens conditioned on the tokens of user's interaction history. Although hierarchical tokenization induces a prefix tree (trie) over items, standard autoregressive modeling with conventional Transformers often flattens item tokens into a linear stream and overlooks the underlying topology.\n  To address this, we propose TrieRec, a trie-aware generative recommendation method that augments Transformers with structural inductive biases via two positional encodings. First, a \\textit{trie-aware absolute positional encoding} aggregates a token's (node's) local structural context (\\eg depth, ancestors, and descendants) into the token representation. Second, a \\textit{topology-aware relative positional encoding} injects pairwise structural relations into self-attention to capture topology-induced semantic relatedness. TrieRec is also model-agnostic, efficient, and hyperparameter-free. In our experiments, we implement TrieRec within three representative GR backbones, achieving notably improvements of 8.83\\% on average across four real-world datasets.","authors":["Zhenxiang Xu","Jiawei Chen","Sirui Chen","Yong He","Jieyu Yang","Chuan Yuan","Ke Ding","Can Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.27566v2","updated":"2026-02-25T08:19:38Z","published":"2025-10-31T15:48:43Z","title":"Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval","summary":"Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.","authors":["Yulong Hui","Chao Chen","Zhihang Fu","Yihao Liu","Jieping Ye","Huanchen Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.13703v3","updated":"2026-02-25T07:20:04Z","published":"2025-04-18T14:03:40Z","title":"C$^3$: Capturing Consensus with Contrastive Learning in Group Recommendation","summary":"Group recommendation aims to recommend tailored items to groups of users, where the key challenge is modeling a consensus that reflects member preferences. Although several deep learning models have improved performance, they still struggle to capture consensus in two important aspects: (1) capturing consensus in small groups (2~5 members), which better reflect real-world scenarios; and (2) balancing individual and group performance while improving overall group accuracy. To address these issues, we propose C$^3$(Capturing Consensus with Contrastive Learning) for group recommendation, which explicitly explores the consensus underlying group decision-making. C$^3$ uses a Transformer encoder to learn both user and group representations, and employs contrastive learning to mitigate overfitting for users with many interactions, resulting in more robust group representations. Experiments on four public datasets show that C$^3$ consistently outperforms state-of-the-art baselines in both user and group recommendation tasks.","authors":["Soyoung Kim","Dongjun Lee","Jaekwang Kim"],"pdf_url":"","comment":"12 pages, 4 figures, accepted by PAKDD 2026 special session"},{"id":"http://arxiv.org/abs/2602.21600v1","updated":"2026-02-25T05:58:16Z","published":"2026-02-25T05:58:16Z","title":"AQR-HNSW: Accelerating Approximate Nearest Neighbor Search via Density-aware Quantization and Multi-stage Re-ranking","summary":"Approximate Nearest Neighbor (ANN) search has become fundamental to modern AI infrastructure, powering recommendation systems, search engines, and large language models across industry leaders from Google to OpenAI. Hierarchical Navigable Small World (HNSW) graphs have emerged as the dominant ANN algorithm, widely adopted in production systems due to their superior recall versus latency balance. However, as vector databases scale to billions of embeddings, HNSW faces critical bottlenecks: memory consumption expands, distance computation overhead dominates query latency, and it suffers suboptimal performance on heterogeneous data distributions. This paper presents Adaptive Quantization and Rerank HNSW (AQR-HNSW), a novel framework that synergistically integrates three strategies to enhance HNSW scalability. AQR-HNSW introduces (1) density-aware adaptive quantization, achieving 4x compression while preserving distance relationships; (2) multi-state re-ranking that reduces unnecessary computations by 35%; and (3) quantization-optimized SIMD implementations delivering 16-64 operations per cycle across architectures. Evaluation on standard benchmarks demonstrates 2.5-3.3x higher queries per second (QPS) than state-of-the-art HNSW implementations while maintaining over 98% recall, with 75% memory reduction for the index graph and 5x faster index construction.","authors":["Ganap Ashit Tewary","Nrusinga Charan Gantayat","Jeff Zhang"],"pdf_url":"","comment":"Accepted at DAC 2026"},{"id":"http://arxiv.org/abs/2602.21598v1","updated":"2026-02-25T05:48:15Z","published":"2026-02-25T05:48:15Z","title":"Retrieval Challenges in Low-Resource Public Service Information: A Case Study on Food Pantry Access","summary":"Public service information systems are often fragmented, inconsistently formatted, and outdated. These characteristics create low-resource retrieval environments that hinder timely access to critical services. We investigate retrieval challenges in such settings through the domain of food pantry access, a socially urgent problem given persistent food insecurity. We develop an AI-powered conversational retrieval system that scrapes and indexes publicly available pantry data and employs a Retrieval-Augmented Generation (RAG) pipeline to support natural language queries via a web interface. We conduct a pilot evaluation study using community-sourced queries to examine system behavior in realistic scenarios. Our analysis reveals key limitations in retrieval robustness, handling underspecified queries, and grounding over inconsistent knowledge bases. This ongoing work exposes fundamental IR challenges in low-resource environments and motivates future research on robust conversational retrieval to improve access to critical public resources.","authors":["Touseef Hasan","Laila Cure","Souvika Sarkar"],"pdf_url":"","comment":"3 pages, 1 figure"},{"id":"http://arxiv.org/abs/2602.21553v1","updated":"2026-02-25T04:19:06Z","published":"2026-02-25T04:19:06Z","title":"Revisiting RAG Retrievers: An Information Theoretic Benchmark","summary":"Retrieval-Augmented Generation (RAG) systems rely critically on the retriever module to surface relevant context for large language models. Although numerous retrievers have recently been proposed, each built on different ranking principles such as lexical matching, dense embeddings, or graph citations, there remains a lack of systematic understanding of how these mechanisms differ and overlap. Existing benchmarks primarily compare entire RAG pipelines or introduce new datasets, providing little guidance on selecting or combining retrievers themselves. Those that do compare retrievers directly use a limited set of evaluation tools which fail to capture complementary and overlapping strengths. This work presents MIGRASCOPE, a Mutual Information based RAG Retriever Analysis Scope. We revisit state-of-the-art retrievers and introduce principled metrics grounded in information and statistical estimation theory to quantify retrieval quality, redundancy, synergy, and marginal contribution. We further show that if chosen carefully, an ensemble of retrievers outperforms any single retriever. We leverage the developed tools over major RAG corpora to provide unique insights on contribution levels of the state-of-the-art retrievers. Our findings provide a fresh perspective on the structure of modern retrieval techniques and actionable guidance for designing robust and efficient RAG systems.","authors":["Wenqing Zheng","Dmitri Kalaev","Noah Fatsi","Daniel Barcklow","Owen Reinert","Igor Melnyk","Senthil Kumar","C. Bayan Bruss"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.19514v6","updated":"2026-02-25T04:01:25Z","published":"2025-11-24T03:00:04Z","title":"SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation","summary":"Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a Generate-Validate-Mine (GVM) pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Empirically, experiments on four benchmarks demonstrate consistent improvements across diverse backbones. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER presents a practical and unified framework for integrating structured LLM reasoning into recommender systems, validated by consistent improvements in both offline benchmarks and online production environments.","authors":["Jie Jiang","Yang Wu","Qian Li","Yuling Xiong","Hongbo Tang","Xun Liu","Haoze Wang","Jun Zhang","Huan Yu","Hailong Shi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21543v1","updated":"2026-02-25T03:58:24Z","published":"2026-02-25T03:58:24Z","title":"Enhancing Multilingual Embeddings via Multi-Way Parallel Text Alignment","summary":"Multilingual pretraining typically lacks explicit alignment signals, leading to suboptimal cross-lingual alignment in the representation space. In this work, we show that training standard pretrained models for cross-lingual alignment with a multi-way parallel corpus in a diverse pool of languages can substantially improve multilingual and cross-lingual representations for NLU tasks. We construct a multi-way parallel dataset using translations of English text from an off-the-shelf NMT model for a pool of six target languages and achieve strong cross-lingual alignment through contrastive learning. This leads to substantial performance gains across both seen and unseen languages for multiple tasks from the MTEB benchmark evaluated for XLM-Roberta and multilingual BERT base models. Using a multi-way parallel corpus for contrastive training yields substantial gains on bitext mining (21.3%), semantic similarity (5.3%), and classification (28.4%) compared to English-centric (En-X) bilingually parallel data, where X is sampled from a pool of multiple target languages. Furthermore, finetuning mE5 model on a small dataset with multi-way parallelism significantly improves bitext mining compared to one without, underscoring the importance of multi-way cross-lingual supervision even for models already pretrained for high-quality sentence embeddings.","authors":["Barah Fazili","Koustava Goswami"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.14640v4","updated":"2026-02-25T02:20:54Z","published":"2025-10-16T12:54:40Z","title":"LUMI: Unsupervised Intent Clustering with Multiple Pseudo-Labels","summary":"In this paper, we propose an intuitive, training-free and label-free method for intent clustering in conversational search. Current approaches to short text clustering use LLM-generated pseudo-labels to enrich text representations or to identify similar text pairs for pooling. The limitations are: (1) each text is assigned only a single label, and refining representations toward a single label can be unstable; (2) text-level similarity is treated as a binary selection, which fails to account for continuous degrees of similarity. Our method LUMI is designed to amplify similarities between texts by using shared pseudo-labels. We first generate pseudo-labels for each text and collect them into a pseudo-label set. Next, we compute the mean of the pseudo-label embeddings and pool it with the text embedding. Finally, we perform text-level pooling: Each text representation is pooled with its similar pairs, where similarity is determined by the degree of shared labels. Our evaluation on four benchmark sets shows that our approach achieves competitive results, better than recent state-of-the-art baselines, while avoiding the need to estimate the number of clusters during embedding refinement, as is required by most methods. Our findings indicate that LUMI can effectively be applied in unsupervised short-text clustering scenarios.","authors":["I-Fan Lin","Faegheh Hasibi","Suzan Verberne"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21480v1","updated":"2026-02-25T01:12:35Z","published":"2026-02-25T01:12:35Z","title":"Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?","summary":"Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.","authors":["Germán T. Eizaguirre","Lars Tissen","Marc Sánchez-Artigas"],"pdf_url":"","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.21456v1","updated":"2026-02-25T00:18:07Z","published":"2026-02-25T00:18:07Z","title":"Revisiting Text Ranking in Deep Research","summary":"Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.","authors":["Chuan Meng","Litu Ou","Sean MacAvaney","Jeff Dalton"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22462v1","updated":"2026-02-25T22:51:31Z","published":"2026-02-25T22:51:31Z","title":"MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation","summary":"Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.","authors":["Raiyan Jahangir","Nafiz Imtiaz Khan","Amritanand Sudheerkumar","Vladimir Filkov"],"pdf_url":"","comment":"arXiv preprint (submitted 25 Feb 2026). Local multi-model pipeline for mammography report generation + classification using prompting, multimodal RAG (ChromaDB), and QLoRA fine-tuning; evaluates MedGemma, LLaVA-Med, Qwen2.5-VL on VinDr-Mammo and DMID; reports BERTScore/ROUGE-L and classification metrics"},{"id":"http://arxiv.org/abs/2602.11836v2","updated":"2026-02-25T19:58:17Z","published":"2026-02-12T11:26:46Z","title":"ULTRA:Urdu Language Transformer-based Recommendation Architecture","summary":"Urdu, as a low-resource language, lacks effective semantic content recommendation systems, particularly in the domain of personalized news retrieval. Existing approaches largely rely on lexical matching or language-agnostic techniques, which struggle to capture semantic intent and perform poorly under varying query lengths and information needs. This limitation results in reduced relevance and adaptability in Urdu content recommendation. We propose ULTRA (Urdu Language Transformer-based Recommendation Architecture),an adaptive semantic recommendation framework designed to address these challenges. ULTRA introduces a dual-embedding architecture with a query-length aware routing mechanism that dynamically distinguishes between short, intent-focused queries and longer, context-rich queries. Based on a threshold-driven decision process, user queries are routed to specialized semantic pipelines optimized for either title/headline-level or full-content/document level representations, ensuring appropriate semantic granularity during retrieval. The proposed system leverages transformer-based embeddings and optimized pooling strategies to move beyond surface-level keyword matching and enable context-aware similarity search. Extensive experiments conducted on a large-scale Urdu news corpus demonstrate that the proposed architecture consistently improves recommendation relevance across diverse query types. Results show gains in precision above 90% compared to single-pipeline baselines, highlighting the effectiveness of query-adaptive semantic alignment for low-resource languages. The findings establish ULTRA as a robust and generalizable content recommendation architecture, offering practical design insights for semantic retrieval systems in low-resource language settings.","authors":["Alishbah Bashir","Fatima Qaiser","Ijaz Hussain"],"pdf_url":"","comment":"25 pages, 24 figures, 10 tables"},{"id":"http://arxiv.org/abs/2602.22278v1","updated":"2026-02-25T10:31:32Z","published":"2026-02-25T10:31:32Z","title":"RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval","summary":"Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM","authors":["Dawei Su","Dongsheng Wang"],"pdf_url":"","comment":"5 pages, 2 figure"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2602.22207v1","updated":"2026-02-25T18:58:25Z","published":"2026-02-25T18:58:25Z","title":"Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets","summary":"The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.","authors":["Hanna Yukhymenko","Anton Alexandrov","Martin Vechev"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2504.17203v4","updated":"2026-02-25T18:55:05Z","published":"2025-04-24T02:27:17Z","title":"High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services","summary":"The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing.","authors":["Shivasankari Kannan","Yeounoh Chung","Amita Gondi","Tristan Swadell","Fatma Ozcan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18292v2","updated":"2026-02-25T18:47:28Z","published":"2026-02-20T15:38:16Z","title":"Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers","summary":"Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.","authors":["Xiaotong Ji","Rasul Tutunov","Matthieu Zimmer","Haitham Bou-Ammar"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.07524v2","updated":"2026-02-25T18:40:10Z","published":"2026-01-12T13:25:21Z","title":"Stagewise Reinforcement Learning and the Geometry of the Regret Landscape","summary":"Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to reinforcement learning, proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that deep reinforcement learning with SGD should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over training manifest as \"opposing staircases\" where regret decreases sharply while the LLC increases.","authors":["Chris Elliott","Einar Urdshals","David Quarel","Matthew Farrugia-Roberts","Daniel Murfet"],"pdf_url":"","comment":"48 pages, 10 figures"},{"id":"http://arxiv.org/abs/2602.22190v1","updated":"2026-02-25T18:34:57Z","published":"2026-02-25T18:34:57Z","title":"GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL","summary":"Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.","authors":["Rui Yang","Qianhui Wu","Zhaoyang Wang","Hanyang Chen","Ke Yang","Hao Cheng","Huaxiu Yao","Baoling Peng","Huan Zhang","Jianfeng Gao","Tong Zhang"],"pdf_url":"","comment":"57 pages, 17 figures"},{"id":"http://arxiv.org/abs/2602.22188v1","updated":"2026-02-25T18:34:03Z","published":"2026-02-25T18:34:03Z","title":"Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach","summary":"Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.","authors":["Nathalie C. Pinheiro","Donghu Guo","Hannah P. Menke","Aniket C. Joshi","Claire E. Heaney","Ahmed H. ElSheikh","Christopher C. Pain"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22179v1","updated":"2026-02-25T18:25:47Z","published":"2026-02-25T18:25:47Z","title":"Learning and Naming Subgroups with Exceptional Survival Characteristics","summary":"In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.","authors":["Mhd Jawad Al Rahwanji","Sascha Xu","Nils Philipp Walter","Jilles Vreeken"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.16762v2","updated":"2026-02-25T18:23:01Z","published":"2025-12-18T16:59:10Z","title":"NRGPT: An Energy-based Alternative for GPT","summary":"Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.","authors":["Nima Dehmamy","Benjamin Hoover","Bishwajit Saha","Leo Kozachkov","Jean-Jacques Slotine","Dmitry Krotov"],"pdf_url":"","comment":"Accepted to ICLR 2026 main conference"},{"id":"http://arxiv.org/abs/2505.23725v2","updated":"2026-02-25T18:22:58Z","published":"2025-05-29T17:55:37Z","title":"MuLoCo: Muon is a practical inner optimizer for DiLoCo","summary":"DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo's performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo's behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon's normalized optimizer steps can affect the pseudogradient's quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K>=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K>2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size.","authors":["Benjamin Thérien","Xiaolong Huang","Aaron Defazio","Irina Rish","Eugene Belilovsky"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20156v3","updated":"2026-02-25T18:14:01Z","published":"2026-02-23T18:59:27Z","title":"Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks","summary":"LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.","authors":["David Schmotz","Luca Beurer-Kellner","Sahar Abdelnabi","Maksym Andriushchenko"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.18182v2","updated":"2026-02-25T18:12:06Z","published":"2026-02-20T12:40:18Z","title":"Capabilities Ain't All You Need: Measuring Propensities in AI","summary":"AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.","authors":["Daniel Romero-Alvarado","Fernando Martínez-Plumed","Lorenzo Pacchiardi","Hugo Save","Siddhesh Milind Pawar","Behzad Mehrbakhsh","Pablo Antonio Moreno Casares","Ben Slater","Paolo Bova","Peter Romero","Zachary R. Tyler","Jonathan Prunty","Luning Sun","Jose Hernandez-Orallo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.19881v3","updated":"2026-02-25T18:06:46Z","published":"2025-06-23T20:46:51Z","title":"Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models","summary":"Are there any conditions under which a generative model's outputs are guaranteed not to infringe the copyrights of its training data? This is the question of \"provable copyright protection\" first posed by Vyas, Kakade, and Barak (ICML 2023). They define near access-freeness (NAF) and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection -- foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copyright protection that we dub being tainted. Then, we introduce our blameless copyright protection framework for defining meaningful guarantees, and instantiate it with clean-room copyright protection. Clean-room copyright protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual \"clean-room setting.\" Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copyright protection when the dataset is golden, a copyright deduplication requirement.","authors":["Aloni Cohen"],"pdf_url":"","comment":"Appeared at NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.22157v1","updated":"2026-02-25T18:05:11Z","published":"2026-02-25T18:05:11Z","title":"Dynamic Personality Adaptation in Large Language Models via State Machines","summary":"The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.","authors":["Leon Pielage","Ole Hätscher","Mitja Back","Bernhard Marschall","Benjamin Risse"],"pdf_url":"","comment":"22 pages, 5 figures, submitted to ICPR 2026"},{"id":"http://arxiv.org/abs/2602.17849v2","updated":"2026-02-25T17:58:32Z","published":"2026-02-19T21:31:33Z","title":"Quad Length Codes for Lossless Compression of e4m3","summary":"Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Quad Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. The coding scheme uses 3 prefix bits to divide the 256 symbols into 8 areas. Each area has a different code length and encodes a different number of symbols. The scheme uses a Look Up Table with 256 entries, significantly simplifying the hardware implementation compared to Huffman trees. The coding scheme can be adapted for different distributions. For the e4m3 data type, the scheme achieves a compressibility of 13.9% in comparison to 15.9% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.","authors":["Aditya Agrawal","Albert Magyar","Hiteshwar Eswaraiah","Patrick Sheridan","Pradeep Janedula","Ravi Krishnan Venkatesan","Krishna Nair","Ravi Iyer"],"pdf_url":"","comment":"The first version proposed lossless compression of BFloat16 using dual length codes. This version proposes lossless compression of e4m3 using quad length codes. The versions will be merged later"},{"id":"http://arxiv.org/abs/2602.22146v1","updated":"2026-02-25T17:54:52Z","published":"2026-02-25T17:54:52Z","title":"Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual","summary":"Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.","authors":["Yining Li","Peizhong Ju","Ness Shroff"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.25017v2","updated":"2026-02-25T17:53:00Z","published":"2025-12-31T18:11:51Z","title":"Convergence of the generalization error for deep gradient flow methods for PDEs","summary":"The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.","authors":["Chenguang Liu","Antonis Papapantoleon","Jasper Rou"],"pdf_url":"","comment":"29 pages"},{"id":"http://arxiv.org/abs/2510.26656v3","updated":"2026-02-25T17:52:16Z","published":"2025-10-30T16:23:46Z","title":"Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems","summary":"In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.","authors":["Georgios Kamaras","Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2602.20946v2","updated":"2026-02-25T17:41:07Z","published":"2026-02-24T14:29:45Z","title":"Some Simple Economics of AGI","summary":"For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.","authors":["Christian Catalini","Xiang Hui","Jane Wu"],"pdf_url":"","comment":"JEL Classification: D82, D83, J23, J24, L23, O33. 112 pages, 3 figures"},{"id":"http://arxiv.org/abs/2602.22136v1","updated":"2026-02-25T17:34:14Z","published":"2026-02-25T17:34:14Z","title":"SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference","summary":"Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \\textbf{\\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.","authors":["Qunyou Liu","Pengbo Yu","Marina Zapater","David Atienza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.03178v4","updated":"2026-02-25T17:27:35Z","published":"2025-03-05T04:48:14Z","title":"Active operator learning with predictive uncertainty quantification for partial differential equations","summary":"With the increased prevalence of neural operators being used to provide rapid solutions to partial differential equations (PDEs), understanding the accuracy of model predictions and the associated error levels is necessary for deploying reliable surrogate models in scientific applications. Existing uncertainty quantification (UQ) frameworks employ ensembles or Bayesian methods, which can incur substantial computational costs during both training and inference. We propose a lightweight predictive UQ method tailored for Deep operator networks (DeepONets) that also generalizes to other operator networks. Numerical experiments on linear and nonlinear PDEs demonstrate that the framework's uncertainty estimates are unbiased and provide accurate out-of-distribution uncertainty predictions with a sufficiently large training dataset. Our framework provides fast inference and uncertainty estimates that can efficiently drive outer-loop analyses that would be prohibitively expensive with conventional solvers. We demonstrate how predictive uncertainties can be used in the context of Bayesian optimization and active learning problems to yield improvements in accuracy and data-efficiency for outer-loop optimization procedures. In the active learning setup, we extend the framework to Fourier Neural Operators (FNO) and describe a generalized method for other operator networks. To enable real-time deployment, we introduce an inference strategy based on precomputed trunk outputs and a sparse placement matrix, reducing evaluation time by more than a factor of five. Our method provides a practical route to uncertainty-aware operator learning in time-sensitive settings.","authors":["Nick Winovich","Mitchell Daneker","Lu Lu","Guang Lin"],"pdf_url":"","comment":"Submitted to the Journal of Computational Physics"},{"id":"http://arxiv.org/abs/2602.22130v1","updated":"2026-02-25T17:21:23Z","published":"2026-02-25T17:21:23Z","title":"Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination","summary":"We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Sihan Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22124v1","updated":"2026-02-25T17:11:49Z","published":"2026-02-25T17:11:49Z","title":"SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents","summary":"Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).","authors":["Patrick Tser Jern Kon","Archana Pradeep","Ang Chen","Alexander P. Ellis","Warren Hunt","Zijian Wang","John Yang","Samuel Thompson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20309v2","updated":"2026-02-25T17:11:08Z","published":"2026-02-23T19:55:54Z","title":"QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models","summary":"Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.","authors":["Jingxuan Zhang","Yunta Hsieh","Zhongwei Wan","Haokun Lin","Xin Wang","Ziqi Wang","Yingtie Lei","Mi Zhang"],"pdf_url":"","comment":"CVPR2026"},{"id":"http://arxiv.org/abs/2602.22122v1","updated":"2026-02-25T17:10:59Z","published":"2026-02-25T17:10:59Z","title":"Probing the Geometry of Diffusion Models with the String Method","summary":"Understanding the geometry of learned distributions is fundamental to improving and interpreting diffusion models, yet systematic tools for exploring their landscape remain limited. Standard latent-space interpolations fail to respect the structure of the learned distribution, often traversing low-density regions. We introduce a framework based on the string method that computes continuous paths between samples by evolving curves under the learned score function. Operating on pretrained models without retraining, our approach interpolates between three regimes: pure generative transport, which yields continuous sample paths; gradient-dominated dynamics, which recover minimum energy paths (MEPs); and finite-temperature string dynamics, which compute principal curves -- self-consistent paths that balance energy and entropy. We demonstrate that the choice of regime matters in practice. For image diffusion models, MEPs contain high-likelihood but unrealistic ''cartoon'' images, confirming prior observations that likelihood maxima appear unrealistic; principal curves instead yield realistic morphing sequences despite lower likelihood. For protein structure prediction, our method computes transition pathways between metastable conformers directly from models trained on static structures, yielding paths with physically plausible intermediates. Together, these results establish the string method as a principled tool for probing the modal structure of diffusion models -- identifying modes, characterizing barriers, and mapping connectivity in complex learned distributions.","authors":["Elio Moreau","Florentin Coeurdoux","Grégoire Ferre","Eric Vanden-Eijnden"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.18615v3","updated":"2026-02-25T17:09:15Z","published":"2025-02-25T20:01:06Z","title":"A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation","summary":"We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.","authors":["Georgios Kamaras","Subramanian Ramamoorthy"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22115v1","updated":"2026-02-25T17:01:52Z","published":"2026-02-25T17:01:52Z","title":"Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing","summary":"Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by NNs, offering correctness guarantees. However, scalability remains a concern in these methods. This paper proposes an approach leveraging domain slicing to facilitate explanation generation for NNs. By reducing the complexity of logical constraints through slicing, we decrease explanation time by up to 40\\% less time, as indicated through comparative experiments. Our findings highlight the efficacy of domain slicing in enhancing explanation efficiency for NNs.","authors":["Luiz Fernando Paulino Queiroz","Carlos Henrique Leitão Cavalcante","Thiago Alves Rocha"],"pdf_url":"","comment":"Preprint version. For the final published version, see the DOI below"},{"id":"http://arxiv.org/abs/2602.22107v1","updated":"2026-02-25T16:56:14Z","published":"2026-02-25T16:56:14Z","title":"Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection","summary":"Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. Our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria.","authors":["Andrea Apicella","Francesco Isgrò","Andrea Pollastro","Roberto Prevete"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04605v4","updated":"2026-02-25T16:49:58Z","published":"2025-08-06T16:25:19Z","title":"Multitask Learning with Stochastic Interpolants","summary":"We propose a framework for learning maps between probability distributions that broadly generalizes the time dynamics of flow and diffusion models. To enable this, we generalize stochastic interpolants by replacing the scalar time variable with vectors, matrices, or linear operators, allowing us to bridge probability distributions across multiple dimensional spaces. This approach enables the construction of versatile generative models capable of fulfilling multiple tasks without task-specific training. Our operator-based interpolants not only provide a unifying theoretical perspective for existing generative models but also extend their capabilities. Through numerical experiments, we demonstrate the zero-shot efficacy of our method on conditional generation and inpainting, fine-tuning and posterior sampling, and multiscale modeling, suggesting its potential as a generic task-agnostic alternative to specialized models.","authors":["Hugo Negrel","Florentin Coeurdoux","Michael S. Albergo","Eric Vanden-Eijnden"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22101v1","updated":"2026-02-25T16:48:07Z","published":"2026-02-25T16:48:07Z","title":"On Imbalanced Regression with Hoeffding Trees","summary":"Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]. Moreover, another recent line of work for batch learning, called hierarchical shrinkage (HS) [Agarwal et al., 2022], has introduced a post-hoc regularization method for decision trees that does not alter the structure of the learned tree. Using a telescoping argument we cast KDE to streaming environments and extend the implementation of HS to incremental decision tree models. Armed with these extensions we investigate the performance of decision trees that may enjoy such options in datasets commonly used for regression in online settings. We conclude that KDE is beneficial in the early parts of the stream, while HS hardly, if ever, offers performance benefits. Our code is publicly available at: https://github.com/marinaAlchirch/DSFA_2026.","authors":["Pantia-Marina Alchirch","Dimitrios I. Diochnos"],"pdf_url":"","comment":"13 pages, 6 figures, 1 table, 2 algorithms, authors' version of paper accepted in PAKDD 2026 special session on Data Science: Foundations and Applications (DSFA)"},{"id":"http://arxiv.org/abs/2511.21104v2","updated":"2026-02-25T16:45:04Z","published":"2025-11-26T06:39:19Z","title":"BRIDGE: Building Representations In Domain Guided Program Synthesis","summary":"Large language models (LLMs) are good at generating code, but remain brittle for formal verification in systems like Lean4. A core scalability challenge is that verified synthesis requires consistent outputs across multiple artifacts: executable code, precise specifications, theorem statements, and ultimately proofs. Existing approaches rarely treat these as a unified pipeline. We present BRIDGE, a structured prompting framework that decomposes verification into three interconnected domains: Code (implementations), Specifications (formal intent), and Theorem Statements (constructive correctness claims), and elicits domain-specific intermediate reasoning to connect them. In Lean4, BRIDGE often adopts a code-first workflow, using the generated implementation as a semantic anchor for downstream specification and theorem statement generation. Across 178 algorithmic problems and five LLMs, BRIDGE improves Lean executable correctness by nearly 1.5x (pass at 5) over direct baselines and can be 2x more sample-efficient at inference time, requiring fewer samples per verified solution at comparable generation lengths. We further find that specification-driven prompting improves Python pass rates by up to 17.5 percent. Beyond inference-time prompting, supervised fine-tuning on BRIDGE-style reasoning traces yields nearly 1.5x higher Lean pass success than code-only SFT, indicating that these intermediate representations are learnable. BRIDGE provides a practical foundation for scaling verified synthesis and motivates future work on expert iteration and full proof generation.","authors":["Robert Joseph George","Carson Eisenach","Udaya Ghai","Dominique Perrault-Joncas","Anima Anandkumar","Dean Foster"],"pdf_url":"","comment":"Approx. 23 pages including appendices, 10 figures, 3 tables. Empirical study of LLM-based verified program synthesis in Lean4 (code, specs, and proofs)"},{"id":"http://arxiv.org/abs/2410.16718v8","updated":"2026-02-25T16:44:01Z","published":"2024-10-22T05:56:57Z","title":"Learning Partial Graph Matching via Optimal Partial Transport","summary":"Partial graph matching extends traditional graph matching by allowing some nodes to remain unmatched, enabling applications in more complex scenarios. However, this flexibility introduces additional complexity, as both the subset of nodes to match and the optimal mapping must be determined. While recent studies have explored deep learning techniques for partial graph matching, a significant limitation remains: the absence of an optimization objective that fully captures the problem's intrinsic nature while enabling efficient solutions. In this paper, we propose a novel optimization framework for partial graph matching, inspired by optimal partial transport. Our approach formulates an objective that enables partial assignments while incorporating matching biases, using weighted total variation as the divergence function to guarantee optimal partial assignments. Our method can achieve efficient, exact solutions within cubic worst case time complexity. Our contributions are threefold: (i) we introduce a novel optimization objective that balances matched and unmatched nodes; (ii) we establish a connection between partial graph matching and linear sum assignment problem, enabling efficient solutions; (iii) we propose a deep graph matching architecture with a novel partial matching loss, providing an end-to-end solution. The empirical evaluations on standard graph matching benchmarks demonstrate the efficacy of the proposed approach.","authors":["Gathika Ratnayaka","James Nichols","Qing Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20070v2","updated":"2026-02-25T16:39:12Z","published":"2026-02-23T17:26:09Z","title":"Training-Free Generative Modeling via Kernelized Stochastic Interpolants","summary":"We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaφ(x)^\\topη_t$, where $η_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.","authors":["Florentin Coeurdoux","Etienne Lempereur","Nathanaël Cuvelle-Magar","Thomas Eboli","Stéphane Mallat","Anastasia Borovykh","Eric Vanden-Eijnden"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22086v1","updated":"2026-02-25T16:34:53Z","published":"2026-02-25T16:34:53Z","title":"MBD-ML: Many-body dispersion from machine learning for molecules and materials","summary":"Van der Waals (vdW) interactions are essential for describing molecules and materials, from drug design and catalysis to battery applications. These omnipresent interactions must also be accurately included in machine-learned force fields. The many-body dispersion (MBD) method stands out as one of the most accurate and transferable approaches to capture vdW interactions, requiring only atomic $C_6$ coefficients and polarizabilities as input. We present MBD-ML, a pretrained message passing neural network that predicts these atomic properties directly from atomic structures. Through seamless integration with libMBD, our method enables the immediate calculation of MBD-inclusive total energies, forces, and stress tensors. By eliminating the need for intermediate electronic structure calculations, MBD-ML offers a practical and streamlined tool that simplifies the incorporation of state-of-the-art vdW interactions into any electronic structure code, as well as empirical and machine-learned force fields.","authors":["Evgeny Moerman","Adil Kabylda","Almaz Khabibrakhmanov","Alexandre Tkatchenko"],"pdf_url":"","comment":"22 pages, 6 figures, Supplementary Information (12 figures)"},{"id":"http://arxiv.org/abs/2602.22083v1","updated":"2026-02-25T16:32:04Z","published":"2026-02-25T16:32:04Z","title":"Coarsening Bias from Variable Discretization in Causal Functionals","summary":"A class of causal effect functionals requires integration over conditional densities of continuous variables, as in mediation effects and nonparametric identification in causal graphical models. Estimating such densities and evaluating the resulting integrals can be statistically and computationally demanding. A common workaround is to discretize the variable and replace integrals with finite sums. Although convenient, discretization alters the population-level functional and can induce non-negligible approximation bias, even under correct identification. Under smoothness conditions, we show that this coarsening bias is first order in the bin width and arises at the level of the target functional, distinct from statistical estimation error. We propose a simple bias-reduced functional that evaluates the outcome regression at within-bin conditional means, eliminating the leading term and yielding a second-order approximation error. We derive plug-in and one-step estimators for the bias-reduced functional. Simulations demonstrate substantial bias reduction and near-nominal confidence interval coverage, even under coarse binning. Our results provide a simple framework for controlling the impact of variable discretization on parameter approximation and estimation.","authors":["Xiaxian Ou","Razieh Nabi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.08786v3","updated":"2026-02-25T16:29:17Z","published":"2026-02-09T15:25:59Z","title":"Empirically Understanding the Value of Prediction in Allocation","summary":"Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.","authors":["Unai Fischer-Abaigar","Emily Aiken","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22066v1","updated":"2026-02-25T16:13:12Z","published":"2026-02-25T16:13:12Z","title":"DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models","summary":"Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, these surrogates are mapped to TSFM-compatible series via the forecasting objective. The symmetric structure enables parameter-free reconstruction of final predictions directly from the surrogates, without additional parametric decoding. A theoretically grounded regularization term is further introduced to enhance robustness against adaptation collapse. Extensive experiments on diverse real-world datasets show that DualWeaver outperforms state-of-the-art multivariate forecasters in both accuracy and stability. We release the code at https://github.com/li-jinpeng/DualWeaver.","authors":["Jinpeng Li","Zhongyi Pei","Huaze Xue","Bojian Zheng","Chen Wang","Jianmin Wang"],"pdf_url":"","comment":"16 pages. Preprint"},{"id":"http://arxiv.org/abs/2602.22061v1","updated":"2026-02-25T16:09:50Z","published":"2026-02-25T16:09:50Z","title":"Learning Quantum Data Distribution via Chaotic Quantum Diffusion Model","summary":"Generative models for quantum data pose significant challenges but hold immense potential in fields such as chemoinformatics and quantum physics. Quantum denoising diffusion probabilistic models (QuDDPMs) enable efficient learning of quantum data distributions by progressively scrambling and denoising quantum states; however, existing implementations typically rely on circuit-based random unitary dynamics that can be costly to realize and sensitive to control imperfections, particularly on analog quantum hardware. We propose the chaotic quantum diffusion model, a framework that generates projected ensembles via chaotic Hamiltonian time evolution, providing a flexible and hardware-compatible diffusion mechanism. Requiring only global, time-independent control, our approach substantially reduces implementation overhead across diverse analog quantum platforms while achieving accuracy comparable to QuDDPMs. This method improves trainability and robustness, broadening the applicability of quantum generative modeling.","authors":["Quoc Hoan Tran","Koki Chinzei","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"","comment":"12 pages, 7 figures; extended version from Poster in Workshop: Machine Learning and the Physical Sciences https://neurips.cc/virtual/2025/loc/san-diego/123072"},{"id":"http://arxiv.org/abs/2602.22056v1","updated":"2026-02-25T16:06:49Z","published":"2026-02-25T16:06:49Z","title":"FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation","summary":"Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.","authors":["Edgar Welte","Yitian Shi","Rosa Wolf","Maximillian Gilles","Rania Rayyes"],"pdf_url":"","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.22055v1","updated":"2026-02-25T16:06:28Z","published":"2026-02-25T16:06:28Z","title":"Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach","summary":"Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univariate feature transformations with a physics-informed loss function and a leakage-free chained prediction pipeline. Using operational and environmental data from five cargo vessels, PI-KAN consistently outperforms the traditional polynomial method and neural network baselines. The model achieves the lowest mean absolute error (MAE) and root mean squared error (RMSE), and the highest coefficient of determination (R^2) for shaft power and fuel consumption across all vessels, while maintaining physically consistent behavior. Interpretability analysis reveals rediscovery of domain-consistent dependencies, such as cubic-like speed-power relationships and cosine-like wave and wind effects. These results demonstrate that PI-KAN achieves both predictive accuracy and interpretability, offering a robust tool for vessel performance monitoring and decision support in operational settings.","authors":["Hamza Haruna Mohammed","Dusica Marijan","Arnbjørn Maressa"],"pdf_url":"","comment":"10 pages, 5 figures, IEEE conference paper format; under review"},{"id":"http://arxiv.org/abs/2509.11517v2","updated":"2026-02-25T16:04:59Z","published":"2025-09-15T02:07:26Z","title":"PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation","summary":"BACKGROUND: Medical large language models (LLMs) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: To build a dataset of questions medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) dataset containing 8,380 questions spanning 12 specialties (2018-2025). We selected ten medical LLMs, including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task specific prompts to answer the questions. We employed parameter-efficient fine tuning (PEFT) and low-rand adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: Medgemma-27b showed the highest accuracy across all specialities, achieving the highest score of 89.29% in Psychiatry; yet, in two specialties, OctoMed-7B exhibited slight superiority: Neurosurgery with 77.27% and 77.38, respectively; and Radiology with 76.13% and 77.39%, respectively. Across specialties, most LLMs with <10 billion parameters exhibited <50% of correct answers. The fine-tuned version of medgemma-4b-it emerged victorious against all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI applications and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profile to Peru's, interested parties should utilize medgemma-27b-text-it.","authors":["Rodrigo M. Carrillo-Larco","Jesus Lovón Melgarejo","Manuel Castillo-Cara","Gusseppe Bravo-Rocca"],"pdf_url":"","comment":"https://github.com/rodrigo-carrillo/PeruMedQA"},{"id":"http://arxiv.org/abs/2508.21421v3","updated":"2026-02-25T15:59:39Z","published":"2025-08-29T08:44:47Z","title":"Rethinking Layer-wise Model Merging through Chain of Merges","summary":"Fine-tuning pretrained models has become a standard pathway to achieve state-of-the-art performance across a wide range of domains, leading to a proliferation of task-specific model variants. As the number of such specialized models increases, merging them into a unified model without retraining has become a critical challenge. Existing merging techniques operate at the level of individual layers, thereby overlooking the inter-layer dependencies inherent in deep networks. We show that this simplification leads to distributional mismatches, particularly in methods that rely on intermediate activations, as changes in early layers are not properly propagated to downstream layers during merging. We identify these mismatches as a form of internal covariate shift, comparable to the phenomenon encountered in the initial phases of neural networks training. To address this, we propose Chain of Merges (CoM), a layer-wise merging procedure that sequentially merges weights across layers while sequentially updating activation statistics. By explicitly accounting for inter-layer interactions, CoM mitigates covariate shift and produces a coherent merged model through a series of conditionally optimal updates. Experiments on standard benchmarks demonstrate that CoM achieves state-of-the-art performance.","authors":["Pietro Buzzega","Riccardo Salami","Angelo Porrello","Simone Calderara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.06657v2","updated":"2026-02-25T15:53:57Z","published":"2024-11-11T01:44:54Z","title":"Renaissance: Investigating the Pretraining of Vision-Language Encoders","summary":"In the past several years there has been an explosion of available models for vision-language (VL) tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. Additionally, the limited programming tools available for modeling make conducting VL research more difficult than necessary. In this paper, we seek to answer several questions related to the pretraining of VL encoders through meta-analysis. To conduct these experiments, we introduce a VL evaluation framework called Renaissance. In our first set of experiments, we show that we can save significant compute at little to no cost to downstream performance, by freezing large parts of VL models during pretraining. In our second set of experiments, we examine the effect of basing a VL transformer on a vision model versus a text model. Renaissance offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. Its source code will be made publicly available upon publication. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","authors":["Clayton Fields","Casey Kennington"],"pdf_url":"","comment":"9 pages"},{"id":"http://arxiv.org/abs/2510.10625v3","updated":"2026-02-25T15:52:01Z","published":"2025-10-12T14:12:28Z","title":"ImpMIA: Leveraging Implicit Bias for Membership Inference Attack","summary":"Determining which data samples were used to train a model, known as Membership Inference Attack (MIA), is a well-studied and important problem with implications on data privacy. SotA methods (which are black-box attacks) rely on training many auxiliary reference models to imitate the behavior of the attacked model. As such, they rely on assumptions which rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. We show that removing these assumptions significantly harms the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks. Building on the maximum-margin implicit bias theory, ImpMIA uses the Karush-Kuhn-Tucker (KKT) optimality conditions to identify training samples -- those whose gradients most strongly reconstruct the trained model's parameters. Our approach is optimization-based, and requires NO training of reference-models, thus removing the need for any knowledge/assumptions regarding the attacked model's training procedure. While ImpMIA is a white-box attack (a setting which assumes access to model weights), this is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). ImpMIA achieves SotA performance compared to both black and white box attacks in settings where only the model weights are known, and a superset of the training data is available.","authors":["Yuval Golbari","Navve Wasserman","Gal Vardi","Michal Irani"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.03941v3","updated":"2026-02-25T15:50:07Z","published":"2024-11-06T14:18:23Z","title":"Modular Deep Learning for Multivariate Time-Series: Decoupling Imputation and Downstream Tasks","summary":"Missing values are pervasive in large-scale time-series data, posing challenges for reliable analysis and decision-making. Many neural architectures have been designed to model and impute the complex and heterogeneous missingness patterns of such data. Most existing methods are end-to-end, rendering imputation tightly coupled with downstream predictive tasks and leading to limited reusability of the trained model, reduced interpretability, and challenges in assessing model quality. In this paper, we call for a modular approach that decouples imputation and downstream tasks, enabling independent optimisation and greater adaptability. Using the largest open-source Python library for deep learning-based time-series analysis, PyPOTS, we evaluate a modular pipeline across six state-of-the-art models that perform imputation and prediction on seven datasets spanning multiple domains. Our results show that a modular approach maintains high performance while prioritising flexibility and reusability - qualities that are crucial for real-world applications. Through this work, we aim to demonstrate how modularity can benefit multivariate time-series analysis, achieving a balance between performance and adaptability.","authors":["Joseph Arul Raj","Linglong Qian","Zina Ibrahim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.21331v3","updated":"2026-02-25T15:47:02Z","published":"2026-01-29T06:47:51Z","title":"Convex Loss Functions for Support Vector Machines (SVMs) and Neural Networks","summary":"We propose a new convex loss for Support Vector Machines, both for the binary classification and for the regression models. Therefore, we show the mathematical derivation of the dual problems and we experiment with them on several small datasets. The minimal dimension of those datasets is due to the difficult scalability of the SVM method to bigger instances. This preliminary study should prove that using pattern correlations inside the loss function could enhance the generalisation performances. Our method consistently achieved comparable or superior performance, with improvements of up to 2.0% in F1 scores for classification tasks and 1.0% reduction in Mean Squared Error (MSE) for regression tasks across various datasets, compared to standard losses. Coherently, results show that generalisation measures are never worse than the standard losses and several times they are better. In our opinion, it should be considered a careful study of this loss, coupled with shallow and deep neural networks. In fact, we present some novel results obtained with those architectures.","authors":["Filippo Portera"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22018v1","updated":"2026-02-25T15:31:30Z","published":"2026-02-25T15:31:30Z","title":"Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data","summary":"Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.","authors":["Sterre de Jonge","Elisabeth J. Vinke","Meike W. Vernooij","Daniel C. Alexander","Alexandra L. Young","Esther E. Bron"],"pdf_url":"","comment":"Accepted for publication, 2026 IEEE 23rd International Symposium on Biomedical Imaging (ISBI), April 2026, London, United Kingdom"},{"id":"http://arxiv.org/abs/2602.22015v1","updated":"2026-02-25T15:29:44Z","published":"2026-02-25T15:29:44Z","title":"Function-Space Empirical Bayes Regularisation with Student's t Priors","summary":"Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.","authors":["Pengcheng Hao","Ercan Engin Kuruoglu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.04382v4","updated":"2026-02-25T15:25:16Z","published":"2025-05-07T13:04:29Z","title":"Discrete Optimal Transport and Voice Conversion","summary":"We propose kDOT, a discrete optimal transport (OT) framework for voice conversion (VC) operating in a pretrained speech embedding space. In contrast to the averaging strategies used in kNN-VC and SinkVC, and the independence assumption adopted in MKL, our method employs the barycentric projection of the discrete OT plan to construct a transport map between source and target speaker embedding distributions.\n  We conduct a comprehensive ablation study over the number of transported embeddings and systematically analyze the impact of source and target utterance duration. Experiments on LibriSpeech demonstrate that OT with barycentric projection consistently improves distribution alignment and often outperforms averaging-based approaches in terms of WER, MOS, and FAD.\n  Furthermore, we show that applying discrete OT as a post-processing step can transform spoofed speech into samples that are misclassified as bona fide by a state-of-the-art spoofing detector. This demonstrates the strong domain adaptation capability of OT in embedding space, while also revealing important security implications for spoof detection systems.","authors":["Anton Selitskiy","Maitreya Kocharekar"],"pdf_url":"","comment":"5 pages, 1 figure, 7 table"},{"id":"http://arxiv.org/abs/2602.22003v1","updated":"2026-02-25T15:21:24Z","published":"2026-02-25T15:21:24Z","title":"Neural solver for Wasserstein Geodesics and optimal transport dynamics","summary":"In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to approximate the relevant functions. This approach not only provides the Wasserstein geodesic but also recovers the OT map, enabling direct sampling from the target distribution. By estimating the OT map, we obtain velocity estimates along particle trajectories, which in turn allow us to learn the full velocity field. The framework is flexible and readily extends to general cost functions, including the commonly used quadratic cost. We demonstrate the effectiveness of our method through experiments on both synthetic and real datasets.","authors":["Hailiang Liu","Yan-Han Chen"],"pdf_url":"","comment":"28 pages, 22 figures"},{"id":"http://arxiv.org/abs/2602.21997v1","updated":"2026-02-25T15:16:43Z","published":"2026-02-25T15:16:43Z","title":"Enhancing LLM-Based Test Generation by Eliminating Covered Code","summary":"Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.","authors":["WeiZhe Xu","Mengyu Liu","Fanxin Kong"],"pdf_url":"","comment":"9 pages, 4 figures, supplementary material included"},{"id":"http://arxiv.org/abs/2602.21995v1","updated":"2026-02-25T15:15:57Z","published":"2026-02-25T15:15:57Z","title":"Outpatient Appointment Scheduling Optimization with a Genetic Algorithm Approach","summary":"The optimization of complex medical appointment scheduling remains a significant operational challenge in multi-center healthcare environments, where clinical safety protocols and patient logistics must be reconciled. This study proposes and evaluates a Genetic Algorithm (GA) framework designed to automate the scheduling of multiple medical acts while adhering to rigorous inter-procedural incompatibility rules. Using a synthetic dataset encompassing 50 medical acts across four healthcare facilities, we compared two GA variants, Pre-Ordered and Unordered, against deterministic First-Come, First-Served (FCFS) and Random Choice baselines. Our results demonstrate that the GA framework achieved a 100% constraint fulfillment rate, effectively resolving temporal overlaps and clinical incompatibilities that the FCFS baseline failed to address in 60% and 40% of cases, respectively. Furthermore, the GA variants demonstrated statistically significant improvements (p < 0.001) in patient-centric metrics, achieving an Idle Time Ratio (ITR) frequently below 0.4 and reducing inter-healthcenter trips. While the GA (Ordered) variant provided a superior initial search locus, both evolutionary models converged to comparable global optima by the 100th generation. These findings suggest that transitioning from manual, human-mediated scheduling to an automated metaheuristic approach enhances clinical integrity, reduces administrative overhead, and significantly improves the patient experience by minimizing wait times and logistical burdens.","authors":["Ana Rodrigues","Rui Rego"],"pdf_url":"","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.01927v2","updated":"2026-02-25T15:06:31Z","published":"2025-03-02T19:29:04Z","title":"QCS-ADME: Quantum Circuit Search for Drug Property Prediction with Imbalanced Data and Regression Adaptation","summary":"The biomedical field is beginning to explore the use of quantum machine learning (QML) for tasks traditionally handled by classical machine learning, especially in predicting ADME (absorption, distribution, metabolism, and excretion) properties, which are essential in drug evaluation. However, ADME tasks pose unique challenges for existing quantum computing systems (QCS) frameworks, as they involve both classification with unbalanced dataset and regression problems. These dual requirements make it necessary to adapt and refine current QCS frameworks to effectively address the complexities of ADME predictions. We propose a novel training-free scoring mechanism to evaluate QML circuit performance on imbalanced classification and regression tasks. Our mechanism demonstrates significant correlation between scoring metrics and test performance on imbalanced classification tasks. Additionally, we develop methods to quantify continuous similarity relationships between quantum states, enabling performance prediction for regression tasks. This represents a novel training-free approach to searching and evaluating QCS circuits specifically for regression applications. Validation on representative ADME tasks-eight imbalanced classification and four regression-demonstrates moderate correlation between our scoring metrics and circuit performance, significantly outperforming baseline scoring methods that show negligible correlation.","authors":["Kangyu Zheng","Tianfan Fu","Zhiding Liang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.19253v2","updated":"2026-02-25T14:54:20Z","published":"2024-11-28T16:42:30Z","title":"Quantum feedback control with a transformer neural network architecture","summary":"Attention-based neural networks such as transformers have revolutionized various fields such as natural language processing, genomics, and vision. Here, we demonstrate the use of transformers for quantum feedback control through both a supervised and reinforcement learning approach. In particular, due to the transformer's ability to capture long-range temporal correlations and training efficiency, we show that it can surpass some of the limitations of previous control approaches, e.g.~those based on recurrent neural networks trained using a similar approach or policy based reinforcement learning. We numerically show, for the example of state stabilization of a two-level system, that our bespoke transformer architecture can achieve near unit fidelity to a target state in a short time even in the presence of inefficient measurement and Hamiltonian perturbations that were not included in the training set as well as the control of non-Markovian systems. We also demonstrate that our transformer can perform energy minimization of non-integrable many-body quantum systems when trained for reinforcement learning tasks. Our approach can be used for quantum error correction, fast control of quantum states in the presence of colored noise, as well as real-time tuning, and characterization of quantum devices.","authors":["Pranav Vaidhyanathan","Florian Marquardt","Mark T. Mitchison","Natalia Ares"],"pdf_url":"","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2512.16902v2","updated":"2026-02-25T14:49:31Z","published":"2025-12-18T18:56:50Z","title":"In-Context Algebra","summary":"We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions in-context. While prior work has studied transformers in settings where the answer relies on fixed parametric or geometric information encoded in token embeddings, we devise a new in-context reasoning task where the assignment of tokens to specific algebraic elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Our findings show that the kinds of reasoning strategies learned by transformers are dependent on the task structure and that models can develop symbolic reasoning mechanisms when trained to reason in-context about variables whose meanings are not fixed.","authors":["Eric Todd","Jannik Brinkmann","Rohit Gandikota","David Bau"],"pdf_url":"","comment":"ICLR 2026. 35 pages, 22 figures. Code and data at https://algebra.baulab.info"},{"id":"http://arxiv.org/abs/2510.13654v3","updated":"2026-02-25T14:48:58Z","published":"2025-10-15T15:15:45Z","title":"Rethinking Evaluation in the Era of Time Series Foundation Models: (Un)known Information Leakage Challenges","summary":"Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. An investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation.","authors":["Marcel Meyer","Sascha Kaltenpoth","Kevin Zalipski","Oliver Müller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21965v1","updated":"2026-02-25T14:48:25Z","published":"2026-02-25T14:48:25Z","title":"Compact Circulant Layers with Spectral Priors","summary":"Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates.","authors":["Joseph Margaryan","Thomas Hamelryck"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21961v1","updated":"2026-02-25T14:44:15Z","published":"2026-02-25T14:44:15Z","title":"Robustness in sparse artificial neural networks trained with adaptive topology","summary":"We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This work highlights the potential of adaptive sparse networks as a promising direction for developing efficient and reliable deep learning models.","authors":["Bendegúz Sulyok","Gergely Palla","Filippo Radicchi","Santo Fortunato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21959v1","updated":"2026-02-25T14:41:07Z","published":"2026-02-25T14:41:07Z","title":"Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions","summary":"To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating and optimizing fuel oil consumption in maritime transport. Our novel contributions include categorizing fuel oil consumption \\& estimation methods into physics-based, machine-learning, and hybrid models, exploring their strengths and limitations. Furthermore, we highlight the importance of data fusion techniques, which combine AIS, onboard sensors, and meteorological data to enhance accuracy. We make the first attempt to discuss the emerging role of Explainable AI in enhancing model transparency for decision-making. Uniquely, key challenges, including data quality, availability, and the need for real-time optimization, are identified, and future research directions are proposed to address these gaps, with a focus on hybrid models, real-time optimization, and the standardization of datasets.","authors":["Dusica Marijan","Hamza Haruna Mohammed","Bakht Zaman"],"pdf_url":"","comment":"23 pages, 4 figures. Published in Journal of Marine Science and Technology (2026)"},{"id":"http://arxiv.org/abs/2602.21957v1","updated":"2026-02-25T14:39:47Z","published":"2026-02-25T14:39:47Z","title":"Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation","summary":"Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective than enforcing shared representations. Specifically, global semantic relations serve as structural constraints for items. Within these constraints, the framework allows item representations to vary locally on each client, which flexibility enables the model to capture fine-grained user personalization while maintaining global consistency. To this end, we propose Cluster-Guided FedRec framework (CGFedRec), a framework that transforms uploaded embeddings into compact cluster labels. In this framework, the server functions as a global structure discoverer to learn item clusters and distributes only the resulting labels. This mechanism explicitly cuts off the downstream transmission of item embeddings, relieving clients from maintaining global shared item embeddings. Consequently, CGFedRec achieves the effective injection of global collaborative signals into local item representations without transmitting full embeddings. Extensive experiments demonstrate that our approach significantly improves communication efficiency while maintaining superior recommendation accuracy across multiple datasets.","authors":["Yuchun Tu","Zhiwei Li","Bingli Sun","Yixuan Li","Xiao Song"],"pdf_url":"","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.21948v1","updated":"2026-02-25T14:32:58Z","published":"2026-02-25T14:32:58Z","title":"Bayesian Generative Adversarial Networks via Gaussian Approximation for Tabular Data Synthesis","summary":"Generative Adversarial Networks (GAN) have been used in many studies to synthesise mixed tabular data. Conditional tabular GAN (CTGAN) have been the most popular variant but struggle to effectively navigate the risk-utility trade-off. Bayesian GAN have received less attention for tabular data, but have been explored with unstructured data such as images and text. The most used technique employed in Bayesian GAN is Markov Chain Monte Carlo (MCMC), but it is computationally intensive, particularly in terms of weight storage. In this paper, we introduce Gaussian Approximation of CTGAN (GACTGAN), an integration of the Bayesian posterior approximation technique using Stochastic Weight Averaging-Gaussian (SWAG) within the CTGAN generator to synthesise tabular data, reducing computational overhead after the training phase. We demonstrate that GACTGAN yields better synthetic data compared to CTGAN, achieving better preservation of tabular structure and inferential statistics with less privacy risk. These results highlight GACTGAN as a simpler, effective implementation of Bayesian tabular synthesis.","authors":["Bahrul Ilmi Nasution","Mark Elliot","Richard Allmendinger"],"pdf_url":"","comment":"28 pages, 5 Figures, Accepted in Transactions on Data Privacy"},{"id":"http://arxiv.org/abs/2601.13780v2","updated":"2026-02-25T14:32:51Z","published":"2026-01-20T09:37:53Z","title":"Principled Latent Diffusion for Graphs via Laplacian Autoencoders","summary":"Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\\times$ speed-up. Our code is available at https://github.com/asiraudin/LG-Flow .","authors":["Antoine Siraudin","Christopher Morris"],"pdf_url":"","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2408.05861v4","updated":"2026-02-25T14:21:54Z","published":"2024-08-11T21:04:14Z","title":"Temporal Knowledge-Graph Memory in a Partially Observable Environment","summary":"Agents in partially observable environments require persistent memory to integrate observations over time. While KGs (knowledge graphs) provide a natural representation for such evolving state, existing benchmarks rarely expose agents to environments where both the world dynamics and the agent's memory are explicitly graph-shaped. We introduce the Room Environment v3, a configurable environment whose hidden state is an RDF KG and whose observations are RDF triples. The agent may extend these observations into a temporal KG when storing them in long-term memory. The environment is easily adjustable in terms of grid size, number of rooms, inner walls, and moving objects.\n  We define a lightweight temporal KG memory for agents, based on RDF-star-style qualifiers (time_added, last_accessed, num_recalled), and evaluate several symbolic baselines that maintain and query this memory under different capacity constraints. Two neural sequence models (LSTM and Transformer) serve as contrasting baselines without explicit KG structure. Agents train on one layout and are evaluated on a held-out layout with the same dynamics but a different query order, exposing train-test generalization gaps. In this setting, temporal qualifiers lead to more stable performance, and the symbolic TKG (temporal knowledge graph) agent achieves roughly fourfold higher test QA (question-answer) accuracy than the neural baselines under the same environment and query conditions. The environment, agent implementations, and experimental scripts are released for reproducible research at https://github.com/humemai/agent-room-env-v3 and https://github.com/humemai/room-env.","authors":["Taewoon Kim","Vincent François-Lavet","Michael Cochez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2404.07849v2","updated":"2026-02-25T14:14:20Z","published":"2024-04-11T15:43:11Z","title":"Overparameterized Multiple Linear Regression as Hyper-Curve Fitting","summary":"This work demonstrates that applying a fixed-effect multiple linear regression (MLR) model to an overparameterized dataset is mathematically equivalent to fitting a hyper-curve parameterized by a single scalar. This reformulation shifts the focus from global coefficients to individual predictors, allowing each to be modeled as a function of a common parameter. We prove that this overparameterized linear framework can yield exact predictions even when the underlying data contains nonlinear dependencies that violate classical linear assumptions. By employing parameterization in terms of the dependent variable and a monomial basis, we validate this approach on both synthetic and experimental datasets. Our results show that the hyper-curve perspective provides a robust framework for regularizing problems with noisy predictors and offers a systematic method for identifying and removing 'improper' predictors that degrade model generalizability.","authors":["E. Atza","N. Budko"],"pdf_url":"","comment":"18 pages, 8 figures, version 2 (IOP style, revised), Python code and data available at: https://github.com/the-iterator/hyper-curve-regression-yarn"},{"id":"http://arxiv.org/abs/2509.18880v3","updated":"2026-02-25T14:13:32Z","published":"2025-09-23T10:21:22Z","title":"Diversity Boosts AI-Generated Text Detection","summary":"Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection.","authors":["Advik Raj Basani","Pin-Yu Chen"],"pdf_url":"","comment":"Accepted to Transactions on Machine Learning Research (TMLR '26). Project page and demos: https://diveye.vercel.app/"},{"id":"http://arxiv.org/abs/2602.21928v1","updated":"2026-02-25T14:05:38Z","published":"2026-02-25T14:05:38Z","title":"Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems","summary":"Root cause analysis (RCA) in networked industrial systems, such as supply chains and power networks, is notoriously difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients. These clients represent heterogeneous physical processes and industrial assets equipped with sensors that generate large volumes of nonlinear, high-dimensional, and heterogeneous IoT data. Classical RCA methods require partial or full knowledge of the system's dependency graph, which is rarely available in these complex networks. While federated learning (FL) offers a natural framework for decentralized settings, most existing FL methods assume homogeneous feature spaces and retrainable client models. These assumptions are not compatible with our problem setting. Different clients have different data features and often run fixed, proprietary models that cannot be modified. This paper presents a federated cross-client interdependency learning methodology for feature-partitioned, nonlinear time-series data, without requiring access to raw sensor streams or modifying proprietary client models. Each proprietary local client model is augmented with a Machine Learning (ML) model that encodes cross-client interdependencies. These ML models are coordinated via a global server that enforces representation consistency while preserving privacy through calibrated differential privacy noise. RCA is performed using model residuals and anomaly flags. We establish theoretical convergence guarantees and validate our approach on extensive simulations and a real-world industrial cybersecurity dataset.","authors":["Ayush Mohanty","Paritosh Ramanan","Nagi Gebraeel"],"pdf_url":"","comment":"Manuscript under review"},{"id":"http://arxiv.org/abs/2602.21926v1","updated":"2026-02-25T14:04:03Z","published":"2026-02-25T14:04:03Z","title":"Bridging Through Absence: How Comeback Researchers Bridge Knowledge Gaps Through Structural Re-emergence","summary":"Understanding the role of researchers who return to academia after prolonged inactivity, termed \"comeback researchers\", is crucial for developing inclusive models of scientific careers. This study investigates the structural and semantic behaviors of comeback researchers, focusing on their role in cross-disciplinary knowledge transfer and network reintegration. Using the AMiner citation dataset, we analyze 113,637 early-career researchers and identify 1,425 comeback cases based on a three-year-or-longer publication gap followed by renewed activity. We find that comeback researchers cite 126% more distinct communities and exhibit 7.6% higher bridging scores compared to dropouts. They also demonstrate 74% higher gap entropy, reflecting more irregular yet strategically impactful publication trajectories. Predictive models trained on these bridging- and entropy-based features achieve a 97% ROC-AUC, far outperforming the 54% ROC-AUC of baseline models using traditional metrics like publication count and h-index. Finally, we substantiate these results via a multi-lens validation. These findings highlight the unique contributions of comeback researchers and offer data-driven tools for their early identification and institutional support.","authors":["Somyajit Chakraborty","Angshuman Jana","Avijit Gayen"],"pdf_url":"","comment":"Preprint; 25 pages, 14 figures, 7 tables, Submitted to Scientometrics 2025"},{"id":"http://arxiv.org/abs/2602.16642v3","updated":"2026-02-25T14:03:15Z","published":"2026-02-18T17:32:43Z","title":"Optimizer choice matters for the emergence of Neural Collapse","summary":"Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.","authors":["Jim Zhao","Tin Sum Cheng","Wojciech Masarczyk","Aurelien Lucchi"],"pdf_url":"","comment":"Published as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21919v1","updated":"2026-02-25T13:55:06Z","published":"2026-02-25T13:55:06Z","title":"Learning in the Null Space: Small Singular Values for Continual Learning","summary":"Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer's input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.","authors":["Cuong Anh Pham","Praneeth Vepakomma","Samuel Horváth"],"pdf_url":"","comment":"17 pages, accepted as Oral presentation at the Third Conference on Parsimony and Learning (CPAL 2026)"},{"id":"http://arxiv.org/abs/2602.21910v1","updated":"2026-02-25T13:38:08Z","published":"2026-02-25T13:38:08Z","title":"The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions","summary":"Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for further advancing the approach.\n  This work analyzes performance limitations of the classical DeepONet architecture. It is shown that the approximation error is dominated by the branch network when the internal dimension is sufficiently large, and that the learned trunk basis can often be replaced by classical basis functions without a significant impact on performance.\n  To investigate this further, a modified DeepONet is constructed in which the trunk network is replaced by the left singular vectors of the training solution matrix. This modification yields several key insights. First, a spectral bias in the branch network is observed, with coefficients of dominant, low-frequency modes learned more effectively. Second, due to singular-value scaling of the branch coefficients, the overall branch error is dominated by modes with intermediate singular values rather than the smallest ones. Third, using a shared branch network for all mode coefficients, as in the standard architecture, improves generalization of small modes compared to a stacked architecture in which coefficients are computed separately. Finally, strong and detrimental coupling between modes in parameter space is identified.","authors":["Alexander Heinlein","Johannes Taraz"],"pdf_url":"","comment":"29 pages, 12 figures"},{"id":"http://arxiv.org/abs/2510.01988v5","updated":"2026-02-25T13:26:31Z","published":"2025-10-02T13:07:37Z","title":"PepCompass: Navigating peptide embedding spaces using Riemannian Geometry","summary":"Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent \"maps\" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $κ$-Stable Riemannian Manifolds $\\mathbb{M}^κ$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.","authors":["Marcin Możejko","Adam Bielecki","Jurand Prądzyński","Marcin Traskowski","Antoni Janowski","Hyun-Su Lee","Marcelo Der Torossian Torres","Michał Kmicikiewicz","Paulina Szymczak","Karol Jurasz","Michał Kucharczyk","Cesar de la Fuente-Nunez","Ewa Szczurek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.04091v2","updated":"2026-02-25T13:23:52Z","published":"2025-10-05T08:30:32Z","title":"Rethinking Consistent Multi-Label Classification Under Inexact Supervision","summary":"Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results on both real-world and synthetic datasets validate the effectiveness of our proposed approaches against state-of-the-art methods.","authors":["Wei Wang","Tianhao Ma","Ming-Kun Xie","Gang Niu","Masashi Sugiyama"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2506.07477v2","updated":"2026-02-25T13:23:05Z","published":"2025-06-09T06:50:59Z","title":"Premise Selection for a Lean Hammer","summary":"Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. A hammer is a tool that integrates premise selection, translation to external automatic theorem provers, and proof reconstruction into one overarching tool to automate tedious reasoning steps. We present LeanPremise, a novel neural premise selection system, and we combine it with existing translation and proof reconstruction components to create LeanHammer, the first end-to-end domain general hammer for the Lean proof assistant. Unlike existing Lean premise selectors, LeanPremise is specifically trained for use with a hammer in dependent type theory. It also dynamically adapts to user-specific contexts, enabling it to effectively recommend premises from libraries outside LeanPremise's training data as well as lemmas defined by the user locally. With comprehensive evaluations, we show that LeanPremise enables LeanHammer to solve 21% more goals than existing premise selectors and generalizes well to diverse domains. Our work helps bridge the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.","authors":["Thomas Zhu","Joshua Clune","Jeremy Avigad","Albert Qiaochu Jiang","Sean Welleck"],"pdf_url":"","comment":"LeanPremise is available at https://github.com/hanwenzhu/premise-selection and LeanHammer is available at https://github.com/JOSHCLUNE/LeanHammer"},{"id":"http://arxiv.org/abs/2507.00031v2","updated":"2026-02-25T13:15:53Z","published":"2025-06-17T23:51:36Z","title":"Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru","summary":"Accurate modeling of human mobility is critical for understanding epidemic spread and deploying timely interventions. In this work, we leverage a large-scale spatio-temporal dataset collected from Peru's national Digital Contact Tracing (DCT) application during the COVID-19 pandemic to forecast mobility flows across urban regions. A key challenge lies in the spatial sparsity of hourly mobility counts across hexagonal grid cells, which limits the predictive power of conventional time series models. To address this, we propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN) technique that augments each cell's features with aggregated signals from its immediate H3 neighbors. We evaluate this strategy on three forecasting backbones: NLinear, PatchTST, and K-U-Net, under various historical input lengths. Experimental results show that SPN consistently improves forecasting performance, achieving up to 9.85 percent reduction in test MSE. Our findings demonstrate that spatial smoothing of sparse mobility signals provides a simple yet effective path toward robust spatio-temporal forecasting during public health crises.","authors":["Chuan Li","Jiang You","Hassine Moungla","Vincent Gauthier","Miguel Nunez-del-Prado","Hugo Alatrista-Salas"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.25800v2","updated":"2026-02-25T13:15:35Z","published":"2025-09-30T05:18:15Z","title":"Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data","summary":"Interventional causal discovery seeks to identify causal relations by leveraging distributional changes introduced by interventions, even in the presence of latent confounders. Beyond the spurious dependencies induced by latent confounders, we highlight a common yet often overlooked challenge in the problem due to post-treatment selection, in which samples are selectively included in datasets after interventions. This fundamental challenge widely exists in biological studies; for example, in gene expression analysis, both observational and interventional samples are retained only if they meet quality control criteria (e.g., highly active cells). Neglecting post-treatment selection may introduce spurious dependencies and distributional changes under interventions, which can mimic causal responses, thereby distorting causal discovery results and challenging existing causal formulations. To address this, we introduce a novel causal formulation that explicitly models post-treatment selection and reveals how its differential reactions to interventions can distinguish causal relations from selection patterns, allowing us to go beyond traditional equivalence classes toward the underlying true causal structure. We then characterize its Markov properties and propose a Fine-grained Interventional equivalence class, named FI-Markov equivalence, represented by a new graphical diagram, F-PAG. Finally, we develop a provably sound and complete algorithm, F-FCI, to identify causal relations, latent confounders, and post-treatment selection up to $\\mathcal{FI}$-Markov equivalence, using both observational and interventional data. Experimental results on synthetic and real-world datasets demonstrate that our method recovers causal relations despite the presence of both selection and latent confounders.","authors":["Gongxu Luo","Loka Li","Guangyi Chen","Haoyue Dai","Kun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22811v3","updated":"2026-02-25T13:11:22Z","published":"2025-05-28T19:40:34Z","title":"Highly Efficient and Effective LLMs with Multi-Boolean Architectures","summary":"Weight binarization has emerged as a promising strategy to reduce the complexity of large language models (LLMs). Existing approaches fall into post-training binarization, which is simple but causes severe performance loss, and training-aware methods, which depend on full-precision latent weights, adding complexity and limiting efficiency. We propose a novel framework that represents LLMs with multi-kernel Boolean parameters and, for the first time, enables direct finetuning LMMs in the Boolean domain, eliminating the need for latent weights. This enhances representational capacity and dramatically reduces complexity during both finetuning and inference. Extensive experiments across diverse LLMs show our method outperforms recent ultra low-bit quantization and binarization techniques.","authors":["Ba-Hien Tran","Van Minh Nguyen"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2504.06533v3","updated":"2026-02-25T13:11:21Z","published":"2025-04-09T02:16:46Z","title":"Rethinking Flexible Graph Similarity Computation: One-step Alignment with Global Guidance","summary":"Graph Edit Distance (GED) is a widely used measure of graph similarity, valued for its flexibility in encoding domain knowledge through operation costs. However, existing learning-based approximation methods follow a modeling paradigm that decouples local candidate match selection from both operation costs and global dependencies between matches. This decoupling undermines their ability to capture the intrinsic flexibility of GED and often forces them to rely on costly iterative refinement to obtain accurate alignments. In this work, we revisit the formulation of GED and revise the prevailing paradigm, and propose Graph Edit Network (GEN), an implementation of the revised formulation that tightly integrates cost-aware expense estimation with globally guided one-step alignment. Specifically, GEN incorporates operation costs into node matching expenses estimation, ensuring match decisions respect the specified cost setting. Furthermore, GEN models match dependencies within and across graphs, capturing each match's impact on the overall alignment. These designs enable accurate GED approximation without iterative refinement. Extensive experiments on real-world and synthetic benchmarks demonstrate that GEN achieves up to a 37.8% reduction in GED predictive errors, while increasing inference throughput by up to 414x. These results highlight GEN's practical efficiency and the effectiveness of the revision. Beyond this implementation, our revision provides a principled framework for advancing learning-based GED approximation.","authors":["Zhouyang Liu","Ning Liu","Yixin Chen","Jiezhong He","Shuai Ma","Dongsheng Li"],"pdf_url":"","comment":"Accepted by ICDE 2026"},{"id":"http://arxiv.org/abs/2602.21889v1","updated":"2026-02-25T13:11:12Z","published":"2026-02-25T13:11:12Z","title":"2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support","summary":"Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.","authors":["Otto Nyberg","Fausto Carcassi","Giovanni Cinà"],"pdf_url":"","comment":"17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2602.21873v1","updated":"2026-02-25T12:57:45Z","published":"2026-02-25T12:57:45Z","title":"GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task","summary":"Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.","authors":["Shiwei Lu","Yuhang He","Jiashuo Li","Qiang Wang","Yihong Gong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.22685v3","updated":"2026-02-25T12:55:13Z","published":"2025-06-27T23:40:27Z","title":"Mitigating Semantic Collapse in Generative Personalization with Test-Time Embedding Adjustment","summary":"In this paper, we investigate the semantic collapsing problem in generative personalization, an under-explored topic where the learned visual concept ($V$) gradually shifts from its original textual meaning and comes to dominate other concepts in multi-concept input prompts. This issue not only reduces the semantic richness of complex input prompts like \"a photo of $V$ wearing glasses and playing guitar\" into simpler, less contextually rich forms such as \"a photo of $V$\" but also leads to simplified output images that fail to capture the intended concept. We identify the root cause as unconstrained optimisation, which allows the learned embedding $V$ to drift arbitrarily in the embedding space, both in direction and magnitude. To address this, we propose a simple yet effective training-free method that adjusts the magnitude and direction of pre-trained embedding at inference time, effectively mitigating the semantic collapsing problem. Our method is broadly applicable across different personalization methods and demonstrates significant improvements in text-image alignment in diverse use cases. Our code is anonymously published at https://github.com/tuananhbui89/Embedding-Adjustment","authors":["Anh Bui","Trang Vu","Trung Le","Junae Kim","Tamas Abraham","Rollin Omari","Amar Kaur","Dinh Phung"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21857v1","updated":"2026-02-25T12:32:04Z","published":"2026-02-25T12:32:04Z","title":"Distill and Align Decomposition for Enhanced Claim Verification","summary":"Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.","authors":["Jabez Magomere","Elena Kochkina","Samuel Mensah","Simerjot Kaur","Fernando Acero","Arturo Oncevay","Charese H. Smiley","Xiaomo Liu","Manuela Veloso"],"pdf_url":"","comment":"EACL Findings 2026"},{"id":"http://arxiv.org/abs/2501.16443v2","updated":"2026-02-25T12:27:06Z","published":"2025-01-27T19:07:06Z","title":"Object-Centric World Models from Few-Shot Annotations for Sample-Efficient Reinforcement Learning","summary":"While deep reinforcement learning (RL) from pixels has achieved remarkable success, its sample inefficiency remains a critical limitation for real-world applications. Model-based RL (MBRL) addresses this by learning a world model to generate simulated experience, but standard approaches that rely on pixel-level reconstruction losses often fail to capture small, task-critical objects in complex, dynamic scenes. We posit that an object-centric (OC) representation can direct model capacity toward semantically meaningful entities, improving dynamics prediction and sample efficiency. In this work, we introduce OC-STORM, an object-centric MBRL framework that enhances a learned world model with object representations extracted by a pretrained segmentation network. By conditioning on a minimal number of annotated frames, OC-STORM learns to track decision-relevant object dynamics and inter-object interactions without extensive labeling or access to privileged information. Empirical results demonstrate that OC-STORM significantly outperforms the STORM baseline on the Atari 100k benchmark and achieves state-of-the-art sample efficiency on challenging boss fights in the visually complex game Hollow Knight. Our findings underscore the potential of integrating OC priors into MBRL for complex visual domains. Project page: https://oc-storm.weipuzhang.com","authors":["Weipu Zhang","Adam Jelley","Trevor McInroe","Amos Storkey","Gang Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21846v1","updated":"2026-02-25T12:25:34Z","published":"2026-02-25T12:25:34Z","title":"Scalable Kernel-Based Distances for Statistical Inference and Integration","summary":"Representing, comparing, and measuring the distance between probability distributions is a key task in computational statistics and machine learning. The choice of representation and the associated distance determine properties of the methods in which they are used: for example, certain distances can allow one to encode robustness or smoothness of the problem. Kernel methods offer flexible and rich Hilbert space representations of distributions that allow the modeller to enforce properties through the choice of kernel, and estimate associated distances at efficient nonparametric rates. In particular, the maximum mean discrepancy (MMD), a kernel-based distance constructed by comparing Hilbert space mean functions, has received significant attention due to its computational tractability and is favoured by practitioners.\n  In this thesis, we conduct a thorough study of kernel-based distances with a focus on efficient computation, with core contributions in Chapters 3 to 6. Part I of the thesis is focused on the MMD, specifically on improved MMD estimation. In Chapter 3 we propose a theoretically sound, improved estimator for MMD in simulation-based inference. Then, in Chapter 4, we propose an MMD-based estimator for conditional expectations, a ubiquitous task in statistical computation. Closing Part I, in Chapter 5 we study the problem of calibration when MMD is applied to the task of integration.\n  In Part II, motivated by the recent developments in kernel embeddings beyond the mean, we introduce a family of novel kernel-based discrepancies: kernel quantile discrepancies. These address some of the pitfalls of MMD, and are shown through both theoretical results and an empirical study to offer a competitive alternative to MMD and its fast approximations. We conclude with a discussion on broader lessons and future work emerging from the thesis.","authors":["Masha Naslidnyk"],"pdf_url":"","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2602.21845v1","updated":"2026-02-25T12:25:29Z","published":"2026-02-25T12:25:29Z","title":"xai-cola: A Python library for sparsifying counterfactual explanations","summary":"Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced by arbitrary generators, reducing superfluous feature changes while preserving their validity. It offers a documented API that takes as input raw tabular data in pandas DataFrame form, a preprocessing object (for standardization and encoding), and a trained scikit-learn or PyTorch model. On this basis, users can either employ the built-in or externally imported CE generators. The library also implements several sparsification policies and includes visualization routines for analysing and comparing sparsified counterfactuals. xai-cola is released under the MIT license and can be installed from PyPI. Empirical experiments indicate that xai-cola produces sparser counterfactuals across several CE generators, reducing the number of modified features by up to 50% in our setting. The source code is available at https://github.com/understanding-ml/COLA.","authors":["Lin Zhu","Lei You"],"pdf_url":"","comment":"5pages, 1 figure"},{"id":"http://arxiv.org/abs/2602.21844v1","updated":"2026-02-25T12:22:48Z","published":"2026-02-25T12:22:48Z","title":"JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning","summary":"Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients (\"privacy stragglers\"), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.","authors":["Ruichen Xu","Ying-Jun Angela Zhang","Jianwei Huang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.07477v2","updated":"2026-02-25T12:17:47Z","published":"2025-09-09T08:02:10Z","title":"MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification","summary":"Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNetV2-S, while improving interpretability: MedicalPatchNet demonstrates improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet","authors":["Patrick Wienholt","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2508.13755v6","updated":"2026-02-25T12:13:07Z","published":"2025-08-19T11:51:40Z","title":"Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration","summary":"Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.","authors":["Zhicheng Yang","Zhijiang Guo","Yinya Huang","Yongxin Wang","Dongchun Xie","Hanhui Li","Yiwei Wang","Xiaodan Liang","Jing Tang"],"pdf_url":"","comment":"20 pages, 17 figures"},{"id":"http://arxiv.org/abs/2602.21824v1","updated":"2026-02-25T11:52:13Z","published":"2026-02-25T11:52:13Z","title":"DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion","summary":"Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic document generation using Vision-Language Models (VLMs) that produces annotated documents from unlabeled seed samples. Our approach generates visually plausible and semantically consistent synthetic documents that follow the distribution of an existing source dataset through clustering-based seed selection with parametrized sampling. By enriching documents with realistic diffusion-based handwriting and contextual visual elements via semantic-visual decoupling, we generate diverse, high-quality annotated synthetic documents. We evaluate across eleven benchmarks spanning key information extraction, question answering, document classification, and document layout analysis. To our knowledge, this is the first work demonstrating that VLMs can generate faithful annotated document datasets at scale from unlabeled seeds that can effectively enrich or approximate real, manually annotated data for diverse document understanding tasks. We show that with only 100 real training samples, our framework achieves on average $87\\%$ of the performance of the full real-world dataset. We publicly release our code and 140k+ synthetic document samples.","authors":["Marcel Lamott","Saifullah Saifullah","Nauman Riaz","Yves-Noel Weweler","Tobias Alt-Veit","Ahmad Sarmad Ali","Muhammad Armaghan Shakir","Adrian Kalwa","Momina Moetesum","Andreas Dengel","Sheraz Ahmed","Faisal Shafait","Ulrich Schwanecke","Adrian Ulges"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.16898v3","updated":"2026-02-25T11:49:07Z","published":"2026-02-18T21:28:56Z","title":"MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation","summary":"Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. MALLVI presents a Multi Agent Large Language and Vision framework that enables closed-loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVI generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step. Rather than using a single model, MALLVI coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning. Experiments in simulation and real-world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks. Code available at https://github.com/iman1234ahmadi/MALLVI .","authors":["Iman Ahmadi","Mehrshad Taji","Arad Mahdinezhad Kashani","AmirHossein Jadidi","Saina Kashani","Babak Khalaj"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.11020v2","updated":"2026-02-25T11:45:18Z","published":"2026-02-11T16:45:23Z","title":"When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging","summary":"We study same-source multi-view learning and adversarial robustness for next-day direction prediction using two deterministic, window-aligned image views derived from the same time series: an OHLCV-rendered chart (ohlcv) and a technical-indicator matrix (indic). To control label ambiguity from near-zero moves, we use an ex-post minimum-movement threshold min_move (tau) based on realized absolute next-day return, defining an offline benchmark on the subset where the absolute next-day return is at least tau. Under leakage-resistant time-block splits with embargo, we compare early fusion (channel stacking) and dual-encoder late fusion with optional cross-branch consistency. We then evaluate pixel-space L-infinity evasion attacks (FGSM/PGD) under view-constrained and joint threat models. We find that fusion is regime dependent: early fusion can suffer negative transfer under noisier settings, whereas late fusion is a more reliable default once labels stabilize. Robustness degrades sharply under tiny budgets with stable view-dependent vulnerabilities; late fusion often helps under view-constrained attacks, but joint perturbations remain challenging.","authors":["Rui Ma"],"pdf_url":"","comment":"Added sensitivity analysis at tau=0.008 for adversarial robustness; corrected the author affiliation"},{"id":"http://arxiv.org/abs/2602.20971v2","updated":"2026-02-25T11:40:21Z","published":"2026-02-24T14:52:20Z","title":"Does Order Matter : Connecting The Law of Robustness to Robust Generalization","summary":"Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al. (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al. (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al. (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.","authors":["Himadri Mandal","Vishnu Varadarajan","Jaee Ponde","Aritra Das","Mihir More","Debayan Gupta"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.00129v4","updated":"2026-02-25T11:32:35Z","published":"2025-10-31T10:25:23Z","title":"Data-Augmented Deep Learning for Downhole Depth Sensing and Validation","summary":"Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network has achieved significant progress in collar recognition, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into a downhole toolstring for CCL log acquisition to facilitate dataset construction. Comprehensive preprocessing methods for data augmentation are proposed, and their effectiveness is evaluated using baseline neural network models. Through systematic experimentation across diverse configurations, the contribution of each augmentation method is analyzed. Results demonstrate that standardization, label distribution smoothing, and random cropping are fundamental prerequisites for model training, while label smoothing regularization, time scaling, and multiple sampling significantly enhance model generalization capabilities. Incorporating the proposed augmentation methods into the two baseline models results in maximum F1 score improvements of 0.027 and 0.024 for the TAN and MAN models, respectively. Furthermore, applying these techniques yields F1 score gains of up to 0.045 for the TAN model and 0.057 for the MAN model compared to prior studies. Performance evaluation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the existing gaps in data augmentation methodologies for training casing collar recognition models under CCL data-limited conditions, and provides a technical foundation for the future automation of downhole operations.","authors":["Si-Yu Xiao","Xin-Di Zhao","Tian-Hao Mao","Yi-Wei Wang","Yu-Qiao Chen","Hong-Yun Zhang","Jian Wang","Jun-Jie Wang","Shuang Liu","Tu-Pei Chen","Yang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21798v1","updated":"2026-02-25T11:22:47Z","published":"2026-02-25T11:22:47Z","title":"Excitation: Momentum For Experts","summary":"We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introduces a competitive update dynamic that amplifies updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization. Notably, we identify a phenomenon of \"structural confusion\" in deep MoEs, where standard optimizers fail to establish functional signal paths; Excitation acts as a specialization catalyst, \"rescuing\" these models and enabling stable training where baselines remain trapped. Excitation is optimizer-, domain-, and model-agnostic, requires minimal integration effort, and introduces neither additional per-parameter optimizer state nor learnable parameters, making it highly viable for memory-constrained settings. Across language and vision tasks, Excitation consistently improves convergence speed and final performance in MoE models, indicating that active update modulation is a key mechanism for effective conditional computation.","authors":["Sagi Shaier"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21797v1","updated":"2026-02-25T11:22:31Z","published":"2026-02-25T11:22:31Z","title":"Neural Learning of Fast Matrix Multiplication Algorithms: A StrassenNet Approach","summary":"Fast matrix multiplication can be described as searching for low-rank decompositions of the matrix--multiplication tensor. We design a neural architecture, \\textsc{StrassenNet}, which reproduces the Strassen algorithm for $2\\times 2$ multiplication. Across many independent runs the network always converges to a rank-$7$ tensor, thus numerically recovering Strassen's optimal algorithm. We then train the same architecture on $3\\times 3$ multiplication with rank $r\\in\\{19,\\dots,23\\}$. Our experiments reveal a clear numerical threshold: models with $r=23$ attain significantly lower validation error than those with $r\\le 22$, suggesting that $r=23$ could actually be the smallest effective rank of the matrix multiplication tensor $3\\times 3$.\n  We also sketch an extension of the method to border-rank decompositions via an $\\varepsilon$--parametrisation and report preliminary results consistent with the known bounds for the border rank of the $3\\times 3$ matrix--multiplication tensor.","authors":["Paolo Andreini","Alessandra Bernardi","Monica Bianchini","Barbara Toniella Corradini","Sara Marziali","Giacomo Nunziati","Franco Scarselli"],"pdf_url":"","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.21788v1","updated":"2026-02-25T11:11:53Z","published":"2026-02-25T11:11:53Z","title":"DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism","summary":"Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.","authors":["Yifan Niu","Han Xiao","Dongyi Liu","Wei Zhou","Jia Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21783v1","updated":"2026-02-25T11:04:49Z","published":"2026-02-25T11:04:49Z","title":"Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control","summary":"Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.","authors":["Beatrice Luciani","Alex van den Berg","Matti Lang","Alexandre L. Ratschat","Laura Marchal-Crespo"],"pdf_url":"","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.12097v2","updated":"2026-02-25T11:03:13Z","published":"2024-04-18T11:29:43Z","title":"MPC of Uncertain Nonlinear Systems with Meta-Learning for Fast Adaptation of Neural Predictive Models","summary":"In this paper, we consider the problem of reference tracking in uncertain nonlinear systems. A neural State-Space Model (NSSM) is used to approximate the nonlinear system, where a deep encoder network learns the nonlinearity from data, and a state-space component captures the temporal relationship. This transforms the nonlinear system into a linear system in a latent space, enabling the application of model predictive control (MPC) to determine effective control actions. Our objective is to design the optimal controller using limited data from the \\textit{target system} (the system of interest). To this end, we employ an implicit model-agnostic meta-learning (iMAML) framework that leverages information from \\textit{source systems} (systems that share similarities with the target system) to expedite training in the target system and enhance its control performance. The framework consists of two phases: the (offine) meta-training phase learns a aggregated NSSM using data from source systems, and the (online) meta-inference phase quickly adapts this aggregated model to the target system using only a few data points and few online training iterations, based on local loss function gradients. The iMAML algorithm exploits the implicit function theorem to exactly compute the gradient during training, without relying on the entire optimization path. By focusing solely on the optimal solution, rather than the path, we can meta-train with less storage complexity and fewer approximations than other contemporary meta-learning algorithms. We demonstrate through numerical examples that our proposed method can yield accurate predictive models by adaptation, resulting in a downstream MPC that outperforms several baselines.","authors":["Jiaqi Yan","Ankush Chakrabarty","Alisa Rupenyan","John Lygeros"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.12981v3","updated":"2026-02-25T10:58:28Z","published":"2025-02-18T16:02:10Z","title":"Riemannian Variational Flow Matching for Material and Protein Design","summary":"We present Riemannian Gaussian Variational Flow Matching (RG-VFM), a geometric extension of Variational Flow Matching (VFM) for generative modeling on manifolds. Motivated by the benefits of VFM, we derive a variational flow matching objective for manifolds with closed-form geodesics based on Riemannian Gaussian distributions. Crucially, in Euclidean space, predicting endpoints (VFM), velocities (FM), or noise (diffusion) is largely equivalent due to affine interpolations. However, on curved manifolds this equivalence breaks down. We formally analyze the relationship between our model and Riemannian Flow Matching (RFM), revealing that the RFM objective lacks a curvature-dependent penalty -- encoded via Jacobi fields -- that is naturally present in RG-VFM. Based on this relationship, we hypothesize that endpoint prediction provides a stronger learning signal by directly minimizing geodesic distances. Experiments on synthetic spherical and hyperbolic benchmarks, as well as real-world tasks in material and protein generation, demonstrate that RG-VFM more effectively captures manifold structure and improves downstream performance over Euclidean and velocity-based baselines. Code available at https://github.com/olgatticus/rg-vfm.","authors":["Olga Zaghen","Floor Eijkelboom","Alison Pouplin","Cong Liu","Max Welling","Jan-Willem van de Meent","Erik J. Bekkers"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21773v1","updated":"2026-02-25T10:48:51Z","published":"2026-02-25T10:48:51Z","title":"Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias","summary":"Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning,\" where models exhibit an ``easy to learn, yet hard to forget\" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.","authors":["JuneHyoung Kwon","MiHyeon Kim","Eunju Lee","Yoonji Lee","Seunghoon Lee","YoungBin Kim"],"pdf_url":"","comment":"Accepted to AAAI 2026"},{"id":"http://arxiv.org/abs/2407.15738v5","updated":"2026-02-25T10:39:10Z","published":"2024-07-22T15:41:23Z","title":"Parallel Split Learning with Global Sampling","summary":"Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.","authors":["Mohammad Kohankhaki","Ahmad Ayad","Mahdi Barhoush","Anke Schmeink"],"pdf_url":"","comment":"Accepted at the 2025 IEEE 3rd International Conference on Foundation and Large Language Models (FLLM). This version corresponds to the accepted manuscript"},{"id":"http://arxiv.org/abs/2602.21766v1","updated":"2026-02-25T10:36:32Z","published":"2026-02-25T10:36:32Z","title":"RAMSeS: Robust and Adaptive Model Selection for Time-Series Anomaly Detection Algorithms","summary":"Time-series data vary widely across domains, making a universal anomaly detector impractical. Methods that perform well on one dataset often fail to transfer because what counts as an anomaly is context dependent. The key challenge is to design a method that performs well in specific contexts while remaining adaptable across domains with varying data complexities. We present the Robust and Adaptive Model Selection for Time-Series Anomaly Detection RAMSeS framework. RAMSeS comprises two branches: (i) a stacking ensemble optimized with a genetic algorithm to leverage complementary detectors. (ii) An adaptive model-selection branch identifies the best single detector using techniques including Thompson sampling, robustness testing with generative adversarial networks, and Monte Carlo simulations. This dual strategy exploits the collective strength of multiple models and adapts to dataset-specific characteristics. We evaluate RAMSeS and show that it outperforms prior methods on F1.","authors":["Mohamed Abdelmaksoud","Sheng Ding","Andrey Morozov","Ziawasch Abedjan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21765v1","updated":"2026-02-25T10:36:17Z","published":"2026-02-25T10:36:17Z","title":"Generalisation of RLHF under Reward Shift and Clipped KL Regularisation","summary":"Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \\emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \\emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.","authors":["Kenton Tang","Yuzhu Chen","Fengxiang He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.23224v2","updated":"2026-02-25T10:32:00Z","published":"2025-11-28T14:31:20Z","title":"Nonstabilizerness Estimation using Graph Neural Networks","summary":"This article proposes a Graph Neural Network (GNN) approach to estimate nonstabilizerness in quantum circuits, measured by the stabilizer Rényi entropy (SRE). Nonstabilizerness is a fundamental resource for quantum advantage, and efficient SRE estimations are highly beneficial in practical applications. We address the nonstabilizerness estimation problem through three supervised learning formulations starting from easier classification tasks to the more challenging regression task. Experimental results show that the proposed GNN manages to capture meaningful features from the graph-based circuit representation, resulting in robust generalization performances achieved across diverse scenarios. In classification tasks, the GNN is trained on product states and generalizes on circuits evolved under Clifford operations, entangled states, and circuits with higher number of qubits. In the regression task, the GNN significantly improves the SRE estimation on out-of-distribution circuits with higher number of qubits and gate counts compared to previous work, for both unstructured random quantum circuits and structured circuits derived from the transverse-field Ising model. Moreover, the graph representation of quantum circuits naturally integrates hardware-specific information. Simulations on noisy quantum hardware highlight the potential of the proposed GNN to predict the SRE measured on quantum devices.","authors":["Vincenzo Lipardi","Domenica Dibenedetto","Georgios Stamoulis","Evert van Nieuwenburg","Mark H. M. Winands"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21757v1","updated":"2026-02-25T10:19:39Z","published":"2026-02-25T10:19:39Z","title":"Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction","summary":"Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE","authors":["Xiannan Huang","Quan Yuan","Chao Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21756v1","updated":"2026-02-25T10:14:30Z","published":"2026-02-25T10:14:30Z","title":"Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing","summary":"Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.","authors":["Deogyong Kim","Junseong Lee","Jeongeun Lee","Changhoe Kim","Junguel Lee","Jungseok Lee","Dongha Lee"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.21750v1","updated":"2026-02-25T10:06:12Z","published":"2026-02-25T10:06:12Z","title":"From Words to Amino Acids: Does the Curse of Depth Persist?","summary":"Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods.","authors":["Aleena Siji","Amir Mohammad Karimi Mamaghan","Ferdinand Kapl","Tobias Höppe","Emmanouil Angelis","Andrea Dittadi","Maurice Brenner","Michael Heinzinger","Karl Henrik Johansson","Kaitlin Maile","Johannes von Oswald","Stefan Bauer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21749v1","updated":"2026-02-25T10:02:57Z","published":"2026-02-25T10:02:57Z","title":"RABot: Reinforcement-Guided Graph Augmentation for Imbalanced and Noisy Social Bot Detection","summary":"Social bot detection is pivotal for safeguarding the integrity of online information ecosystems. Although recent graph neural network (GNN) solutions achieve strong results, they remain hindered by two practical challenges: (i) severe class imbalance arising from the high cost of generating bots, and (ii) topological noise introduced by bots that skillfully mimic human behavior and forge deceptive links. We propose the Reinforcement-guided graph Augmentation social Bot detector (RABot), a multi-granularity graph-augmentation framework that addresses both issues in a unified manner. RABot employs a neighborhood-aware oversampling strategy that linearly interpolates minority-class embeddings within local subgraphs, thereby stabilizing the decision boundary under low-resource regimes. Concurrently, a reinforcement-learning-driven edge-filtering module combines similarity-based edge features with adaptive threshold optimization to excise spurious interactions during message passing, yielding a cleaner topology. Extensive experiments on three real-world benchmarks and four GNN backbones demonstrate that RABot consistently surpasses state-of-the-art baselines. In addition, since its augmentation and filtering modules are orthogonal to the underlying architecture, RABot can be seamlessly integrated into existing GNN pipelines to boost performance with minimal overhead.","authors":["Longlong Zhang","Xi Wang","Haotong Du","Yangyi Xu","Zhuo Liu","Yang Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21741v1","updated":"2026-02-25T09:52:32Z","published":"2026-02-25T09:52:32Z","title":"Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization","summary":"We describe our end-to-end system for Bengali long-form speech recognition (ASR) and speaker diarization submitted to the DL Sprint 4.0 competition on Kaggle. Bengali presents substantial challenges for both tasks: a large phoneme inventory, significant dialectal variation, frequent code-mixing with English, and a relative scarcity of large-scale labelled corpora. For ASR we achieve a best private Word Error Rate (WER) of 0.37738 and public WER of 0.36137, combining a BengaliAI fine-tuned Whisper medium model with Demucs source separation for vocal isolation, silence-boundary chunking, and carefully tuned generation hyperparameters. For speaker diarization we reach a best private Diarization Error Rate (DER) of 0.27671 and public DER of 0.20936 by replacing the default segmentation model inside the pyannote.audio pipeline with a Bengali-fine-tuned variant, pairing it with wespeaker-voxceleb-resnet34-LM embeddings and centroid-based agglomerative clustering. Our experiments demonstrate that domain-specific fine-tuning of the segmentation component, vocal source separation, and natural silence-aware chunking are the three most impactful design choices for low-resource Bengali speech processing.","authors":["MD. Sagor Chowdhury","Adiba Fairooz Chowdhury"],"pdf_url":"","comment":"6 pages, 5 figures, 3 tables; system paper submitted to DL Sprint 4.0 (Kaggle)"},{"id":"http://arxiv.org/abs/2508.21438v2","updated":"2026-02-25T09:44:44Z","published":"2025-08-29T09:05:56Z","title":"Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing","summary":"The development of continuous biomanufacturing processes requires robust and early anomaly detection, since even minor deviations can compromise yield and stability, leading to disruptions in scheduling, reduced weekly production, and diminished economic performance. These processes are inherently complex and exhibit non-linear dynamics with intricate relationships between process variables, thus making advanced methods for anomaly detection essential for efficient operation. In this work, we present a novel framework for unsupervised anomaly detection in continuous biomanufacturing based on an ensemble of generative adversarial networks (GANs). We first establish a benchmark dataset simulating both normal and anomalous operation regimes in a continuous process for the production of a small molecule. We then demonstrate the effectiveness of our GAN-based framework in detecting anomalies caused by sudden feedstock variability. Finally, we evaluate the impact of using a hybrid quantum/classical GAN approach with both a simulated quantum circuit and a real photonic quantum processor on anomaly detection performance. We find that the hybrid approach yields improved anomaly detection rates. Our work shows the potential of hybrid quantum/classical approaches for solving real-world problems in complex continuous biomanufacturing processes.","authors":["Rajiv Kailasanathan","William R. Clements","Mohammad Reza Boskabadi","Shawn M. Gibford","Emmanouil Papadakis","Christopher J. Savoie","Seyed Soheil Mansouri"],"pdf_url":"","comment":"Accepted in the Journal of Industrial & Engineering Chemistry Research"},{"id":"http://arxiv.org/abs/2512.02435v2","updated":"2026-02-25T09:39:03Z","published":"2025-12-02T05:45:40Z","title":"Efficient Cross-Domain Offline Reinforcement Learning with Dynamics- and Value-Aligned Data Filtering","summary":"Cross-domain offline reinforcement learning (RL) aims to train a well-performing agent in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between source and target domains, naively merging the two datasets may incur inferior performance. Recent advances address this issue by selectively leveraging source domain samples whose dynamics align well with the target domain. However, our work demonstrates that dynamics alignment alone is insufficient, by examining the limitations of prior frameworks and deriving a new target domain sub-optimality bound for the policy learned on the source domain. More importantly, our theory underscores an additional need for \\textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain, a critical dimension overlooked by existing works. Motivated by such theoretical insight, we propose \\textbf{\\underline{D}}ynamics- and \\textbf{\\underline{V}}alue-aligned \\textbf{\\underline{D}}ata \\textbf{\\underline{F}}iltering (DVDF) method, a novel unified cross-domain RL framework that selectively incorporates source domain samples exhibiting strong alignment in \\textit{both dynamics and values}. We empirically study a range of dynamics shift scenarios, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, even in the challenging setting where the target domain dataset contains an extremely limited amount of data. Extensive experiments demonstrate that DVDF consistently outperforms strong baselines with significant improvements.","authors":["Zhongjian Qiao","Rui Yang","Jiafei Lyu","Chenjia Bai","Xiu Li","Siyang Gao","Shuang Qiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.12032v3","updated":"2026-02-25T09:32:40Z","published":"2025-01-21T10:53:17Z","title":"Accelerating Recommender Model ETL with a Streaming FPGA-GPU Dataflow","summary":"The real-time performance of recommender models depends on the continuous integration of massive volumes of new user interaction data into training pipelines. While GPUs have scaled model training throughput, the data preprocessing stage - commonly expressed as Extract-Transform-Load (ETL) pipelines - has emerged as the dominant bottleneck. Production systems often dedicate clusters of CPU servers to support a single GPU node, leading to high operational cost. To address this issue, we present PipeRec, a hardware-accelerated ETL engine co-designed with online recommender model training. PipeRec introduces a training-aware ETL abstraction that exposes freshness, ordering, and batching semantics while compiling software-defined operators into reconfigurable FPGA dataflows and overlaps ETL with GPU training to maximize utilization under I/O constraints. To eliminate CPU bottlenecks, PipeRec implements a format-aware packer that streams training-ready batches directly into GPU memory via P2P DMA transfers, enabling zero-copy ingest and efficient GPU consumption. Our evaluation on three datasets shows that PipeRec accelerates ETL throughput by over 10x compared to CPU-based pipelines and up to 17x over state-of-the-art GPU ETL systems. When integrated with training, PipeRec maintains 64-91% GPU utilization and reduces end-to-end training time to 9.94% of the time taken by CPU-GPU pipelines.","authors":["Yu Zhu","Wenqi Jiang","Piyumi Jasin Pathiranage","Yongjun He","Gustavo Alonso"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.21865v2","updated":"2026-02-25T09:32:19Z","published":"2025-09-26T04:40:42Z","title":"Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding","summary":"Retrieval-Augmented Generation (RAG) is a framework for grounding Large Language Models (LLMs) in external, up-to-date information. However, recent advancements in context window size allow LLMs to process inputs of up to 128K tokens or more, offering an alternative strategy: supplying the full document context directly to the model, rather than relying on RAG to retrieve a subset of contexts. Nevertheless, this emerging alternative strategy has notable limitations: (i) it is token-inefficient to handle large and potentially redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon; and (iii) under limited model capacity, it amplifies distraction, ultimately degrading LLM output quality. In this paper, we propose LDAR (Learning Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve contexts in a way that mitigates interference from distracting passages, thereby achieving significantly higher performance with reduced token usage compared to long-context approaches. Extensive experiments across diverse LLM architectures and six knowledge-intensive benchmarks demonstrate the effectiveness and robustness of our approach, highlighting the importance of balancing the trade-off between information coverage and distraction.","authors":["Seongwoong Shim","Myunsoo Kim","Jae Hyeon Cho","Byung-Jun Lee"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21721v1","updated":"2026-02-25T09:27:40Z","published":"2026-02-25T09:27:40Z","title":"Private and Robust Contribution Evaluation in Federated Learning","summary":"Cross-silo federated learning allows multiple organizations to collaboratively train machine learning models without sharing raw data, but client updates can still leak sensitive information through inference attacks. Secure aggregation protects privacy by hiding individual updates, yet it complicates contribution evaluation, which is critical for fair rewards and detecting low-quality or malicious participants. Existing marginal-contribution methods, such as the Shapley value, are incompatible with secure aggregation, and practical alternatives, such as Leave-One-Out, are crude and rely on self-evaluation.\n  We introduce two marginal-difference contribution scores compatible with secure aggregation. Fair-Private satisfies standard fairness axioms, while Everybody-Else eliminates self-evaluation and provides resistance to manipulation, addressing a largely overlooked vulnerability. We provide theoretical guarantees for fairness, privacy, robustness, and computational efficiency, and evaluate our methods on multiple medical image datasets and CIFAR10 in cross-silo settings. Our scores consistently outperform existing baselines, better approximate Shapley-induced client rankings, and improve downstream model performance as well as misbehavior detection. These results demonstrate that fairness, privacy, robustness, and practical utility can be achieved jointly in federated contribution evaluation, offering a principled solution for real-world cross-silo deployments.","authors":["Delio Jaramillo Velez","Gergely Biczok","Alexandre Graell i Amat","Johan Ostman","Balazs Pejo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21717v1","updated":"2026-02-25T09:25:24Z","published":"2026-02-25T09:25:24Z","title":"C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation","summary":"Tabular data is the primary data format in industrial relational databases, underpinning modern data analytics and decision-making. However, the increasing scale of tabular data poses significant computational and storage challenges to learning-based analytical systems. This highlights the need for data-efficient learning, which enables effective model training and generalization using substantially fewer samples. Dataset condensation (DC) has emerged as a promising data-centric paradigm that synthesizes small yet informative datasets to preserve data utility while reducing storage and training costs. However, existing DC methods are computationally intensive due to reliance on complex gradient-based optimization. Moreover, they often overlook key characteristics of tabular data, such as heterogeneous features and class imbalance. To address these limitations, we introduce C$^{2}$TC (Class-Adaptive Clustering for Tabular Condensation), the first training-free tabular dataset condensation framework that jointly optimizes class allocation and feature representation, enabling efficient and scalable condensation. Specifically, we reformulate the dataset condensation objective into a novel class-adaptive cluster allocation problem (CCAP), which eliminates costly training and integrates adaptive label allocation to handle class imbalance. To solve the NP-hard CCAP, we develop HFILS, a heuristic local search that alternates between soft allocation and class-wise clustering to efficiently obtain high-quality solutions. Moreover, a hybrid categorical feature encoding (HCFE) is proposed for semantics-preserving clustering of heterogeneous discrete attributes. Extensive experiments on 10 real-world datasets demonstrate that C$^{2}$TC improves efficiency by at least 2 orders of magnitude over state-of-the-art baselines, while achieving superior downstream performance.","authors":["Sijia Xu","Fan Li","Xiaoyang Wang","Zhengyi Yang","Xuemin Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.22049v2","updated":"2026-02-25T09:18:59Z","published":"2025-10-24T22:17:49Z","title":"Massive Memorization with Hundreds of Trillions of Parameters for Sequential Transducer Generative Recommenders","summary":"Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.","authors":["Zhimin Chen","Chenyu Zhao","Ka Chun Mo","Yunjiang Jiang","Jane H. Lee","Khushhall Chandra Mahajan","Ning Jiang","Kai Ren","Jinhui Li","Wen-Yun Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21707v1","updated":"2026-02-25T09:13:24Z","published":"2026-02-25T09:13:24Z","title":"Learning spatially adaptive sparsity level maps for arbitrary convolutional dictionaries","summary":"State-of-the-art learned reconstruction methods often rely on black-box modules that, despite their strong performance, raise questions about their interpretability and robustness. Here, we build on a recently proposed image reconstruction method, which is based on embedding data-driven information into a model-based convolutional dictionary regularization via neural network-inferred spatially adaptive sparsity level maps. By means of improved network design and dedicated training strategies, we extend the method to achieve filter-permutation invariance as well as the possibility to change the convolutional dictionary at inference time. We apply our method to low-field MRI and compare it to several other recent deep learning-based methods, also on in vivo data, in which the benefit for the use of a different dictionary is showcased. We further assess the method's robustness when tested on in- and out-of-distribution data. When tested on the latter, the proposed method suffers less from the data distribution shift compared to the other learned methods, which we attribute to its reduced reliance on training data due to its underlying model-based reconstruction component.","authors":["Joshua Schulz","David Schote","Christoph Kolbitsch","Kostas Papafitsoros","Andreas Kofler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20714v2","updated":"2026-02-25T09:10:17Z","published":"2026-02-24T09:19:28Z","title":"WeirNet: A Large-Scale 3D CFD Benchmark for Geometric Surrogate Modeling of Piano Key Weirs","summary":"Reliable prediction of hydraulic performance is challenging for Piano Key Weir (PKW) design because discharge capacity depends on three-dimensional geometry and operating conditions. Surrogate models can accelerate hydraulic-structure design, but progress is limited by scarce large, well-documented datasets that jointly capture geometric variation, operating conditions, and functional performance. This study presents WeirNet, a large 3D CFD benchmark dataset for geometric surrogate modeling of PKWs. WeirNet contains 3,794 parametric, feasibility-constrained rectangular and trapezoidal PKW geometries, each scheduled at 19 discharge conditions using a consistent free-surface OpenFOAM workflow, resulting in 71,387 completed simulations that form the benchmark and with complete discharge coefficient labels. The dataset is released as multiple modalities compact parametric descriptors, watertight surface meshes and high-resolution point clouds together with standardized tasks and in-distribution and out-of-distribution splits. Representative surrogate families are benchmarked for discharge coefficient prediction. Tree-based regressors on parametric descriptors achieve the best overall accuracy, while point- and mesh-based models remain competitive and offer parameterization-agnostic inference. All surrogates evaluate in milliseconds per sample, providing orders-of-magnitude speedups over CFD runtimes. Out-of-distribution results identify geometry shift as the dominant failure mode compared to unseen discharge values, and data-efficiency experiments show diminishing returns beyond roughly 60% of the training data. By publicly releasing the dataset together with simulation setups and evaluation pipelines, WeirNet establishes a reproducible framework for data-driven hydraulic modeling and enables faster exploration of PKW designs during the early stages of hydraulic planning.","authors":["Lisa Lüddecke","Michael Hohmann","Sebastian Eilermann","Jan Tillmann-Mumm","Pezhman Pourabdollah","Mario Oertel","Oliver Niggemann"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21703v1","updated":"2026-02-25T09:09:12Z","published":"2026-02-25T09:09:12Z","title":"Brain Tumor Segmentation with Special Emphasis on the Non-Enhancing Brain Tumor Compartment","summary":"A U-Net based deep learning architecture is designed to segment brain tumors as they appear on various MRI modalities. Special emphasis is lent to the non-enhancing tumor compartment. The latter has not been considered anymore in recent brain tumor segmentation challenges like the MICCAI challenges. However, it is considered to be indicative of the survival time of the patient as well as of areas of further tumor growth. Hence it deems essential to have means to automatically delineate its extension within the tumor.","authors":["T. Schaffer","A. Brawanski","S. Wein","A. M. Tomé","E. W. Lang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.22083v4","updated":"2026-02-25T09:07:06Z","published":"2025-05-28T08:06:25Z","title":"Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz","summary":"In this work, we introduce the first type of non-Euclidean neural quantum state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent neural networks (RNNs)), to be used in the Variational Monte Carlo method of approximating the ground state energy for quantum many-body systems. In particular, we examine the performances of NQS ansatzes constructed from both conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical settings of the one- and two-dimensional transverse field Ising models (TFIM) and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$ systems. By virtue of the fact that, for all of the experiments performed in this work, hyperbolic GRU can yield performances comparable to or better than Euclidean RNNs, which have been extensively studied in these settings in the literature, our work is a proof-of-concept for the viability of hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum many-body systems. Furthermore, in settings where the Hamiltonian displays a clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ & $J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor interactions, our results show that hyperbolic GRU definitively outperforms its Euclidean version in almost all instances. The fact that these results are reminiscent of the established ones from natural language processing where hyperbolic GRU almost always outperforms Euclidean RNNs when the training data exhibit a tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin systems that involve different degrees of nearest neighbor interactions. Finally, with this work, we hope to initiate future studies of other types of non-Euclidean NQS beyond hyperbolic GRU.","authors":["H. L. Dao"],"pdf_url":"","comment":"v2: additional experiments and results included, typo corrected. v3: inference experiments redone, all results updated, conclusions remain qualitatively the same. v4: minor updates of some figures, more descriptions added, matches the published version on EPJP"},{"id":"http://arxiv.org/abs/2602.21701v1","updated":"2026-02-25T09:04:15Z","published":"2026-02-25T09:04:15Z","title":"Learning Complex Physical Regimes via Coverage-oriented Uncertainty Quantification: An application to the Critical Heat Flux","summary":"A central challenge in scientific machine learning (ML) is the correct representation of physical systems governed by multi-regime behaviours. In these scenarios, standard data analysis techniques often fail to capture the nature of the data, as the system's response varies significantly across the state space due to its stochasticity and the different physical regimes. Uncertainty quantification (UQ) should thus not be viewed merely as a safety assessment, but as a support to the learning task itself, guiding the model to internalise the behaviour of the data. We address this by focusing on the Critical Heat Flux (CHF) benchmark and dataset presented by the OECD/NEA Expert Group on Reactor Systems Multi-Physics. This case study represents a test for scientific ML due to the non-linear dependence of CHF on the inputs and the existence of distinct microscopic physical regimes. These regimes exhibit diverse statistical profiles, a complexity that requires UQ techniques to internalise the data behaviour and ensure reliable predictions. In this work, we conduct a comparative analysis of UQ methodologies to determine their impact on physical representation. We contrast post-hoc methods, specifically conformal prediction, against end-to-end coverage-oriented pipelines, including (Bayesian) heteroscedastic regression and quality-driven losses. These approaches treat uncertainty not as a final metric, but as an active component of the optimisation process, modelling the prediction and its behaviour simultaneously. We show that while post-hoc methods ensure statistical calibration, coverage-oriented learning effectively reshapes the model's representation to match the complex physical regimes. The result is a model that delivers not only high predictive accuracy but also a physically consistent uncertainty estimation that adapts dynamically to the intrinsic variability of the CHF.","authors":["Michele Cazzola","Alberto Ghione","Lucia Sargentini","Julien Nespoulous","Riccardo Finotello"],"pdf_url":"","comment":"34 pages, 14 figures"},{"id":"http://arxiv.org/abs/2602.21693v1","updated":"2026-02-25T08:51:03Z","published":"2026-02-25T08:51:03Z","title":"TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts","summary":"Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on future developments, which serve as guidance for time series forecasting. To seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering both strong adaptability and interpretability.","authors":["Jiafeng Lin","Yuxuan Wang","Huakun Luo","Zhongyi Pei","Jianmin Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.03255v2","updated":"2026-02-25T08:50:49Z","published":"2025-09-26T09:25:16Z","title":"SciTS: Scientific Time Series Understanding and Generation with LLMs","summary":"The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences as text or convert them into images. Such approaches may be insufficient for comprehensive scientific time series understanding and generation. Existing unified time series models typically specialise in either forecasting or analysis, and their effectiveness on non-periodic, heterogeneous scientific signals remains unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12 scientific domains and 43 tasks, with over 50k+ instances, both univariate and multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz in frequency. We benchmark 17 models, including text-only LLMs, multimodal LLMs, and unified time series models, and find that general-purpose LLMs exhibit stronger generalisability than specialised time series models, while representing time series as text or images limits their performance due to excessively long sequences and loss of numerical precision, respectively. We then introduce TimeOmni, a framework that equips LLMs with the ability to understand and generate time series while remaining compatible with general-purpose LLM training. This work fills a gap in both dedicated benchmarks and modelling frameworks for scientific time series, paving the way for LLMs to understand and generate complex temporal scientific data.","authors":["Wen Wu","Ziyang Zhang","Liwei Liu","Xuenan Xu","Jimin Zhuang","Ke Fan","Qitan Lv","Junlin Liu","Chen Zhang","Zheqi Yuan","Siyuan Hou","Tianyi Lin","Kai Chen","Bowen Zhou","Chao Zhang"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.21684v1","updated":"2026-02-25T08:36:45Z","published":"2026-02-25T08:36:45Z","title":"Primary-Fine Decoupling for Action Generation in Robotic Imitation","summary":"Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Xingyu Lu","Houqiang Li"],"pdf_url":"","comment":"The Fourteenth International Conference on Learning Representations (ICLR), 2026"},{"id":"http://arxiv.org/abs/2602.21680v1","updated":"2026-02-25T08:33:39Z","published":"2026-02-25T08:33:39Z","title":"Hierarchical Lead Critic based Multi-Agent Reinforcement Learning","summary":"Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.","authors":["David Eckel","Henri Meeß"],"pdf_url":"","comment":"16 pages, 10 Figures, Preprint"},{"id":"http://arxiv.org/abs/2602.21677v1","updated":"2026-02-25T08:25:16Z","published":"2026-02-25T08:25:16Z","title":"Trie-Aware Transformers for Generative Recommendation","summary":"Generative recommendation (GR) aligns with advances in generative AI by casting next-item prediction as token-level generation rather than score-based ranking. Most GR methods adopt a two-stage pipeline: (i) \\textit{item tokenization}, which maps each item to a sequence of discrete, hierarchically organized tokens; and (ii) \\textit{autoregressive generation}, which predicts the next item's tokens conditioned on the tokens of user's interaction history. Although hierarchical tokenization induces a prefix tree (trie) over items, standard autoregressive modeling with conventional Transformers often flattens item tokens into a linear stream and overlooks the underlying topology.\n  To address this, we propose TrieRec, a trie-aware generative recommendation method that augments Transformers with structural inductive biases via two positional encodings. First, a \\textit{trie-aware absolute positional encoding} aggregates a token's (node's) local structural context (\\eg depth, ancestors, and descendants) into the token representation. Second, a \\textit{topology-aware relative positional encoding} injects pairwise structural relations into self-attention to capture topology-induced semantic relatedness. TrieRec is also model-agnostic, efficient, and hyperparameter-free. In our experiments, we implement TrieRec within three representative GR backbones, achieving notably improvements of 8.83\\% on average across four real-world datasets.","authors":["Zhenxiang Xu","Jiawei Chen","Sirui Chen","Yong He","Jieyu Yang","Chuan Yuan","Ke Ding","Can Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21674v1","updated":"2026-02-25T08:20:52Z","published":"2026-02-25T08:20:52Z","title":"Error-awareness Accelerates Active Automata Learning","summary":"Active automata learning (AAL) algorithms can learn a behavioral model of a system from interacting with it. The primary challenge remains scaling to larger models, in particular in the presence of many possible inputs to the system. Modern AAL algorithms fail to scale even if, in every state, most inputs lead to errors. In various challenging problems from the literature, these errors are observable, i.e., they emit a known error output. Motivated by these problems, we study learning these systems more efficiently. Further, we consider various degrees of knowledge about which inputs are non-error producing at which state. For each level of knowledge, we provide a matching adaptation of the state-of-the-art AAL algorithm L# to make the most of this domain knowledge. Our empirical evaluation demonstrates that the methods accelerate learning by orders of magnitude with strong but realistic domain knowledge to a single order of magnitude with limited domain knowledge.","authors":["Loes Kruger","Sebastian Junges","Jurriaan Rot"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2407.15160v3","updated":"2026-02-25T08:14:26Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"Large language models based on the transformer architecture can solve highly complex tasks, yet their fundamental limitations on simple algorithmic problems remain poorly understood. In this work, we focus on basic counting tasks and investigate how the difficulty of these tasks scales with the transformer embedding dimension, the context length, and the vocabulary size. We reveal a sharp theoretical phase transition governed by the relationship between the embedding dimension and the vocabulary size. When the dimension is at least as large as the vocabulary, transformers can perfectly maintain token counts. However, when the vocabulary exceeds the embedding dimension, the interference between non-orthogonal token representations forces the network weights to scale polynomially. This renders the exact counting algorithm numerically unstable and practically unlearnable. We empirically validate this bottleneck by training transformers from scratch, demonstrating a strict performance drop at the theoretical threshold and catastrophic out of distribution failure when scaling the vocabulary or context length. Furthermore, we show that state-of-the-art pretrained models suffer from similar failure cases. Our work reveals a critical blind spot absent from the current literature regarding the connection among these three parameters, proving that vocabulary size fundamentally dictates the difficulty of counting tasks.","authors":["Gilad Yehudai","Haim Kaplan","Guy Dar","Royi Rassin","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.18502v2","updated":"2026-02-25T08:12:27Z","published":"2025-05-24T04:43:24Z","title":"Knowledge Fusion of Large Language Models Via Modular SkillPacks","summary":"Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.","authors":["Guodong Du","Zhuo Li","Xuanning Zhou","Junlin Li","Zesheng Shi","Wanyu Lin","Ho-Kin Tang","Xiucheng Li","Fangming Liu","Wenya Wang","Min Zhang","Jing Li"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.12635v2","updated":"2026-02-25T08:07:27Z","published":"2026-02-13T05:41:31Z","title":"Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats","summary":"As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.","authors":["Pengxiang Zhao","Hui-Ling Zhen","Xing Li","Han Bao","Weizhe Lin","Zhiyuan Yang","Ziwei Yu","Xin Wang","Mingxuan Yuan","Xianzhi Yu","Zhenhua Dong"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.03801v2","updated":"2026-02-25T08:01:08Z","published":"2025-05-02T08:00:48Z","title":"Large Language Model Compression with Global Rank and Sparsity Optimization","summary":"Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-rank and sparse matrices, while the second involves determining weight allocation across different layers, as redundancy varies considerably among them. To address these challenges, we propose a novel two-stage LLM compression method with the capability of global resource allocation for rank and sparsity. It is noteworthy that the overall optimization space is vast, making comprehensive optimization computationally prohibitive. Therefore, to reduce the optimization space, our first stage utilizes robust principal component analysis to decompose the weight matrices of LLMs into low-rank and sparse components, which span the low dimensional and sparse spaces containing the resultant low-rank and sparse matrices, respectively. In the second stage, we propose a probabilistic global allocation strategy to jointly identify the low-rank and sparse structures within the above two spaces. The appealing feature of our approach is its ability to automatically detect the redundancy across different layers and to manage the interaction between the sparse and low-rank components. Extensive experimental results indicate that our method significantly surpasses state-of-the-art techniques for sparsification and composite approximation.","authors":["Changhai Zhou","Qian Qiao","Yuhua Zhou","Yuxin Wu","Shichao Weng","Weizhong Zhang","Cheng Jin"],"pdf_url":"","comment":"33 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.21158v2","updated":"2026-02-25T07:50:58Z","published":"2026-02-24T18:04:54Z","title":"SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards","summary":"Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.","authors":["Dengjia Zhang","Xiaoou Liu","Lu Cheng","Yaqing Wang","Kenton Murray","Hua Wei"],"pdf_url":"","comment":"Accepted by PAKDD'26"},{"id":"http://arxiv.org/abs/2510.21686v2","updated":"2026-02-25T07:46:34Z","published":"2025-10-24T17:44:40Z","title":"Multimodal Datasets with Controllable Mutual Information","summary":"We introduce a framework for generating highly multimodal datasets with explicitly calculable mutual information (MI) between modalities. This enables the construction of benchmark datasets that provide a novel testbed for systematic studies of mutual information estimators and multimodal self-supervised learning (SSL) techniques. Our framework constructs realistic datasets with known MI using a flow-based generative model and a structured causal framework for generating correlated latent variables. We benchmark a suite of MI estimators on datasets with varying ground truth MI values and verify that regression performance improves as the MI increases between input modalities and the target value. Finally, we describe how our framework can be applied to contexts including multi-detector astrophysics and SSL studies in the highly multimodal regime.","authors":["Raheem Karim Hashmani","Garrett W. Merz","Helen Qu","Mariel Pettee","Kyle Cranmer"],"pdf_url":"","comment":"16 pages, 7 figures, 2 tables. Our code is publicly available at https://github.com/RKHashmani/MmMi-Datasets. Datasets generated based on Figure 1 can be found at https://huggingface.co/datasets/RKHashmani/mmmi-dag1-2modalities-cifar10"},{"id":"http://arxiv.org/abs/2510.11789v2","updated":"2026-02-25T07:26:58Z","published":"2025-10-13T18:00:04Z","title":"Minimax Rates for Learning Pairwise Interactions in Attention-Style Models","summary":"We study the convergence rate of learning pairwise interactions in single-layer attention-style models, where tokens interact through a weight matrix and a nonlinear activation function. We prove that the minimax rate is $M^{-\\frac{2β}{2β+1}}$, where $M$ is the sample size and $β$ is the Hölder smoothness of the activation function. Importantly, this rate is independent of the embedding dimension $d$, the number of tokens $N$, and the rank $r$ of the weight matrix, provided that $rd \\le (M/\\log M)^{\\frac{1}{2β+1}}$. These results highlight a fundamental statistical efficiency of attention-style models, even when the weight matrix and activation are not separately identifiable, and provide a theoretical understanding of attention mechanisms and guidance on training.","authors":["Shai Zucker","Xiong Wang","Fei Lu","Inbar Seroussi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.07922v3","updated":"2026-02-25T07:24:40Z","published":"2025-11-11T07:18:55Z","title":"SERL: Self-Examining Reinforcement Learning on Open-Domain","summary":"Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.","authors":["Weixuan Ou","Yanzhao Zheng","Shuoshuo Sun","Wei Zhang","Baohua Dong","Hangcheng Zhu","Ruohui Huang","Gang Yu","Pengwei Yan","Yifan Qiao"],"pdf_url":"","comment":"Accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)"},{"id":"http://arxiv.org/abs/2602.21648v1","updated":"2026-02-25T07:20:43Z","published":"2026-02-25T07:20:43Z","title":"Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction","summary":"Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort.\n  After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions.\n  Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state.\n  This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.","authors":["Toktam Khatibi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21647v1","updated":"2026-02-25T07:20:23Z","published":"2026-02-25T07:20:23Z","title":"Mitigating Structural Noise in Low-Resource S2TT: An Optimized Cascaded Nepali-English Pipeline with Punctuation Restoration","summary":"This paper presents and evaluates an optimized cascaded Nepali speech-to-English text translation (S2TT) system, focusing on mitigating structural noise introduced by Automatic Speech Recognition (ASR). We first establish highly proficient ASR and NMT components: a Wav2Vec2-XLS-R-300m model achieved a state-of-the-art 2.72% CER on OpenSLR-54, and a multi-stage fine-tuned MarianMT model reached a 28.32 BLEU score on the FLORES-200 benchmark. We empirically investigate the influence of punctuation loss, demonstrating that unpunctuated ASR output significantly degrades translation quality, causing a massive 20.7% relative BLEU drop on the FLORES benchmark. To overcome this, we propose and evaluate an intermediate Punctuation Restoration Module (PRM). The final S2TT pipeline was tested across three configurations on a custom dataset. The optimal configuration, which applied the PRM directly to ASR output, achieved a 4.90 BLEU point gain over the direct ASR-to-NMT baseline (BLEU 36.38 vs. 31.48). This improvement was validated by human assessment, which confirmed the optimized pipeline's superior Adequacy (3.673) and Fluency (3.804). This work validates that targeted punctuation restoration is the most effective intervention for mitigating structural noise in the Nepali S2TT pipeline. It establishes an optimized baseline and demonstrates a critical architectural insight for developing cascaded speech translation systems for similar low-resource languages.","authors":["Tangsang Chongbang","Pranesh Pyara Shrestha","Amrit Sarki","Anku Jaiswal"],"pdf_url":"","comment":"13 pages, 4 figures, 12 tables"},{"id":"http://arxiv.org/abs/2507.14206v2","updated":"2026-02-25T07:17:42Z","published":"2025-07-15T02:54:24Z","title":"A Comprehensive Benchmark for Electrocardiogram Time-Series","summary":"Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial for assessing cardiac health and diagnosing various diseases. Given its time-series format, ECG data is often incorporated into pre-training datasets for large-scale time-series model training. However, existing studies often overlook its unique characteristics and specialized downstream applications, which differ significantly from other time-series data, leading to an incomplete understanding of its properties. In this paper, we present an in-depth investigation of ECG signals and establish a comprehensive benchmark, which includes (1) categorizing its downstream applications into four distinct evaluation tasks, (2) identifying limitations in traditional evaluation metrics for ECG analysis, and introducing a novel metric; (3) benchmarking state-of-the-art time-series models and proposing a new architecture. Extensive experiments demonstrate that our proposed benchmark is comprehensive and robust. The results validate the effectiveness of the proposed metric and model architecture, which establish a solid foundation for advancing research in ECG signal analysis.","authors":["Zhijiang Tang","Jiaxin Qi","Yuhua Zheng","Jianqiang Huang"],"pdf_url":"","comment":"ACM MM 2025"},{"id":"http://arxiv.org/abs/2512.09069v2","updated":"2026-02-25T07:03:17Z","published":"2025-12-09T19:34:30Z","title":"KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification","summary":"Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency-accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD-OCT.","authors":["Erfan Nourbakhsh","Nasrin Sanjari","Ali Nourbakhsh"],"pdf_url":"","comment":"7 pages, 5 figures (Accepted at ICSPIS 2025)"},{"id":"http://arxiv.org/abs/2602.21634v1","updated":"2026-02-25T06:58:18Z","published":"2026-02-25T06:58:18Z","title":"AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction","summary":"Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.","authors":["Chaowei Wu","Huazhu Chen","Congde Yuan","Qirui Yang","Guoqing Song","Yue Gao","Li Luo","Frank Youhua Chen","Mengzhuo Guo"],"pdf_url":"","comment":"12 pages, 4 figures, submitted to KDD 2026: 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ADS Track"},{"id":"http://arxiv.org/abs/2510.16071v2","updated":"2026-02-25T06:50:59Z","published":"2025-10-17T09:01:59Z","title":"MNO: Multiscale Neural Operator for 3D Computational Fluid Dynamics","summary":"Neural operators have emerged as a powerful data-driven paradigm for solving partial differential equations (PDEs), while their accuracy and scalability are still limited, particularly on irregular domains where fluid flows exhibit rich multiscale structures. In this work, we introduce the Multiscale Neural Operator (MNO), a new architecture for computational fluid dynamics (CFD) on 3D unstructured point clouds. MNO explicitly decomposes information across three scales: a global dimension-shrinkage attention module for long-range dependencies, a local graph attention module for neighborhood-level interactions, and a micro point-wise attention module for fine-grained details. This design preserves multiscale inductive biases while remaining computationally efficient. We evaluate MNO on diverse benchmarks, covering steady-state and unsteady flow scenarios with up to 300k points. Across all tasks, MNO consistently outperforms state-of-the-art baselines, reducing prediction errors by 5% to 50%. The results highlight the importance of explicit multiscale design for neural operators and establish MNO as a scalable framework for learning complex fluid dynamics on irregular domains.","authors":["Qinxuan Wang","Chuang Wang","Mingyu Zhang","Jingwei Sun","Peipei Yang","Shuo Tang","Shiming Xiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.14659v2","updated":"2026-02-25T06:36:29Z","published":"2025-09-18T06:33:44Z","title":"Aligning Audio Captions with Human Preferences","summary":"Current audio captioning relies on supervised learning with paired audio-caption data, which is costly to curate and may not reflect human preferences in real-world scenarios. To address this, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To capture nuanced preferences, we train a Contrastive Language-Audio Pretraining (CLAP) based reward model using human-labeled pairwise preference data. This reward model is integrated into an RL framework to fine-tune any baseline captioning system without ground-truth annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over baseline models, particularly when baselines fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating effective alignment with human preferences and scalability in real-world use.","authors":["Kartik Hegde","Rehana Mahfuz","Yinyi Guo","Erik Visser"],"pdf_url":"","comment":"Submitted for review to Interspeech 2026"},{"id":"http://arxiv.org/abs/2602.21620v1","updated":"2026-02-25T06:32:23Z","published":"2026-02-25T06:32:23Z","title":"Revisiting the Bertrand Paradox via Equilibrium Analysis of No-regret Learners","summary":"We study the discrete Bertrand pricing game with a non-increasing demand function. The game has $n \\ge 2$ players who simultaneously choose prices from the set $\\{1/k, 2/k, \\ldots, 1\\}$, where $k\\in\\mathbb{N}$. The player who sets the lowest price captures the entire demand; if multiple players tie for the lowest price, they split the demand equally.\n  We study the Bertrand paradox, where classical theory predicts low prices, yet real markets often sustain high prices. To understand this gap, we analyze a repeated-game model in which firms set prices using no-regret learners. Our goal is to characterize the equilibrium outcomes that can arise under different no-regret learning guarantees. We are particularly interested in questions such as whether no-external-regret learners can converge to undesirable high-price outcomes, and how stronger guarantees such as no-swap regret shape the emergence of competitive low-price behavior. We address these and related questions through a theoretical analysis, complemented by experiments that support the theory and reveal surprising phenomena for no-swap regret learners.","authors":["Arnab Maiti","Junyan Liu","Kevin Jamieson","Lillian J. Ratliff"],"pdf_url":"","comment":"36 pages, 34 figures"},{"id":"http://arxiv.org/abs/2602.02137v3","updated":"2026-02-25T06:25:37Z","published":"2026-02-02T14:18:52Z","title":"DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations","summary":"Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.","authors":["Minghao Li","Ruihang Wang","Rui Tan","Yonggang Wen"],"pdf_url":"","comment":"Accepted as a full paper at HSCC/ICCPS 2026"},{"id":"http://arxiv.org/abs/2502.14183v3","updated":"2026-02-25T06:15:06Z","published":"2025-02-20T01:26:00Z","title":"Glycemic-Aware and Architecture-Agnostic Training Framework for Blood Glucose Forecasting in Type 1 Diabetes","summary":"Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels and avoid dysglycemia, including hyperglycemia and hypoglycemia. Despite advances in automated insulin delivery (AID) systems, achieving optimal glycemic control remains challenging. These systems integrate data from wearable devices such as insulin pumps and continuous glucose monitors (CGMs), helping reduce variability and improve time in range. However, they often fail to prevent dysglycemia due to limitations in prediction algorithms that cannot accurately anticipate glycemic excursions. This limitation highlights the need for more advanced glucose forecasting methods. To address this need, we introduce GLIMMER (Glucose Level Indicator Model with Modified Error Rate), a modular and architecture-agnostic training framework for glucose forecasting. GLIMMER combines structured preprocessing, a region-aware loss formulation, and genetic algorithm-based weight optimization to emphasize prediction accuracy in dysglycemic regions. We evaluate GLIMMER using two datasets: the publicly available OhioT1DM dataset and a newly collected AZT1D dataset consisting of data from 25 individuals with T1D. Our analyses demonstrate that GLIMMER consistently improves forecasting performance across baseline architectures, reducing RMSE and MAE by up to 24.6% and 29.6%, respectively. Additionally, GLIMMER achieves a recall of 98.4% and an F1-score of 86.8% for dysglycemia prediction, highlighting strong performance in clinically high-risk regions. Compared with state-of-the-art models containing millions of parameters-such as TimesNet (18.7M), BG-BERT (2.1M), and Gluformer (11.2M)-GLIMMER attains comparable accuracy while using only 10K parameters, demonstrating its efficiency as a lightweight and architecture-agnostic solution for glycemic forecasting.","authors":["Saman Khamesian","Asiful Arefeen","Maria Adela Grando","Bithika M. Thompson","Hassan Ghasemzadeh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21601v1","updated":"2026-02-25T06:01:17Z","published":"2026-02-25T06:01:17Z","title":"Deep Clustering based Boundary-Decoder Net for Inter and Intra Layer Stress Prediction of Heterogeneous Integrated IC Chip","summary":"High stress occurs when 3D heterogeneous IC packages are subjected to thermal cycling at extreme temperatures. Stress mainly occurs at the interface between different materials. We investigate stress image using latent space representation which is based on using deep generative model (DGM). However, most DGM approaches are unsupervised, meaning they resort to image pairing (input and output) to train DGM. Instead, we rely on a recent boundary-decoder (BD) net, which uses boundary condition and image pairing for stress modeling. The boundary net maps material parameters to the latent space co-shared by its image counterpart. Because such a setup is dimensionally wise ill-posed, we further couple BD net with deep clustering. To access the performance of our proposed method, we simulate an IC chip dataset comprising of 1825 stress images. We compare our new approach using variants of BD net as well as a baseline approach. We show that our approach is able to outperform all the comparison in terms of train and test error reduction.","authors":["Kart Leong Lim","Ji Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.12415v5","updated":"2026-02-25T05:53:52Z","published":"2026-01-18T13:57:44Z","title":"Orthogonalized Policy Optimization:Policy Optimization as Orthogonal Projection in Hilbert Space","summary":"We propose Orthogonalized Policy Optimization (OPO), a principled framework for large language model alignment derived from optimization in the Hilbert function space L2(pi_k). Lifting policy updates from the probability simplex into L2(pi_k) transforms the nonlinear normalization constraint into a linear orthogonality condition <v, 1>_{pi_k} = 0 on the density fluctuation field v = pi/pi_k - 1. By the Hilbert projection theorem, the unique closed-form update is v_star = (omega_alpha - E[omega_alpha]) / mu, where the subtracted mean acts as a chemical potential enforcing probability conservation. This interpretation reveals advantage z-score normalization as a conservation-law projection rather than a variance-reduction heuristic.\n  OPO cleanly decouples sampling geometry, controlled by the escort exponent alpha, from optimization geometry, governed by the stiffness parameter mu, a separation not attainable under KL-based objectives. The same update can also be derived as a Euclidean mirror-descent step and as the linear-response law of near-equilibrium statistical mechanics, establishing its structural uniqueness within ratio geometry.\n  Structurally, OPO induces constant curvature, non-saturating linear gradient dynamics, and an intrinsic chi-square trust region. Experiments on MATH benchmarks show that the Hilbert projection formulation prevents gradient saturation typical of KL-constrained methods. By sustaining non-vanishing gradients in high-confidence regimes, OPO avoids premature plateaus and achieves stronger long-horizon training rewards and improved out-of-distribution generalization compared to clipping-based baselines.","authors":["Wang Zixian"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21597v1","updated":"2026-02-25T05:46:42Z","published":"2026-02-25T05:46:42Z","title":"NGDB-Zoo: Towards Efficient and Scalable Neural Graph Databases Training","summary":"Neural Graph Databases (NGDBs) facilitate complex logical reasoning over incomplete knowledge structures, yet their training efficiency and expressivity are constrained by rigid query-level batching and structure-exclusive embeddings. We present NGDB-Zoo, a unified framework that resolves these bottlenecks by synergizing operator-level training with semantic augmentation. By decoupling logical operators from query topologies, NGDB-Zoo transforms the training loop into a dynamically scheduled data-flow execution, enabling multi-stream parallelism and achieving a $1.8\\times$ - $6.8\\times$ throughput compared to baselines. Furthermore, we formalize a decoupled architecture to integrate high-dimensional semantic priors from Pre-trained Text Encoders (PTEs) without triggering I/O stalls or memory overflows. Extensive evaluations on six benchmarks, including massive graphs like ogbl-wikikg2 and ATLAS-Wiki, demonstrate that NGDB-Zoo maintains high GPU utilization across diverse logical patterns and significantly mitigates representation friction in hybrid neuro-symbolic reasoning.","authors":["Zhongwei Xie","Jiaxin Bai","Shujie Liu","Haoyu Huang","Yufei Li","Yisen Gao","Hong Ting Tsang","Yangqiu Song"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20223v2","updated":"2026-02-25T05:40:23Z","published":"2026-02-23T13:37:44Z","title":"MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning","summary":"Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN.","authors":["Wall Kim","Chaeyoung Song","Hanul Kim"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.21593v1","updated":"2026-02-25T05:38:08Z","published":"2026-02-25T05:38:08Z","title":"Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection","summary":"Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations that invalidate such bindings. To expose this overlooked vulnerability, we introduce a Coherence-Preserving Semantic Injection (CSI) attack that leverages LLM-guided semantic manipulation under embedding-space similarity constraints. This alignment enforces visual-semantic consistency while selectively perturbing watermark-relevant semantics, ultimately inducing detector misclassification. Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness of current semantic watermark designs when confronted with LLM-driven semantic perturbations.","authors":["Zheng Gao","Xiaoyu Li","Zhicheng Bao","Xiaoyan Feng","Jiaojiao Jiang"],"pdf_url":"","comment":"Accepted by The Web Conference 2026 (Short Paper Track)"},{"id":"http://arxiv.org/abs/2509.23597v3","updated":"2026-02-25T05:33:12Z","published":"2025-09-28T03:06:30Z","title":"Characteristic Root Analysis and Regularization for Linear Time Series Forecasting","summary":"Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including \\textbf{Reduced-Rank Regression (RRR)} and \\textbf{Direct Weight Rank Reduction (DWRR)}, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called \\textbf{Root Purge}, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models.","authors":["Zheng Wang","Kaixuan Zhang","Wanfang Chen","Xiaonan Lu","Longyuan Li","Tobias Schlagenhauf"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21588v1","updated":"2026-02-25T05:19:43Z","published":"2026-02-25T05:19:43Z","title":"ABM-UDE: Developing Surrogates for Epidemic Agent-Based Models via Scientific Machine Learning","summary":"Agent-based epidemic models (ABMs) encode behavioral and policy heterogeneity but are too slow for nightly hospital planning. We develop county-ready surrogates that learn directly from exascale ABM trajectories using Universal Differential Equations (UDEs): mechanistic SEIR-family ODEs with a neural-parameterized contact rate $κ_φ(u,t)$ (no additive residual). Our contributions are threefold: we adapt multiple shooting and an observer-based prediction-error method (PEM) to stabilize identification of neural-augmented epidemiological dynamics across intervention-driven regime shifts; we enforce positivity and mass conservation and show the learned contact-rate parameterization yields a well-posed vector field; and we quantify accuracy, calibration, and compute against ABM ensembles and UDE baselines. On a representative ExaEpi scenario, PEM-UDE reduces mean MSE by 77% relative to single-shooting UDE (3.00 vs. 13.14) and by 20% relative to MS-UDE (3.75). Reliability improves in parallel: empirical coverage of ABM $10$-$90$% and $25$-$75$% bands rises from 0.68/0.43 (UDE) and 0.79/0.55 (MS-UDE) to 0.86/0.61 with PEM-UDE and 0.94/0.69 with MS+PEM-UDE, indicating calibrated uncertainty rather than overconfident fits. Inference runs in seconds on commodity CPUs (20-35 s per $\\sim$90-day forecast), enabling nightly ''what-if'' sweeps on a laptop. Relative to a $\\sim$100 CPU-hour ABM reference run, this yields $\\sim10^{4}\\times$ lower wall-clock per scenario. This closes the realism-cadence gap, supports threshold-aware decision-making (e.g., maintaining ICU occupancy $<75$%), preserves mechanistic interpretability, and enables calibrated, risk-aware scenario planning on standard institutional hardware. Beyond epidemics, the ABM$\\to$UDE recipe provides a portable path to distill agent-based simulators into fast, trustworthy surrogates for other scientific domains.","authors":["Sharv Murgai","Utkarsh Utkarsh","Kyle C. Nguyen","Alan Edelman","Erin C. S. Acquesta","Christopher Vincent Rackauckas"],"pdf_url":"","comment":"25 pages, 4 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2602.22011v1","updated":"2026-02-25T15:27:54Z","published":"2026-02-25T15:27:54Z","title":"A Generic Web Component for WebRTC Pub-Sub","summary":"We present video-io, a generic web component to publish or subscribe to a media stream in WebRTC (web real-time communication) applications. Unlike a call or conference room abstraction of existing video conferencing services, it uses a named stream abstraction, which is useful in many scenarios beyond just a call or conference. It keeps most of the application logic in the endpoint using the extensive application interface of this component, and keeps any vendor specific access control or signaling negotiation in a service-specific connector implementation. This allows an app developer to write once, and be able to run the web app on different servers or services. We also demonstrate its flexibility by implementing the connector for ten different existing systems and services. Decoupling the app from the hosted vendor service promotes innovation in the endpoint beyond what a single vendor locked client app can offer.","authors":["Kundan Singh"],"pdf_url":"","comment":"11 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2601.13879v3","updated":"2026-02-25T12:15:14Z","published":"2026-01-20T11:45:38Z","title":"Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring","summary":"While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.","authors":["Dongxu Zhang","Yiding Sun","Cheng Tan","Wenbiao Yan","Ning Yang","Jihua Zhu","Haijun Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.15759v2","updated":"2026-02-25T11:49:46Z","published":"2025-06-18T13:07:04Z","title":"Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration","summary":"Recent advancements in 4D generation have demonstrated its remarkable capability in synthesizing photorealistic renderings of dynamic 3D scenes. However, despite achieving impressive visual performance, almost all existing methods overlook the generation of spatial audio aligned with the corresponding 4D scenes, posing a significant limitation to truly immersive audiovisual experiences. To mitigate this issue, we propose Sonic4D, a novel framework that enables spatial audio generation for immersive exploration of 4D scenes. Specifically, our method is composed of three stages: 1) To capture both the dynamic visual content and raw auditory information from a monocular video, we first employ pre-trained expert models to generate the 4D scene and its corresponding monaural audio. 2) Subsequently, to transform the monaural audio into spatial audio, we localize and track the sound sources within the 4D scene, where their 3D spatial coordinates at different timestamps are estimated via a pixel-level visual grounding strategy. 3) Based on the estimated sound source locations, we further synthesize plausible spatial audio that varies across different viewpoints and timestamps using physics-based simulation. Extensive experiments have demonstrated that our proposed method generates realistic spatial audio consistent with the synthesized 4D scene in a training-free manner, significantly enhancing the immersive experience for users. Generated audio and video examples are available at https://x-drunker.github.io/Sonic4D-project-page.","authors":["Siyi Xie","Hanxin Zhu","Tianyu He","Xin Li","Zhibo Chen"],"pdf_url":"","comment":"17 pages, 7 figures. Project page: https://x-drunker.github.io/Sonic4D-project-page/"},{"id":"http://arxiv.org/abs/2602.12304v2","updated":"2026-02-25T04:58:18Z","published":"2026-02-12T03:25:41Z","title":"OmniCustom: Sync Audio-Video Customization Via Joint Audio-Video Generation Model","summary":"Existing mainstream video customization methods focus on generating identity-consistent videos based on given reference images and textual prompts. Benefiting from the rapid advancement of joint audio-video generation, this paper proposes a more compelling new task: sync audio-video customization, which aims to synchronously customize both video identity and audio timbre. Specifically, given a reference image $I^{r}$ and a reference audio $A^{r}$, this novel task requires generating videos that maintain the identity of the reference image while imitating the timbre of the reference audio, with spoken content freely specifiable through user-provided textual prompts. To this end, we propose OmniCustom, a powerful DiT-based audio-video customization framework that can synthesize a video following reference image identity, audio timbre, and text prompts all at once in a zero-shot manner. Our framework is built on three key contributions. First, identity and audio timbre control are achieved through separate reference identity and audio LoRA modules that operate through self-attention layers within the base audio-video generation model. Second, we introduce a contrastive learning objective alongside the standard flow matching objective. It uses predicted flows conditioned on reference inputs as positive examples and those without reference conditions as negative examples, thereby enhancing the model ability to preserve identity and timbre. Third, we train OmniCustom on our constructed large-scale, high-quality audio-visual human dataset. Extensive experiments demonstrate that OmniCustom outperforms existing methods in generating audio-video content with consistent identity and timbre fidelity. Project page: https://omnicustom-project.github.io/page/.","authors":["Maomao Li","Zhen Li","Kaipeng Zhang","Guosheng Yin","Zhifeng Li","Dong Xu"],"pdf_url":"","comment":"code: https://github.com/OmniCustom-project/OmniCustom"},{"id":"http://arxiv.org/abs/2602.21482v1","updated":"2026-02-25T01:17:24Z","published":"2026-02-25T01:17:24Z","title":"Perceptual Quality Optimization of Image Super-Resolution","summary":"Single-image super-resolution (SR) has achieved remarkable progress with deep learning, yet most approaches rely on distortion-oriented losses or heuristic perceptual priors, which often lead to a trade-off between fidelity and visual quality. To address this issue, we propose an \\textit{Efficient Perceptual Bi-directional Attention Network (Efficient-PBAN)} that explicitly optimizes SR towards human-preferred quality. Unlike patch-based quality models, Efficient-PBAN avoids extensive patch sampling and enables efficient image-level perception. The proposed framework is trained on our self-constructed SR quality dataset that covers a wide range of state-of-the-art SR methods with corresponding human opinion scores. Using this dataset, Efficient-PBAN learns to predict perceptual quality in a way that correlates strongly with subjective judgments. The learned metric is further integrated into SR training as a differentiable perceptual loss, enabling closed-loop alignment between reconstruction and perceptual assessment. Extensive experiments demonstrate that our approach delivers superior perceptual quality. Code is publicly available at https://github.com/Lighting-YXLI/Efficient-PBAN.","authors":["Wei Zhou","Yixiao Li","Hadi Amirpour","Xiaoshuai Hao","Jiang Liu","Peng Wang","Hantao Liu"],"pdf_url":"","comment":"6 pages, 2 figures, accepted in ICASSP 26"},{"id":"http://arxiv.org/abs/2503.23655v4","updated":"2026-02-25T00:09:27Z","published":"2025-03-31T01:38:59Z","title":"A 3D-Cascading Crossing Coupling Framework for Hyperchaotic Map Construction and Its Application to Color Image Encryption","summary":"This paper focuses on hyperchaotic-map construction and proposes a 3D-Cascading Crossing Coupling framework (3D-CCC), which cascades, crosses, and couples three one-dimensional chaotic maps to form a three-dimensional hyperchaotic system. The framework avoids modulo-1 operations and introduces bounded-state and denominator safeguards for stable digital implementation. A general 3D-CCC formulation is established, and its derivative/Jacobian structure is analyzed to characterize multidirectional expansion. By instantiating ICMIC, Logistic, and Sine maps, a concrete system (3D-ILS) is derived. Phase portraits, bifurcation behavior, sensitivity tests, and Lyapunov-exponent analysis indicate pronounced ergodicity and hyperchaotic dynamics. As an application of the constructed map, a one-round RGB image-encryption scheme is developed using cross-channel bit mixing with joint permutation-diffusion. Under the reported settings, the cipher reaches near-ideal entropy (average 7.9993), NPCR of 96.61\\%, UACI of 33.46\\%, and an effective key space of about $2^{309}$. These results support the effectiveness of 3D-CCC as a practical framework for hyperchaotic-system design, with image encryption as one representative application.","authors":["Jilei Sun","Dianhong Wu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22299v1","updated":"2026-02-25T18:24:06Z","published":"2026-02-25T18:24:06Z","title":"Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads","summary":"Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagement metrics. Analyzing this brief window is challenging due to the multimodal nature of video content, which blends visual, auditory, and textual elements. Traditional methods often miss the nuanced interplay of these components, requiring advanced frameworks for thorough evaluation.\n  This study presents a framework using transformer-based multimodal large language models (MLLMs) to analyze the hooking period of video ads. It tests two frame sampling strategies, uniform random sampling and key frame selection, to ensure balanced and representative acoustic feature extraction, capturing the full range of design elements. The hooking video is processed by state-of-the-art MLLMs to generate descriptive analyses of the ad's initial impact, which are distilled into coherent topics using BERTopic for high-level abstraction. The framework also integrates features such as audio attributes and aggregated ad targeting information, enriching the feature set for further analysis.\n  Empirical validation on large-scale real-world data from social media platforms demonstrates the efficacy of our framework, revealing correlations between hooking period features and key performance metrics like conversion per investment. The results highlight the practical applicability and predictive power of the approach, offering valuable insights for optimizing video ad strategies. This study advances video ad analysis by providing a scalable methodology for understanding and enhancing the initial moments of video advertisements.","authors":["Kunpeng Zhang","Poppy Zhang","Shawndra Hill","Amel Awadelkarim"],"pdf_url":"","comment":"11 pages, 5 figures, 3 tables"}]},"2026-02-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2602.23351v1","updated":"2026-02-26T18:54:06Z","published":"2026-02-26T18:54:06Z","title":"Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning","summary":"The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.","authors":["Amita Kamath","Jack Hessel","Khyathi Chandu","Jena D. Hwang","Kai-Wei Chang","Ranjay Krishna"],"pdf_url":"","comment":"TACL 2026"},{"id":"http://arxiv.org/abs/2602.23329v1","updated":"2026-02-26T18:37:23Z","published":"2026-02-26T18:37:23Z","title":"LLM Novice Uplift on Dual-Use, In Silico Biology Tasks","summary":"Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.","authors":["Chen Bo Calvin Zhang","Christina Q. Knight","Nicholas Kruus","Jason Hausenloy","Pedro Medeiros","Nathaniel Li","Aiden Kim","Yury Orlovskiy","Coleman Breen","Bryce Cai","Jasper Götting","Andrew Bo Liu","Samira Nedungadi","Paula Rodriguez","Yannis Yiming He","Mohamed Shaaban","Zifan Wang","Seth Donoughe","Julian Michael"],"pdf_url":"","comment":"59 pages, 33 figures"},{"id":"http://arxiv.org/abs/2504.12522v2","updated":"2026-02-26T18:17:44Z","published":"2025-04-16T23:02:23Z","title":"Evaluating the Diversity and Quality of LLM Generated Content","summary":"Recent work suggests that preference-tuning techniques -- such as Reinforcement Learning from Human Feedback (RLHF) methods like PPO and GRPO, as well as alternatives like DPO -- reduce diversity, creating a dilemma given that these models are widely deployed in applications requiring varied outputs. We argue that diversity without consideration of quality has limited practical value. To address this issue, we introduce a framework for measuring effective semantic diversity -- diversity among outputs that meet quality thresholds -- which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: when using diversity metrics that do not explicitly consider quality, preference-tuned models -- particularly those trained via RL -- often produce outputs with lower diversity; however, these same preference-tuned models generate greater effective semantic diversity than supervised fine-tuned (SFT) or base models. Our analysis further shows another trend: while larger models may exhibit greater effective semantic diversity than smaller models, the smaller models are consistently more parameter-efficient at producing unique content within a fixed sampling budget. These findings have practical implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.","authors":["Alexander Shypula","Shuo Li","Botong Zhang","Vishakh Padmakumar","Kayo Yin","Osbert Bastani"],"pdf_url":"","comment":"Published at COLM 2025"},{"id":"http://arxiv.org/abs/2602.23300v1","updated":"2026-02-26T18:08:40Z","published":"2026-02-26T18:08:40Z","title":"A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations","summary":"Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.","authors":["Soumya Dutta","Smruthi Balaji","Sriram Ganapathy"],"pdf_url":"","comment":"Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2510.19060v3","updated":"2026-02-26T18:05:42Z","published":"2025-10-21T20:30:20Z","title":"PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions","summary":"While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $ρ$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.","authors":["Amith Ananthram","Elias Stengel-Eskin","Lorena A. Bradford","Julia Demarest","Adam Purvis","Keith Krut","Robert Stein","Rina Elster Pantalony","Mohit Bansal","Kathleen McKeown"],"pdf_url":"","comment":"Accepted at ICLR 2026. 26 pages, 9 figures. Metric/benchmark available at https://github.com/amith-ananthram/posh"},{"id":"http://arxiv.org/abs/2602.23286v1","updated":"2026-02-26T17:59:51Z","published":"2026-02-26T17:59:51Z","title":"SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables","summary":"Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.","authors":["Sungho Park","Jueun Kim","Wook-Shin Han"],"pdf_url":"","comment":"10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: https://sparta-projectpage.github.io/"},{"id":"http://arxiv.org/abs/2602.23266v1","updated":"2026-02-26T17:39:56Z","published":"2026-02-26T17:39:56Z","title":"Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems","summary":"Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.","authors":["Siyuan Liu","Jiahui Xu","Feng Jiang","Kuang Wang","Zefeng Zhao","Chu-Ren Huang","Jinghang Gu","Changqing Yin","Haizhou Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23258v1","updated":"2026-02-26T17:31:43Z","published":"2026-02-26T17:31:43Z","title":"AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning","summary":"While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.","authors":["Yutong Wang","Siyuan Xiong","Xuebo Liu","Wenkang Zhou","Liang Ding","Miao Zhang","Min Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.05154v4","updated":"2026-02-26T17:20:53Z","published":"2025-10-02T17:36:23Z","title":"Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs","summary":"Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.","authors":["Shenzhe Zhu","Shu Yang","Michiel A. Bakker","Alex Pentland","Jiaxin Pei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.07885v3","updated":"2026-02-26T17:09:14Z","published":"2025-11-11T06:33:30Z","title":"Intelligence per Watt: Measuring Intelligence Efficiency of Local AI","summary":"Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness here: https://github.com/HazyResearch/intelligence-per-watt.","authors":["Jon Saad-Falcon","Avanika Narayan","Hakki Orhun Akengin","J. Wes Griffin","Herumb Shandilya","Adrian Gamarra Lafuente","Medhya Goel","Rebecca Joseph","Shlok Natarajan","Etash Kumar Guha","Shang Zhu","Ben Athiwaratkun","John Hennessy","Azalia Mirhoseini","Christopher Ré"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23225v1","updated":"2026-02-26T17:04:57Z","published":"2026-02-26T17:04:57Z","title":"Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?","summary":"Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.","authors":["Pengxiang Li","Dilxat Muhtar","Lu Yin","Tianlong Chen","Shiwei Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23200v1","updated":"2026-02-26T16:50:36Z","published":"2026-02-26T16:50:36Z","title":"InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models","summary":"Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\\%$ speedup over previous work and up to $88\\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods.","authors":["Sayed Mohammadreza Tayaranian Hosseini","Amir Ardakani","Warren J. Gross"],"pdf_url":"","comment":"16 pages, 4 figures, 4 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2602.23197v1","updated":"2026-02-26T16:49:15Z","published":"2026-02-26T16:49:15Z","title":"Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models","summary":"Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.","authors":["Chungpa Lee","Jy-yong Sohn","Kangwook Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23184v1","updated":"2026-02-26T16:41:17Z","published":"2026-02-26T16:41:17Z","title":"MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations","summary":"We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark","authors":["Sara Rosenthal","Yannis Katsis","Vraj Shah","Lihong He","Lucian Popa","Marina Danilevsky"],"pdf_url":"","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2602.23163v1","updated":"2026-02-26T16:27:24Z","published":"2026-02-26T16:27:24Z","title":"A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring","summary":"Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.","authors":["Usman Anwar","Julianna Piskorz","David D. Baek","David Africa","Jim Weatherall","Max Tegmark","Christian Schroeder de Witt","Mihaela van der Schaar","David Krueger"],"pdf_url":"","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2602.00564v2","updated":"2026-02-26T16:18:49Z","published":"2026-01-31T07:09:17Z","title":"Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs","summary":"Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.","authors":["Xiang Zheng","Weiqi Zhai","Wei Wang","Boyu Yang","Wenbo Li","Ruixiang Luo","Haoxiang Sun","Yucheng Wang","Zhengze Li","Meng Wang","Yuetian Du","Guojie Lin","Yaxuan Wang","Xiaoxiao Xu","Yanhu Mo","Xuan Ren","Hu Wei","Bing Zhao"],"pdf_url":"","comment":"8 pages, and 3 figures"},{"id":"http://arxiv.org/abs/2602.23136v1","updated":"2026-02-26T15:52:48Z","published":"2026-02-26T15:52:48Z","title":"Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs","summary":"Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.","authors":["Jayadev Billa"],"pdf_url":"","comment":"22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper"},{"id":"http://arxiv.org/abs/2507.08491v2","updated":"2026-02-26T15:16:15Z","published":"2025-07-11T11:16:01Z","title":"A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench","summary":"There are currently two main paradigms for evaluating large language models (LLMs), reference-based evaluation and preference-based evaluation. The first, carried over from the evaluation of machine learning models in general, relies on pre-defined task instances, for which reference task executions are available. The second, best exemplified by the LM-arena, relies on (often self-selected) users bringing their own intents to a site that routes these to several models in parallel, among whose responses the user then selects their most preferred one. The former paradigm hence excels at control over what is tested, while the latter comes with higher ecological validity, testing actual use cases interactively. Recently, a third complementary paradigm has emerged that combines some of the strengths of these approaches, offering control over multi-turn, reference-free, repeatable interactions, while stressing goal-directedness: dialogue game based evaluation. While the utility of this approach has been shown by several projects, its adoption has been held back by the lack of a mature, easily re-usable implementation. In this paper, we present clembench, which has been in continuous development since 2023 and has in its latest release been optimized for ease of general use. We describe how it can be used to benchmark one's own models (using a provided set of benchmark game instances in English), as well as how easily the benchmark itself can be extended with new, tailor-made targeted tests.","authors":["David Schlangen","Sherzod Hakimov","Chalamalasetti Kranti","Jonathan Jordan","Philipp Sadler"],"pdf_url":"","comment":"All code required to run the benchmark, as well as extensive documentation, is available at https://github.com/clembench/clembench"},{"id":"http://arxiv.org/abs/2602.23079v1","updated":"2026-02-26T15:05:13Z","published":"2026-02-26T15:05:13Z","title":"Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent","summary":"The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.","authors":["Boyang Zhang","Yang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.24597v3","updated":"2026-02-26T15:04:01Z","published":"2025-09-29T11:03:16Z","title":"Inducing Dyslexia in Vision Language Models","summary":"Dyslexia, a neurodevelopmental disorder characterized by persistent reading difficulties, is often linked to reduced activity of the visual word form area (VWFA) in the ventral occipito-temporal cortex. Traditional approaches to studying dyslexia, such as behavioral and neuroimaging methods, have provided valuable insights but remain limited in their ability to test causal hypotheses about the underlying mechanisms of reading impairments. In this study, we use large-scale vision-language models (VLMs) to simulate dyslexia by functionally identifying and perturbing artificial analogues of word processing. Using stimuli from cognitive neuroscience, we identify visual-word-form-selective units within VLMs and demonstrate that they predict human VWFA neural responses. Ablating model VWF units leads to selective impairments in reading tasks while general visual and language comprehension abilities remain intact. In particular, the resulting model matches dyslexic humans' phonological deficits without a significant change in orthographic processing, and mirrors dyslexic behavior in font sensitivity. Taken together, our modeling results replicate key characteristics of dyslexia and establish a computational framework for investigating brain disorders.","authors":["Melika Honarmand","Ayati Sharma","Badr AlKhamissi","Johannes Mehrer","Martin Schrimpf"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23075v1","updated":"2026-02-26T15:02:22Z","published":"2026-02-26T15:02:22Z","title":"CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery","summary":"Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.","authors":["Mengze Hong","Di Jiang","Chen Jason Zhang","Zichang Guo","Yawen Li","Jun Chen","Shaobo Cui","Zhiyang Su"],"pdf_url":"","comment":"Accepted by TheWebConf 2026 Demo Track"},{"id":"http://arxiv.org/abs/2602.23071v1","updated":"2026-02-26T15:00:59Z","published":"2026-02-26T15:00:59Z","title":"Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody","summary":"While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.","authors":["Yuqi Shi","Hao Yang","Xiyao Lu","Jinsong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23070v1","updated":"2026-02-26T14:59:24Z","published":"2026-02-26T14:59:24Z","title":"Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment","summary":"Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Ghor-882, a comprehensive 882-hour multi-speaker Bengali dataset. In this paper, detailing our submission to the DL Sprint 4.0 competition, we systematically evaluate various architectures and approaches for long-form Bengali speech. For ASR, we demonstrate that raw data scaling is ineffective; instead, targeted fine-tuning utilizing perfectly aligned annotations paired with synthetic acoustic degradation (noise and reverberation) emerges as the singular most effective approach. Conversely, for speaker diarization, we observed that global open-source state-of-the-art models (such as Diarizen) performed surprisingly poorly on this complex dataset. Extensive model retraining yielded negligible improvements; instead, strategic, heuristic post-processing of baseline model outputs proved to be the primary driver for increasing accuracy. Ultimately, this work outlines a highly optimized dual pipeline achieving a $\\sim$0.019 Real-Time Factor (RTF), establishing a practical, empirically backed benchmark for low-resource, long-form speech processing.","authors":["Sanjid Hasan","Risalat Labib","A H M Fuad","Bayazid Hasan"],"pdf_url":"","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.22072v4","updated":"2026-02-26T14:54:29Z","published":"2025-09-26T08:53:13Z","title":"Fine-tuning Done Right in Model Editing","summary":"Fine-tuning, a foundational method for adapting large language models, has long been considered ineffective for model editing. Here, we challenge this belief, arguing that the reported failure arises not from the inherent limitation of fine-tuning itself, but from adapting it to the sequential nature of the editing task, a single-pass depth-first pipeline that optimizes each sample to convergence before moving on. While intuitive, this depth-first pipeline coupled with sample-wise updating over-optimizes each edit and induces interference across edits. Our controlled experiments reveal that simply restoring fine-tuning to the standard breadth-first (i.e., epoch-based) pipeline with mini-batch optimization substantially improves its effectiveness for model editing. Moreover, fine-tuning in editing also suffers from suboptimal tuning parameter locations inherited from prior methods. Through systematic analysis of tuning locations, we derive LocFT-BF, a simple and effective localized editing method built on the restored fine-tuning framework. Extensive experiments across diverse LLMs and datasets demonstrate that LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x beyond prior practice, without sacrificing general capabilities. By clarifying a long-standing misconception and introducing a principled localized tuning strategy, we advance fine-tuning from an underestimated baseline to a leading method for model editing, establishing a solid foundation for future research.","authors":["Wanli Yang","Rui Tang","Hongyu Zang","Du Su","Qi Cao","Jingang Wang","Huawei Shen","Xueqi Cheng","Fei Sun"],"pdf_url":"","comment":"Accepted as a conference paper at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19463v2","updated":"2026-02-26T14:54:23Z","published":"2026-02-23T03:17:27Z","title":"PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives","summary":"As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.","authors":["Emma Jiren Wang","Siying Hu","Zhicong Lu"],"pdf_url":"","comment":"19 pages, 8 figures; Accepted by ACM CHI 2026. In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI'26)"},{"id":"http://arxiv.org/abs/2602.23062v1","updated":"2026-02-26T14:49:11Z","published":"2026-02-26T14:49:11Z","title":"Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department","summary":"Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours \"unknown\" answers), which need to be corrected.","authors":["Gabriela Anna Kaczmarek","Pietro Ferrazzi","Lorenzo Porta","Vicky Rubini","Bernardo Magnini"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23061v1","updated":"2026-02-26T14:48:49Z","published":"2026-02-26T14:48:49Z","title":"MoDora: Tree-Based Semi-Structured Document Analysis System","summary":"Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.\n  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.","authors":["Bangrui Xu","Qihang Yao","Zirui Tang","Xuanhe Zhou","Yeye He","Shihan Yu","Qianqian Xu","Bin Wang","Guoliang Li","Conghui He","Fan Wu"],"pdf_url":"","comment":"Extension of our SIGMOD 2026 paper. Please refer to source code available at https://github.com/weAIDB/MoDora"},{"id":"http://arxiv.org/abs/2602.21480v2","updated":"2026-02-26T14:47:19Z","published":"2026-02-25T01:12:35Z","title":"Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?","summary":"Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.","authors":["Germán T. Eizaguirre","Lars Tissen","Marc Sánchez-Artigas"],"pdf_url":"","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.14812v2","updated":"2026-02-26T14:44:33Z","published":"2026-02-16T15:04:35Z","title":"Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque","summary":"Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.","authors":["Jaione Bengoetxea","Itziar Gonzalez-Dios","Rodrigo Agerri"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23057v1","updated":"2026-02-26T14:42:16Z","published":"2026-02-26T14:42:16Z","title":"Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention","summary":"Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner.\n  We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.","authors":["Jeongin Bae","Baeseong Park","Gunho Park","Minsub Kim","Joonhyung Lee","Junhee Yoo","Sunghyeon Woo","Jiwon Ryu","Se Jung Kwon","Dongsoo Lee"],"pdf_url":"","comment":"Preprint. 14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2509.21294v3","updated":"2026-02-26T14:27:02Z","published":"2025-09-25T15:13:00Z","title":"UPDESH: Synthesizing Grounded Instruction Tuning Data for 13 Indic Languages","summary":"Developing culturally grounded multilingual AI systems remains challenging, particularly for low-resource languages. While synthetic data offers promise, its effectiveness in multilingual and multicultural contexts is underexplored. We investigate bottom-up synthetic data generation using large open-source LLMs (>= 235B parameters) grounded in language-specific Wikipedia content, complementing dominant top-down translation-based approaches from English. We introduce Updesh, a high-quality large-scale synthetic instruction-following dataset comprising 9.5M data points across 13 Indian languages and English, encompassing diverse reasoning and generative tasks. Comprehensive evaluation using automated metrics and 10K human assessments confirms high data quality. Downstream evaluations performed by fine-tuning models on various datasets and assessing performance across 13 diverse multilingual datasets and model comparative evaluations, demonstrate that models trained on Updesh consistently obtain significant improvements on NLU, NLG evaluations. Finally, through ablation studies and cultural evaluations, we show that context-aware, culturally grounded data generation is essential for effective multilingual AI development.","authors":["Pranjal A. Chitale","Varun Gumma","Sanchit Ahuja","Prashant Kodali","Manan Uppadhyay","Deepthi Sudharsan","Sunayana Sitaram"],"pdf_url":"","comment":"Under Review"},{"id":"http://arxiv.org/abs/2510.26577v2","updated":"2026-02-26T14:19:55Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models","summary":"Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5 % to 20%. The code is available at https://github.com/EAGLE-Research/sglang-eagle4.","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.21306v2","updated":"2026-02-26T13:46:37Z","published":"2025-10-24T10:04:23Z","title":"PARL: Prompt-based Agents for Reinforcement Learning","summary":"Large language models (LLMs) have demonstrated high performance on tasks expressed in natural language, particularly in zero- or few-shot settings. These are typically framed as supervised (e.g., classification) or unsupervised (e.g., clustering) problems. However, limited work evaluates LLMs as agents in reinforcement learning (RL) tasks (e.g., playing games), where learning occurs through interaction with an environment and a reward system. While prior work focused on representing tasks that rely on a language representation, we study structured, non-linguistic reasoning - such as interpreting positions in a grid world. We therefore introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that uses LLMs as RL agents through prompting, without any fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling the model to learn through trial-and-error interaction. We evaluate PARL on three standard RL tasks that do not entirely rely on natural language. We show that it can match or outperform traditional RL agents in simple environments by leveraging pretrained knowledge. However, we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions.","authors":["Yarik Menchaca Resendiz","Roman Klinger"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.07452v2","updated":"2026-02-26T13:37:58Z","published":"2025-10-08T18:58:41Z","title":"PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing","summary":"Language models (LMs) may memorize personally identifiable information (PII) from training data, enabling adversaries to extract it during inference. Existing defense mechanisms such as differential privacy (DP) reduce this leakage, but incur large drops in utility. Based on a comprehensive study using circuit discovery to identify the computational circuits responsible PII leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware Targeted Circuit PatcHing), a novel approach that first identifies and subsequently directly edits PII circuits to reduce leakage. PATCH achieves better privacy-utility trade-off than existing defenses, e.g., reducing recall of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis shows that PII leakage circuits persist even after the application of existing defense mechanisms. In contrast, PATCH can effectively mitigate their impact.","authors":["Anthony Hughes","Vasisht Duddu","N. Asokan","Nikolaos Aletras","Ning Ma"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.12125v2","updated":"2026-02-26T13:26:22Z","published":"2026-02-12T16:14:29Z","title":"Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation","summary":"On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.","authors":["Wenkai Yang","Weijie Liu","Ruobing Xie","Kai Yang","Saiyong Yang","Yankai Lin"],"pdf_url":"","comment":"v2, update results under stronger teachers with more RL training steps"},{"id":"http://arxiv.org/abs/2602.22958v1","updated":"2026-02-26T12:53:48Z","published":"2026-02-26T12:53:48Z","title":"Frequency-Ordered Tokenization for Better Text Compression","summary":"We present frequency-ordered tokenization, a simple preprocessing technique that improves lossless text compression by exploiting the power-law frequency distribution of natural language tokens (Zipf's law). The method tokenizes text with Byte Pair Encoding (BPE), reorders the vocabulary so that frequent tokens receive small integer identifiers, and encodes the result with variable-length integers before passing it to any standard compressor. On enwik8 (100 MB Wikipedia), this yields improvements of 7.08 percentage points (pp) for zlib, 1.69 pp for LZMA, and 0.76 pp for zstd (all including vocabulary overhead), outperforming the classical Word Replacing Transform. Gains are consistent at 1 GB scale (enwik9) and across Chinese and Arabic text. We further show that preprocessing accelerates compression for computationally expensive algorithms: the total wall-clock time including preprocessing is 3.1x faster than raw zstd-22 and 2.4x faster than raw LZMA, because the preprocessed input is substantially smaller. The method can be implemented in under 50 lines of code.","authors":["Maximilian Kalcher"],"pdf_url":"","comment":"5 pages, 4 figures, 9 tables"},{"id":"http://arxiv.org/abs/2602.08237v3","updated":"2026-02-26T12:46:45Z","published":"2026-02-09T03:23:23Z","title":"Document Reconstruction Unlocks Scalable Long-Context RLVR","summary":"Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.","authors":["Yao Xiao","Lei Wang","Yue Deng","Guanzheng Chen","Ziqi Jin","Jung-jae Kim","Xiaoli Li","Roy Ka-wei Lee","Lidong Bing"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22918v1","updated":"2026-02-26T12:06:02Z","published":"2026-02-26T12:06:02Z","title":"Where Vision Becomes Text: Locating the OCR Routing Bottleneck in Vision-Language Models","summary":"Vision-language models (VLMs) can read text from images, but where does this optical character recognition (OCR) information enter the language processing stream? We investigate the OCR routing mechanism across three architecture families (Qwen3-VL, Phi-4, InternVL3.5) using causal interventions. By computing activation differences between original images and text-inpainted versions, we identify architecture-specific OCR bottlenecks whose dominant location depends on the vision-language integration strategy: DeepStack models (Qwen) show peak sensitivity at mid-depth (about 50%) for scene text, while single-stage projection models (Phi-4, InternVL) peak at early layers (6-25%), though the exact layer of maximum effect varies across datasets. The OCR signal is remarkably low-dimensional: PC1 captures 72.9% of variance. Crucially, principal component analysis (PCA) directions learned on one dataset transfer to others, demonstrating shared text-processing pathways. Surprisingly, in models with modular OCR circuits (notably Qwen3-VL-4B), OCR removal can improve counting performance (up to +6.9 percentage points), suggesting OCR interferes with other visual processing in sufficiently modular architectures.","authors":["Jonathan Steinberg","Oren Gal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.14479v6","updated":"2026-02-26T11:56:20Z","published":"2025-05-20T15:13:32Z","title":"Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach","summary":"Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.","authors":["Oren Sultan","Eitan Stern","Dafna Shahaf"],"pdf_url":"","comment":"long paper"},{"id":"http://arxiv.org/abs/2602.22911v1","updated":"2026-02-26T11:55:25Z","published":"2026-02-26T11:55:25Z","title":"NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion","summary":"Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.","authors":["Hung-Hsuan Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22897v1","updated":"2026-02-26T11:35:04Z","published":"2026-02-26T11:35:04Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Shijian Wang","Guanting Dong","Jiajie Jin","Hao Wang","Yinuo Wang","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.04403v2","updated":"2026-02-26T11:09:21Z","published":"2025-09-04T17:13:59Z","title":"Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios","summary":"Multimodal large language models (MLLMs) are rapidly evolving, presenting increasingly complex safety challenges. However, current dataset construction methods, which are risk-oriented, fail to cover the growing complexity of real-world multimodal safety scenarios (RMS). And due to the lack of a unified evaluation metric, their overall effectiveness remains unproven. This paper introduces a novel image-oriented self-adaptive dataset construction method for RMS, which starts with images and end constructing paired text and guidance responses. Using the image-oriented method, we automatically generate an RMS dataset comprising 35k image-text pairs with guidance responses. Additionally, we introduce a standardized safety dataset evaluation metric: fine-tuning a safety judge model and evaluating its capabilities on other safety datasets.Extensive experiments on various tasks demonstrate the effectiveness of the proposed image-oriented pipeline. The results confirm the scalability and effectiveness of the image-oriented approach, offering a new perspective for the construction of real-world multimodal safety datasets. The dataset is presented at https://huggingface.co/datasets/NewCityLetter/RMS2/tree/main.","authors":["Jingen Qu","Lijun Li","Bo Zhang","Yichen Yan","Jing Shao"],"pdf_url":"","comment":"Accepted at EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2602.22871v1","updated":"2026-02-26T11:08:39Z","published":"2026-02-26T11:08:39Z","title":"Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching","summary":"Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.","authors":["Roy Miles","Aysim Toker","Andreea-Maria Oncescu","Songcen Xu","Jiankang Deng","Ismail Elezi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22868v1","updated":"2026-02-26T11:08:11Z","published":"2026-02-26T11:08:11Z","title":"Rejection Mixing: Fast Semantic Propagation of Mask Tokens for Efficient DLLM Inference","summary":"Diffusion Large Language Models (DLLMs) promise fast non-autoregressive inference but suffer a severe quality-speed trade-off in parallel decoding. This stems from the ''combinatorial contradiction'' phenomenon, where parallel tokens form semantically inconsistent combinations. We address this by integrating continuous representations into the discrete decoding process, as they preserve rich inter-position dependency. We propose ReMix (Rejection Mixing), a framework that introduces a novel Continuous Mixing State as an intermediate between the initial masked state and the final decoded token state. This intermediate state allows a token's representation to be iteratively refined in a continuous space, resolving mutual conflicts with other tokens before collapsing into a final discrete sample. Furthermore, a rejection rule reverts uncertain representations from the continuous state back to the masked state for reprocessing, ensuring stability and preventing error propagation. ReMix thus mitigates combinatorial contradictions by enabling continuous-space refinement during discrete diffusion decoding. Extensive experiments demonstrate that ReMix, as a training-free method, achieves a $2-8 \\times$ inference speedup without any quality degradation.","authors":["Yushi Ye","Feng Hong","Huangjie Zheng","Xu Chen","Zhiyong Chen","Yanfeng Wang","Jiangchao Yao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22865v1","updated":"2026-02-26T11:01:38Z","published":"2026-02-26T11:01:38Z","title":"Effective QA-driven Annotation of Predicate-Argument Relations Across Languages","summary":"Explicit representations of predicate-argument relations form the basis of interpretable semantic analysis, supporting reasoning, generation, and evaluation. However, attaining such semantic structures requires costly annotation efforts and has remained largely confined to English. We leverage the Question-Answer driven Semantic Role Labeling (QA-SRL) framework -- a natural-language formulation of predicate-argument relations -- as the foundation for extending semantic annotation to new languages. To this end, we introduce a cross-linguistic projection approach that reuses an English QA-SRL parser within a constrained translation and word-alignment pipeline to automatically generate question-answer annotations aligned with target-language predicates. Applied to Hebrew, Russian, and French -- spanning diverse language families -- the method yields high-quality training data and fine-tuned, language-specific parsers that outperform strong multilingual LLM baselines (GPT-4o, LLaMA-Maverick). By leveraging QA-SRL as a transferable natural-language interface for semantics, our approach enables efficient and broadly accessible predicate-argument parsing across languages.","authors":["Jonathan Davidov","Aviv Slobodkin","Shmuel Tomi Klein","Reut Tsarfaty","Ido Dagan","Ayal Klein"],"pdf_url":"","comment":"Accepted to EACL 2026 (Main Conference)"},{"id":"http://arxiv.org/abs/2602.22846v1","updated":"2026-02-26T10:37:05Z","published":"2026-02-26T10:37:05Z","title":"Improving Neural Argumentative Stance Classification in Controversial Topics with Emotion-Lexicon Features","summary":"Argumentation mining comprises several subtasks, among which stance classification focuses on identifying the standpoint expressed in an argumentative text toward a specific target topic. While arguments-especially about controversial topics-often appeal to emotions, most prior work has not systematically incorporated explicit, fine-grained emotion analysis to improve performance on this task. In particular, prior research on stance classification has predominantly utilized non-argumentative texts and has been restricted to specific domains or topics, limiting generalizability. We work on five datasets from diverse domains encompassing a range of controversial topics and present an approach for expanding the Bias-Corrected NRC Emotion Lexicon using DistilBERT embeddings, which we feed into a Neural Argumentative Stance Classification model. Our method systematically expands the emotion lexicon through contextualized embeddings to identify emotionally charged terms not previously captured in the lexicon. Our expanded NRC lexicon (eNRC) improves over the baseline across all five datasets (up to +6.2 percentage points in F1 score), outperforms the original NRC on four datasets (up to +3.0), and surpasses the LLM-based approach on nearly all corpora. We provide all resources-including eNRC, the adapted corpora, and model architecture-to enable other researchers to build upon our work.","authors":["Mohammad Yeghaneh Abkenar","Weixing Wang","Manfred Stede","Davide Picca","Mark A. Finlayson","Panagiotis Ioannidis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.16164v2","updated":"2026-02-26T10:34:11Z","published":"2025-05-22T03:08:27Z","title":"Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task","summary":"Large language models (LLMs) are increasingly explored as substitutes for human participants in cognitive tasks, but their ability to simulate human behavioral variability remains unclear. This study examines whether LLMs can approximate individual differences in the phonemic fluency task, where participants generate words beginning with a target letter. We evaluated 34 distinct models across 45 configurations from major closed-source and open-source providers, and compared outputs to responses from 106 human participants. While some models, especially Claude 3.7 Sonnet, approximated human averages and lexical preferences, none reproduced the scope of human variability. LLM outputs were consistently less diverse, with newer models and thinking-enabled modes often reducing rather than increasing variability. Network analysis further revealed fundamental differences in retrieval structure between humans and the most human-like model. Ensemble simulations combining outputs from diverse models also failed to recover human-level diversity, likely due to high vocabulary overlap across models. These results highlight key limitations in using LLMs to simulate human cognition and behavior.","authors":["Mengyang Qiu","Zoe Brisebois","Siena Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22831v1","updated":"2026-02-26T10:17:57Z","published":"2026-02-26T10:17:57Z","title":"Moral Preferences of LLMs Under Directed Contextual Influence","summary":"Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.","authors":["Phil Blandfort","Tushar Karayil","Urja Pawar","Robert Graham","Alex McKenzie","Dmitrii Krasheninnikov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22828v1","updated":"2026-02-26T10:11:15Z","published":"2026-02-26T10:11:15Z","title":"TCM-DiffRAG: Personalized Syndrome Differentiation Reasoning Method for Traditional Chinese Medicine based on Knowledge Graph and Chain of Thought","summary":"Background: Retrieval augmented generation (RAG) technology can empower large language models (LLMs) to generate more accurate, professional, and timely responses without fine tuning. However, due to the complex reasoning processes and substantial individual differences involved in traditional Chinese medicine (TCM) clinical diagnosis and treatment, traditional RAG methods often exhibit poor performance in this domain. Objective: To address the limitations of conventional RAG approaches in TCM applications, this study aims to develop an improved RAG framework tailored to the characteristics of TCM reasoning. Methods: We developed TCM-DiffRAG, an innovative RAG framework that integrates knowledge graphs (KG) with chains of thought (CoT). TCM-DiffRAG was evaluated on three distinctive TCM test datasets. Results: The experimental results demonstrated that TCM-DiffRAG achieved significant performance improvements over native LLMs. For example, the qwen-plus model achieved scores of 0.927, 0.361, and 0.038, which were significantly enhanced to 0.952, 0.788, and 0.356 with TCM-DiffRAG. The improvements were even more pronounced for non-Chinese LLMs. Additionally, TCM-DiffRAG outperformed directly supervised fine-tuned (SFT) LLMs and other benchmark RAG methods. Conclusions: TCM-DiffRAG shows that integrating structured TCM knowledge graphs with Chain of Thought based reasoning substantially improves performance in individualized diagnostic tasks. The joint use of universal and personalized knowledge graphs enables effective alignment between general knowledge and clinical reasoning. These results highlight the potential of reasoning-aware RAG frameworks for advancing LLM applications in traditional Chinese medicine.","authors":["Jianmin Li","Ying Chang","Su-Kit Tang","Yujia Liu","Yanwen Wang","Shuyuan Lin","Binkai Ou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.20505v3","updated":"2026-02-26T10:08:30Z","published":"2025-10-23T12:48:18Z","title":"RELOOP: Recursive Retrieval with Multi-Hop Reasoner and Planners for Heterogeneous QA","summary":"Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets. This paper introduces RELOOP, a structure aware framework using Hierarchical Sequence (HSEQ) that (i) linearize documents, tables, and knowledge graphs into a reversible hierarchical sequence with lightweight structural tags, and (ii) perform structure-aware iteration to collect just-enough evidence before answer synthesis. A Head Agent provides guidance that leads retrieval, while an Iteration Agent selects and expands HSeq via structure-respecting actions (e.g., parent/child hops, table row/column neighbors, KG relations); Finally the head agent composes canonicalized evidence to genearte the final answer, with an optional refinement loop to resolve detected contradictions. Experiments on HotpotQA (text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1 gains over strong single-pass, multi-hop, and agentic RAG baselines with high efficiency. Besides, RELOOP exhibits three key advantages: (1) a format-agnostic unification that enables a single policy to operate across text, tables, and KGs without per-dataset specialization; (2) \\textbf{guided, budget-aware iteration} that reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and (3) evidence canonicalization for reliable QA, improving answers consistency and auditability.","authors":["Ruiyi Yang","Hao Xue","Imran Razzak","Hakim Hacid","Flora D. Salim"],"pdf_url":"","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2602.22827v1","updated":"2026-02-26T10:08:02Z","published":"2026-02-26T10:08:02Z","title":"TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models","summary":"This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.","authors":["Reihaneh Iranmanesh","Saeedeh Davoudi","Pasha Abrishamchian","Ophir Frieder","Nazli Goharian"],"pdf_url":"","comment":"11 pages, 3 figures, Fifteenth biennial Language Resources and Evaluation Conference (LREC) 2026 (to appear)"},{"id":"http://arxiv.org/abs/2510.25726v2","updated":"2026-02-26T09:46:48Z","published":"2025-10-29T17:32:49Z","title":"The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution","summary":"Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.","authors":["Junlong Li","Wenshuo Zhao","Jian Zhao","Weihao Zeng","Haoze Wu","Xiaochen Wang","Rui Ge","Yuxuan Cao","Yuzhen Huang","Wei Liu","Junteng Liu","Zhaochen Su","Yiyang Guo","Fan Zhou","Lueyang Zhang","Juan Michelini","Xingyao Wang","Xiang Yue","Shuyan Zhou","Graham Neubig","Junxian He"],"pdf_url":"","comment":"ICLR 2026, Website: https://toolathlon.xyz/"},{"id":"http://arxiv.org/abs/2602.22790v1","updated":"2026-02-26T09:23:09Z","published":"2026-02-26T09:23:09Z","title":"Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift","summary":"The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.","authors":["Hyunwoo Kim","Hanau Yi","Jaehee Bae","Yumin Kim"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22787v1","updated":"2026-02-26T09:21:12Z","published":"2026-02-26T09:21:12Z","title":"Probing for Knowledge Attribution in Large Language Models","summary":"Large language models (LLMs) often generate fluent but unfounded claims, or hallucinations, which fall into two types: (i) faithfulness violations - misusing user context - and (ii) factuality violations - errors from internal knowledge. Proper mitigation depends on knowing whether a model's answer is based on the prompt or its internal weights. This work focuses on the problem of contributive attribution: identifying the dominant knowledge source behind each output. We show that a probe, a simple linear classifier trained on model hidden representations, can reliably predict contributive attribution. For its training, we introduce AttriWiki, a self-supervised data pipeline that prompts models to recall withheld entities from memory or read them from context, generating labelled examples automatically. Probes trained on AttriWiki data reveal a strong attribution signal, achieving up to 0.96 Macro-F1 on Llama-3.1-8B, Mistral-7B, and Qwen-7B, transferring to out-of-domain benchmarks (SQuAD, WebQuestions) with 0.94-0.99 Macro-F1 without retraining. Attribution mismatches raise error rates by up to 70%, demonstrating a direct link between knowledge source confusion and unfaithful answers. Yet, models may still respond incorrectly even when attribution is correct, highlighting the need for broader detection frameworks.","authors":["Ivo Brink","Alexander Boer","Dennis Ulmer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22775v1","updated":"2026-02-26T09:11:34Z","published":"2026-02-26T09:11:34Z","title":"TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation","summary":"As mental health chatbots proliferate to address the global treatment gap, a critical question emerges: How do we design for relational safety the quality of interaction patterns that unfold across conversations rather than the correctness of individual responses? Current safety evaluations assess single-turn crisis responses, missing the therapeutic dynamics that determine whether chatbots help or harm over time. We introduce TherapyProbe, a design probe methodology that generates actionable design knowledge by systematically exploring chatbot conversation trajectories through adversarial multi-agent simulation. Using open-source models, TherapyProbe surfaces relational safety failures interaction patterns like \"validation spirals\" where chatbots progressively reinforce hopelessness, or \"empathy fatigue\" where responses become mechanical over turns. Our contribution is translating these failures into a Safety Pattern Library of 23 failure archetypes with corresponding design recommendations. We contribute: (1) a replicable methodology requiring no API costs, (2) a clinically-grounded failure taxonomy, and (3) design implications for developers, clinicians, and policymakers.","authors":["Joydeep Chandra","Satyam Kumar Navneet","Yong Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22766v1","updated":"2026-02-26T08:56:23Z","published":"2026-02-26T08:56:23Z","title":"Imagination Helps Visual Reasoning, But Not Yet in Latent Space","summary":"Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.","authors":["You Li","Chi Chen","Yanghao Li","Fanhu Zeng","Kaiyu Huang","Jinan Xu","Maosong Sun"],"pdf_url":"","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2602.22765v1","updated":"2026-02-26T08:55:58Z","published":"2026-02-26T08:55:58Z","title":"Towards Better RL Training Data Utilization via Second-Order Rollout","summary":"Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training","authors":["Zhe Yang","Yudong Wang","Rang Li","Zhifang Sui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.15339v3","updated":"2026-02-26T08:45:20Z","published":"2025-06-18T10:42:22Z","title":"DeVisE: Behavioral Testing of Medical Large Language Models","summary":"Large language models (LLMs) are increasingly applied in clinical decision support, yet current evaluations rarely reveal whether their outputs reflect genuine medical reasoning or superficial correlations. We introduce DeVisE (Demographics and Vital signs Evaluation), a behavioral testing framework that probes fine-grained clinical understanding through controlled counterfactuals. Using intensive care unit (ICU) discharge notes from MIMIC-IV, we construct both raw (real-world) and template-based (synthetic) variants with single-variable perturbations in demographic (age, gender, ethnicity) and vital sign attributes. We evaluate eight LLMs, spanning general-purpose and medical variants, under zero-shot setting. Model behavior is analyzed through (1) input-level sensitivity, capturing how counterfactuals alter perplexity, and (2) downstream reasoning, measuring their effect on predicted ICU length-of-stay and mortality. Overall, our results show that standard task metrics obscure clinically relevant differences in model behavior, with models differing substantially in how consistently and proportionally they adjust predictions to counterfactual perturbations.","authors":["Camila Zurdo Tagliabue","Heloisa Oss Boll","Aykut Erdem","Erkut Erdem","Iacer Calixto"],"pdf_url":"","comment":"Camera-ready version published at Findings of the EACL 2026"},{"id":"http://arxiv.org/abs/2602.22755v1","updated":"2026-02-26T08:43:07Z","published":"2026-02-26T08:43:07Z","title":"AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors","summary":"We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.","authors":["Abhay Sheshadri","Aidan Ewart","Kai Fronsdal","Isha Gupta","Samuel R. Bowman","Sara Price","Samuel Marks","Rowan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22752v1","updated":"2026-02-26T08:40:21Z","published":"2026-02-26T08:40:21Z","title":"Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction","summary":"The transition of Large Language Models (LLMs) from exploratory tools to active \"silicon subjects\" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current \"naive prompting\" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.","authors":["Nils Schwager","Simon Münker","Alistair Plum","Achim Rettinger"],"pdf_url":"","comment":"14 pages, 1 figure, 7 tables. Accepted to the 15th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) at EACL 2026, Rabat, Morocco"},{"id":"http://arxiv.org/abs/2602.22730v1","updated":"2026-02-26T08:13:42Z","published":"2026-02-26T08:13:42Z","title":"Extending Czech Aspect-Based Sentiment Analysis with Opinion Terms: Dataset and LLM Benchmarks","summary":"This paper introduces a novel Czech dataset in the restaurant domain for aspect-based sentiment analysis (ABSA), enriched with annotations of opinion terms. The dataset supports three distinct ABSA tasks involving opinion terms, accommodating varying levels of complexity. Leveraging this dataset, we conduct extensive experiments using modern Transformer-based models, including large language models (LLMs), in monolingual, cross-lingual, and multilingual settings. To address cross-lingual challenges, we propose a translation and label alignment methodology leveraging LLMs, which yields consistent improvements. Our results highlight the strengths and limitations of state-of-the-art models, especially when handling the linguistic intricacies of low-resource languages like Czech. A detailed error analysis reveals key challenges, including the detection of subtle opinion terms and nuanced sentiment expressions. The dataset establishes a new benchmark for Czech ABSA, and our proposed translation-alignment approach offers a scalable solution for adapting ABSA resources to other low-resource languages.","authors":["Jakub Šmíd","Pavel Přibáň","Pavel Král"],"pdf_url":"","comment":"Accepted for the 15th edition of the Language Resources and Evaluation Conference (LREC 2026)"},{"id":"http://arxiv.org/abs/2602.22723v1","updated":"2026-02-26T07:56:24Z","published":"2026-02-26T07:56:24Z","title":"Human Label Variation in Implicit Discourse Relation Recognition","summary":"There is growing recognition that many NLP tasks lack a single ground truth, as human judgments reflect diverse perspectives. To capture this variation, models have been developed to predict full annotation distributions rather than majority labels, while perspectivist models aim to reproduce the interpretations of individual annotators. In this work, we compare these approaches on Implicit Discourse Relation Recognition (IDRR), a highly ambiguous task where disagreement often arises from cognitive complexity rather than ideological bias. Our experiments show that existing annotator-specific models perform poorly in IDRR unless ambiguity is reduced, whereas models trained on label distributions yield more stable predictions. Further analysis indicates that frequent cognitively demanding cases drive inconsistency in human interpretation, posing challenges for perspectivist modeling in IDRR.","authors":["Frances Yung","Daniil Ignatev","Merel Scholman","Vera Demberg","Massimo Poesio"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22721v1","updated":"2026-02-26T07:49:50Z","published":"2026-02-26T07:49:50Z","title":"Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA","summary":"Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.\n  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\\% table compression and a 2.2$\\times$ reduction in monetary cost.","authors":["Fengyu Li","Junhao Zhu","Kaishi Song","Lu Chen","Zhongming Yao","Tianyi Li","Christian S. Jensen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22698v1","updated":"2026-02-26T07:20:40Z","published":"2026-02-26T07:20:40Z","title":"Tokenization, Fusion and Decoupling: Bridging the Granularity Mismatch Between Large Language Models and Knowledge Graphs","summary":"Leveraging Large Language Models (LLMs) for Knowledge Graph Completion (KGC) is promising but hindered by a fundamental granularity mismatch. LLMs operate on fragmented token sequences, whereas entities are the fundamental units in knowledge graphs (KGs) scenarios. Existing approaches typically constrain predictions to limited candidate sets or align entities with the LLM's vocabulary by pooling multiple tokens or decomposing entities into fixed-length token sequences, which fail to capture both the semantic meaning of the text and the structural integrity of the graph. To address this, we propose KGT, a novel framework that uses dedicated entity tokens to enable efficient, full-space prediction. Specifically, we first introduce specialized tokenization to construct feature representations at the level of dedicated entity tokens. We then fuse pre-trained structural and textual features into these unified embeddings via a relation-guided gating mechanism, avoiding training from scratch. Finally, we implement decoupled prediction by leveraging independent heads to separate and combine semantic and structural reasoning. Experimental results show that KGT consistently outperforms state-of-the-art methods across multiple benchmarks.","authors":["Siyue Su","Jian Yang","Bo Li","Guanglin Niu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22697v1","updated":"2026-02-26T07:19:57Z","published":"2026-02-26T07:19:57Z","title":"Reinforcing Real-world Service Agents: Balancing Utility and Cost in Task-oriented Dialogue","summary":"The rapid evolution of Large Language Models (LLMs) has accelerated the transition from conversational chatbots to general agents. However, effectively balancing empathetic communication with budget-aware decision-making remains an open challenge. Since existing methods fail to capture these complex strategic trade-offs, we propose InteractCS-RL, a framework that reframes task-oriented dialogue as a multi-granularity reinforcement learning process. Specifically, we first establish a User-centric Interaction Framework to provide a high-fidelity training gym, enabling agents to dynamically explore diverse strategies with persona-driven users. Then, we introduce Cost-aware Multi-turn Policy Optimization (CMPO) with a hybrid advantage estimation strategy. By integrating generative process credits and employing a PID-Lagrangian cost controller, CMPO effectively guides the policy to explore Pareto boundary between user reward and global cost constraints. Extensive experiments on customized real business scenarios demonstrate that InteractCS-RL significantly outperform other baselines across three evaluation dimensions. Further evaluation on tool-agent-user interaction benchmarks verify InteractCS-RL robustness across diverse domains.","authors":["Ning Gao","Wei Zhang","Yuqin Dai","Ling Shi","Ziyin Wang","Yujie Wang","Wei He","Jinpeng Wang","Chaozheng Wang"],"pdf_url":"","comment":"35 pages, 8 tables, 3 figures"},{"id":"http://arxiv.org/abs/2602.22696v1","updated":"2026-02-26T07:18:45Z","published":"2026-02-26T07:18:45Z","title":"Enhancing Persuasive Dialogue Agents by Synthesizing Cross-Disciplinary Communication Strategies","summary":"Current approaches to developing persuasive dialogue agents often rely on a limited set of predefined persuasive strategies that fail to capture the complexity of real-world interactions. We applied a cross-disciplinary approach to develop a framework for designing persuasive dialogue agents that draws on proven strategies from social psychology, behavioral economics, and communication theory. We validated our proposed framework through experiments on two distinct datasets: the Persuasion for Good dataset, which represents a specific in-domain scenario, and the DailyPersuasion dataset, which encompasses a wide range of scenarios. The proposed framework achieved strong results for both datasets and demonstrated notable improvement in the persuasion success rate as well as promising generalizability. Notably, the proposed framework also excelled at persuading individuals with initially low intent, which addresses a critical challenge for persuasive dialogue agents.","authors":["Shinnosuke Nozue","Yuto Nakano","Yotaro Watanabe","Meguru Takasaki","Shoji Moriya","Reina Akama","Jun Suzuki"],"pdf_url":"","comment":"Accepted to the EMNLP 2025 Industry Track; 26 pages"},{"id":"http://arxiv.org/abs/2602.22675v1","updated":"2026-02-26T06:46:41Z","published":"2026-02-26T06:46:41Z","title":"Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization","summary":"Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose \\emph{Search More, Think Less} (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.","authors":["Qianben Chen","Tianrui Qin","King Zhu","Qiexiang Wang","Chengjun Yu","Shu Xu","Jiaqi Wu","Jiayu Zhang","Xinpeng Liu","Xin Gui","Jingyi Cao","Piaohong Wang","Dingfeng Shi","He Zhu","Tiannan Wang","Yuqing Wang","Maojia Song","Tianyu Zheng","Ge Zhang","Jian Yang","Jiaheng Liu","Minghao Liu","Yuchen Eleanor Jiang","Wangchunshu Zhou"],"pdf_url":"","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.21262v2","updated":"2026-02-26T06:37:29Z","published":"2026-02-24T04:09:21Z","title":"Under the Influence: Quantifying Persuasion and Vigilance in Large Language Models","summary":"With increasing integration of Large Language Models (LLMs) into areas of high-stakes human decision-making, it is important to understand the risks they introduce as advisors. To be useful advisors, LLMs must sift through large amounts of content, written with both benevolent and malicious intent, and then use this information to convince a user to take a specific action. This involves two social capacities: vigilance (the ability to determine which information to use, and which to discard) and persuasion (synthesizing the available evidence to make a convincing argument). While existing work has investigated these capacities in isolation, there has been little prior investigation of how these capacities may be linked. Here, we use a simple multi-turn puzzle-solving game, Sokoban, to study LLMs' abilities to persuade and be rationally vigilant towards other LLM agents. We find that puzzle-solving performance, persuasive capability, and vigilance are dissociable capacities in LLMs. Performing well on the game does not automatically mean a model can detect when it is being misled, even if the possibility of deception is explicitly mentioned. However, LLMs do consistently modulate their token use, using fewer tokens to reason when advice is benevolent and more when it is malicious, even if they are still persuaded to take actions leading them to failure. To our knowledge, our work presents the first investigation of the relationship between persuasion, vigilance, and task performance in LLMs, and suggests that monitoring all three independently will be critical for future work in AI safety.","authors":["Sasha Robinson","Kerem Oktar","Katherine M. Collins","Ilia Sucholutsky","Kelsey R. Allen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.17072v2","updated":"2026-02-26T06:36:24Z","published":"2026-02-19T04:27:47Z","title":"BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios","summary":"Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.","authors":["Yunseung Lee","Subin Kim","Youngjun Kwak","Jaegul Choo"],"pdf_url":"","comment":"LREC 2026"},{"id":"http://arxiv.org/abs/2602.22661v1","updated":"2026-02-26T06:26:02Z","published":"2026-02-26T06:26:02Z","title":"dLLM: Simple Diffusion Language Modeling","summary":"Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\n  To address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.","authors":["Zhanhui Zhou","Lingjie Chen","Hanghang Tong","Dawn Song"],"pdf_url":"","comment":"Code available at: https://github.com/ZHZisZZ/dllm"},{"id":"http://arxiv.org/abs/2510.05725v2","updated":"2026-02-26T06:25:31Z","published":"2025-10-07T09:44:24Z","title":"Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies","summary":"Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence. Code is available at https://github.com/chunsanHong/UPO.","authors":["Chunsan Hong","Seonho An","Min-Soo Kim","Jong Chul Ye"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22658v1","updated":"2026-02-26T06:17:56Z","published":"2026-02-26T06:17:56Z","title":"Deepfake Word Detection by Next-token Prediction using Fine-tuned Whisper","summary":"Deepfake speech utterances can be forged by replacing one or more words in a bona fide utterance with semantically different words synthesized by speech generative models. While a dedicated synthetic word detector could be developed, we investigate a cost-effective method that fine-tunes a pre-trained Whisper model to detect synthetic words while transcribing the input utterance via next-token prediction. We further investigate using partially vocoded utterances as the fine-tuning data, thereby reducing the cost of data collection. Our experiments demonstrate that, on in-domain test data, the fine-tuned Whisper yields low synthetic-word detection error rates and transcription error rates. On out-of-domain test data with synthetic words produced by unseen speech generative models, the fine-tuned Whisper remains on par with a dedicated ResNet-based detection model; however, the overall performance degradation calls for strategies to improve its generalization capability.","authors":["Hoan My Tran","Xin Wang","Wanying Ge","Xuechen Liu","Junichi Yamagishi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22647v1","updated":"2026-02-26T06:00:56Z","published":"2026-02-26T06:00:56Z","title":"Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators","summary":"Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.","authors":["Zhengyang Su","Isay Katsman","Yueqi Wang","Ruining He","Lukasz Heldt","Raghunandan Keshavan","Shao-Chuan Wang","Xinyang Yi","Mingyan Gao","Onkar Dalal","Lichan Hong","Ed Chi","Ningren Han"],"pdf_url":"","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.13359v2","updated":"2026-02-26T05:45:42Z","published":"2025-04-17T21:58:29Z","title":"Cost-of-Pass: An Economic Framework for Evaluating Language Models","summary":"Widespread adoption of AI systems hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics accounting for both performance and costs. Building on production theory, we develop an economically grounded framework to evaluate language models' productivity by combining accuracy and inference cost. We formalize cost-of-pass: the expected monetary cost of generating a correct solution. We then define the frontier cost-of-pass: the minimum cost-of-pass achievable across available models or the human-expert(s), using the approx. cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking the frontier cost-of-pass over the past year reveals significant progress, particularly for complex quant. tasks where the cost roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers -- estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quant., knowledge-intensive, and complex quant. tasks, respectively. Finally, we assess the cost-reductions from common inference-time techniques (majority voting and self-refinement), and a budget-aware technique (TALE-EP). We find that performance-oriented methods with marginal performance gains rarely justify the costs, while TALE-EP shows some promise. Overall, our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency and our framework provides a principled tool for measuring this progress and guiding deployment.","authors":["Mehmet Hamza Erol","Batu El","Mirac Suzgun","Mert Yuksekgonul","James Zou"],"pdf_url":"","comment":"Code is available at: https://github.com/mhamzaerol/Cost-of-Pass"},{"id":"http://arxiv.org/abs/2506.18582v2","updated":"2026-02-26T05:26:04Z","published":"2025-06-23T12:35:41Z","title":"Parallel Continuous Chain-of-Thought with Jacobi Iteration","summary":"Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at https://github.com/whyNLP/PCCoT.","authors":["Haoyi Wu","Zhihao Teng","Kewei Tu"],"pdf_url":"","comment":"Accepted to EMNLP 2025 main conference"},{"id":"http://arxiv.org/abs/2602.22623v1","updated":"2026-02-26T04:55:57Z","published":"2026-02-26T04:55:57Z","title":"ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL","summary":"We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to \"recover\" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.","authors":["Xingyu Lu","Jinpeng Wang","YiFan Zhang","Shijie Ma","Xiao Hu","Tianke Zhang","Haonan fan","Kaiyu Jiang","Changyi Liu","Kaiyu Tang","Bin Wen","Fan Yang","Tingting Gao","Han Li","Chun Yuan"],"pdf_url":"","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.21947v2","updated":"2026-02-26T04:36:33Z","published":"2026-02-25T14:32:15Z","title":"Large Language Models are Algorithmically Blind","summary":"Large language models (LLMs) demonstrate remarkable breadth of knowledge, yet their ability to reason about computational processes remains poorly understood. Closing this gap matters for practitioners who rely on LLMs to guide algorithm selection and deployment. We address this limitation using causal discovery as a testbed and evaluate eight frontier LLMs against ground truth derived from large-scale algorithm executions and find systematic, near-total failure. Models produce ranges far wider than true confidence intervals yet still fail to contain the true algorithmic mean in the majority of instances; most perform worse than random guessing and the marginal above-random performance of the best model is most consistent with benchmark memorization rather than principled reasoning. We term this failure algorithmic blindness and argue it reflects a fundamental gap between declarative knowledge about algorithms and calibrated procedural prediction.","authors":["Sohan Venkatesh","Ashish Mahendran Kurapath","Tejas Melkote"],"pdf_url":"","comment":"19 pages, 8 figures, 15 tables"},{"id":"http://arxiv.org/abs/2602.12150v3","updated":"2026-02-26T04:35:19Z","published":"2026-02-12T16:33:58Z","title":"GPT-4o Lacks Core Features of Theory of Mind","summary":"Do Large Language Models (LLMs) possess a Theory of Mind (ToM)? Research into this question has focused on evaluating LLMs against benchmarks and found success across a range of social tasks. However, these evaluations do not test for the actual representations posited by ToM: namely, a causal model of mental states and behavior. Here, we use a cognitively-grounded definition of ToM to develop and test a new evaluation framework. Specifically, our approach probes whether LLMs have a coherent, domain-general, and consistent model of how mental states cause behavior -- regardless of whether that model matches a human-like ToM. We find that even though LLMs succeed in approximating human judgments in a simple ToM paradigm, they fail at a logically equivalent task and exhibit low consistency between their action predictions and corresponding mental state inferences. As such, these findings suggest that the social proficiency exhibited by LLMs is not the result of a domain-general or consistent ToM.","authors":["John Muchovej","Amanda Royka","Shane Lee","Julian Jara-Ettinger"],"pdf_url":"","comment":"Submitted to CogSci 2025; see more at https://jmuchovej.com/projects/llm-tom. Note: \"abstractness\" is the second feature we test for, but due to arXiv's abstract requirements, the text has been altered"},{"id":"http://arxiv.org/abs/2505.24449v2","updated":"2026-02-26T04:21:31Z","published":"2025-05-30T10:36:19Z","title":"When Large Multimodal Models Confront Evolving Knowledge: Challenges and Explorations","summary":"Large Multimodal Models (LMMs) store vast amounts of pretrained knowledge but struggle to remain aligned with real-world updates, making it difficult to avoid capability degradation when acquiring evolving knowledge. Furthermore, most current work focuses on exploring static textual knowledge injection, neglecting dynamic multimodal evolving knowledge injection, leaving the potential of LMMs for multimodal knowledge injection as an open question. To address this, we first propose a pipeline to construct MMEVOKE, a benchmark for evaluating LMMs' ability in multimodal evolving knowledge injection. MMEVOKE contains 9,422 samples spanning 159 subtypes. Then, based on extensive experiments with MMEVOKE, we reveal challenges such as poor injection performance and capability degradation in existing knowledge injection methods through knowledge injection tests and general capability tests. Finally, to tackle these challenges, we introduce knowledge augmentation and knowledge retention methods, finding that knowledge-aware augmentation strengthens knowledge injection performance, and that Data Replay and MoE methods effectively mitigate capability degradation.","authors":["Kailin Jiang","Yuntao Du","Yukai Ding","Yuchen Ren","Ning Jiang","Zhi Gao","Zilong Zheng","Lei Liu","Bin Li","Qing Li"],"pdf_url":"","comment":"ICLR 2026, Project Page: https://evoke-lmm.github.io/"},{"id":"http://arxiv.org/abs/2510.05534v2","updated":"2026-02-26T03:59:20Z","published":"2025-10-07T02:47:25Z","title":"Revisiting Self-Play Preference Optimization: On the Role of Prompt Difficulty","summary":"Self-play preference optimization has emerged as a prominent paradigm for aligning large language models (LLMs). It typically involves a language model to generate on-policy responses for prompts and a reward model (RM) to guide the selection of chosen and rejected responses, which can be further trained with direct preference optimization (DPO). However, the role of prompts remains underexplored, despite being a core component in this pipeline. In this work, we investigate how prompts of varying difficulty influence self-play preference optimization. We use the mean reward of\n  sampled responses of a prompt as a proxy for its difficulty. We first find that difficult prompts exhibit substantially inferior self-play optimization performance compared to easy prompts for language models. Moreover, incorporating difficult prompts into training fails to enhance overall performance and, in fact, leads to slight degradation compared to training on easy prompts alone. Third, there is a clear upward trend in optimization performance as prompt difficulty decreases. We also observe that the performance gap between difficult and easy prompts tends to close as the model capacity increases, suggesting that prompt difficulty interacts with the model capacity. Building on these findings, we explore strategies to mitigate the adversary effect of difficult prompts on final performance. We demonstrate that only training on a small portion (30%) of the easiest prompts improves overall self-play performance on AlpacaEval~2 and Arena-Hard. We also report failed attempts and lessons learned.","authors":["Yao Xiao","Jung-jae Kim","Roy Ka-wei Lee","Lidong Bing"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.11221v2","updated":"2026-02-26T03:55:17Z","published":"2026-02-11T12:32:15Z","title":"The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task","summary":"The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such as web search engines, or leverage the curated knowledge store provided by the organizers. System performance was evaluated using the AVerImaTeC score, defined as a conditional verdict accuracy in which a verdict is considered correct only when the associated evidence score exceeds a predefined threshold. The shared task attracted 14 submissions during the development phase and 6 submissions during the testing phase. All participating systems in the testing phase outperformed the baseline provided. The winning team, HUMANE, achieved an AVerImaTeC score of 0.5455. This paper provides a detailed description of the shared task, presents the complete evaluation results, and discusses key insights and lessons learned.","authors":["Rui Cao","Zhenyun Deng","Yulong Chen","Michael Schlichtkrull","Andreas Vlachos"],"pdf_url":"","comment":"Shared Task Overview and Summary for the Ninth FEVER Workshop, Co-located at EACL 2026"},{"id":"http://arxiv.org/abs/2403.15226v3","updated":"2026-02-26T03:53:55Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models","summary":"In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22592v1","updated":"2026-02-26T03:51:58Z","published":"2026-02-26T03:51:58Z","title":"pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training","summary":"Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.","authors":["Wenzheng Zhang","Bingzheng Liu","Yang Hu","Xiaoying Bai","Wentao Zhang","Bin Cui"],"pdf_url":"","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.22586v1","updated":"2026-02-26T03:41:49Z","published":"2026-02-26T03:41:49Z","title":"TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion","summary":"Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.","authors":["Donghong Cai","Jiarui Feng","Yanbo Wang","Da Zheng","Yixin Chen","Muhan Zhang"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2602.22584v1","updated":"2026-02-26T03:35:09Z","published":"2026-02-26T03:35:09Z","title":"Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA","summary":"Industrial advertising question answering (QA) is a high-stakes task in which hallucinated content, particularly fabricated URLs, can lead to financial loss, compliance violations, and legal risk. Although Retrieval-Augmented Generation (RAG) is widely adopted, deploying it in production remains challenging because industrial knowledge is inherently relational, frequently updated, and insufficiently aligned with generation objectives. We propose a reinforced co-adaptation framework that jointly optimizes retrieval and generation through two components: (1) Graph-aware Retrieval (GraphRAG), which models entity-relation structure over a high-citation knowledge subgraph for multi-hop, domain-specific evidence selection; and (2) evidence-constrained reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional rewards covering faithfulness, style compliance, safety, and URL validity. Experiments on an internal advertising QA dataset show consistent gains across expert-judged dimensions including accuracy, completeness, and safety, while reducing the hallucination rate by 72\\%. A two-week online A/B test demonstrates a 28.6\\% increase in like rate, a 46.2\\% decrease in dislike rate, and a 92.7\\% reduction in URL hallucination. The system has been running in production for over half a year and has served millions of QA interactions.","authors":["Wenwei Li","Ming Xu","Tianle Xia","Lingxiang Hu","Yiding Sun","Linfang Shang","Liqun Liu","Peng Shu","Huan Yu","Jie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22583v1","updated":"2026-02-26T03:34:23Z","published":"2026-02-26T03:34:23Z","title":"Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance","summary":"Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.","authors":["Weida Liang","Yiyou Sun","Shuyuan Nan","Chuang Li","Dawn Song","Kenji Kawaguchi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22576v1","updated":"2026-02-26T03:31:00Z","published":"2026-02-26T03:31:00Z","title":"Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training","summary":"Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.","authors":["Tianle Xia","Ming Xu","Lingxiang Hu","Yiding Sun","Wenwei Li","Linfang Shang","Liqun Liu","Peng Shu","Huan Yu","Jie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.01780v2","updated":"2026-02-26T03:19:15Z","published":"2025-08-03T14:36:42Z","title":"LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?","summary":"Model Context Protocol (MCP) has become a key infrastructure for connecting LLMs with external tools, scaling to 10,000+ MCP servers with diverse tools. Unfortunately, there is still a large gap between real-world MCP usage and current evaluation: they typically assume single-server settings and directly inject tools into the model's context, bypassing the challenges of large-scale retrieval and multi-tool composition. To bridge this gap, we propose LiveMCPBench, which evaluates 95 real-world daily tasks explicitly constructed to stress diverse tools and scaled multi-server routing. The benchmark includes a ready-to-deploy tool suite of 70 servers with 527 tools, ensuring reproducibility without scattered API configuration. We further introduce an LLM-as-a-Judge evaluation framework that directly verifies task outcomes, handling dynamic data sources and multiple valid solution paths. We benchmark 12 state-of-the-art LLMs and observe a substantial performance gap: while Claude-Sonnet-4 reaches 78.95% task success, most models achieve only 30-50%. Our analysis reveals that the active tool composition strongly correlates with task success, whereas retrieval errors account for nearly half of all failures, highlighting retrieval as the dominant bottleneck. Together, these results provide the first large-scale, reproducible diagnosis of MCP agent capabilities and point towards future research on improving retrieval robustness and encouraging effective tool composition. Our code and data are publicly available at https://icip-cas.github.io/LiveMCPBench.","authors":["Guozhao Mo","Wenliang Zhong","Jiawei Chen","Qianhao Yuan","Xuanang Chen","Yaojie Lu","Hongyu Lin","Ben He","Xianpei Han","Le Sun"],"pdf_url":"","comment":"Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench"},{"id":"http://arxiv.org/abs/2505.18502v3","updated":"2026-02-26T02:59:19Z","published":"2025-05-24T04:43:24Z","title":"Knowledge Fusion of Large Language Models Via Modular SkillPacks","summary":"Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.","authors":["Guodong Du","Zhuo Li","Xuanning Zhou","Junlin Li","Zesheng Shi","Wanyu Lin","Ho-Kin Tang","Xiucheng Li","Fangming Liu","Wenya Wang","Min Zhang","Jing Li"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22556v1","updated":"2026-02-26T02:49:36Z","published":"2026-02-26T02:49:36Z","title":"Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation","summary":"Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.","authors":["Zihang Xu","Haozhi Xie","Ziqi Miao","Wuxuan Gong","Chen Qian","Lijun Li"],"pdf_url":"","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.22543v1","updated":"2026-02-26T02:34:49Z","published":"2026-02-26T02:34:49Z","title":"Ruyi2 Technical Report","summary":"Large Language Models (LLMs) face significant challenges regarding deployment costs and latency, necessitating adaptive computing strategies. Building upon the AI Flow framework, we introduce Ruyi2 as an evolution of our adaptive model series designed for efficient variable-depth computation. While early-exit architectures offer a viable efficiency-performance balance, the Ruyi model and existing methods often struggle with optimization complexity and compatibility with large-scale distributed training. To bridge this gap, Ruyi2 introduces a stable \"Familial Model\" based on Megatron-LM. By using 3D parallel training, it achieves a 2-3 times speedup over Ruyi, while performing comparably to same-sized Qwen3 models. These results confirm that family-based parameter sharing is a highly effective strategy, establishing a new \"Train Once, Deploy Many\" paradigm and providing a key reference for balancing architectural efficiency with high-performance capabilities.","authors":["Huan Song","Shuyu Tian","Junyi Hao","Minxiu Xu","Hongjun An","Yiliang Song","Jiawei Shao","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22538v1","updated":"2026-02-26T02:26:45Z","published":"2026-02-26T02:26:45Z","title":"RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format","summary":"Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.","authors":["Zhehao Huang","Yuhang Liu","Baijiong Lin","Yixin Lou","Zhengbao He","Hanling Tian","Tao Li","Xiaolin Huang"],"pdf_url":"","comment":"41 pages, ICLR 2026 Oral"},{"id":"http://arxiv.org/abs/2602.22530v1","updated":"2026-02-26T02:09:43Z","published":"2026-02-26T02:09:43Z","title":"Dynamic Level Sets","summary":"A mathematical concept is identified and analyzed that is implicit in the 2012 paper Turing Incomputable Computation, presented at the Alan Turing Centenary Conference (Turing 100, Manchester). The concept, called dynamic level sets, is distinct from mathematical concepts in the standard literature on dynamical systems, topology, and computability theory. A new mathematical object is explained and why it may have escaped prior characterizations, including the classical result of de Leeuw, Moore, Shannon, and Shapiro (1956) that probabilistic Turing machines compute no more than deterministic ones.","authors":["Michael Stephen Fiske"],"pdf_url":"","comment":"7 pages"},{"id":"http://arxiv.org/abs/2509.03113v4","updated":"2026-02-26T01:48:53Z","published":"2025-09-03T08:13:52Z","title":"Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection","summary":"Multimodal large language models achieve strong performance across diverse tasks but remain prone to hallucinations, where outputs are not grounded in visual inputs. This issue can be attributed to two main biases: text-visual bias, the overreliance on prompts and prior outputs, and co-occurrence bias, spurious correlations between frequently paired objects. We propose Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-based method, that addresses both biases without auxiliary models, and is readily applicable to existing models without finetuning. The core of our approach is bias estimation, which uses first-order Taylor gradients to understand the contribution of individual tokens-visual features and text tokens-to the current output. Based on this analysis, GACD mitigates hallucinations through two components: (1) suppressing spurious visual features correlated with the output objects, and (2) rebalancing cross-modal contributions by strengthening visual features relative to text. Experiments across multiple benchmarks demonstrate that GACD effectively reduces hallucinations and improves the visual grounding of MLLM outputs.","authors":["Shan Wang","Maying Shen","Nadine Chang","Chuong Nguyen","Hongdong Li","Jose M. Alvarez"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22524v1","updated":"2026-02-26T01:46:40Z","published":"2026-02-26T01:46:40Z","title":"Iterative Prompt Refinement for Dyslexia-Friendly Text Summarization Using GPT-4o","summary":"Dyslexia affects approximately 10% of the global population and presents persistent challenges in reading fluency and text comprehension. While existing assistive technologies address visual presentation, linguistic complexity remains a substantial barrier to equitable access. This paper presents an empirical study on dyslexia-friendly text summarization using an iterative prompt-based refinement pipeline built on GPT-4o. We evaluate the pipeline on approximately 2,000 news article samples, applying a readability target of Flesch Reading Ease >= 90. Results show that the majority of summaries meet the readability threshold within four attempts, with many succeeding on the first try. A composite score combining readability and semantic fidelity shows stable performance across the dataset, ranging from 0.13 to 0.73 with a typical value near 0.55. These findings establish an empirical baseline for accessibility-driven NLP summarization and motivate further human-centered evaluation with dyslexic readers.","authors":["Samay Bhojwani","Swarnima Kain","Lisong Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22523v1","updated":"2026-02-26T01:35:32Z","published":"2026-02-26T01:35:32Z","title":"Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents","summary":"While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.","authors":["Ryan Liu","Dilip Arumugam","Cedegao E. Zhang","Sean Escola","Xaq Pitkow","Thomas L. Griffiths"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22522v1","updated":"2026-02-26T01:33:54Z","published":"2026-02-26T01:33:54Z","title":"Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing","summary":"Taiwanese Hakka is a low-resource, endangered language that poses significant challenges for automatic speech recognition (ASR), including high dialectal variability and the presence of two distinct writing systems (Hanzi and Pinyin). Traditional ASR models often encounter difficulties in this context, as they tend to conflate essential linguistic content with dialect-specific variations across both phonological and lexical dimensions. To address these challenges, we propose a unified framework grounded in the Recurrent Neural Network Transducers (RNN-T). Central to our approach is the introduction of dialect-aware modeling strategies designed to disentangle dialectal \"style\" from linguistic \"content\", which enhances the model's capacity to learn robust and generalized representations. Additionally, the framework employs parameter-efficient prediction networks to concurrently model ASR (Hanzi and Pinyin). We demonstrate that these tasks create a powerful synergy, wherein the cross-script objective serves as a mutual regularizer to improve the primary ASR tasks. Experiments conducted on the HAT corpus reveal that our model achieves 57.00% and 40.41% relative error rate reduction on Hanzi and Pinyin ASR, respectively. To our knowledge, this is the first systematic investigation into the impact of Hakka dialectal variations on ASR and the first single model capable of jointly addressing these tasks.","authors":["An-Ci Peng","Kuan-Tang Huang","Tien-Hong Lo","Hung-Shin Lee","Hsin-Min Wang","Berlin Chen"],"pdf_url":"","comment":"Accepted to LREC 2026"},{"id":"http://arxiv.org/abs/2602.14162v2","updated":"2026-02-26T01:17:29Z","published":"2026-02-15T14:23:50Z","title":"Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering","summary":"Existing multimodal document question answering methods predominantly adopt a Pre-Ingestion (PI) strategy: during the indexing phase, a Vision Language Model (VLM) is called on every page to generate page descriptions that are then encoded into vectors, and questions are answered via embedding similarity retrieval. However, this approach faces a dual dilemma on visual-dense engineering documents: VLM blind descriptions inevitably lose critical visual details, and embedding retrieval systematically fails on highly similar documents. This paper proposes the Deferred Visual Ingestion (DVI) framework: zero VLM calls during preprocessing, leveraging only document structural information (table of contents, drawing numbers) to automatically build a hierarchical index through the HDNC (Hierarchical Drawing Number Clustering) algorithm; during inference, candidate pages are located via BM25 retrieval, and the original images along with the specific question are sent to a VLM for targeted analysis. Large-scale experiments on three datasets validate the effectiveness of DVI: on Bridge engineering drawings (1,323 questions), end-to-end QA accuracy reaches 65.6\\% vs. PI's 24.3\\% (+41.3pp); on Steel catalog (186 questions), 30.6\\% vs. 16.1\\% (+14.5pp); on CircuitVQA, a public benchmark (9,315 questions), retrieval ImgR@3 achieves 31.2\\% vs. 0.7\\%. On the Bridge dataset, we evaluated ColPali (ICLR 2025 visual retrieval SOTA), which achieved only 20.1\\% PageR@3, demonstrating that the failure of embedding retrieval on homogeneous engineering documents is structural rather than due to insufficient model capability. Ablation studies show that HDNC zero-cost automatic indexing yields a +27.5pp retrieval improvement, and VLM conversion rate analysis confirms that the bottleneck lies on the retrieval side rather than the comprehension side.","authors":["Tao Xu"],"pdf_url":"","comment":"24 pages, 4 figures, 7 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2602.23363v1","updated":"2026-02-26T18:59:46Z","published":"2026-02-26T18:59:46Z","title":"MediX-R1: Open Ended Medical Reinforcement Learning","summary":"We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com","authors":["Sahal Shaji Mullappilly","Mohammed Irfan Kurpath","Omair Mohamed","Mohamed Zidan","Fahad Khan","Salman Khan","Rao Anwer","Hisham Cholakkal"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2501.02158v2","updated":"2026-02-26T18:59:39Z","published":"2025-01-04T01:53:51Z","title":"Joint Optimization for 4D Human-Scene Reconstruction in the Wild","summary":"Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.","authors":["Zhizheng Liu","Joe Lin","Wayne Wu","Bolei Zhou"],"pdf_url":"","comment":"Project Page: https://vail-ucla.github.io/JOSH/"},{"id":"http://arxiv.org/abs/2602.23361v1","updated":"2026-02-26T18:59:33Z","published":"2026-02-26T18:59:33Z","title":"VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale","summary":"We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.","authors":["Sven Elflein","Ruilong Li","Sérgio Agostinho","Zan Gojcic","Laura Leal-Taixé","Qunjie Zhou","Aljosa Osep"],"pdf_url":"","comment":"CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt"},{"id":"http://arxiv.org/abs/2602.23359v1","updated":"2026-02-26T18:59:05Z","published":"2026-02-26T18:59:05Z","title":"SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation","summary":"We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.","authors":["Vaibhav Agrawal","Rishubh Parihar","Pradhaan Bhat","Ravi Kiran Sarvadevabhatla","R. Venkatesh Babu"],"pdf_url":"","comment":"Project page: https://seethrough3d.github.io. Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23358v1","updated":"2026-02-26T18:59:03Z","published":"2026-02-26T18:59:03Z","title":"A Dataset is Worth 1 MB","summary":"A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.","authors":["Elad Kimchi Shoshani","Leeyam Gabay","Yedid Hoshen"],"pdf_url":"","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.23357v1","updated":"2026-02-26T18:57:52Z","published":"2026-02-26T18:57:52Z","title":"Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training","summary":"Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.","authors":["Aheli Saha","René Schuster","Didier Stricker"],"pdf_url":"","comment":"12 pages, International Conference on Pattern Recognition Applications and Methods"},{"id":"http://arxiv.org/abs/2602.23351v1","updated":"2026-02-26T18:54:06Z","published":"2026-02-26T18:54:06Z","title":"Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning","summary":"The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.","authors":["Amita Kamath","Jack Hessel","Khyathi Chandu","Jena D. Hwang","Kai-Wei Chang","Ranjay Krishna"],"pdf_url":"","comment":"TACL 2026"},{"id":"http://arxiv.org/abs/2602.23339v1","updated":"2026-02-26T18:45:33Z","published":"2026-02-26T18:45:33Z","title":"Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?","summary":"Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.","authors":["Tilemachos Aravanis","Vladan Stojnić","Bill Psomas","Nikos Komodakis","Giorgos Tolias"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.06092v2","updated":"2026-02-26T18:27:23Z","published":"2025-06-06T13:52:33Z","title":"LinGuinE: Longitudinal Guidance Estimation for Volumetric Tumour Segmentation","summary":"Longitudinal volumetric tumour segmentation is critical for radiotherapy planning and response assessment, yet this problem is underexplored and most methods produce single-timepoint semantic masks, lack lesion correspondence, and offer limited radiologist control. We introduce LinGuinE (Longitudinal Guidance Estimation), a PyTorch framework that combines image registration and guided segmentation to deliver lesion-level tracking and volumetric masks across all scans in a longitudinal study from a single radiologist prompt. LinGuinE is temporally direction agnostic, requires no training on longitudinal data, and allows any registration and semi-automatic segmentation algorithm to be repurposed for the task. We evaluate various combinations of registration and segmentation algorithms within the framework. LinGuinE achieves state-of-the-art segmentation and tracking performance across four datasets with a total of 456 longitudinal studies. Tumour segmentation performance shows minimal degradation with increasing temporal separation. We conduct ablation studies to determine the impact of autoregression, pathology specific finetuning, and the use of real radiologist prompts. We release our code and substantial public benchmarking for longitudinal segmentation, facilitating future research.","authors":["Nadine Garibli","Mayank Patwari","Bence Csiba","Yi Wei","Kostantinos Sidiropoulos"],"pdf_url":"","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2602.23306v1","updated":"2026-02-26T18:10:41Z","published":"2026-02-26T18:10:41Z","title":"ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding","summary":"Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.","authors":["Yiran Guan","Sifan Tu","Dingkang Liang","Linghao Zhu","Jianzhong Ju","Zhenbo Luo","Jian Luan","Yuliang Liu","Xiang Bai"],"pdf_url":"","comment":"Accept by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.23297v1","updated":"2026-02-26T18:07:52Z","published":"2026-02-26T18:07:52Z","title":"PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM","summary":"Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.","authors":["Yiqing Wang","Chunming He","Ming-Chen Lu","Mercy Pawar","Leslie Niziol","Maria Woodward","Sina Farsiu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23295v1","updated":"2026-02-26T18:07:10Z","published":"2026-02-26T18:07:10Z","title":"ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation","summary":"In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.","authors":["Ayush Roy","Wei-Yang Alex Lee","Rudrasis Chakraborty","Vishnu Suresh Lokhande"],"pdf_url":"","comment":"CVPE 2026"},{"id":"http://arxiv.org/abs/2510.19060v3","updated":"2026-02-26T18:05:42Z","published":"2025-10-21T20:30:20Z","title":"PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions","summary":"While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $ρ$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.","authors":["Amith Ananthram","Elias Stengel-Eskin","Lorena A. Bradford","Julia Demarest","Adam Purvis","Keith Krut","Robert Stein","Rina Elster Pantalony","Mohit Bansal","Kathleen McKeown"],"pdf_url":"","comment":"Accepted at ICLR 2026. 26 pages, 9 figures. Metric/benchmark available at https://github.com/amith-ananthram/posh"},{"id":"http://arxiv.org/abs/2602.23294v1","updated":"2026-02-26T18:04:09Z","published":"2026-02-26T18:04:09Z","title":"Towards Long-Form Spatio-Temporal Video Grounding","summary":"In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.","authors":["Xin Gu","Bing Fan","Jiali Yao","Zhipeng Zhang","Yan Huang","Cheng Han","Heng Fan","Libo Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2408.17251v2","updated":"2026-02-26T18:03:25Z","published":"2024-08-30T12:50:15Z","title":"Abstracted Gaussian Prototypes for True One-Shot Concept Learning","summary":"We introduce a cluster-based generative image segmentation framework to encode higher-level representations of visual concepts based on one-shot learning inspired by the Omniglot Challenge. The inferred parameters of each component of a Gaussian Mixture Model (GMM) represent a distinct topological subpart of a visual concept. Sampling new data from these parameters generates augmented subparts to build a more robust prototype for each concept, i.e., the Abstracted Gaussian Prototype (AGP). This framework addresses one-shot classification tasks using a cognitively-inspired similarity metric and addresses one-shot generative tasks through a novel AGP-VAE pipeline employing variational autoencoders (VAEs) to generate new class variants. Results from human judges reveal that the generative pipeline produces novel examples and classes of visual concepts that are broadly indistinguishable from those made by humans. The proposed framework leads to impressive, but not state-of-the-art, classification accuracy; thus, the contribution is two-fold: 1) the system is low in theoretical and computational complexity yet achieves the standard of 'true' one-shot learning by operating in a fully standalone manner unlike existing approaches that draw heavily on pre-training or knowledge engineering; and 2) in contrast with existing neural network approaches, the AGP approach addresses the importance of broad task capability emphasized in the Omniglot challenge (successful performance on classification and generative tasks). These two points are critical in advancing our understanding of how learning and reasoning systems can produce viable, robust, and flexible concepts based on literally no more than a single example.","authors":["Chelsea Zou","Kenneth J. Kurtz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23292v1","updated":"2026-02-26T18:03:24Z","published":"2026-02-26T18:03:24Z","title":"PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning","summary":"Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).","authors":["Fuqiang Chen","Ranran Zhang","Wanming Hu","Deboch Eyob Abera","Yue Peng","Boyun Zheng","Yiwen Sun","Jing Cai","Wenjian Qin"],"pdf_url":"","comment":"Accepted by TMI"},{"id":"http://arxiv.org/abs/2602.23290v1","updated":"2026-02-26T18:02:44Z","published":"2026-02-26T18:02:44Z","title":"LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction","summary":"The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.","authors":["Zhengyang Wei","Renzhi Jing","Yiyi He","Jenny Suckale"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04228v2","updated":"2026-02-26T17:37:05Z","published":"2025-08-06T09:03:16Z","title":"LayerT2V: A Unified Multi-Layer Video Generation Framework","summary":"Text-to-video generation has advanced rapidly, but existing methods typically output only the final composited video and lack editable layered representations, limiting their use in professional workflows. We propose \\textbf{LayerT2V}, a unified multi-layer video generation framework that produces multiple semantically consistent outputs in a single inference pass: the full video, an independent background layer, and multiple foreground RGB layers with corresponding alpha mattes. Our key insight is that recent video generation backbones use high compression in both time and space, enabling us to serialize multiple layer representations along the temporal dimension and jointly model them on a shared generation trajectory. This turns cross-layer consistency into an intrinsic objective, improving semantic alignment and temporal coherence. To mitigate layer ambiguity and conditional leakage, we augment a shared DiT backbone with LayerAdaLN and layer-aware cross-attention modulation. LayerT2V is trained in three stages: alpha mask VAE adaptation, joint multi-layer learning, and multi-foreground extension. We also introduce \\textbf{VidLayer}, the first large-scale dataset for multi-layer video generation. Extensive experiments demonstrate that LayerT2V substantially outperforms prior methods in visual fidelity, temporal consistency, and cross-layer coherence.","authors":["Guangzhao Li","Kangrui Cen","Baixuan Zhao","Yi Xin","Siqi Luo","Guangtao Zhai","Lei Zhang","Xiaohong Liu"],"pdf_url":"","comment":"Project Page is https://layert2v.github.io/"},{"id":"http://arxiv.org/abs/2602.23262v1","updated":"2026-02-26T17:36:48Z","published":"2026-02-26T17:36:48Z","title":"Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling","summary":"Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.","authors":["Jasmine Bayrooti","Weiwei Kong","Natalia Ponomareva","Carlos Esteves","Ameesh Makadia","Amanda Prorok"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.20570v2","updated":"2026-02-26T17:33:06Z","published":"2025-08-28T09:08:30Z","title":"Dyslexify: A Mechanistic Defense Against Typographic Attacks in CLIP","summary":"Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce Dyslexify - a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, dyslexify improves performance by up to 22.06% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%, and demonstrate its utility in a medical foundation model for skin lesion diagnosis. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.","authors":["Lorenz Hufe","Constantin Venhoff","Erblina Purelku","Maximilian Dreyer","Sebastian Lapuschkin","Wojciech Samek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23259v1","updated":"2026-02-26T17:32:30Z","published":"2026-02-26T17:32:30Z","title":"Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving","summary":"With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.","authors":["Jiangxin Sun","Feng Xue","Teng Long","Chang Liu","Jian-Fang Hu","Wei-Shi Zheng","Nicu Sebe"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23235v1","updated":"2026-02-26T17:12:40Z","published":"2026-02-26T17:12:40Z","title":"Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents","summary":"Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.","authors":["Zhou Xu","Bowen Zhou","Qi Wang","Shuwen Feng","Jingyu Xiao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23231v1","updated":"2026-02-26T17:10:58Z","published":"2026-02-26T17:10:58Z","title":"Skarimva: Skeleton-based Action Recognition is a Multi-view Application","summary":"Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.","authors":["Daniel Bermuth","Alexander Poeppel","Wolfgang Reif"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23229v1","updated":"2026-02-26T17:08:18Z","published":"2026-02-26T17:08:18Z","title":"Large Multimodal Models as General In-Context Classifiers","summary":"Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their \"in-context\" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.","authors":["Marco Garosi","Matteo Farina","Alessandro Conti","Massimiliano Mancini","Elisa Ricci"],"pdf_url":"","comment":"CVPR Findings 2026. Project website at https://circle-lmm.github.io/"},{"id":"http://arxiv.org/abs/2602.23228v1","updated":"2026-02-26T17:08:08Z","published":"2026-02-26T17:08:08Z","title":"MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction","summary":"With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external \"tool\" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.","authors":["Yizhi Li","Xiaohan Chen","Miao Jiang","Wentao Tang","Gaoang Wang"],"pdf_url":"","comment":"6 pages, CSCWD 2026"},{"id":"http://arxiv.org/abs/2602.23224v1","updated":"2026-02-26T17:04:36Z","published":"2026-02-26T17:04:36Z","title":"UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception","summary":"We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.","authors":["Mohammad Mahdavian","Gordon Tan","Binbin Xu","Yuan Ren","Dongfeng Bai","Bingbing Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23217v1","updated":"2026-02-26T17:00:45Z","published":"2026-02-26T17:00:45Z","title":"Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks","summary":"This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.","authors":["Alaa El Ichi","Khalide Jbilou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23214v1","updated":"2026-02-26T16:58:43Z","published":"2026-02-26T16:58:43Z","title":"Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction","summary":"Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.","authors":["Chenhe Du","Xuanyu Tian","Qing Wu","Muyu Liu","Jingyi Yu","Hongjiang Wei","Yuyao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23212v1","updated":"2026-02-26T16:56:51Z","published":"2026-02-26T16:56:51Z","title":"Through BrokenEyes: How Eye Disorders Impact Face Detection?","summary":"Vision disorders significantly impact millions of lives, altering how visual information is processed and perceived. In this work, a computational framework was developed using the BrokenEyes system to simulate five common eye disorders: Age-related macular degeneration, cataract, glaucoma, refractive errors, and diabetic retinopathy and analyze their effects on neural-like feature representations in deep learning models. Leveraging a combination of human and non-human datasets, models trained under normal and disorder-specific conditions revealed critical disruptions in feature maps, particularly for cataract and glaucoma, which align with known neural processing challenges in these conditions. Evaluation metrics such as activation energy and cosine similarity quantified the severity of these distortions, providing insights into the interplay between degraded visual inputs and learned representations.","authors":["Prottay Kumar Adhikary"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23205v1","updated":"2026-02-26T16:53:41Z","published":"2026-02-26T16:53:41Z","title":"EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents","summary":"Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.","authors":["Wenjia Wang","Liang Pan","Huaijin Pi","Yuke Lou","Xuqian Ren","Yifan Wu","Zhouyingcheng Liao","Lei Yang","Rishabh Dabral","Christian Theobalt","Taku Komura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23204v1","updated":"2026-02-26T16:53:36Z","published":"2026-02-26T16:53:36Z","title":"Motion-aware Event Suppression for Event Cameras","summary":"In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\\% in segmentation accuracy while operating at a 53\\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\\%.","authors":["Roberto Pellerito","Nico Messikommer","Giovanni Cioffi","Marco Cannici","Davide Scaramuzza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23203v1","updated":"2026-02-26T16:51:24Z","published":"2026-02-26T16:51:24Z","title":"ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation","summary":"Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.","authors":["Junhu Fu","Shuyu Liang","Wutong Li","Chen Ma","Peng Huang","Kehao Wang","Ke Chen","Shengli Lin","Pinghong Zhou","Zeju Li","Yuanyuan Wang","Yi Guo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23192v1","updated":"2026-02-26T16:44:47Z","published":"2026-02-26T16:44:47Z","title":"FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification","summary":"Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.","authors":["Thomas Woergaard","Raghavendra Selvan"],"pdf_url":"","comment":"Source code available at https://github.com/saintslab/FairQuant"},{"id":"http://arxiv.org/abs/2602.23191v1","updated":"2026-02-26T16:44:37Z","published":"2026-02-26T16:44:37Z","title":"Uni-Animator: Towards Unified Visual Colorization","summary":"We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.","authors":["Xinyuan Chen","Yao Xu","Shaowen Wang","Pengjie Song","Bowen Deng"],"pdf_url":"","comment":"10 pages, 8 figures. Submitted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23177v1","updated":"2026-02-26T16:38:44Z","published":"2026-02-26T16:38:44Z","title":"Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms","summary":"Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.","authors":["Bin Zeng","Johannes Künzel","Anna Hilsmann","Peter Eisert"],"pdf_url":"","comment":"published at VISAPP 2026"},{"id":"http://arxiv.org/abs/2412.06491v3","updated":"2026-02-26T16:36:51Z","published":"2024-12-09T13:48:15Z","title":"PPT: Pretraining with Pseudo-Labeled Trajectories for Motion Forecasting","summary":"Accurately predicting how agents move in dynamic scenes is essential for safe autonomous driving. State-of-the-art motion forecasting models rely on datasets with manually annotated or post-processed trajectories. However, building these datasets is costly, generally manual, hard to scale, and lacks reproducibility. They also introduce domain gaps that limit generalization across environments. We introduce PPT (Pretraining with Pseudo-labeled Trajectories), a simple and scalable pretraining framework that uses unprocessed and diverse trajectories automatically generated from off-the-shelf 3D detectors and tracking. Unlike data annotation pipelines aiming for clean, single-label annotations, PPT is a pretraining framework embracing off-the-shelf trajectories as useful signals for learning robust representations. With optional finetuning on a small amount of labeled data, models pretrained with PPT achieve strong performance across standard benchmarks, particularly in low-data regimes, and in cross-domain, end-to-end, and multi-class settings. PPT is easy to implement and improves generalization in motion forecasting.","authors":["Yihong Xu","Yuan Yin","Éloi Zablocki","Tuan-Hung Vu","Alexandre Boulch","Matthieu Cord"],"pdf_url":"","comment":"8 pages, 6 figures, accepted to ICRA 2026"},{"id":"http://arxiv.org/abs/2602.21101v2","updated":"2026-02-26T16:35:02Z","published":"2026-02-24T17:02:56Z","title":"Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones","summary":"Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.","authors":["Rong Zou","Marco Cannici","Davide Scaramuzza"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23172v1","updated":"2026-02-26T16:34:49Z","published":"2026-02-26T16:34:49Z","title":"Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking","summary":"Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.","authors":["Maximilian Luz","Rohit Mohan","Thomas Nürnberg","Yakov Miron","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23169v1","updated":"2026-02-26T16:31:27Z","published":"2026-02-26T16:31:27Z","title":"Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration","summary":"Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.","authors":["Xiaole Tang","Xiaoyi He","Jiayi Xu","Xiang Gu","Jian Sun"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23166v1","updated":"2026-02-26T16:30:46Z","published":"2026-02-26T16:30:46Z","title":"AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios","summary":"Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.","authors":["Zhaochen Su","Jincheng Gao","Hangyu Guo","Zhenhua Liu","Lueyang Zhang","Xinyu Geng","Shijue Huang","Peng Xia","Guanyu Jiang","Cheng Wang","Yue Zhang","Yi R. Fung","Junxian He"],"pdf_url":"","comment":"The project website is available at \\url{https://agentvista-bench.github.io/}, and the code is available at \\url{https://github.com/hkust-nlp/AgentVista}"},{"id":"http://arxiv.org/abs/2602.23165v1","updated":"2026-02-26T16:30:07Z","published":"2026-02-26T16:30:07Z","title":"DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation","summary":"Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.","authors":["Yichen Peng","Jyun-Ting Song","Siyeol Jung","Ruofan Liu","Haiyang Liu","Xuangeng Chu","Ruicong Liu","Erwin Wu","Hideki Koike","Kris Kitani"],"pdf_url":"","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2510.12670v2","updated":"2026-02-26T16:28:27Z","published":"2025-10-14T16:05:31Z","title":"TerraCodec: Compressing Optical Earth Observations","summary":"Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented and lacks publicly available, large-scale pretrained codecs. Moreover, prior work has largely focused on image compression, leaving temporal redundancy and EO video codecs underexplored. To address these gaps, we introduce TerraCodec (TEC), a family of learned codecs pretrained on Sentinel-2 EO data. TEC includes efficient multispectral image variants and a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of today's neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. TerraCodec outperforms classical codecs, achieving 3-10x higher compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish neural codecs as a promising direction for Earth observation. Our code and models are publically available at https://github.com/IBM/TerraCodec.","authors":["Julen Costa-Watanabe","Isabelle Wittmann","Benedikt Blumenstiel","Konrad Schindler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23153v1","updated":"2026-02-26T16:16:02Z","published":"2026-02-26T16:16:02Z","title":"Efficient Encoder-Free Fourier-based 3D Large Multimodal Model","summary":"Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.","authors":["Guofeng Mei","Wei Lin","Luigi Riz","Yujiao Wu","Yiming Wang","Fabio Poiesi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23146v1","updated":"2026-02-26T16:11:53Z","published":"2026-02-26T16:11:53Z","title":"Partial recovery of meter-scale surface weather","summary":"Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.","authors":["Jonathan Giezendanner","Qidong Yang","Eric Schmitt","Anirban Chandra","Daniel Salles Civitarese","Johannes Jakubik","Jeremy Vila","Detlef Hohl","Campbell Watson","Sherrie Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23141v1","updated":"2026-02-26T16:04:36Z","published":"2026-02-26T16:04:36Z","title":"No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors","summary":"We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.","authors":["Tao Liu","Gang Wan","Kan Ren","Shibo Wen"],"pdf_url":"","comment":"CVPR2026"},{"id":"http://arxiv.org/abs/2510.04714v4","updated":"2026-02-26T16:03:04Z","published":"2025-10-06T11:33:09Z","title":"Object-Centric Representation Learning for Enhanced 3D Semantic Scene Graph Prediction","summary":"3D Semantic Scene Graph Prediction aims to detect objects and their semantic relationships in 3D scenes, and has emerged as a crucial technology for robotics and AR/VR applications. While previous research has addressed dataset limitations and explored various approaches including Open-Vocabulary settings, they frequently fail to optimize the representational capacity of object and relationship features, showing excessive reliance on Graph Neural Networks despite insufficient discriminative capability. In this work, we demonstrate through extensive analysis that the quality of object features plays a critical role in determining overall scene graph accuracy. To address this challenge, we design a highly discriminative object feature encoder and employ a contrastive pretraining strategy that decouples object representation learning from the scene graph prediction. This design not only enhances object classification accuracy but also yields direct improvements in relationship prediction. Notably, when plugging in our pretrained encoder into existing frameworks, we observe substantial performance improvements across all evaluation metrics. Additionally, whereas existing approaches have not fully exploited the integration of relationship information, we effectively combine both geometric and semantic features to achieve superior relationship prediction. Comprehensive experiments on the 3DSSG dataset demonstrate that our approach significantly outperforms previous state-of-the-art methods. Our code is publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.","authors":["KunHo Heo","GiHyun Kim","SuYeon Kim","MyeongAh Cho"],"pdf_url":"","comment":"Accepted by NeurIPS 2025. Code: https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes"},{"id":"http://arxiv.org/abs/2602.23133v1","updated":"2026-02-26T15:50:15Z","published":"2026-02-26T15:50:15Z","title":"From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification","summary":"With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.","authors":["Xin Yuan","Zhiyong Zhang","Xin Xu","Zheng Wang","Chia-Wen Lin"],"pdf_url":"","comment":"Accepted by IEEE TMM 2026"},{"id":"http://arxiv.org/abs/2509.24421v3","updated":"2026-02-26T15:33:44Z","published":"2025-09-29T08:10:07Z","title":"Proxy-GS: Unified Occlusion Priors for Training and Inference in Structured 3D Gaussian Splatting","summary":"3D Gaussian Splatting (3DGS) has emerged as an efficient approach for achieving photorealistic rendering. Recent MLP-based variants further improve visual fidelity but introduce substantial decoding overhead during rendering. To alleviate computation cost, several pruning strategies and level-of-detail (LOD) techniques have been introduced, aiming to effectively reduce the number of Gaussian primitives in large-scale scenes. However, our analysis reveals that significant redundancy still remains due to the lack of occlusion awareness. In this work, we propose Proxy-GS, a novel pipeline that exploits a proxy to introduce Gaussian occlusion awareness from any view. At the core of our approach is a fast proxy system capable of producing precise occlusion depth maps at a resolution of 1000x1000 under 1ms. This proxy serves two roles: first, it guides the culling of anchors and Gaussians to accelerate rendering speed. Second, it guides the densification towards surfaces during training, avoiding inconsistencies in occluded regions, and improving the rendering quality. In heavily occluded scenarios, such as the MatrixCity Streets dataset, Proxy-GS not only equips MLP-based Gaussian splatting with stronger rendering capability but also achieves faster rendering speed. Specifically, it achieves more than 2.5x speedup over Octree-GS, and consistently delivers substantially higher rendering quality. Code will be public upon acceptance.","authors":["Yuanyuan Gao","Yuning Gong","Yifei Liu","Li Jingfeng","Dingwen Zhang","Yanci Zhang","Dan Xu","Xiao Sun","Zhihang Zhong"],"pdf_url":"","comment":"Project page: https://gyy456.github.io/Proxy-GS"},{"id":"http://arxiv.org/abs/2602.23120v1","updated":"2026-02-26T15:33:22Z","published":"2026-02-26T15:33:22Z","title":"TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement","summary":"Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.","authors":["Arian Sabaghi","José Oramas"],"pdf_url":"","comment":"This paper consists of 8 pages including 6 figures. Accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23117v1","updated":"2026-02-26T15:30:36Z","published":"2026-02-26T15:30:36Z","title":"Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation","summary":"Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.","authors":["Xiaosen Wang","Zhijin Ge","Bohan Liu","Zheng Fang","Fengfan Zhou","Ruixuan Zhang","Shaokang Wang","Yuyang Luo"],"pdf_url":"","comment":"Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack"},{"id":"http://arxiv.org/abs/2505.07734v2","updated":"2026-02-26T15:30:36Z","published":"2025-05-12T16:42:19Z","title":"LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention","summary":"Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.","authors":["Jiangling Zhang","Weijie Zhu","Jirui Huang","Yaxiong Chen"],"pdf_url":"","comment":"Accepted to ECAI 2025"},{"id":"http://arxiv.org/abs/2602.23115v1","updated":"2026-02-26T15:27:49Z","published":"2026-02-26T15:27:49Z","title":"FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time","summary":"Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.","authors":["David Dirnfeld","Fabien Delattre","Pedro Miraldo","Erik Learned-Miller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23114v1","updated":"2026-02-26T15:27:17Z","published":"2026-02-26T15:27:17Z","title":"WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning","summary":"Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .","authors":["Xudong Yan","Songhe Feng","Jiaxin Wang","Xin Su","Yi Jin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23103v1","updated":"2026-02-26T15:17:42Z","published":"2026-02-26T15:17:42Z","title":"SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation","summary":"Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.","authors":["Fuhao Zhang","Lei Liu","Jialin Zhang","Ya-Nan Zhang","Nan Mu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23101v1","updated":"2026-02-26T15:16:04Z","published":"2026-02-26T15:16:04Z","title":"Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras","summary":"Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.","authors":["Paul Kielty","Timothy Hanley","Peter Corcoran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.13507v2","updated":"2026-02-26T15:15:41Z","published":"2026-02-13T22:36:42Z","title":"Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening","summary":"Video-based assessments offer a scalable pathway for remote Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4 - 85.3% and accuracies of 71.5 - 80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2 - 57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\\_video\\_benchmarking-A2C5","authors":["Md Saiful Islam","Ekram Hossain","Abdelrahman Abdelkader","Tariq Adnan","Fazla Rabbi Mashrur","Sooyong Park","Praveen Kumar","Qasim Sudais","Natalia Chunga","Nami Shah","Jan Freyberg","Christopher Kanan","Ruth Schneider","Ehsan Hoque"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.16758v2","updated":"2026-02-26T15:11:32Z","published":"2024-11-24T10:03:24Z","title":"Motion-Aware Animatable Gaussian Avatars Deblurring","summary":"The creation of 3D human avatars from multi-view videos is a significant yet challenging task in computer vision. However, existing techniques rely on high-quality, sharp images as input, which are often impractical to obtain in real-world scenarios due to variations in human motion speed and intensity. This paper introduces a novel method for directly reconstructing sharp 3D human Gaussian avatars from blurry videos. The proposed approach incorporates a 3D-aware, physics-based model of blur formation caused by human motion, together with a 3D human motion model designed to resolve ambiguities in motion-induced blur. This framework enables the joint optimization of the avatar representation and motion parameters from a coarse initialization. Comprehensive benchmarks are established using both a synthetic dataset and a real-world dataset captured with a 360-degree synchronous hybrid-exposure camera system. Extensive evaluations demonstrate the effectiveness and robustness of the model across diverse conditions.","authors":["Muyao Niu","Yifan Zhan","Qingtian Zhu","Zhuoxiao Li","Wei Wang","Zhihang Zhong","Xiao Sun","Yinqiang Zheng"],"pdf_url":"","comment":"CVPR2026, https://github.com/MyNiuuu/MAD-Avatar"},{"id":"http://arxiv.org/abs/2602.23088v1","updated":"2026-02-26T15:10:39Z","published":"2026-02-26T15:10:39Z","title":"Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy","summary":"Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.","authors":["Matthew Sutton","Katrin Amunts","Timo Dickscheid","Christian Schiffer"],"pdf_url":"","comment":"8 pages, 3 figures, submitted for inclusion at a conference"},{"id":"http://arxiv.org/abs/2602.23069v1","updated":"2026-02-26T14:58:59Z","published":"2026-02-26T14:58:59Z","title":"Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception","summary":"Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel \"Align then Adapt\" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \\% accuracy on 3D action recognition, $+8.7 \\%$ on 4 D action segmentation, and 84.06\\% on 4D semantic segmentation.","authors":["Yiding Sun","Jihua Zhu","Haozhe Cheng","Chaoyi Lu","Zhichuan Yang","Lin Chen","Yaonan Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23058v1","updated":"2026-02-26T14:42:53Z","published":"2026-02-26T14:42:53Z","title":"GeoWorld: Geometric World Models","summary":"Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.","authors":["Zeyu Zhang","Danning Li","Ian Reid","Richard Hartley"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2503.22841v3","updated":"2026-02-26T14:37:01Z","published":"2025-03-28T19:26:45Z","title":"GmNet: Revisiting Gating Mechanisms From A Frequency View","summary":"Gating mechanisms have emerged as an effective strategy integrated into model designs beyond recurrent neural networks for addressing long-range dependency problems. In a broad understanding, it provides adaptive control over the information flow while maintaining computational efficiency. However, there is a lack of theoretical analysis on how the gating mechanism works in neural networks. In this paper, inspired by the \\textit{convolution theorem}, we systematically explore the effect of gating mechanisms on the training dynamics of neural networks from a frequency perspective. We investigate the interact between the element-wise product and activation functions in managing the responses to different frequency components. Leveraging these insights, we propose a Gating Mechanism Network (GmNet), a lightweight model designed to efficiently utilize the information of various frequency components. It minimizes the low-frequency bias present in existing lightweight models. GmNet achieves impressive performance in terms of both effectiveness and efficiency in the image classification task.","authors":["Yifan Wang","Xu Ma","Yitian Zhang","Zhongruo Wang","Sung-Cheol Kim","Vahid Mirjalili","Vidya Renganathan","Yun Fu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.17361v3","updated":"2026-02-26T14:31:39Z","published":"2025-11-21T16:26:31Z","title":"SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation","summary":"Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75% reduction in memory footprint, 124% faster inference, and a 5.9% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be made available at https://github.com/seamie6/SuperQuadricOcc.","authors":["Seamie Hayes","Reenu Mohandas","Tim Brophy","Alexandre Boulch","Ganesh Sistu","Ciaran Eising"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23043v1","updated":"2026-02-26T14:26:49Z","published":"2026-02-26T14:26:49Z","title":"D-FINE-seg: Object Detection and Instance Segmentation Framework with multi-backend deployment","summary":"Transformer-based real-time object detectors achieve strong accuracy-latency trade-offs, and D-FINE is among the top-performing recent architectures. However, real-time instance segmentation with transformers is still less common. We present D-FINE-seg, an instance segmentation extension of D-FINE that adds: a lightweight mask head, segmentation-aware training, including box cropped BCE and dice mask losses, auxiliary and denoising mask supervision, and adapted Hungarian matching cost. On the TACO dataset, D-FINE-seg improves F1-score over Ultralytics YOLO26 under a unified TensorRT FP16 end-to-end benchmarking protocol, while maintaining competitive latency. Second contribution is an end-to-end pipeline for training, exporting, and optimized inference across ONNX, TensorRT, OpenVINO for both object detection and instance segmentation tasks. This framework is released as open-source under the Apache-2.0 license. GitHub repository - https://github.com/ArgoHA/D-FINE-seg.","authors":["Argo Saakyan","Dmitry Solntsev"],"pdf_url":"","comment":"6 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2602.23040v1","updated":"2026-02-26T14:24:48Z","published":"2026-02-26T14:24:48Z","title":"PackUV: Packed Gaussian UV Maps for 4D Volumetric Video","summary":"Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications.\n  We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure.\n  To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.","authors":["Aashish Rai","Angela Xing","Anushka Agarwal","Xiaoyan Cong","Zekun Li","Tao Lu","Aayush Prakash","Srinath Sridhar"],"pdf_url":"","comment":"https://ivl.cs.brown.edu/packuv"},{"id":"http://arxiv.org/abs/2512.01292v3","updated":"2026-02-26T14:18:12Z","published":"2025-12-01T05:26:43Z","title":"Diffusion Model in Latent Space for Medical Image Segmentation Task","summary":"Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.","authors":["Huynh Trinh Ngoc","Toan Nguyen Hai","Ba Luong Son","Long Tran Quoc"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23031v1","updated":"2026-02-26T14:11:30Z","published":"2026-02-26T14:11:30Z","title":"Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images","summary":"Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model's semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model's ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.","authors":["Zhangjian Ji","Huijia Yan","Shaotong Qiao","Kai Feng","Wei Wei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23029v1","updated":"2026-02-26T14:11:10Z","published":"2026-02-26T14:11:10Z","title":"WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval","summary":"Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a \"retrieve-verify-refine\" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.","authors":["Tianyue Wang","Leigang Qu","Tianyu Yang","Xiangzhao Hao","Yifan Xu","Haiyun Guo","Jinqiao Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.20903v3","updated":"2026-02-26T14:04:48Z","published":"2026-02-24T13:40:23Z","title":"TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering","summary":"Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., Seedream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.","authors":["Hanshen Zhu","Yuliang Liu","Xuecheng Wu","An-Lan Wang","Hao Feng","Dingkang Yang","Chao Feng","Can Huang","Jingqun Tang","Xiang Bai"],"pdf_url":"","comment":"Accepted by CVPR 2026; Code: https://github.com/CIawevy/TextPecker"},{"id":"http://arxiv.org/abs/2602.23022v1","updated":"2026-02-26T14:00:07Z","published":"2026-02-26T14:00:07Z","title":"DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis","summary":"Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.","authors":["Xinglong Luo","Ao Luo","Zhengning Wang","Yueqi Yang","Chaoyu Feng","Lei Lei","Bing Zeng","Shuaicheng Liu"],"pdf_url":"","comment":"Accepted by CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23013v1","updated":"2026-02-26T13:52:57Z","published":"2026-02-26T13:52:57Z","title":"SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling","summary":"Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.","authors":["Camile Lendering","Erkut Akdag","Egor Bondarev"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23010v1","updated":"2026-02-26T13:52:42Z","published":"2026-02-26T13:52:42Z","title":"HELMLAB: An Analytical, Data-Driven Color Space for Perceptual Distance in UI Design Systems","summary":"We present HELMLAB, a 72-parameter analytical color space for UI design systems. The forward transform maps CIE XYZ to a perceptually-organized Lab representation through learned matrices, per-channel power compression, Fourier hue correction, and embedded Helmholtz-Kohlrausch lightness adjustment. A post-pipeline neutral correction guarantees that achromatic colors map to a=b=0 (chroma < 10^-6), and a rigid rotation of the chromatic plane improves hue-angle alignment without affecting the distance metric, which is invariant under isometries. On the COMBVD dataset (3,813 color pairs), HELMLAB achieves a STRESS of 23.22, a 20.4% reduction from CIEDE2000 (29.18). Cross-validation on He et al. 2022 and MacAdam 1974 shows competitive cross-dataset performance. The transform is invertible with round-trip errors below 10^-14. Gamut mapping, design-token export, and dark/light mode adaptation utilities are included for use in web and mobile design systems.","authors":["Gorkem Yildiz"],"pdf_url":"","comment":"9 pages, 6 figures. Code and demo available at: https://github.com/Grkmyldz148/helmlab"},{"id":"http://arxiv.org/abs/2602.02334v2","updated":"2026-02-26T13:40:03Z","published":"2026-02-02T16:58:17Z","title":"VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations","summary":"Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating codebook learning with contrastive learning and a novel information leakage loss to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.","authors":["Fatemeh Zargarbashi","Dhruv Agrawal","Jakob Buhmann","Martin Guay","Stelian Coros","Robert W. Sumner"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02700v4","updated":"2026-02-26T13:16:26Z","published":"2025-12-02T12:30:05Z","title":"VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm","summary":"Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.","authors":["Zhenkai Wu","Xiaowen Ma","Zhenliang Ni","Dengming Zhang","Han Shu","Xin Jiang","Xinghao Chen"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2602.22974v1","updated":"2026-02-26T13:13:43Z","published":"2026-02-26T13:13:43Z","title":"An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets","summary":"Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.","authors":["L. Martino","M. M. Garcia","P. S. Paradas","E. Curbelo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22968v1","updated":"2026-02-26T13:07:31Z","published":"2026-02-26T13:07:31Z","title":"Certified Circuits: Stability Guarantees for Mechanistic Circuits","summary":"Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!","authors":["Alaa Anani","Tobias Lorenz","Bernt Schiele","Mario Fritz","Jonas Fischer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.01031v2","updated":"2026-02-26T12:56:09Z","published":"2025-10-01T15:37:20Z","title":"Secure and reversible face anonymization with diffusion models","summary":"Face anonymization aims to protect sensitive identity information by altering faces while preserving visual realism and utility for downstream computer vision tasks. Current methods struggle to simultaneously ensure high image quality, strong security guarantees, and controlled reversibility for authorized identity recovery at a later time. To improve the image quality of generated anonymized faces, recent methods have adopted diffusion models. However, these new diffusion-based anonymization methods do not provide a mechanism to restrict de-anonymization to trusted parties, limiting their real-world applicability. In this paper, we present the first diffusion-based framework for secure, reversible face anonymization via secret-key conditioning. Our method injects the secret key directly into the diffusion process, enabling anonymization and authorized face reconstruction while preventing unauthorized de-anonymization. The use of deterministic forward and reverse diffusion steps guarantees exact identity recovery when the correct secret key is available. Experiments on CelebA-HQ and LFW demonstrate that our approach achieves better anonymization and de-anonymization capabilities than prior work. We also show that our method remains robust to incorrect or adversarial key de-anonymization. Our code will be made publicly available.","authors":["Pol Labarbarie","Vincent Itier","William Puech"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22960v1","updated":"2026-02-26T12:54:46Z","published":"2026-02-26T12:54:46Z","title":"UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models","summary":"World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.","authors":["Tianxing Xu","Zixuan Wang","Guangyuan Wang","Li Hu","Zhongyi Zhang","Peng Zhang","Bang Zhang","Song-Hai Zhang"],"pdf_url":"","comment":"Project Page: https://humanaigc.github.io/ucm-webpage/"},{"id":"http://arxiv.org/abs/2602.22959v1","updated":"2026-02-26T12:53:50Z","published":"2026-02-26T12:53:50Z","title":"Can Agents Distinguish Visually Hard-to-Separate Diseases in a Zero-Shot Setting? A Pilot Study","summary":"The rapid progress of multimodal large language models (MLLMs) has led to increasing interest in agent-based systems. While most prior work in medical imaging concentrates on automating routine clinical workflows, we study an underexplored yet clinically significant setting: distinguishing visually hard-to-separate diseases in a zero-shot setting. We benchmark representative agents on two imaging-only proxy diagnostic tasks, (1) melanoma vs. atypical nevus and (2) pulmonary edema vs. pneumonia, where visual features are highly confounded despite substantial differences in clinical management. We introduce a multi-agent framework based on contrastive adjudication. Experimental results show improved diagnostic performance (an 11-percentage-point gain in accuracy on dermoscopy data) and reduced unsupported claims on qualitative samples, although overall performance remains insufficient for clinical deployment. We acknowledge the inherent uncertainty in human annotations and the absence of clinical context, which further limit the translation to real-world settings. Within this controlled setting, this pilot study provides preliminary insights into zero-shot agent performance in visually confounded scenarios.","authors":["Zihao Zhao","Frederik Hauke","Juliana De Castilhos","Sven Nebelung","Daniel Truhn"],"pdf_url":"","comment":"Code available at https://github.com/TruhnLab/Contrastive-Agent-Reasoning"},{"id":"http://arxiv.org/abs/2602.22955v1","updated":"2026-02-26T12:50:32Z","published":"2026-02-26T12:50:32Z","title":"MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis","summary":"Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco","authors":["Feng Guo","Jiaxiang Liu","Yang Li","Qianqian Shi","Mingkun Xu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2511.05898v2","updated":"2026-02-26T12:49:30Z","published":"2025-11-08T07:45:21Z","title":"Q$^2$: Quantization-Aware Gradient Balancing and Attention Alignment for Low-Bit Quantization","summary":"Quantization-aware training (QAT) has achieved remarkable success in low-bit ($\\leq$4-bit) quantization for classification networks. However, when applied to more complex visual tasks such as object detection and image segmentation, performance still suffers significant degradation. A key cause of this limitation has been largely overlooked in the literature. In this work, we revisit this phenomenon from a new perspective and identify a major failure factor: gradient imbalance at feature fusion stages, induced by accumulated quantization errors. This imbalance biases the optimization trajectory and impedes convergence under low-bit quantization. Based on this diagnosis, we propose Q$^2$, a two-pronged framework comprising: (1) Quantization-aware Gradient Balancing Fusion (Q-GBFusion), a closed-loop mechanism that dynamically rebalances gradient contributions during feature fusion; and (2) Quantization-aware Attention Distribution Alignment (Q-ADA), a parameter-free supervision strategy that reconstructs the supervision distribution using semantic relevance and quantization sensitivity, yielding more stable and reliable supervision to stabilize training and accelerate convergence. Extensive experiments show that our method, as a plug-and-play and general strategy, can be integrated into various state-of-the-art QAT pipelines, achieving an average +2.5\\% mAP gain on object detection and a +3.7\\% mDICE improvement on image segmentation. Notably, it is applied only during training and introduces no inference-time overhead, making it highly practical for real-world deployment.","authors":["Zhaoyang Wang","Dong Wang"],"pdf_url":"","comment":"24 pages,6 figures"},{"id":"http://arxiv.org/abs/2502.02088v5","updated":"2026-02-26T12:46:39Z","published":"2025-02-04T08:14:34Z","title":"Dual-IPO: Dual-Iterative Preference Optimization for Text-to-Video Generation","summary":"Recent advances in video generation have enabled thrilling experiences in producing realistic videos driven by scalable diffusion transformers. However, they usually fail to produce satisfactory outputs that are aligned to users' authentic demands and preferences. In this work, we introduce Dual-Iterative Optimization (Dual-IPO), an iterative paradigm that sequentially optimizes both the reward model and the video generation model for improved synthesis quality and human preference alignment. For the reward model, our framework ensures reliable and robust reward signals via CoT-guided reasoning, voting-based self-consistency, and preference certainty estimation. Given this, we optimize video foundation models with guidance of signals from reward model's feedback, thus improving the synthesis quality in subject consistency, motion smoothness and aesthetic quality, etc. The reward model and video generation model complement each other and are progressively improved in the multi-round iteration, without requiring tediously manual preference annotations. Comprehensive experiments demonstrate that the proposed Dual-IPO can effectively and consistently improve the video generation quality of base model with various architectures and sizes, even help a model with only 2B parameters surpass a 5B one. Moreover, our analysis experiments and ablation studies identify the rational of our systematic design and the efficacy of each component.","authors":["Xiaomeng Yang","Mengping Yang","Jia Gong","Luozheng Qin","Zhiyu Tan","Hao Li"],"pdf_url":"","comment":"To appear in ICLR 2026, GitHub Code: https://github.com/SAIS-FUXI/IPO"},{"id":"http://arxiv.org/abs/2602.22949v1","updated":"2026-02-26T12:41:24Z","published":"2026-02-26T12:41:24Z","title":"OpenFS: Multi-Hand-Capable Fingerspelling Recognition with Implicit Signing-Hand Detection and Frame-Wise Letter-Conditioned Synthesis","summary":"Fingerspelling is a component of sign languages in which words are spelled out letter by letter using specific hand poses. Automatic fingerspelling recognition plays a crucial role in bridging the communication gap between Deaf and hearing communities, yet it remains challenging due to the signing-hand ambiguity issue, the lack of appropriate training losses, and the out-of-vocabulary (OOV) problem. Prior fingerspelling recognition methods rely on explicit signing-hand detection, which often leads to recognition failures, and on a connectionist temporal classification (CTC) loss, which exhibits the peaky behavior problem. To address these issues, we develop OpenFS, an open-source approach for fingerspelling recognition and synthesis. We propose a multi-hand-capable fingerspelling recognizer that supports both single- and multi-hand inputs and performs implicit signing-hand detection by incorporating a dual-level positional encoding and a signing-hand focus (SF) loss. The SF loss encourages cross-attention to focus on the signing hand, enabling implicit signing-hand detection during recognition. Furthermore, without relying on the CTC loss, we introduce a monotonic alignment (MA) loss that enforces the output letter sequence to follow the temporal order of the input pose sequence through cross-attention regularization. In addition, we propose a frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words. This generator enables the construction of a new synthetic benchmark, called FSNeo. Through comprehensive experiments, we demonstrate that our approach achieves state-of-the-art performance in recognition and validate the effectiveness of the proposed recognizer and generator. Codes and data are available in: https://github.com/JunukCha/OpenFS.","authors":["Junuk Cha","Jihyeon Kim","Han-Mu Park"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22948v1","updated":"2026-02-26T12:36:56Z","published":"2026-02-26T12:36:56Z","title":"ToProVAR: Efficient Visual Autoregressive Modeling via Tri-Dimensional Entropy-Aware Semantic Analysis and Sparsity Optimization","summary":"Visual Autoregressive(VAR) models enhance generation quality but face a critical efficiency bottleneck in later stages. In this paper, we present a novel optimization framework for VAR models that fundamentally differs from prior approaches such as FastVAR and SkipVAR. Instead of relying on heuristic skipping strategies, our method leverages attention entropy to characterize the semantic projections across different dimensions of the model architecture. This enables precise identification of parameter dynamics under varying token granularity levels, semantic scopes, and generation scales. Building on this analysis, we further uncover sparsity patterns along three critical dimensions-token, layer, and scale-and propose a set of fine-grained optimization strategies tailored to these patterns. Extensive evaluation demonstrates that our approach achieves aggressive acceleration of the generation process while significantly preserving semantic fidelity and fine details, outperforming traditional methods in both efficiency and quality. Experiments on Infinity-2B and Infinity-8B models demonstrate that ToProVAR achieves up to 3.4x acceleration with minimal quality loss, effectively mitigating the issues found in prior work. Our code will be made publicly available.","authors":["Jiayu Chen","Ruoyu Lin","Zihao Zheng","Jingxin Li","Maoliang Li","Guojie Luo","Xiang chen"],"pdf_url":"","comment":"ToProVAR is honored to be accepted by ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22941v1","updated":"2026-02-26T12:31:30Z","published":"2026-02-26T12:31:30Z","title":"Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings","summary":"Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.","authors":["Julian Ziegler","Daniel Matthes","Finn Gerdts","Patrick Frenzel","Torsten Warnke","Matthias Englert","Tina Koevari","Mirco Fuchs"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22938v1","updated":"2026-02-26T12:27:06Z","published":"2026-02-26T12:27:06Z","title":"pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation","summary":"Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.","authors":["Shentong Mo","Xufang Luo","Dongsheng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22932v1","updated":"2026-02-26T12:24:17Z","published":"2026-02-26T12:24:17Z","title":"MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding","summary":"Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.","authors":["Wenhui Tan","Xiaoyi Yu","Jiaze Li","Yijing Chen","Jianzhong Ju","Zhenbo Luo","Ruihua Song","Jian Luan"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2503.10981v4","updated":"2026-02-26T12:15:44Z","published":"2025-03-14T01:04:38Z","title":"CLIP-Free, Label Free, Unsupervised Concept Bottleneck Models","summary":"Concept Bottleneck Models (CBMs) map dense feature representations into human-interpretable concepts which are then combined linearly to make a prediction. However, modern CBMs rely on the CLIP model to obtain image-concept annotations, and it remains unclear how to design CBMs without the CLIP bottleneck. Methods that do not use CLIP instead require manual, labor intensive annotation to associate feature representations with concepts. Furthermore, all CBMs necessitate training a linear classifier to map the extracted concepts to class labels. In this work, we lift all three limitations simultaneously by proposing a method that converts any frozen visual classifier into a CBM without requiring image-concept labels (label-free), without relying on the CLIP model (CLIP-free), and by deriving the linear classifier in an unsupervised manner. Our method is formulated by aligning the original classifier's distribution (over discrete class indices) with its corresponding vision-language counterpart distribution derived from textual class names, while preserving the classifier's performance. The approach requires no ground-truth image-class annotations, and is highly data-efficient and preserves the classifier's reasoning process. Applied and tested on over 40 visual classifiers, our resulting unsupervised, label-free and CLIP-free CBM (U-F$^2$-CBM) sets a new state of the art, surpassing even supervised CLIP-based CBMs. We also show that our method can be used for zero-shot image captioning, outperforming existing methods based on CLIP, and achieving state-of-art.","authors":["Fawaz Sammani","Jonas Fischer","Nikos Deligiannis"],"pdf_url":"","comment":"CVPR 2026 (Findings)"},{"id":"http://arxiv.org/abs/2602.22923v1","updated":"2026-02-26T12:12:40Z","published":"2026-02-26T12:12:40Z","title":"WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents","summary":"While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.","authors":["Runwei Guan","Shaofeng Liang","Ningwei Ouyang","Weichen Fei","Shanliang Yao","Wei Dai","Chenhao Ge","Penglei Sun","Xiaohui Zhu","Tao Huang","Ryan Wen Liu","Hui Xiong"],"pdf_url":"","comment":"11 pages,8 figures"},{"id":"http://arxiv.org/abs/2409.02108v3","updated":"2026-02-26T12:11:30Z","published":"2024-09-03T17:59:05Z","title":"Unveiling Deep Shadows: A Survey and Benchmark on Image and Video Shadow Detection, Removal, and Generation in the Deep Learning Era","summary":"Shadows, formed by the occlusion of light, play an essential role in visual perception and directly influence scene understanding, image quality, and visual realism. This paper presents a unified survey and benchmark of deep-learning-based shadow detection, removal, and generation across images and videos. We introduce consistent taxonomies for architectures, supervision strategies, and learning paradigms; review major datasets and evaluation protocols; and re-train representative methods under standardized settings to enable fair comparison. Our benchmark reveals key findings, including inconsistencies in prior reports, strong dependence on model design and resolution, and limited cross-dataset generalization due to dataset bias. By synthesizing insights across the three tasks, we highlight shared illumination cues and priors that connect detection, removal, and generation. We further outline future directions involving unified all-in-one frameworks, semantics- and geometry-aware reasoning, shadow-based AIGC authenticity analysis, and the integration of physics-guided priors into multimodal foundation models. Corrected datasets, trained models, and evaluation tools are released to support reproducible research.","authors":["Xiaowei Hu","Zhenghao Xing","Tianyu Wang","Chi-Wing Fu","Pheng-Ann Heng"],"pdf_url":"","comment":"Accepted by International Journal of Computer Vision (IJCV). Publicly available results, trained models, and evaluation metrics at https://github.com/xw-hu/Unveiling-Deep-Shadows"},{"id":"http://arxiv.org/abs/2602.22920v1","updated":"2026-02-26T12:08:02Z","published":"2026-02-26T12:08:02Z","title":"OSDaR-AR: Enhancing Railway Perception Datasets via Multi-modal Augmented Reality","summary":"Although deep learning has significantly advanced the perception capabilities of intelligent transportation systems, railway applications continue to suffer from a scarcity of high-quality, annotated data for safety-critical tasks like obstacle detection. While photorealistic simulators offer a solution, they often struggle with the ``sim-to-real\" gap; conversely, simple image-masking techniques lack the spatio-temporal coherence required to obtain augmented single- and multi-frame scenes with the correct appearance and dimensions. This paper introduces a multi-modal augmented reality framework designed to bridge this gap by integrating photorealistic virtual objects into real-world railway sequences from the OSDaR23 dataset. Utilizing Unreal Engine 5 features, our pipeline leverages LiDAR point-clouds and INS/GNSS data to ensure accurate object placement and temporal stability across RGB frames. This paper also proposes a segmentation-based refinement strategy for INS/GNSS data to significantly improve the realism of the augmented sequences, as confirmed by the comparative study presented in the paper. Carefully designed augmented sequences are collected to produce OSDaR-AR, a public dataset designed to support the development of next-generation railway perception systems. The dataset is available at the following page: https://syndra.retis.santannapisa.it/osdarar.html","authors":["Federico Nesti","Gianluca D'Amico","Mauro Marinoni","Giorgio Buttazzo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22919v1","updated":"2026-02-26T12:06:43Z","published":"2026-02-26T12:06:43Z","title":"Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins","summary":"A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.","authors":["Haofan Wu","Nay Aung","Theodoros N. Arvanitis","Joao A. C. Lima","Steffen E. Petersen","Le Zhang"],"pdf_url":"","comment":"10 pages, 8 figures. Submitted to IEEE Transactions on Medical Imaging (TMI). Code will be released after review"},{"id":"http://arxiv.org/abs/2602.22917v1","updated":"2026-02-26T12:05:56Z","published":"2026-02-26T12:05:56Z","title":"Towards Multimodal Domain Generalization with Few Labels","summary":"Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.","authors":["Hongzhao Li","Hao Dong","Hualei Wan","Shupan Li","Mingliang Xu","Muhammad Haris Khan"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22897v1","updated":"2026-02-26T11:35:04Z","published":"2026-02-26T11:35:04Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Shijian Wang","Guanting Dong","Jiajie Jin","Hao Wang","Yinuo Wang","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21893v2","updated":"2026-02-26T11:19:43Z","published":"2026-02-25T13:21:49Z","title":"EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion","summary":"Accurate depth estimation plays a critical role in the navigation of endoscopic surgical robots, forming the foundation for 3D reconstruction and safe instrument guidance. Fine-tuning pretrained models heavily relies on endoscopic surgical datasets with precise depth annotations. While existing self-supervised depth estimation techniques eliminate the need for accurate depth annotations, their performance degrades in environments with weak textures and variable lighting, leading to sparse reconstruction with invalid depth estimation. Depth completion using sparse depth maps can mitigate these issues and improve accuracy. Despite the advances in depth completion techniques in general fields, their application in endoscopy remains limited. To overcome these limitations, we propose EndoDDC, an endoscopy depth completion method that integrates images, sparse depth information with depth gradient features, and optimizes depth maps through a diffusion model, addressing the issues of weak texture and light reflection in endoscopic environments. Extensive experiments on two publicly available endoscopy datasets show that our approach outperforms state-of-the-art models in both depth accuracy and robustness. This demonstrates the potential of our method to reduce visual errors in complex endoscopic environments. Our code will be released at https://github.com/yinheng-lin/EndoDDC.","authors":["Yinheng Lin","Yiming Huang","Beilei Cui","Long Bai","Huxin Gao","Hongliang Ren","Jiewen Lai"],"pdf_url":"","comment":"Accepted by ICRA 2026"},{"id":"http://arxiv.org/abs/2503.21449v2","updated":"2026-02-26T11:16:01Z","published":"2025-03-27T12:41:42Z","title":"Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving","summary":"Semantic scene understanding is crucial for robotics and computer vision applications. In autonomous driving, 3D semantic segmentation plays an important role for enabling safe navigation. Despite significant advances in the field, the complexity of collecting and annotating 3D data is a bottleneck in this developments. To overcome that data annotation limitation, synthetic simulated data has been used to generate annotated data on demand. There is still, however, a domain gap between real and simulated data. More recently, diffusion models have been in the spotlight, enabling close-to-real data synthesis. Those generative models have been recently applied to the 3D data domain for generating scene-scale data with semantic annotations. Still, those methods either rely on image projection or decoupled models trained with different resolutions in a coarse-to-fine manner. Such intermediary representations impact the generated data quality due to errors added in those transformations. In this work, we propose a novel approach able to generate 3D semantic scene-scale data without relying on any projection or decoupled trained multi-resolution models, achieving more realistic semantic scene data generation compared to previous state-of-the-art methods. Besides improving 3D semantic scene-scale data synthesis, we thoroughly evaluate the use of the synthetic scene samples as labeled data to train a semantic segmentation network. In our experiments, we show that using the synthetic annotated data generated by our method as training data together with the real semantic segmentation labels, leads to an improvement in the semantic segmentation model performance. Our results show the potential of generated scene-scale point clouds to generate more training data to extend existing datasets, reducing the data annotation effort. Our code is available at https://github.com/PRBonn/3DiSS.","authors":["Lucas Nunes","Rodrigo Marcuzzi","Jens Behley","Cyrill Stachniss"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.11727v2","updated":"2026-02-26T11:11:12Z","published":"2024-11-18T16:57:41Z","title":"Aligning Few-Step Diffusion Models with Dense Reward Difference Learning","summary":"Few-step diffusion models enable efficient high-resolution image synthesis but struggle to align with specific downstream objectives due to limitations of existing reinforcement learning (RL) methods in low-step regimes with limited state spaces and suboptimal sample quality. To address this, we propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework tailored for few-step diffusion models. SDPO introduces a dual-state trajectory sampling mechanism, tracking both noisy and predicted clean states at each step to provide dense reward feedback and enable low-variance, mixed-step optimization. For further efficiency, we develop a latent similarity-based dense reward prediction strategy to minimize costly dense reward queries. Leveraging these dense rewards, SDPO optimizes a dense reward difference learning objective that enables more frequent and granular policy updates. Additional refinements, including stepwise advantage estimates, temporal importance weighting, and step-shuffled gradient updates, further enhance long-term dependency, low-step priority, and gradient stability. Our experiments demonstrate that SDPO consistently delivers superior reward-aligned results across diverse few-step settings and tasks. Code is available at https://github.com/ZiyiZhang27/sdpo.","authors":["Ziyi Zhang","Li Shen","Sen Zhang","Deheng Ye","Yong Luo","Miaojing Shi","Dongjing Shan","Bo Du","Dacheng Tao"],"pdf_url":"","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2509.04403v2","updated":"2026-02-26T11:09:21Z","published":"2025-09-04T17:13:59Z","title":"Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios","summary":"Multimodal large language models (MLLMs) are rapidly evolving, presenting increasingly complex safety challenges. However, current dataset construction methods, which are risk-oriented, fail to cover the growing complexity of real-world multimodal safety scenarios (RMS). And due to the lack of a unified evaluation metric, their overall effectiveness remains unproven. This paper introduces a novel image-oriented self-adaptive dataset construction method for RMS, which starts with images and end constructing paired text and guidance responses. Using the image-oriented method, we automatically generate an RMS dataset comprising 35k image-text pairs with guidance responses. Additionally, we introduce a standardized safety dataset evaluation metric: fine-tuning a safety judge model and evaluating its capabilities on other safety datasets.Extensive experiments on various tasks demonstrate the effectiveness of the proposed image-oriented pipeline. The results confirm the scalability and effectiveness of the image-oriented approach, offering a new perspective for the construction of real-world multimodal safety datasets. The dataset is presented at https://huggingface.co/datasets/NewCityLetter/RMS2/tree/main.","authors":["Jingen Qu","Lijun Li","Bo Zhang","Yichen Yan","Jing Shao"],"pdf_url":"","comment":"Accepted at EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2602.22867v1","updated":"2026-02-26T11:07:51Z","published":"2026-02-26T11:07:51Z","title":"SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation","summary":"Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\\pm 35^\\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.","authors":["Qinfeng Zhu","Yunxi Jiang","Lei Fan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.21058v2","updated":"2026-02-26T11:05:29Z","published":"2025-12-24T08:52:08Z","title":"Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control","summary":"In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The dataset and code can be obtained from https://github.com/Hanminghao/UniPath.","authors":["Minghao Han","Yichen Liu","Yizhou Liu","Zizhi Chen","Jingqun Tang","Xuecheng Wu","Dingkang Yang","Lihua Zhang"],"pdf_url":"","comment":"accepted by CVPR 2026; 32 pages, 17 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2602.22862v1","updated":"2026-02-26T10:56:01Z","published":"2026-02-26T10:56:01Z","title":"GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion","summary":"This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.","authors":["Enda Xiang","Haoxiang Ma","Xinzhu Ma","Zicheng Liu","Di Huang"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.22859v1","updated":"2026-02-26T10:53:57Z","published":"2026-02-26T10:53:57Z","title":"From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models","summary":"As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.","authors":["Hongrui Jia","Chaoya Jiang","Shikun Zhang","Wei Ye"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22843v1","updated":"2026-02-26T10:32:08Z","published":"2026-02-26T10:32:08Z","title":"A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling","summary":"Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a \"scale-at-all-costs\" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.","authors":["Chong Wang","Yabin Zhang","Yunhe Gao","Maya Varma","Clemence Mottez","Faidra Patsatzi","Jiaming Liu","Jin Long","Jean-Benoit Delbrouck","Sergios Gatidis","Akshay S. Chaudhari","Curtis P. Langlotz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.02686v2","updated":"2026-02-26T10:18:58Z","published":"2025-12-02T12:14:19Z","title":"ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data","summary":"Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.","authors":["Yuxing Liu","Zheng Li","Huanhuan Liang","Ji Zhang","Zeyu Sun","Yong Liu"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2602.22831v1","updated":"2026-02-26T10:17:57Z","published":"2026-02-26T10:17:57Z","title":"Moral Preferences of LLMs Under Directed Contextual Influence","summary":"Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.","authors":["Phil Blandfort","Tushar Karayil","Urja Pawar","Robert Graham","Alex McKenzie","Dmitrii Krasheninnikov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22829v1","updated":"2026-02-26T10:12:25Z","published":"2026-02-26T10:12:25Z","title":"Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification","summary":"Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.","authors":["G. A. S. L Ranasinghe","J. A. S. T. Jayakody","M. C. L. De Silva","G. Thilakarathne","G. M. R. I. Godaliyadda","H. M. V. R. Herath","M. P. B. Ekanayake","S. K. Navaratnarajah"],"pdf_url":"","comment":"Under Review at IEEE Access. 17 pages, 15 figures"},{"id":"http://arxiv.org/abs/2510.04504v2","updated":"2026-02-26T10:10:17Z","published":"2025-10-06T05:45:56Z","title":"Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation","summary":"Diffusion models have achieved impressive results in generating high-quality images. Yet, they often struggle to faithfully align the generated images with the input prompts. This limitation is associated with synchronous denoising, where all pixels simultaneously evolve from random noise to clear images. As a result, during generation, the prompt-related regions can only reference the unrelated regions at the same noise level, failing to obtain clear context and ultimately impairing text-to-image alignment. To address this issue, we propose asynchronous diffusion models -- a novel framework that allocates distinct timesteps to different pixels and reformulates the pixel-wise denoising process. By dynamically modulating the timestep schedules of individual pixels, prompt-related regions are denoised more gradually than unrelated regions, thereby allowing them to leverage clearer inter-pixel context. Consequently, these prompt-related regions achieve better alignment in the final images. Extensive experiments demonstrate that our asynchronous diffusion models can significantly improve text-to-image alignment across diverse prompts. The code repository for this work is available at https://github.com/hu-zijing/AsynDM.","authors":["Zijing Hu","Yunze Tong","Fengda Zhang","Junkun Yuan","Jun Xiao","Kun Kuang"],"pdf_url":"","comment":"Accepted to ICLR 2026, 25 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.10922v3","updated":"2026-02-26T10:05:51Z","published":"2024-10-14T12:08:12Z","title":"Towards Privacy-Guaranteed Label Unlearning in Vertical Federated Learning: Few-Shot Forgetting without Disclosure","summary":"This paper addresses the critical challenge of unlearning in Vertical Federated Learning (VFL), a setting that has received far less attention than its horizontal counterpart. Specifically, we propose the first method tailored to \\textit{label unlearning} in VFL, where labels play a dual role as both essential inputs and sensitive information. To this end, we employ a representation-level manifold mixup mechanism to generate synthetic embeddings for both unlearned and retained samples. This is to provide richer signals for the subsequent gradient-based label forgetting and recovery steps. These augmented embeddings are then subjected to gradient-based label forgetting, effectively removing the associated label information from the model. To recover performance on the retained data, we introduce a recovery-phase optimization step that refines the remaining embeddings. This design achieves effective label unlearning while maintaining computational efficiency. We validate our method through extensive experiments on diverse datasets, including MNIST, CIFAR-10, CIFAR-100, ModelNet, Brain Tumor MRI, COVID-19 Radiography, and Yahoo Answers demonstrate strong efficacy and scalability. Overall, this work establishes a new direction for unlearning in VFL, showing that re-imagining mixup as an efficient mechanism can unlock practical and utility-preserving unlearning. The code is publicly available at \\href{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}","authors":["Hanlin Gu","Hong Xi Tae","Chee Seng Chan","Lixin Fan"],"pdf_url":"","comment":"We introduce the first method for label unlearning in vertical federated learning (VFL), focused on preventing label leakage by the active party"},{"id":"http://arxiv.org/abs/2602.22821v1","updated":"2026-02-26T10:03:31Z","published":"2026-02-26T10:03:31Z","title":"CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation","summary":"Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.","authors":["Tong Wang","Yaolei Qi","Siwen Wang","Imran Razzak","Guanyu Yang","Yutong Xie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22819v1","updated":"2026-02-26T10:01:37Z","published":"2026-02-26T10:01:37Z","title":"Face Time Traveller : Travel Through Ages Without Losing Identity","summary":"Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.","authors":["Purbayan Kar","Ayush Ghadiya","Vishal Chudasama","Pankaj Wasnik","C. V. Jawahar"],"pdf_url":"","comment":"Accepted at CVPR 2026 (Findings Track)"},{"id":"http://arxiv.org/abs/2602.19424v2","updated":"2026-02-26T09:59:43Z","published":"2026-02-23T01:43:32Z","title":"Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images","summary":"Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.","authors":["Yuxuan Yang","Zhonghao Yan","Yi Zhang","Bo Yun","Muxi Diao","Guowei Zhao","Kongming Liang","Wenbin Li","Zhanyu Ma"],"pdf_url":"","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2509.16552v2","updated":"2026-02-26T09:54:58Z","published":"2025-09-20T06:36:30Z","title":"ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting","summary":"3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.","authors":["Xiaoyang Yan","Muleilan Pei","Shaojie Shen"],"pdf_url":"","comment":"Accepted by ICRA 2026"},{"id":"http://arxiv.org/abs/2602.22809v1","updated":"2026-02-26T09:46:06Z","published":"2026-02-26T09:46:06Z","title":"PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning","summary":"With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.","authors":["Mingde Yao","Zhiyuan You","Tam-King Man","Menglu Wang","Tianfan Xue"],"pdf_url":"","comment":"A fully automated, intelligent photo-editing agent that autonomously plans multi-step aesthetic enhancements, smartly chooses diverse editing tools, and enables everyday users to achieve professional-looking results without crafting complex prompts. Project page: https://github.com/mdyao/PhotoAgent"},{"id":"http://arxiv.org/abs/2602.19190v2","updated":"2026-02-26T09:45:03Z","published":"2026-02-22T13:40:17Z","title":"FUSAR-GPT : A Spatiotemporal Feature-Embedded and Two-Stage Decoupled Visual Language Model for SAR Imagery","summary":"Research on the intelligent interpretation of all-weather, all-time Synthetic Aperture Radar (SAR) is crucial for advancing remote sensing applications. In recent years, although Visual Language Models (VLMs) have demonstrated strong open-world understanding capabilities on RGB images, their performance is severely limited when directly applied to the SAR field due to the complexity of the imaging mechanism, sensitivity to scattering features, and the scarcity of high-quality text corpora. To systematically address this issue, we constructed the inaugural SAR Image-Text-AlphaEarth feature triplet dataset and developed FUSAR-GPT, a VLM specifically for SAR. FUSAR-GPT innovatively introduces a geospatial baseline model as a 'world knowledge' prior and embeds multi-source remote-sensing temporal features into the model's visual backbone via 'spatiotemporal anchors', enabling dynamic compensation for the sparse representation of targets in SAR images. Furthermore, we designed a two-stage SFT strategy to decouple the knowledge injection and task execution of large models. The spatiotemporal feature embedding and the two-stage decoupling paradigm enable FUSAR-GPT to achieve state-of-the-art performance across several typical remote sensing visual-language benchmark tests, significantly outperforming mainstream baseline models by over 12%.","authors":["Xiaokun Zhang","Yi Yang","Ziqi Ye"," Baiyun","Xiaorong Guo","Qingchen Fang","Ruyi Zhang","Xinpeng Zhou","Haipeng Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22800v1","updated":"2026-02-26T09:37:27Z","published":"2026-02-26T09:37:27Z","title":"GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation","summary":"Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.","authors":["Hanliang Du","Zhangji Lu","Zewei Cai","Qijian Tang","Qifeng Yu","Xiaoli Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22791v1","updated":"2026-02-26T09:25:52Z","published":"2026-02-26T09:25:52Z","title":"Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning","summary":"Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.","authors":["Taishu Arashima","Hiroshi Kera","Kazuhiko Kawamoto"],"pdf_url":"","comment":"11 pages main, 5 pages supplementary material"},{"id":"http://arxiv.org/abs/2602.22785v1","updated":"2026-02-26T09:19:59Z","published":"2026-02-26T09:19:59Z","title":"SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation","summary":"We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.","authors":["Ling Wang","Hao-Xiang Guo","Xinzhou Wang","Fuchun Sun","Kai Sun","Pengkun Liu","Hang Xiao","Zhong Wang","Guangyuan Fu","Eric Li","Yang Liu","Yikai Wang"],"pdf_url":"","comment":"published at iclr 2026"},{"id":"http://arxiv.org/abs/2510.06139v2","updated":"2026-02-26T09:15:39Z","published":"2025-10-07T17:14:10Z","title":"Deforming Videos to Masks: Flow Matching for Referring Video Segmentation","summary":"Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.","authors":["Zanyi Wang","Dengyang Jiang","Liuzhuozheng Li","Sizhe Dang","Chengzu Li","Harry Yang","Guang Dai","Mengmeng Wang","Jingdong Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22779v1","updated":"2026-02-26T09:15:34Z","published":"2026-02-26T09:15:34Z","title":"TrajTok: Learning Trajectory Tokens enables better Video Understanding","summary":"Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.","authors":["Chenhao Zheng","Jieyu Zhang","Jianing Zhang","Weikai Huang","Ashutosh Kumar","Quan Kong","Oncel Tuzel","Chun-Liang Li","Ranjay Krishna"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2501.16904v2","updated":"2026-02-26T09:05:50Z","published":"2025-01-28T12:44:27Z","title":"Diffusion or Non-Diffusion Adversarial Defenses: Rethinking the Relation between Classifier and Adversarial Purifier","summary":"Adversarial defense research continues to face challenges in combating against advanced adversarial attacks, yet with diffusion models increasingly favoring their defensive capabilities. Unlike most prior studies that focus on diffusion models for test-time defense, we explore the generalization loss in classifiers caused by diffusion models. We compare diffusion-based and non-diffusion-based adversarial purifiers, demonstrating that non-diffusion models can also achieve well performance under a practical setting of non-adaptive attack. While non-diffusion models show promising adversarial robustness, they particularly excel in defense transferability and color generalization without relying on additional data beyond the training set. Notably, a non-diffusion model trained on CIFAR-10 achieves state-of-the-art performance when tested directly on ImageNet, surpassing existing diffusion-based models trained specifically on ImageNet.","authors":["Yuan-Chih Chen","Chun-Shien Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.09666v4","updated":"2026-02-26T09:05:04Z","published":"2025-09-11T17:57:59Z","title":"Unified Multimodal Models as Auto-Encoders","summary":"Image-to-text (I2T) understanding and text-to-image (T2I) generation are two fundamental, important yet traditionally isolated multimodal tasks. Despite their intrinsic connection, existing approaches typically optimize them independently, missing the opportunity for mutual enhancement. In this paper, we argue that the both tasks can be connected under a shared Auto-Encoder perspective, where text serves as the intermediate latent representation bridging the two directions - encoding images into textual semantics (I2T) and decoding text back into images (T2I). Our key insight is that if the encoder truly \"understands\" the image, it should capture all essential structure, and if the decoder truly \"understands\" the text, it should recover that structure faithfully. Building upon this principle, we propose Unified-GRPO, a post-training method based on reinforcement learning that jointly optimizes both modules through reconstructive rewards, maximizing the semantic consistency between the input and the generated images. Under this reconstruction objective, the encoder is encouraged to extract as much accurate and comprehensive semantic information from the input image to maximize reconstruction quality, while the decoder is simultaneously optimized to generate conditioned on the encoder's prior, enabling a self-evolving improvement. Empirically, we find that using text as the intermediate representation and training under a reconstructive RL paradigm effectively benefits both I2T and T2I. The I2T module gains stronger fine-grained visual perception, such as small-object recognition, grounding, etc, while its dense embeddings and language priors, in turn, provide richer semantic signals that improve T2I fidelity and complex instruction following. These results demonstrate that the reconstructive RL establishes a mutually reinforcing cross-modal synergy within the auto-encoding framework.","authors":["Zhiyuan Yan","Kaiqing Lin","Zongjian Li","Junyan Ye","Hui Han","Haochen Wang","Zhendong Wang","Bin Lin","Hao Li","Xinyan Xiao","Jingdong Wang","Haifeng Wang","Li Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.23276v2","updated":"2026-02-26T08:51:13Z","published":"2026-01-30T18:47:54Z","title":"Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging","summary":"Astronomical imaging remains noise-limited under practical observing conditions. Standard calibration pipelines remove structured artifacts but largely leave stochastic noise unresolved. Although learning-based denoising has shown strong potential, progress is constrained by scarce paired training data and the requirement for physically interpretable models in scientific workflows. We propose a physics-based noise synthesis framework tailored to CCD noise formation in the telescope. The pipeline models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we stack multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for supervised learning. Extensive experiments on our real-world multi-band dataset curated from two ground-based telescopes demonstrate the effectiveness of our framework in both photometric and scientific accuracy.","authors":["Shuhong Liu","Xining Ge","Ziying Gu","Quanfeng Xu","Lin Gu","Ziteng Cui","Xuangeng Chu","Jun Liu","Dong Li","Tatsuya Harada"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.12099v2","updated":"2026-02-26T08:50:49Z","published":"2026-02-12T15:55:19Z","title":"GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning","summary":"Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.","authors":[" GigaBrain Team","Boyuan Wang","Bohan Li","Chaojun Ni","Guan Huang","Guosheng Zhao","Hao Li","Jie Li","Jindi Lv","Jingyu Liu","Lv Feng","Mingming Yu","Peng Li","Qiuping Deng","Tianze Liu","Xinyu Zhou","Xinze Chen","Xiaofeng Wang","Yang Wang","Yifan Li","Yifei Nie","Yilong Li","Yukun Zhou","Yun Ye","Zhichao Liu","Zheng Zhu"],"pdf_url":"","comment":"https://gigabrain05m.github.io/"},{"id":"http://arxiv.org/abs/2503.13587v2","updated":"2026-02-26T08:48:02Z","published":"2025-03-17T17:59:50Z","title":"UniFuture: A 4D Driving World Model for Future Generation and Perception","summary":"We present UniFuture, a unified 4D Driving World Model designed to simulate the dynamic evolution of the 3D physical world. Unlike existing driving world models that focus solely on 2D pixel-level video generation (lacking geometry) or static perception (lacking temporal dynamics), our approach bridges appearance and geometry to construct a holistic 4D representation. Specifically, we treat future RGB images and depth maps as coupled projections of the same 4D reality and model them jointly within a single framework. To achieve this, we introduce a Dual-Latent Sharing (DLS) scheme, which maps visual and geometric modalities into a shared spatio-temporal latent space, implicitly entangling texture with structure. Furthermore, we propose a Multi-scale Latent Interaction (MLI) mechanism, which enforces bidirectional consistency: geometry constrains visual synthesis to prevent structural hallucinations, while visual semantics refine geometric estimation. During inference, UniFuture can forecast high-fidelity, geometrically consistent 4D scene sequences (image-depth pairs) from a single current frame. Extensive experiments on the nuScenes and Waymo datasets demonstrate that our method outperforms specialized models in both future generation and geometry perception, highlighting the efficacy of unified 4D modeling for autonomous driving. The code is available at https://github.com/dk-liang/UniFuture.","authors":["Dingkang Liang","Dingyuan Zhang","Xin Zhou","Sifan Tu","Tianrui Feng","Xiaofan Li","Yumeng Zhang","Mingyang Du","Xiao Tan","Xiang Bai"],"pdf_url":"","comment":"Accepted by ICRA 2026"},{"id":"http://arxiv.org/abs/2602.22759v1","updated":"2026-02-26T08:47:48Z","published":"2026-02-26T08:47:48Z","title":"Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval","summary":"Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.","authors":["Yuan-Chih Chen","Chun-Shien Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.10611v3","updated":"2026-02-26T08:46:23Z","published":"2026-01-15T17:27:44Z","title":"Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding","summary":"Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).","authors":["Christopher Clark","Jieyu Zhang","Zixian Ma","Jae Sung Park","Mohammadreza Salehi","Rohun Tripathi","Sangho Lee","Zhongzheng Ren","Chris Dongjoo Kim","Yinuo Yang","Vincent Shao","Yue Yang","Weikai Huang","Ziqi Gao","Taira Anderson","Jianrui Zhang","Jitesh Jain","George Stoica","Winson Han","Ali Farhadi","Ranjay Krishna"],"pdf_url":"","comment":"Fixed results in Table 7"},{"id":"http://arxiv.org/abs/2602.22745v1","updated":"2026-02-26T08:34:09Z","published":"2026-02-26T08:34:09Z","title":"SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation","summary":"Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.","authors":["Fengming Liu","Tat-Jen Cham","Chuanxia Zheng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2406.09293v4","updated":"2026-02-26T08:31:53Z","published":"2024-06-13T16:29:46Z","title":"StableMaterials: Enhancing Diversity in Material Generation via Semi-Supervised Learning","summary":"We introduce StableMaterials, a novel approach for generating photorealistic physical-based rendering (PBR) materials that integrate semi-supervised learning with Latent Diffusion Models (LDMs). Our method employs adversarial training to distill knowledge from existing large-scale image generation models, minimizing the reliance on annotated data and enhancing the diversity in generation. This distillation approach aligns the distribution of the generated materials with that of image textures from an SDXL model, enabling the generation of novel materials that are not present in the initial training dataset. Furthermore, we employ a diffusion-based refiner model to improve the visual quality of the samples and achieve high-resolution generation. Finally, we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps. We detail the architecture and training process of StableMaterials, the integration of semi-supervised training within existing LDM frameworks and show the advantages of our approach. Comparative evaluations with state-of-the-art methods show the effectiveness of StableMaterials, highlighting its potential applications in computer graphics and beyond. StableMaterials is publicly available at https://gvecchio.com/stablematerials.","authors":["Giuseppe Vecchio"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22742v1","updated":"2026-02-26T08:29:25Z","published":"2026-02-26T08:29:25Z","title":"ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control","summary":"Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.","authors":["Akihisa Watanabe","Qing Yu","Edgar Simo-Serra","Kent Fujiwara"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22740v1","updated":"2026-02-26T08:29:04Z","published":"2026-02-26T08:29:04Z","title":"AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation","summary":"Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios","authors":["Tongfei Chen","Shuo Yang","Yuguang Yang","Linlin Yang","Runtang Guo","Changbai Li","He Long","Chunyu Xie","Dawei Leng","Baochang Zhang"],"pdf_url":"","comment":"ICLR 2026 conference paper"},{"id":"http://arxiv.org/abs/2602.22734v1","updated":"2026-02-26T08:16:47Z","published":"2026-02-26T08:16:47Z","title":"Asymmetric Idiosyncrasies in Multimodal Models","summary":"In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.","authors":["Muzi Tao","Chufan Shi","Huijuan Wang","Shengbang Tong","Xuezhe Ma"],"pdf_url":"","comment":"Project page: https://muzi-tao.github.io/asymmetric-idiosyncrasies/"},{"id":"http://arxiv.org/abs/2602.22731v1","updated":"2026-02-26T08:13:47Z","published":"2026-02-26T08:13:47Z","title":"Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring","summary":"Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.","authors":["Miguel Ángel Muñoz-Bañón","Nived Chebrolu","Sruthi M. Krishna Moorthy","Yifu Tao","Fernando Torres","Roberto Salguero-Gómez","Maurice Fallon"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.05115v2","updated":"2026-02-26T08:08:56Z","published":"2025-08-07T07:47:16Z","title":"RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer","summary":"Audio-driven portrait animation aims to synthesize realistic and natural talking head videos from an input audio signal and a single reference image. While existing methods achieve high-quality results by leveraging high-dimensional intermediate representations and explicitly modeling motion dynamics, their computational complexity renders them unsuitable for real-time deployment. Real-time inference imposes stringent latency and memory constraints, often necessitating the use of highly compressed latent representations. However, operating in such compact spaces hinders the preservation of fine-grained spatiotemporal details, thereby complicating audio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a unified framework for generating high-quality talking portraits under real-time constraints. Specifically, RAP introduces a hybrid attention mechanism for fine-grained audio control, and a static-dynamic training-inference paradigm that avoids explicit motion supervision. Through these techniques, RAP achieves precise audio-driven control, mitigates long-term temporal drift, and maintains high visual fidelity. Extensive experiments demonstrate that RAP achieves state-of-the-art performance while operating under real-time constraints.","authors":["Fangyu Du","Taiqing Li","Qian Qiao","Tan Yu","Ziwei Zhang","Dingcheng Zhen","Xu Jia","Yang Yang","Shunshun Yin","Siyuan Liu"],"pdf_url":"","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.22727v1","updated":"2026-02-26T08:08:25Z","published":"2026-02-26T08:08:25Z","title":"HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models","summary":"Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.","authors":["Yangguang Lin","Quan Fang","Yufei Li","Jiachen Sun","Junyu Gao","Jitao Sang"],"pdf_url":"","comment":"accepted at CVPR 2026"},{"id":"http://arxiv.org/abs/2510.12099v2","updated":"2026-02-26T08:03:13Z","published":"2025-10-14T03:06:28Z","title":"G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior","summary":"Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, DeepBlending and Mip-NeRF 360 show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at https://dali-jack.github.io/g4splat-web/.","authors":["Junfeng Ni","Yixin Chen","Zhifei Yang","Yu Liu","Ruijie Lu","Song-Chun Zhu","Siyuan Huang"],"pdf_url":"","comment":"ICLR'26. Project page: https://dali-jack.github.io/g4splat-web/"},{"id":"http://arxiv.org/abs/2502.14377v5","updated":"2026-02-26T07:56:19Z","published":"2025-02-20T09:10:05Z","title":"RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers","summary":"The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the \"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta.","authors":["Ke Cao","Jing Wang","Ao Ma","Jiasong Feng","Xuanhua He","Run Ling","Haowei Liu","Jian Lu","Wei Feng","Haozhe Wang","Hongjuan Pei","Yihua Shao","Zhanjie Zhang","Jie Zhang"],"pdf_url":"","comment":"AAAI 2026"},{"id":"http://arxiv.org/abs/2602.22717v1","updated":"2026-02-26T07:42:25Z","published":"2026-02-26T07:42:25Z","title":"IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling","summary":"Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.","authors":["Shuoqi Chen","Yujia Wu","Geoffrey P. Luke"],"pdf_url":"","comment":"12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint"},{"id":"http://arxiv.org/abs/2602.22716v1","updated":"2026-02-26T07:42:15Z","published":"2026-02-26T07:42:15Z","title":"SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs","summary":"3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.","authors":["Guanting Ye","Qiyan Zhao","Wenhao Yu","Liangyu Yuan","Mingkai Li","Xiaofeng Zhang","Jianmin Ji","Yanyong Zhang","Qing Jiang","Ka-Veng Yuen"],"pdf_url":"","comment":"CVPR 2026"},{"id":"http://arxiv.org/abs/2509.17562v4","updated":"2026-02-26T07:40:53Z","published":"2025-09-22T10:57:42Z","title":"Visual Instruction Pretraining for Domain-Specific Foundation Models","summary":"Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at https://github.com/zcablii/ViTP.","authors":["Yuxuan Li","Yicheng Zhang","Wenhao Tang","Yimian Dai","Ming-Ming Cheng","Xiang Li","Jian Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.26454v3","updated":"2026-02-26T07:39:57Z","published":"2025-09-30T16:08:59Z","title":"Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection and Defect Detection","summary":"Ensuring that every vehicle leaving a modern production line is built to the correct \\emph{variant} specification and is free from visible defects is an increasingly complex challenge. We present the \\textbf{Automated Vehicle Inspection (AVI)} platform, an end-to-end, \\emph{multi-view} perception system that couples deep-learning detectors with a semantic rule engine to deliver \\emph{variant-aware} quality control in real time. Eleven synchronized cameras capture a full 360° sweep of each vehicle; task-specific views are then routed to specialised modules: YOLOv8 for part detection, EfficientNet for ICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for scratch-and-dent segmentation. A view-aware fusion layer standardises evidence, while a VIN-conditioned rule engine compares detected features against the expected manifest, producing an interpretable pass/fail report in \\(\\approx\\! 300\\,\\text{ms}\\). On a mixed data set of Original Equipment Manufacturer(OEM) vehicle data sets of four distinct models plus public scratch/dent images, AVI achieves \\textbf{ 93 \\%} verification accuracy, \\textbf{86 \\%} defect-detection recall, and sustains \\(\\mathbf{3.3}\\) vehicles/min, surpassing single-view or no segmentation baselines by large margins. To our knowledge, this is the first publicly reported system that unifies multi-camera feature validation with defect detection in a deployable automotive setting in industry.","authors":["Yash Kulkarni","Raman Jha","Renu Kachhoria"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22712v1","updated":"2026-02-26T07:37:45Z","published":"2026-02-26T07:37:45Z","title":"UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects","summary":"Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.","authors":["Yuankai Chen","Kai Lin","Qihong Wu","Xinxuan Yang","Jiashuo Lai","Ruoen Chen","Haonan Shi","Minfan He","Meihua Wang"],"pdf_url":"","comment":"6 pages, 6 figures, published to 2026 International Conference on Computer Supported Cooperative Work in Design"},{"id":"http://arxiv.org/abs/2602.01577v2","updated":"2026-02-26T07:17:49Z","published":"2026-02-02T03:14:05Z","title":"Visible Light Positioning With Lamé Curve LEDs: A Generic Approach for Camera Pose Estimation","summary":"Camera-based visible light positioning (VLP) is a promising technique for accurate and low-cost indoor camera pose estimation (CPE). To reduce the number of required light-emitting diodes (LEDs), advanced methods commonly exploit LED shape features for positioning. Although interesting, they are typically restricted to a single LED geometry, leading to failure in heterogeneous LED-shape scenarios. To address this challenge, this paper investigates Lamé curves as a unified representation of common LED shapes and proposes a generic VLP algorithm using Lamé curve-shaped LEDs, termed LC-VLP. In the considered system, multiple ceiling-mounted Lamé curve-shaped LEDs periodically broadcast their curve parameters via visible light communication, which are captured by a camera-equipped receiver. Based on the received LED images and curve parameters, the receiver can estimate the camera pose using LC-VLP. Specifically, an LED database is constructed offline to store the curve parameters, while online positioning is formulated as a nonlinear least-squares problem and solved iteratively. To provide a reliable initialization, a correspondence-free perspective-n-points (FreePnP) algorithm is further developed, enabling approximate CPE without any pre-calibrated reference points. The performance of LC-VLP is verified by both simulations and experiments. Simulations show that LC-VLP outperforms state-of-the-art methods in both circular- and rectangular-LED scenarios, achieving reductions of over 40% in position error and 25% in rotation error. Experiments further show that LC-VLP can achieve an average position accuracy of less than 4 cm.","authors":["Wenxuan Pan","Yang Yang","Dong Wei","Zhiyu Zhu","Jintao Wang","Huan Wu","Yao Nie"],"pdf_url":"","comment":"Submitted to an IEEE journal for possible publication"},{"id":"http://arxiv.org/abs/2602.22695v1","updated":"2026-02-26T07:17:49Z","published":"2026-02-26T07:17:49Z","title":"GFRRN: Explore the Gaps in Single Image Reflection Removal","summary":"Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.","authors":["Yu Chen","Zewei He","Xingyu Liu","Zixuan Chen","Zheming Lu"],"pdf_url":"","comment":"CVPR26"},{"id":"http://arxiv.org/abs/2509.21965v3","updated":"2026-02-26T07:13:04Z","published":"2025-09-26T06:52:35Z","title":"PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data","summary":"Segmenting 3D objects into parts is a long-standing challenge in computer vision. To overcome taxonomy constraints and generalize to unseen 3D objects, recent works turn to open-world part segmentation. These approaches typically transfer supervision from 2D foundation models, such as SAM, by lifting multi-view masks into 3D. However, this indirect paradigm fails to capture intrinsic geometry, leading to surface-only understanding, uncontrolled decomposition, and limited generalization. We present PartSAM, the first promptable part segmentation model trained natively on large-scale 3D data. Following the design philosophy of SAM, PartSAM employs an encoder-decoder architecture in which a triplane-based dual-branch encoder produces spatially structured tokens for scalable part-aware representation learning. To enable large-scale supervision, we further introduce a model-in-the-loop annotation pipeline that curates over five million 3D shape-part pairs from online assets, providing diverse and fine-grained labels. This combination of scalable architecture and diverse 3D data yields emergent open-world capabilities: with a single prompt, PartSAM achieves highly accurate part identification, and in a Segment-Every-Part mode, it automatically decomposes shapes into both surface and internal structures. Extensive experiments show that PartSAM outperforms state-of-the-art methods by large margins across multiple benchmarks, marking a decisive step toward foundation models for 3D part understanding.","authors":["Zhe Zhu","Le Wan","Rui Xu","Yiheng Zhang","Honghua Chen","Zhiyang Dou","Cheng Lin","Yuan Liu","Mingqiang Wei"],"pdf_url":"","comment":"ICLR 2026. Project Page: https://czvvd.github.io/PartSAMPage/"},{"id":"http://arxiv.org/abs/2602.22689v1","updated":"2026-02-26T07:07:11Z","published":"2026-02-26T07:07:11Z","title":"No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings","summary":"Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.","authors":["Joonsung Jeon","Woo Jae Kim","Suhyeon Ha","Sooel Son","Sung-Eui Yoon"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22683v1","updated":"2026-02-26T06:55:48Z","published":"2026-02-26T06:55:48Z","title":"SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses","summary":"The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.","authors":["Zhuohang Jiang","Xu Yuan","Haohao Qu","Shanru Lin","Kanglong Liu","Wenqi Fan","Qing Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22678v1","updated":"2026-02-26T06:51:25Z","published":"2026-02-26T06:51:25Z","title":"ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport","summary":"Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.","authors":["Quoc-Khang Tran","Minh-Thien Nguyen","Nguyen-Khang Pham"],"pdf_url":"","comment":"Preprint submitted to Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2512.14187v2","updated":"2026-02-26T06:50:15Z","published":"2025-12-16T08:33:08Z","title":"Establishing Stochastic Object Models from Noisy Data via Ambient Measurement-Integrated Diffusion","summary":"Task-based measures of image quality (IQ) are critical for evaluating medical imaging systems, which must account for randomness including anatomical variability. Stochastic object models (SOMs) provide a statistical description of such variability, but conventional mathematical SOMs fail to capture realistic anatomy, while data-driven approaches typically require clean data rarely available in clinical tasks. To address this challenge, we propose AMID, an unsupervised Ambient Measurement-Integrated Diffusion with noise decoupling, which establishes clean SOMs directly from noisy measurements. AMID introduces a measurement-integrated strategy aligning measurement noise with the diffusion trajectory, and explicitly models coupling between measurement and diffusion noise across steps, an ambient loss is thus designed base on it to learn clean SOMs. Experiments on real CT and mammography datasets show that AMID outperforms existing methods in generation fidelity and yields more reliable task-based IQ evaluation, demonstrating its potential for unsupervised medical imaging analysis.","authors":["Jianwei Sun","Xiaoning Lei","Wenhao Cai","Xichen Xu","Yanshu Wang","Hu Gao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22674v1","updated":"2026-02-26T06:45:11Z","published":"2026-02-26T06:45:11Z","title":"SPMamba-YOLO: An Underwater Object Detection Network Based on Multi-Scale Feature Enhancement and Global Context Modeling","summary":"Underwater object detection is a critical yet challenging research problem owing to severe light attenuation, color distortion, background clutter, and the small scale of underwater targets. To address these challenges, we propose SPMamba-YOLO, a novel underwater object detection network that integrates multi-scale feature enhancement with global context modeling. Specifically, a Spatial Pyramid Pooling Enhanced Layer Aggregation Network (SPPELAN) module is introduced to strengthen multi-scale feature aggregation and expand the receptive field, while a Pyramid Split Attention (PSA) mechanism enhances feature discrimination by emphasizing informative regions and suppressing background interference. In addition, a Mamba-based state space modeling module is incorporated to efficiently capture long-range dependencies and global contextual information, thereby improving detection robustness in complex underwater environments. Extensive experiments on the URPC2022 dataset demonstrate that SPMamba-YOLO outperforms the YOLOv8n baseline by more than 4.9\\% in mAP@0.5, particularly for small and densely distributed underwater objects, while maintaining a favorable balance between detection accuracy and computational cost.","authors":["Guanghao Liao","Zhen Liu","Liyuan Cao","Yonghui Yang","Qi Li"],"pdf_url":"","comment":"31 pages, 10 figures, 6 tables. This paper presents SPMamba-YOLO, an underwater object detection framework integrating multi-scale feature enhancement and global context modeling. The work is under review"},{"id":"http://arxiv.org/abs/2602.22667v1","updated":"2026-02-26T06:37:43Z","published":"2026-02-26T06:37:43Z","title":"Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes","summary":"Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.","authors":["Changqing Zhou","Yueru Luo","Han Zhang","Zeyu Jiang","Changhao Chen"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2602.22666v1","updated":"2026-02-26T06:35:23Z","published":"2026-02-26T06:35:23Z","title":"ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals","summary":"Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.","authors":["Xuelu Li","Zhaonan Wang","Xiaogang Wang","Lei Wu","Manyi Li","Changhe Tu"],"pdf_url":"","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2602.23342v1","updated":"2026-02-26T18:48:29Z","published":"2026-02-26T18:48:29Z","title":"AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search","summary":"On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.\n  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.","authors":["Weijian Chen","Haotian Liu","Yangshen Deng","Long Xiang","Liang Huang","Gezi Li","Bo Tang"],"pdf_url":"","comment":"The paper has been accepted by SIGMOD 2026"},{"id":"http://arxiv.org/abs/2602.23335v1","updated":"2026-02-26T18:40:28Z","published":"2026-02-26T18:40:28Z","title":"Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset","summary":"AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.","authors":["Dany Haddad","Dan Bareket","Joseph Chee Chang","Jay DeYoung","Jena D. Hwang","Uri Katz","Mark Polak","Sangho Suh","Harshit Surana","Aryeh Tiktinsky","Shriya Atmakuri","Jonathan Bragg","Mike D'Arcy","Sergey Feldman","Amal Hassan-Ali","Rubén Lozano","Bodhisattwa Prasad Majumder","Charles McGrady","Amanpreet Singh","Brooke Vlahos","Yoav Goldberg","Doug Downey"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23286v1","updated":"2026-02-26T17:59:51Z","published":"2026-02-26T17:59:51Z","title":"SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables","summary":"Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.","authors":["Sungho Park","Jueun Kim","Wook-Shin Han"],"pdf_url":"","comment":"10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: https://sparta-projectpage.github.io/"},{"id":"http://arxiv.org/abs/2602.23234v1","updated":"2026-02-26T17:11:26Z","published":"2026-02-26T17:11:26Z","title":"Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments","summary":"Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.","authors":["Evangelia Christakopoulou","Vivekkumar Patel","Hemanth Velaga","Sandip Gaikwad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23132v1","updated":"2026-02-26T15:48:09Z","published":"2026-02-26T15:48:09Z","title":"From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation","summary":"Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \\textbf{FatsMB}, a framework based diffusion model that guides preference generation \\textit{\\textbf{F}rom Behavior-\\textbf{A}gnostic \\textbf{T}o Behavior-\\textbf{S}pecific} in latent spaces, enabling diverse and accurate \\textit{\\textbf{M}ulti-\\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.","authors":["Ruochen Yang","Xiaodong Li","Jiawei Sheng","Jiangxia Cao","Xinkui Lin","Shen Wang","Shuang Yang","Zhaojie Liu","Tingwen Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23105v1","updated":"2026-02-26T15:19:43Z","published":"2026-02-26T15:19:43Z","title":"MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System","summary":"Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.","authors":["Yusheng Huang","Pengbo Xu","Shen Wang","Changxin Lao","Jiangxia Cao","Shuang Wen","Shuang Yang","Zhaojie Liu","Han Li","Kun Gai"],"pdf_url":"","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2602.23075v1","updated":"2026-02-26T15:02:22Z","published":"2026-02-26T15:02:22Z","title":"CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery","summary":"Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.","authors":["Mengze Hong","Di Jiang","Chen Jason Zhang","Zichang Guo","Yawen Li","Jun Chen","Shaobo Cui","Zhiyang Su"],"pdf_url":"","comment":"Accepted by TheWebConf 2026 Demo Track"},{"id":"http://arxiv.org/abs/2602.23061v1","updated":"2026-02-26T14:48:49Z","published":"2026-02-26T14:48:49Z","title":"MoDora: Tree-Based Semi-Structured Document Analysis System","summary":"Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.\n  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.","authors":["Bangrui Xu","Qihang Yao","Zirui Tang","Xuanhe Zhou","Yeye He","Shihan Yu","Qianqian Xu","Bin Wang","Guoliang Li","Conghui He","Fan Wu"],"pdf_url":"","comment":"Extension of our SIGMOD 2026 paper. Please refer to source code available at https://github.com/weAIDB/MoDora"},{"id":"http://arxiv.org/abs/2602.21480v2","updated":"2026-02-26T14:47:19Z","published":"2026-02-25T01:12:35Z","title":"Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?","summary":"Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.","authors":["Germán T. Eizaguirre","Lars Tissen","Marc Sánchez-Artigas"],"pdf_url":"","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.16541v2","updated":"2026-02-26T14:13:24Z","published":"2026-02-18T15:40:30Z","title":"From Latent to Observable Position-Based Click Models in Carousel Interfaces","summary":"Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors.\n  In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation.\n  Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.","authors":["Santiago de Leon-Martinez","Robert Moro","Maria Bielikova"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2412.10745v2","updated":"2026-02-26T13:53:14Z","published":"2024-12-14T08:28:52Z","title":"Enhancing Event Extraction from Short Stories through Contextualized Prompts","summary":"Event extraction is an important natural language processing (NLP) task of identifying events in an unstructured text. Although a plethora of works deal with event extraction from new articles, clinical text etc., only a few works focus on event extraction from literary content. Detecting events in short stories presents several challenges to current systems, encompassing a different distribution of events as compared to other domains and the portrayal of diverse emotional conditions. This paper presents \\texttt{Vrittanta-EN}, a collection of 1000 English short stories annotated for real events. Exploring this field could result in the creation of techniques and resources that support literary scholars in improving their effectiveness. This could simultaneously influence the field of Natural Language Processing. Our objective is to clarify the intricate idea of events in the context of short stories. Towards the objective, we collected 1,000 short stories written mostly for children in the Indian context. Further, we present fresh guidelines for annotating event mentions and their categories, organized into \\textit{seven distinct classes}. The classes are {\\tt{COGNITIVE-MENTAL-STATE(CMS), COMMUNICATION(COM), CONFLICT(CON), GENERAL-ACTIVITY(GA), LIFE-EVENT(LE), MOVEMENT(MOV), and OTHERS(OTH)}}. Subsequently, we apply these guidelines to annotate the short story dataset. Later, we apply the baseline methods for automatically detecting and categorizing events. We also propose a prompt-based method for event detection and classification. The proposed method outperforms the baselines, while having significant improvement of more than 4\\% for the class \\texttt{CONFLICT} in event classification task.","authors":["Chaitanya Kirti","Ayon Chattopadhyay","Ashish Anand","Prithwijit Guha"],"pdf_url":"","comment":"47 pages, 8 figures, Planning to submit in Elsevier (Computer Speech and Language Journal)"},{"id":"http://arxiv.org/abs/2602.23012v1","updated":"2026-02-26T13:52:54Z","published":"2026-02-26T13:52:54Z","title":"Sequential Regression for Continuous Value Prediction using Residual Quantization","summary":"Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.\n  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.","authors":["Runpeng Cui","Zhipeng Sun","Chi Lu","Peng Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22913v1","updated":"2026-02-26T12:00:46Z","published":"2026-02-26T12:00:46Z","title":"SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress","summary":"With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.","authors":["Yang Yu","Lei Kou","Huaikuan Yi","Bin Chen","Yayu Cao","Lei Shen","Chao Zhang","Bing Wang","Xiaoyi Zeng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22903v1","updated":"2026-02-26T11:47:32Z","published":"2026-02-26T11:47:32Z","title":"PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA","summary":"Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.","authors":["Yunpeng Hong","Chenyang Bu","Jie Zhang","Yi He","Di Wu","Xindong Wu"],"pdf_url":"","comment":"2026 SIGKDD accept"},{"id":"http://arxiv.org/abs/2504.13703v4","updated":"2026-02-26T10:14:55Z","published":"2025-04-18T14:03:40Z","title":"C$^3$: Capturing Consensus with Contrastive Learning in Group Recommendation","summary":"Group recommendation aims to recommend tailored items to groups of users, where the key challenge is modeling a consensus that reflects member preferences. Although several existing deep learning models have achieved performance improvements, they still fail to capture consensus in various aspects: (1) Capturing consensus in small-group (2~5 members) recommendation systems, which align more closely with real-world scenarios, remains a significant challenge; (2) Most existing models significantly enhance the overall group performance but struggle with balancing individual and group performance. To address these issues, we propose Capturing Consensus with Contrastive Learning in Group Recommendation (C$^3$), which focuses on exploring the consensus behind group decision-making. A Transformer encoder is used to learn both group and user representations, and contrastive learning mitigates overfitting for users with many interactions, yielding more robust group representations. Experiments on four public datasets demonstrate that C$^3$ significantly outperforms state-of-the-art baselines in both user and group recommendation tasks.","authors":["Soyoung Kim","Dongjun Lee","Jaekwang Kim"],"pdf_url":"","comment":"12 pages, 4 figures, accepted by PAKDD 2026 special session"},{"id":"http://arxiv.org/abs/2602.09448v3","updated":"2026-02-26T08:33:54Z","published":"2026-02-10T06:33:10Z","title":"The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training","summary":"Prior synthetic query generation for dense retrieval produces one query per document, focusing on quality. We systematically study multi-query synthesis, discovering a quality-diversity trade-off: quality benefits in-domain, diversity benefits out-of-domain (OOD). Experiments on 31 datasets show diversity especially benefits multi-hop retrieval. Analysis reveals diversity benefit correlates with query complexity (r>=0.95), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides thresholds (CW>10: use diversity; CW<7: avoid it) and enables CW-weighted training that improves OOD even with single-query data.","authors":["Xincan Feng","Noriki Nishida","Yusuke Sakai","Yuji Matsumoto"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.22732v1","updated":"2026-02-26T08:15:26Z","published":"2026-02-26T08:15:26Z","title":"Generative Recommendation for Large-Scale Advertising","summary":"Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.","authors":["Ben Xue","Dan Liu","Lixiang Wang","Mingjie Sun","Peng Wang","Pengfei Zhang","Shaoyun Shi","Tianyu Xu","Yunhao Sha","Zhiqiang Liu","Bo Kong","Bo Wang","Hang Yang","Jieting Xue","Junhao Wang","Shengyu Wang","Shuping Hui","Wencai Ye","Xiao Lin","Yongzhi Li","Yuhang Chen","Zhihui Yin","Quan Chen","Shiyang Wen","Wenjin Wu","Han Li","Guorui Zhou","Changcheng Li","Peng Jiang"],"pdf_url":"","comment":"13 pages, 6 figures, under review"},{"id":"http://arxiv.org/abs/2411.00341v2","updated":"2026-02-26T06:20:56Z","published":"2024-11-01T03:43:50Z","title":"A Survey on Bundle Recommendation: Methods, Applications, and Challenges","summary":"In recent years, bundle recommendation systems have gained significant attention in both academia and industry due to their ability to enhance user experience and increase sales by recommending a set of items as a bundle rather than individual items. This survey provides a comprehensive review on bundle recommendation, beginning by a taxonomy for exploring product bundling. We classify it into two categories based on bundling strategy from various application domains, i.e., discriminative and generative bundle recommendation. Then we formulate the corresponding tasks of the two categories and systematically review their methods: 1) representation learning from bundle and item levels and interaction modeling for discriminative bundle recommendation; 2) representation learning from item level and bundle generation for generative bundle recommendation. Subsequently, we survey the resources of bundle recommendation including datasets and evaluation metrics, and conduct reproducibility experiments on mainstream models. Lastly, we discuss the main challenges and highlight the promising future directions in the field of bundle recommendation, aiming to serve as a useful resource for researchers and practitioners. Our code and datasets are publicly available at https://github.com/WUT-IDEA/bundle-recommendation-survey.","authors":["Meng Sun","Lin Li","Ming Li","Xiaohui Tao","Dong Zhang","Qing Xie","Peipei Wang","Jimmy Xiangji Huang"],"pdf_url":"","comment":"Accepted by ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2602.22647v1","updated":"2026-02-26T06:00:56Z","published":"2026-02-26T06:00:56Z","title":"Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators","summary":"Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.","authors":["Zhengyang Su","Isay Katsman","Yueqi Wang","Ruining He","Lukasz Heldt","Raghunandan Keshavan","Shao-Chuan Wang","Xinyang Yi","Mingyan Gao","Onkar Dalal","Lichan Hong","Ed Chi","Ningren Han"],"pdf_url":"","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2602.22632v1","updated":"2026-02-26T05:17:24Z","published":"2026-02-26T05:17:24Z","title":"Fine-grained Semantics Integration for Large Language Model-based Recommendation","summary":"Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.","authors":["Jiawen Feng","Xiaoyu Kong","Leheng Sheng","Bin Wu","Chao Yi","Feifang Yang","Xiang-Rong Sheng","Han Zhu","Xiang Wang","Jiancan Wu","Xiangnan He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22591v1","updated":"2026-02-26T03:51:31Z","published":"2026-02-26T03:51:31Z","title":"Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking","summary":"Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.\n  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal \"bell-curve\" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.","authors":["Haodong Chen","Shengyao Zhuang","Zheng Yao","Guido Zuccon","Teerapong Leelanupab"],"pdf_url":"","comment":"10 pages, 5 figures, 1 table. Code available at https://github.com/ielab/Selective-ICR"},{"id":"http://arxiv.org/abs/2602.22576v1","updated":"2026-02-26T03:31:00Z","published":"2026-02-26T03:31:00Z","title":"Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training","summary":"Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.","authors":["Tianle Xia","Ming Xu","Lingxiang Hu","Yiding Sun","Wenwei Li","Linfang Shang","Liqun Liu","Peng Shu","Huan Yu","Jie Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.27566v3","updated":"2026-02-26T02:42:05Z","published":"2025-10-31T15:48:43Z","title":"Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval","summary":"Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.","authors":["Yulong Hui","Chao Chen","Zhihang Fu","Yihao Liu","Jieping Ye","Huanchen Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22547v1","updated":"2026-02-26T02:38:40Z","published":"2026-02-26T02:38:40Z","title":"Towards Dynamic Dense Retrieval with Routing Strategy","summary":"The \\textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.\n  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \\textit{dynamic dense retrieval} (DDR). DDR uses \\textit{prefix tuning} as a \\textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.","authors":["Zhan Su","Fengran Mo","Jinghan Zhang","Yuchen Hui","Jia Ao Sun","Bingbing Wen","Jian-Yun Nie"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22529v1","updated":"2026-02-26T02:08:39Z","published":"2026-02-26T02:08:39Z","title":"Generative Agents Navigating Digital Libraries","summary":"In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.","authors":["Saber Zerhoudi","Michael Granitzer"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22521v1","updated":"2026-02-26T01:32:45Z","published":"2026-02-26T01:32:45Z","title":"TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering","summary":"The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.","authors":["Jiayi Wu","Zhengyu Wu","Xunkai Li","Rong-Hua Li","Guoren Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.14162v2","updated":"2026-02-26T01:17:29Z","published":"2026-02-15T14:23:50Z","title":"Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering","summary":"Existing multimodal document question answering methods predominantly adopt a Pre-Ingestion (PI) strategy: during the indexing phase, a Vision Language Model (VLM) is called on every page to generate page descriptions that are then encoded into vectors, and questions are answered via embedding similarity retrieval. However, this approach faces a dual dilemma on visual-dense engineering documents: VLM blind descriptions inevitably lose critical visual details, and embedding retrieval systematically fails on highly similar documents. This paper proposes the Deferred Visual Ingestion (DVI) framework: zero VLM calls during preprocessing, leveraging only document structural information (table of contents, drawing numbers) to automatically build a hierarchical index through the HDNC (Hierarchical Drawing Number Clustering) algorithm; during inference, candidate pages are located via BM25 retrieval, and the original images along with the specific question are sent to a VLM for targeted analysis. Large-scale experiments on three datasets validate the effectiveness of DVI: on Bridge engineering drawings (1,323 questions), end-to-end QA accuracy reaches 65.6\\% vs. PI's 24.3\\% (+41.3pp); on Steel catalog (186 questions), 30.6\\% vs. 16.1\\% (+14.5pp); on CircuitVQA, a public benchmark (9,315 questions), retrieval ImgR@3 achieves 31.2\\% vs. 0.7\\%. On the Bridge dataset, we evaluated ColPali (ICLR 2025 visual retrieval SOTA), which achieved only 20.1\\% PageR@3, demonstrating that the failure of embedding retrieval on homogeneous engineering documents is structural rather than due to insufficient model capability. Ablation studies show that HDNC zero-cost automatic indexing yields a +27.5pp retrieval improvement, and VLM conversion rate analysis confirms that the bottleneck lies on the retrieval side rather than the comprehension side.","authors":["Tao Xu"],"pdf_url":"","comment":"24 pages, 4 figures, 7 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2602.23360v1","updated":"2026-02-26T18:59:32Z","published":"2026-02-26T18:59:32Z","title":"Model Agreement via Anchoring","summary":"Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.\n  We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.","authors":["Eric Eaton","Surbhi Goel","Marcel Hussing","Michael Kearns","Aaron Roth","Sikata Bela Sengupta","Jessica Sorrell"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23358v1","updated":"2026-02-26T18:59:03Z","published":"2026-02-26T18:59:03Z","title":"A Dataset is Worth 1 MB","summary":"A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.","authors":["Elad Kimchi Shoshani","Leeyam Gabay","Yedid Hoshen"],"pdf_url":"","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.23353v1","updated":"2026-02-26T18:55:06Z","published":"2026-02-26T18:55:06Z","title":"SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport","summary":"The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.","authors":["Simon Roschmann","Paul Krzakala","Sonia Mazelet","Quentin Bouniot","Zeynep Akata"],"pdf_url":"","comment":"Preprint"},{"id":"http://arxiv.org/abs/2602.23349v1","updated":"2026-02-26T18:52:22Z","published":"2026-02-26T18:52:22Z","title":"FlashOptim: Optimizers for Memory Efficient Training","summary":"Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.\n  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.","authors":["Jose Javier Gonzalez Ortiz","Abhay Gupta","Chris Renard","Davis Blalock"],"pdf_url":"","comment":"Source code is available at https://github.com/databricks/flashoptim"},{"id":"http://arxiv.org/abs/2602.23341v1","updated":"2026-02-26T18:47:06Z","published":"2026-02-26T18:47:06Z","title":"Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms","summary":"Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]","authors":["Alkis Kalavasis","Anay Mehrotra","Manolis Zampetakis","Felix Zhou","Ziyu Zhu"],"pdf_url":"","comment":"Abstract truncated to arXiv limits. To appear in ICLR'26"},{"id":"http://arxiv.org/abs/2602.23336v1","updated":"2026-02-26T18:41:31Z","published":"2026-02-26T18:41:31Z","title":"Differentiable Zero-One Loss via Hypersimplex Projections","summary":"Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.","authors":["Camilo Gomez","Pengyang Wang","Liansheng Tang"],"pdf_url":"","comment":"To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages"},{"id":"http://arxiv.org/abs/2602.23321v1","updated":"2026-02-26T18:29:48Z","published":"2026-02-26T18:29:48Z","title":"Deep ensemble graph neural networks for probabilistic cosmic-ray direction and energy reconstruction in autonomous radio arrays","summary":"Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced on ground-based radio detector arrays.\n  In our approach, triggered antennas are represented as a graph structure, which serves as input for a graph neural network (GNN). By incorporating physical knowledge into both the GNN architecture and the input data, we improve the precision and reduce the required size of the training set with respect to a fully data-driven approach. This method achieves an angular resolution of 0.092° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data with realistic noise conditions.\n  We also employ uncertainty estimation methods to enhance the reliability of our predictions, quantifying the confidence of the GNN's outputs and providing confidence intervals for both direction and energy reconstruction. Finally, we investigate strategies to verify the model's consistency and robustness under real life variations, with the goal of identifying scenarios in which predictions remain reliable despite domain shifts between simulation and reality.","authors":["Arsène Ferrière","Aurélien Benoit-Lévy","Olivier Martineau-Huynh","Matías Tueros"],"pdf_url":"","comment":"Submitted to Astroparticle Physics Journal"},{"id":"http://arxiv.org/abs/2602.23320v1","updated":"2026-02-26T18:28:04Z","published":"2026-02-26T18:28:04Z","title":"ParamMem: Augmenting Language Agents with Parametric Reflective Memory","summary":"Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.","authors":["Tianjun Yao","Yongqiang Chen","Yujia Zheng","Pan Li","Zhiqiang Shen","Kun Zhang"],"pdf_url":"","comment":"20 pages"},{"id":"http://arxiv.org/abs/2506.06092v2","updated":"2026-02-26T18:27:23Z","published":"2025-06-06T13:52:33Z","title":"LinGuinE: Longitudinal Guidance Estimation for Volumetric Tumour Segmentation","summary":"Longitudinal volumetric tumour segmentation is critical for radiotherapy planning and response assessment, yet this problem is underexplored and most methods produce single-timepoint semantic masks, lack lesion correspondence, and offer limited radiologist control. We introduce LinGuinE (Longitudinal Guidance Estimation), a PyTorch framework that combines image registration and guided segmentation to deliver lesion-level tracking and volumetric masks across all scans in a longitudinal study from a single radiologist prompt. LinGuinE is temporally direction agnostic, requires no training on longitudinal data, and allows any registration and semi-automatic segmentation algorithm to be repurposed for the task. We evaluate various combinations of registration and segmentation algorithms within the framework. LinGuinE achieves state-of-the-art segmentation and tracking performance across four datasets with a total of 456 longitudinal studies. Tumour segmentation performance shows minimal degradation with increasing temporal separation. We conduct ablation studies to determine the impact of autoregression, pathology specific finetuning, and the use of real radiologist prompts. We release our code and substantial public benchmarking for longitudinal segmentation, facilitating future research.","authors":["Nadine Garibli","Mayank Patwari","Bence Csiba","Yi Wei","Kostantinos Sidiropoulos"],"pdf_url":"","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2602.23312v1","updated":"2026-02-26T18:20:26Z","published":"2026-02-26T18:20:26Z","title":"Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction","summary":"Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.","authors":["Rafael R. Baptista","André de Lima Salgado","Ricardo V. Godoy","Marcelo Becker","Thiago Boaventura","Gustavo J. G. Lahr"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.04733v3","updated":"2026-02-26T18:16:20Z","published":"2025-05-07T18:46:02Z","title":"Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting","summary":"We introduce a framework for robust uncertainty quantification in situations where labeled training data are corrupted, through noisy or missing labels. We build on conformal prediction, a statistical tool for generating prediction sets that cover the test label with a pre-specified probability. The validity of conformal prediction, however, holds under the i.i.d assumption, which does not hold in our setting due to the corruptions in the data. To account for this distribution shift, the privileged conformal prediction (PCP) method proposed leveraging privileged information (PI) -- additional features available only during training -- to re-weight the data distribution, yielding valid prediction sets under the assumption that the weights are accurate. In this work, we analyze the robustness of PCP to inaccuracies in the weights. Our analysis indicates that PCP can still yield valid uncertainty estimates even when the weights are poorly estimated. Furthermore, we introduce uncertain imputation (UI), a new conformal method that does not rely on weight estimation. Instead, we impute corrupted labels in a way that preserves their uncertainty. Our approach is supported by theoretical guarantees and validated empirically on both synthetic and real benchmarks. Finally, we show that these techniques can be integrated into a triply robust framework, ensuring statistically valid predictions as long as at least one underlying method is valid.","authors":["Shai Feldman","Stephen Bates","Yaniv Romano"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21189v2","updated":"2026-02-26T18:11:36Z","published":"2026-02-24T18:43:08Z","title":"Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training","summary":"Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.","authors":["Anas Barakat","Souradip Chakraborty","Khushbu Pahwa","Amrit Singh Bedi"],"pdf_url":"","comment":"updated related work discussion"},{"id":"http://arxiv.org/abs/2602.20833v2","updated":"2026-02-26T18:10:20Z","published":"2026-02-24T12:18:42Z","title":"DRESS: A Continuous Framework for Structural Graph Refinement","summary":"The Weisfeiler-Lehman (WL) hierarchy is a cornerstone framework for graph isomorphism testing and structural analysis. However, scaling beyond 1-WL to 3-WL and higher requires tensor-based operations that scale as $\\mathcal{O}(n^3)$ or $\\mathcal{O}(n^4)$, making them computationally prohibitive for large graphs. In this paper, we start from the Original-DRESS equation (Castrillo, León, and Gómez, 2018) -- a parameter-free, continuous dynamical system on edges -- and show that it distinguishes the prism graph from $K_{3,3}$, a pair that 1-WL provably cannot separate. We then generalize it to Motif-DRESS, which replaces triangle neighborhoods with arbitrary structural motifs and converges to a unique fixed point under three sufficient conditions, and further to Generalized-DRESS, an abstract template parameterized by the choice of neighborhood operator, aggregation function and norm. Finally, we introduce $Δ$-DRESS, which runs DRESS on each node-deleted subgraph $G \\setminus \\{v\\}$, connecting the framework to the Kelly--Ulam reconstruction conjecture. Both Motif-DRESS and $Δ$-DRESS empirically distinguish Strongly Regular Graphs (SRGs) -- such as the Rook and Shrikhande graphs -- that confound 3-WL. Our results establish the DRESS family as a highly scalable framework that empirically surpasses both 1-WL and 3-WL on well-known benchmark graphs, without the prohibitive $\\mathcal{O}(n^4)$ computational cost.","authors":["Eduar Castrillo Velilla"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23305v1","updated":"2026-02-26T18:09:49Z","published":"2026-02-26T18:09:49Z","title":"A Proper Scoring Rule for Virtual Staining","summary":"Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.","authors":["Samuel Tonks","Steve Hood","Ryan Musso","Ceridwen Hopely","Steve Titus","Minh Doan","Iain Styles","Alexander Krull"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23303v1","updated":"2026-02-26T18:09:16Z","published":"2026-02-26T18:09:16Z","title":"Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications","summary":"Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.","authors":["Ilya Balabin","Thomas M. Kaiser"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23296v1","updated":"2026-02-26T18:07:45Z","published":"2026-02-26T18:07:45Z","title":"Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity","summary":"Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.","authors":["Quang-Huy Nguyen","Jiaqi Wang","Wei-Shinn Ku"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23295v1","updated":"2026-02-26T18:07:10Z","published":"2026-02-26T18:07:10Z","title":"ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation","summary":"In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.","authors":["Ayush Roy","Wei-Yang Alex Lee","Rudrasis Chakraborty","Vishnu Suresh Lokhande"],"pdf_url":"","comment":"CVPE 2026"},{"id":"http://arxiv.org/abs/2602.01434v2","updated":"2026-02-26T18:06:09Z","published":"2026-02-01T20:47:36Z","title":"Phase Transitions for Feature Learning in Neural Networks","summary":"According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\\boldsymbol x}_i,y_i)$, where the covariate vectors ${\\boldsymbol x}_i\\in\\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\\boldsymbol x}_i$ through a $k$-dimensional projection ${\\boldsymbol Θ}_*^{\\sf T}{\\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\\boldsymbol Θ}_*$.\n  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\\to\\infty$, $n/d\\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\\text{alg}}$, for $δ_{\\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\\text{alg}}$. Here we derive an analogous threshold $δ_{\\text{NN}}$ for two-layer networks. Our characterization of $δ_{\\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.\n  The threshold $δ_{\\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.","authors":["Andrea Montanari","Zihao Wang"],"pdf_url":"","comment":"75 pages; 17 pdf figures; v2 is a minor revision of v1"},{"id":"http://arxiv.org/abs/2505.17517v4","updated":"2026-02-26T18:04:18Z","published":"2025-05-23T06:16:58Z","title":"The Spacetime of Diffusion Models: An Information Geometry Perspective","summary":"We present a novel geometric perspective on the latent space of diffusion models. We first show that the standard pullback approach, utilizing the deterministic probability flow ODE decoder, is fundamentally flawed. It provably forces geodesics to decode as straight segments in data space, effectively ignoring any intrinsic data geometry beyond the ambient Euclidean space. Complementing this view, diffusion also admits a stochastic decoder via the reverse SDE, which enables an information geometric treatment with the Fisher-Rao metric. However, a choice of $x_T$ as the latent representation collapses this metric due to memorylessness. We address this by introducing a latent spacetime $z=(x_t,t)$ that indexes the family of denoising distributions $p(x_0 | x_t)$ across all noise scales, yielding a nontrivial geometric structure. We prove these distributions form an exponential family and derive simulation-free estimators for curve lengths, enabling efficient geodesic computation. The resulting structure induces a principled Diffusion Edit Distance, where geodesics trace minimal sequences of noise and denoise edits between data. We also demonstrate benefits for transition path sampling in molecular systems, including constrained variants such as low-variance transitions and region avoidance. Code is available at: https://github.com/rafalkarczewski/spacetime-geometry.","authors":["Rafał Karczewski","Markus Heinonen","Alison Pouplin","Søren Hauberg","Vikas Garg"],"pdf_url":"","comment":"ICLR 2026 (Oral)"},{"id":"http://arxiv.org/abs/2602.23280v1","updated":"2026-02-26T17:53:46Z","published":"2026-02-26T17:53:46Z","title":"Physics Informed Viscous Value Representations","summary":"Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.","authors":["Hrishikesh Viswanath","Juanwu Lu","S. Talha Bukhari","Damon Conover","Ziran Wang","Aniket Bera"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23277v1","updated":"2026-02-26T17:52:08Z","published":"2026-02-26T17:52:08Z","title":"Zeroth-Order Stackelberg Control in Combinatorial Congestion Games","summary":"We study Stackelberg (leader--follower) tuning of network parameters (tolls, capacities, incentives) in combinatorial congestion games, where selfish users choose discrete routes (or other combinatorial strategies) and settle at a congestion equilibrium. The leader minimizes a system-level objective (e.g., total travel time) evaluated at equilibrium, but this objective is typically nonsmooth because the set of used strategies can change abruptly. We propose ZO-Stackelberg, which couples a projection-free Frank--Wolfe equilibrium solver with a zeroth-order outer update, avoiding differentiation through equilibria. We prove convergence to generalized Goldstein stationary points of the true equilibrium objective, with explicit dependence on the equilibrium approximation error, and analyze subsampled oracles: if an exact minimizer is sampled with probability $κ_m$, then the Frank--Wolfe error decays as $\\mathcal{O}(1/(κ_m T))$. We also propose stratified sampling as a practical way to avoid a vanishing $κ_m$ when the strategies that matter most for the Wardrop equilibrium concentrate in a few dominant combinatorial classes (e.g., short paths). Experiments on real-world networks demonstrate that our method achieves orders-of-magnitude speedups over a differentiation-based baseline while converging to follower equilibria.","authors":["Saeed Masiha","Sepehr Elahi","Negar Kiyavash","Patrick Thiran"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2507.03772v3","updated":"2026-02-26T17:39:06Z","published":"2025-07-04T18:45:10Z","title":"Skewed Score: A statistical framework to assess autograders","summary":"The evaluation of large language model (LLM) outputs is increasingly performed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or autograders. While autograders offer a scalable alternative to human evaluation, they have shown mixed reliability and may exhibit systematic biases, depending on response type, scoring methodology, domain specificity, or other factors. Here we propose a statistical framework based on Bayesian generalised linear models (GLMs) that enables researchers to simultaneously assess their autograders while addressing their primary research questions (e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores or pairwise preferences) as a function of properties of the grader (e.g., human vs. autograder) and the evaluated item (e.g., response length or the LLM that generated it), allowing for explicit quantification of scoring differences and potential biases within a unified framework. In addition, our method can be used to augment traditional metrics such as inter-rater agreement, by providing uncertainty estimates and clarifying sources of disagreement. Overall, this approach contributes to more robust and interpretable use of autograders in LLM evaluation, enabling both performance analysis and bias detection.","authors":["Magda Dubois","Harry Coppock","Mario Giulianelli","Timo Flesch","Lennart Luettgau","Cozmin Ududec"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04228v2","updated":"2026-02-26T17:37:05Z","published":"2025-08-06T09:03:16Z","title":"LayerT2V: A Unified Multi-Layer Video Generation Framework","summary":"Text-to-video generation has advanced rapidly, but existing methods typically output only the final composited video and lack editable layered representations, limiting their use in professional workflows. We propose \\textbf{LayerT2V}, a unified multi-layer video generation framework that produces multiple semantically consistent outputs in a single inference pass: the full video, an independent background layer, and multiple foreground RGB layers with corresponding alpha mattes. Our key insight is that recent video generation backbones use high compression in both time and space, enabling us to serialize multiple layer representations along the temporal dimension and jointly model them on a shared generation trajectory. This turns cross-layer consistency into an intrinsic objective, improving semantic alignment and temporal coherence. To mitigate layer ambiguity and conditional leakage, we augment a shared DiT backbone with LayerAdaLN and layer-aware cross-attention modulation. LayerT2V is trained in three stages: alpha mask VAE adaptation, joint multi-layer learning, and multi-foreground extension. We also introduce \\textbf{VidLayer}, the first large-scale dataset for multi-layer video generation. Extensive experiments demonstrate that LayerT2V substantially outperforms prior methods in visual fidelity, temporal consistency, and cross-layer coherence.","authors":["Guangzhao Li","Kangrui Cen","Baixuan Zhao","Yi Xin","Siqi Luo","Guangtao Zhai","Lei Zhang","Xiaohong Liu"],"pdf_url":"","comment":"Project Page is https://layert2v.github.io/"},{"id":"http://arxiv.org/abs/2408.01503v2","updated":"2026-02-26T17:28:25Z","published":"2024-08-02T18:02:51Z","title":"Efficient Graph Coloring with Neural Networks: A Physics-Inspired Approach for Large Graphs","summary":"Combinatorial optimization problems near algorithmic phase transitions represent a fundamental challenge for both classical algorithms and machine learning approaches. Among them, graph coloring stands as a prototypical constraint satisfaction problem exhibiting sharp dynamical and satisfiability thresholds. Here we introduce a physics-inspired neural framework that learns to solve large-scale graph coloring instances by combining graph neural networks with statistical-mechanics principles. Our approach integrates a planting-based supervised signal, symmetry-breaking regularization, and iterative noise-annealed neural dynamics to navigate clustered solution landscapes. When the number of iterations scales quadratically with graph size, the learned solver reaches algorithmic thresholds close to the theoretical dynamical transition in random graphs and achieves near-optimal detection performance in the planted inference regime. The model generalizes from small training graphs to instances orders of magnitude larger, demonstrating that neural architectures can learn scalable algorithmic strategies that remain effective in hard connectivity regions. These results establish a general paradigm for learning neural solvers that operate near fundamental phase boundaries in combinatorial optimization and inference.","authors":["Lorenzo Colantonio","Andrea Cacioppo","Federico Scarpati","Maria Chiara Angelini","Federico Ricci-Tersenghi","Stefano Giagu"],"pdf_url":"","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2602.23234v1","updated":"2026-02-26T17:11:26Z","published":"2026-02-26T17:11:26Z","title":"Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments","summary":"Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.","authors":["Evangelia Christakopoulou","Vivekkumar Patel","Hemanth Velaga","Sandip Gaikwad"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19964v2","updated":"2026-02-26T17:10:56Z","published":"2026-02-23T15:28:27Z","title":"On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference","summary":"Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.","authors":["Moritz A. Zanger","Yijun Wu","Pascal R. Van der Vaart","Wendelin Böhmer","Matthijs T. J. Spaan"],"pdf_url":"","comment":"8 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2511.07885v3","updated":"2026-02-26T17:09:14Z","published":"2025-11-11T06:33:30Z","title":"Intelligence per Watt: Measuring Intelligence Efficiency of Local AI","summary":"Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness here: https://github.com/HazyResearch/intelligence-per-watt.","authors":["Jon Saad-Falcon","Avanika Narayan","Hakki Orhun Akengin","J. Wes Griffin","Herumb Shandilya","Adrian Gamarra Lafuente","Medhya Goel","Rebecca Joseph","Shlok Natarajan","Etash Kumar Guha","Shang Zhu","Ben Athiwaratkun","John Hennessy","Azalia Mirhoseini","Christopher Ré"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.05993v2","updated":"2026-02-26T17:05:08Z","published":"2025-03-08T00:29:00Z","title":"SODAs: Sparse Optimization for the Discovery of Differential and Algebraic Equations","summary":"Differential-algebraic equations (DAEs) integrate ordinary differential equations (ODEs) with algebraic constraints, providing a fundamental framework for developing models of dynamical systems characterized by timescale separation, conservation laws, and physical constraints. While sparse optimization has revolutionized model development by allowing data-driven discovery of parsimonious models from a library of possible equations, existing approaches for dynamical systems assume DAEs can be reduced to ODEs by eliminating variables before model discovery. This assumption limits the applicability of such methods for DAE systems with unknown constraints and time scales. We introduce Sparse Optimization for Differential-Algebraic Systems (SODAs), a data-driven method for the identification of DAEs in their explicit form. By discovering the algebraic and dynamic components sequentially without prior identification of the algebraic variables, this approach leads to a sequence of convex optimization problems. It has the advantage of discovering interpretable models that preserve the structure of the underlying physical system. To this end, SODAs improves numerical stability when handling high correlations between library terms, caused by near-perfect algebraic relationships, by iteratively refining the conditioning of the candidate library. We demonstrate the performance of our method on biological, mechanical, and electrical systems, showcasing its robustness to noise in both simulated time series and real-time experimental data.","authors":["Manu Jayadharan","Christina Catlett","Arthur N. Montanari","Niall M. Mangan"],"pdf_url":"","comment":"22 pages, 5 figures"},{"id":"http://arxiv.org/abs/2602.23219v1","updated":"2026-02-26T17:01:14Z","published":"2026-02-26T17:01:14Z","title":"Takeuchi's Information Criteria as Generalization Measures for DNNs Close to NTK Regime","summary":"Generalization measures have been studied extensively in the machine learning community to better characterize generalization gaps. However, establishing a reliable generalization measure for statistically singular models such as deep neural networks (DNNs) is difficult due to their complex nature. This study focuses on Takeuchi's information criterion (TIC) to investigate the conditions under which this classical measure can effectively explain the generalization gaps of DNNs. Importantly, the developed theory indicates the applicability of TIC near the neural tangent kernel (NTK) regime. In a series of experiments, we trained more than 5,000 DNN models with 12 architectures, including large models (e.g., VGG-16), on four datasets, and estimated the corresponding TIC values to examine the relationship between the generalization gap and the TIC estimates. We applied several TIC approximation methods with feasible computational costs and assessed the accuracy trade-off. Our experimental results indicate that the estimated TIC values correlate well with the generalization gap under conditions close to the NTK regime. However, we show both theoretically and empirically that outside the NTK regime such correlation disappears. Finally, we demonstrate that TIC provides better trial pruning ability than existing methods for hyperparameter optimization.","authors":["Hiroki Naganuma","Taiji Suzuki","Rio Yokota","Masahiro Nomura","Kohta Ishikawa","Ikuro Sato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.21545v2","updated":"2026-02-26T17:01:08Z","published":"2026-02-25T04:04:00Z","title":"Muon+: Towards Better Muon via One Additional Normalization Step","summary":"The Muon optimizer has demonstrated promising performance in pre-training large language models through gradient (or momentum) orthogonalization. In this work, we propose a simple yet effective enhancement to Muon, namely Muon+, which introduces an additional normalization step after orthogonalization. We demonstrate the effectiveness of Muon+ through extensive pre-training experiments across a wide range of model scales and architectures. Our evaluation includes GPT-style models ranging from 130M to 774M parameters and LLaMA-style models ranging from 60M to 1B parameters. We comprehensively evaluate the effectiveness of Muon+ in the compute-optimal training regime and further extend the token-to-parameter (T2P) ratio to an industrial level of $\\approx 200$. Experimental results show that Muon+ provides a consistent boost on training and validation perplexity over Muon. We provide our code here: https://github.com/K1seki221/MuonPlus.","authors":["Ruijie Zhang","Yequan Zhao","Ziyue Liu","Zhengyang Wang","Zheng Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23214v1","updated":"2026-02-26T16:58:43Z","published":"2026-02-26T16:58:43Z","title":"Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction","summary":"Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.","authors":["Chenhe Du","Xuanyu Tian","Qing Wu","Muyu Liu","Jingyi Yu","Hongjiang Wei","Yuyao Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2508.04724v2","updated":"2026-02-26T16:53:54Z","published":"2025-08-05T15:11:25Z","title":"Understanding protein function with a multimodal retrieval-augmented foundation model","summary":"Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models.","authors":["Timothy Fei Truong","Tristan Bepler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23201v1","updated":"2026-02-26T16:50:52Z","published":"2026-02-26T16:50:52Z","title":"Tell Me What To Learn: Generalizing Neural Memory to be Controllable in Natural Language","summary":"Modern machine learning models are deployed in diverse, non-stationary environments where they must continually adapt to new tasks and evolving knowledge. Continual fine-tuning and in-context learning are costly and brittle, whereas neural memory methods promise lightweight updates with minimal forgetting. However, existing neural memory models typically assume a single fixed objective and homogeneous information streams, leaving users with no control over what the model remembers or ignores over time. To address this challenge, we propose a generalized neural memory system that performs flexible updates based on learning instructions specified in natural language. Our approach enables adaptive agents to learn selectively from heterogeneous information sources, supporting settings, such as healthcare and customer service, where fixed-objective memory updates are insufficient.","authors":["Max S. Bennett","Thomas P. Zollo","Richard Zemel"],"pdf_url":"","comment":"58 Pages, 16 Figures, Code at https://github.com/maxbennett/Generalized-Neural-Memory"},{"id":"http://arxiv.org/abs/2602.23200v1","updated":"2026-02-26T16:50:36Z","published":"2026-02-26T16:50:36Z","title":"InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models","summary":"Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\\%$ speedup over previous work and up to $88\\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods.","authors":["Sayed Mohammadreza Tayaranian Hosseini","Amir Ardakani","Warren J. Gross"],"pdf_url":"","comment":"16 pages, 4 figures, 4 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2602.23197v1","updated":"2026-02-26T16:49:15Z","published":"2026-02-26T16:49:15Z","title":"Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models","summary":"Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.","authors":["Chungpa Lee","Jy-yong Sohn","Kangwook Lee"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23192v1","updated":"2026-02-26T16:44:47Z","published":"2026-02-26T16:44:47Z","title":"FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification","summary":"Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.","authors":["Thomas Woergaard","Raghavendra Selvan"],"pdf_url":"","comment":"Source code available at https://github.com/saintslab/FairQuant"},{"id":"http://arxiv.org/abs/2602.23188v1","updated":"2026-02-26T16:43:28Z","published":"2026-02-26T16:43:28Z","title":"Efficient Real-Time Adaptation of ROMs for Unsteady Flows Using Data Assimilation","summary":"We propose an efficient retraining strategy for a parameterized Reduced Order Model (ROM) that attains accuracy comparable to full retraining while requiring only a fraction of the computational time and relying solely on sparse observations of the full system. The architecture employs an encode-process-decode structure: a Variational Autoencoder (VAE) to perform dimensionality reduction, and a transformer network to evolve the latent states and model the dynamics. The ROM is parameterized by an external control variable, the Reynolds number in the Navier-Stokes setting, with the transformer exploiting attention mechanisms to capture both temporal dependencies and parameter effects. The probabilistic VAE enables stochastic sampling of trajectory ensembles, providing predictive means and uncertainty quantification through the first two moments. After initial training on a limited set of dynamical regimes, the model is adapted to out-of-sample parameter regions using only sparse data. Its probabilistic formulation naturally supports ensemble generation, which we employ within an ensemble Kalman filtering framework to assimilate data and reconstruct full-state trajectories from minimal observations. We further show that, for the dynamical system considered, the dominant source of error in out-of-sample forecasts stems from distortions of the latent manifold rather than changes in the latent dynamics. Consequently, retraining can be limited to the autoencoder, allowing for a lightweight, computationally efficient, real-time adaptation procedure with very sparse fine-tuning data.","authors":["Ismaël Zighed","Andrea Nóvoa","Luca Magri","Taraneh Sayadi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23182v1","updated":"2026-02-26T16:40:23Z","published":"2026-02-26T16:40:23Z","title":"Closing the gap on tabular data with Fourier and Implicit Categorical Features","summary":"While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last \"unconquered castle\" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We address this performance gap by using statistical-based feature processing techniques to identify features that are strongly correlated with the target once discretized. We further mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier. We show that our proposed feature preprocessing significantly boosts the performance of deep learning models and enables them to achieve a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark.","authors":["Marius Dragoi","Florin Gogianu","Elena Burceanu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23179v1","updated":"2026-02-26T16:39:04Z","published":"2026-02-26T16:39:04Z","title":"Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models","summary":"Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.","authors":["Gal Kesten-Pomeranz","Yaniv Nikankin","Anja Reusch","Tomer Tsaban","Ora Schueler-Furman","Yonatan Belinkov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23167v1","updated":"2026-02-26T16:31:23Z","published":"2026-02-26T16:31:23Z","title":"SettleFL: Trustless and Scalable Reward Settlement Protocol for Federated Learning on Permissionless Blockchains (Extended version)","summary":"In open Federated Learning (FL) environments where no central authority exists, ensuring collaboration fairness relies on decentralized reward settlement, yet the prohibitive cost of permissionless blockchains directly clashes with the high-frequency, iterative nature of model training. Existing solutions either compromise decentralization or suffer from scalability bottlenecks due to linear on-chain costs. To address this, we present SettleFL, a trustless and scalable reward settlement protocol designed to minimize total economic friction by offering a family of two interoperable protocols. Leveraging a shared domain-specific circuit architecture, SettleFL offers two interoperable strategies: (1) a Commit-and-Challenge variant that minimizes on-chain costs via optimistic execution and dispute-driven arbitration, and (2) a Commit-with-Proof variant that guarantees instant finality through per-round validity proofs. This design allows the protocol to flexibly adapt to varying latency and cost constraints while enforcing rational robustness without trusted coordination. We conduct extensive experiments combining real FL workloads and controlled simulations. Results show that SettleFL remains practical when scaling to 800 participants, achieving substantially lower gas cost.","authors":["Shuang Liang","Yang Hua","Linshan Jiang","Peishen Yan","Tao Song","Bin Yao","Haibing Guan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23164v1","updated":"2026-02-26T16:28:09Z","published":"2026-02-26T16:28:09Z","title":"MetaOthello: A Controlled Study of Multiple World Models in Transformers","summary":"Foundation models must handle multiple generative processes, yet mechanistic interpretability largely studies capabilities in isolation; it remains unclear how a single transformer organizes multiple, potentially conflicting \"world models\". Previous experiments on Othello playing neural-networks test world-model learning but focus on a single game with a single set of rules. We introduce MetaOthello, a controlled suite of Othello variants with shared syntax but different rules or tokenizations, and train small GPTs on mixed-variant data to study how multiple world models are organized in a shared representation space. We find that transformers trained on mixed-game data do not partition their capacity into isolated sub-models; instead, they converge on a mostly shared board-state representation that transfers causally across variants. Linear probes trained on one variant can intervene on another's internal state with effectiveness approaching that of matched probes. For isomorphic games with token remapping, representations are equivalent up to a single orthogonal rotation that generalizes across layers. When rules partially overlap, early layers maintain game-agnostic representations while a middle layer identifies game identity, and later layers specialize. MetaOthello offers a path toward understanding not just whether transformers learn world models, but how they organize many at once.","authors":["Aviral Chawla","Galen Hall","Juniper Lovato"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23159v1","updated":"2026-02-26T16:19:02Z","published":"2026-02-26T16:19:02Z","title":"Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge","summary":"Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \\emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \\textit{FinSurvival Challenge 2025} as a case study in benchmarking \\emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.","authors":["Oshani Seneviratne","Fernando Spadea","Adrien Pavao","Aaron Micah Green","Kristin P. Bennett"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23146v1","updated":"2026-02-26T16:11:53Z","published":"2026-02-26T16:11:53Z","title":"Partial recovery of meter-scale surface weather","summary":"Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.","authors":["Jonathan Giezendanner","Qidong Yang","Eric Schmitt","Anirban Chandra","Daniel Salles Civitarese","Johannes Jakubik","Jeremy Vila","Detlef Hohl","Campbell Watson","Sherrie Wang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23142v1","updated":"2026-02-26T16:08:50Z","published":"2026-02-26T16:08:50Z","title":"Prediction of Diffusion Coefficients in Mixtures with Tensor Completion","summary":"Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.","authors":["Zeno Romero","Kerstin Münnemann","Hans Hasse","Fabian Jirasek"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.10503v4","updated":"2026-02-26T16:08:47Z","published":"2025-03-13T16:05:56Z","title":"Sample Compression for Self Certified Continual Learning","summary":"Continual learning algorithms aim to learn from a sequence of tasks. In order to avoid catastrophic forgetting, most existing approaches rely on heuristics and do not provide computable learning guarantees. In this paper, we introduce Continual Pick-to-Learn (CoP2L), a method grounded in sample compression theory that retains representative samples for each task in a principled and efficient way. This allows us to derive non-vacuous, numerically computable upper bounds on the generalization loss of the learned predictors after each task. We evaluate CoP2L on standard continual learning benchmarks under Class-Incremental and Task-Incremental settings, showing that it effectively mitigates catastrophic forgetting. It turns out that CoP2L is empirically competitive with baseline methods while certifying predictor reliability in continual learning with a non-vacuous bound.","authors":["Jacob Comeau","Mathieu Bazinet","Pascal Germain","Cem Subakan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23136v1","updated":"2026-02-26T15:52:48Z","published":"2026-02-26T15:52:48Z","title":"Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs","summary":"Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.","authors":["Jayadev Billa"],"pdf_url":"","comment":"22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper"},{"id":"http://arxiv.org/abs/2602.23135v1","updated":"2026-02-26T15:51:51Z","published":"2026-02-26T15:51:51Z","title":"DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding","summary":"Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.","authors":["Tyler Bonnet","Marek Rei"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23132v1","updated":"2026-02-26T15:48:09Z","published":"2026-02-26T15:48:09Z","title":"From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation","summary":"Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \\textbf{FatsMB}, a framework based diffusion model that guides preference generation \\textit{\\textbf{F}rom Behavior-\\textbf{A}gnostic \\textbf{T}o Behavior-\\textbf{S}pecific} in latent spaces, enabling diverse and accurate \\textit{\\textbf{M}ulti-\\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.","authors":["Ruochen Yang","Xiaodong Li","Jiawei Sheng","Jiangxia Cao","Xinkui Lin","Shen Wang","Shuang Yang","Zhaojie Liu","Tingwen Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23128v1","updated":"2026-02-26T15:42:13Z","published":"2026-02-26T15:42:13Z","title":"Bound to Disagree: Generalization Bounds via Certifiable Surrogates","summary":"Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.","authors":["Mathieu Bazinet","Valentina Zantedeschi","Pascal Germain"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.15429v2","updated":"2026-02-26T15:29:30Z","published":"2025-09-18T21:08:38Z","title":"Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data","summary":"Single-cell RNA-seq provides detailed molecular snapshots of individual cells but is notoriously noisy. Variability stems from biological differences and technical factors, such as amplification bias and limited RNA capture efficiency, making it challenging to adapt computational pipelines to heterogeneous datasets or evolving technologies. As a result, most studies still rely on principal component analysis (PCA) for dimensionality reduction, valued for its interpretability and robustness, in spite of its known bias in high dimensions. Here, we improve upon PCA with a Random Matrix Theory (RMT)-based approach that guides the inference of sparse principal components using existing sparse PCA algorithms. We first introduce a novel biwhitening algorithm which self-consistently estimates the magnitude of transcriptomic noise affecting each gene in individual cells, without assuming a specific noise distribution. This enables the use of an RMT-based criterion to automatically select the sparsity level, rendering sparse PCA nearly parameter-free. Our mathematically grounded approach retains the interpretability of PCA while enabling robust, hands-off inference of sparse principal components. Across seven single-cell RNA-seq technologies and four sparse PCA algorithms, we show that this method systematically improves the reconstruction of the principal subspace and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification tasks.","authors":["Victor Chardès"],"pdf_url":"","comment":"16 figures"},{"id":"http://arxiv.org/abs/2602.00299v2","updated":"2026-02-26T15:28:01Z","published":"2026-01-30T20:45:45Z","title":"Agentic Framework for Epidemiological Modeling","summary":"Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.","authors":["Rituparna Datta","Zihan Guan","Baltazar Espinoza","Yiqi Su","Priya Pitre","Srini Venkatramanan","Naren Ramakrishnan","Anil Vullikanti"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23116v1","updated":"2026-02-26T15:27:53Z","published":"2026-02-26T15:27:53Z","title":"Regularized Online RLHF with Generalized Bilinear Preferences","summary":"We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\\tilde{O}(ηd^4 (\\log T)^2)$. (2) Explore-Then-Commit achieves $\\mathrm{poly}(d)$-free regret $\\tilde{O}(\\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.","authors":["Junghyun Lee","Minju Hong","Kwang-Sung Jun","Chulhee Yun","Se-Young Yun"],"pdf_url":"","comment":"43 pages, 1 table"},{"id":"http://arxiv.org/abs/2602.23113v1","updated":"2026-02-26T15:27:14Z","published":"2026-02-26T15:27:14Z","title":"Learning Physical Operators using Neural Operators","summary":"Neural operators have emerged as promising surrogate models for solving partial differential equations (PDEs), but struggle to generalise beyond training distributions and are often constrained to a fixed temporal discretisation. This work introduces a physics-informed training framework that addresses these limitations by decomposing PDEs using operator splitting methods, training separate neural operators to learn individual non-linear physical operators while approximating linear operators with fixed finite-difference convolutions. This modular mixture-of-experts architecture enables generalisation to novel physical regimes by explicitly encoding the underlying operator structure. We formulate the modelling task as a neural ordinary differential equation (ODE) where these learned operators constitute the right-hand side, enabling continuous-in-time predictions through standard ODE solvers and implicitly enforcing PDE constraints. Demonstrated on incompressible and compressible Navier-Stokes equations, our approach achieves better convergence and superior performance when generalising to unseen physics. The method remains parameter-efficient, enabling temporal extrapolation beyond training horizons, and provides interpretable components whose behaviour can be verified against known physics.","authors":["Vignesh Gopakumar","Ander Gray","Dan Giles","Lorenzo Zanisi","Matt J. Kusner","Timo Betcke","Stanislas Pamela","Marc Peter Deisenroth"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23111v1","updated":"2026-02-26T15:23:34Z","published":"2026-02-26T15:23:34Z","title":"PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training","summary":"Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.","authors":["Yanyi Li","Yimu Zhang","Cong Fang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.19929v2","updated":"2026-02-26T15:21:16Z","published":"2025-09-24T09:38:11Z","title":"Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later","summary":"Uncertainty Quantification (UQ) is paramount for inference in engineering. A common inference task is to recover full-field information of physical systems from a small number of noisy observations, a usually highly ill-posed problem. Sharing information from multiple distinct yet related physical systems can alleviate this ill-possendess. Critically, engineering systems often have complicated variable geometries prohibiting the use of standard multi-system Bayesian UQ. In this work, we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework for learning geometry-aware generative models of physical responses that serve as highly informative geometry-conditioned priors for Bayesian inversion. Following a ''learn first, observe later'' paradigm, GABI distills information from large datasets of systems with varying geometries, without requiring knowledge of governing PDEs, boundary conditions, or observation processes, into a rich latent prior. At inference time, this prior is seamlessly combined with the likelihood of a specific observation process, yielding a geometry-adapted posterior distribution. Our proposed framework is architecture agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling yields an efficient implementation that utilizes modern GPU hardware. We test our method on: steady-state heat over rectangular domains; Reynold-Averaged Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source localization on 3D car bodies; RANS airflow over terrain. We find: the predictive accuracy to be comparable to deterministic supervised learning approaches in the restricted setting where supervised learning is applicable; UQ to be well calibrated and robust on challenging problems with complex geometries.","authors":["Arnaud Vadeboncoeur","Gregory Duthé","Mark Girolami","Eleni Chatzi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.22123v3","updated":"2026-02-26T15:12:43Z","published":"2026-01-29T18:47:46Z","title":"Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics","summary":"Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.","authors":["Winfried Ripken","Michael Plainer","Gregor Lied","Thorben Frank","Oliver T. Unke","Stefan Chmiela","Frank Noé","Klaus-Robert Müller"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23089v1","updated":"2026-02-26T15:10:45Z","published":"2026-02-26T15:10:45Z","title":"Physics-informed neural particle flow for the Bayesian update step","summary":"The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.","authors":["Domonkos Csuzdi","Tamás Bécsi","Olivér Törő"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23085v1","updated":"2026-02-26T15:08:59Z","published":"2026-02-26T15:08:59Z","title":"Q-Tag: Watermarking Quantum Circuit Generative Models","summary":"Quantum cloud platforms have become the most widely adopted and mainstream approach for accessing quantum computing resources, due to the scarcity and operational complexity of quantum hardware. In this service-oriented paradigm, quantum circuits, which constitute high-value intellectual property, are exposed to risks of unauthorized access, reuse, and misuse. Digital watermarking has been explored as a promising mechanism for protecting quantum circuits by embedding ownership information for tracing and verification. However, driven by recent advances in generative artificial intelligence, the paradigm of quantum circuit design is shifting from individually and manually constructed circuits to automated synthesis based on quantum circuit generative models (QCGMs). In such generative settings, protecting only individual output circuits is insufficient, and existing post hoc, circuit-centric watermarking methods are not designed to integrate with the generative process, often failing to simultaneously ensure stealthiness, functional correctness, and robustness at scale. These limitations highlight the need for a new watermarking paradigm that is natively integrated with quantum circuit generative models. In this work, we present the first watermarking framework for QCGMs, which embeds ownership signals into the generation process while preserving circuit fidelity. We introduce a symmetric sampling strategy that aligns watermark encoding with the model's Gaussian prior, and a synchronization mechanism that counteracts adversarial watermark attack through latent drift correction. Empirical results confirm that our method achieves high-fidelity circuit generation and robust watermark detection across a range of perturbations, paving the way for scalable, secure copyright protection in AI-powered quantum design.","authors":["Yang Yang","Yuzhu Long","Han Fang","Zhaoyun Chen","Zhonghui Li","Weiming Zhang","Guoping Guo"],"pdf_url":"","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2602.23079v1","updated":"2026-02-26T15:05:13Z","published":"2026-02-26T15:05:13Z","title":"Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent","summary":"The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.","authors":["Boyang Zhang","Yang Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.24597v3","updated":"2026-02-26T15:04:01Z","published":"2025-09-29T11:03:16Z","title":"Inducing Dyslexia in Vision Language Models","summary":"Dyslexia, a neurodevelopmental disorder characterized by persistent reading difficulties, is often linked to reduced activity of the visual word form area (VWFA) in the ventral occipito-temporal cortex. Traditional approaches to studying dyslexia, such as behavioral and neuroimaging methods, have provided valuable insights but remain limited in their ability to test causal hypotheses about the underlying mechanisms of reading impairments. In this study, we use large-scale vision-language models (VLMs) to simulate dyslexia by functionally identifying and perturbing artificial analogues of word processing. Using stimuli from cognitive neuroscience, we identify visual-word-form-selective units within VLMs and demonstrate that they predict human VWFA neural responses. Ablating model VWF units leads to selective impairments in reading tasks while general visual and language comprehension abilities remain intact. In particular, the resulting model matches dyslexic humans' phonological deficits without a significant change in orthographic processing, and mirrors dyslexic behavior in font sensitivity. Taken together, our modeling results replicate key characteristics of dyslexia and establish a computational framework for investigating brain disorders.","authors":["Melika Honarmand","Ayati Sharma","Badr AlKhamissi","Johannes Mehrer","Martin Schrimpf"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23061v1","updated":"2026-02-26T14:48:49Z","published":"2026-02-26T14:48:49Z","title":"MoDora: Tree-Based Semi-Structured Document Analysis System","summary":"Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.\n  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.","authors":["Bangrui Xu","Qihang Yao","Zirui Tang","Xuanhe Zhou","Yeye He","Shihan Yu","Qianqian Xu","Bin Wang","Guoliang Li","Conghui He","Fan Wu"],"pdf_url":"","comment":"Extension of our SIGMOD 2026 paper. Please refer to source code available at https://github.com/weAIDB/MoDora"},{"id":"http://arxiv.org/abs/2602.23060v1","updated":"2026-02-26T14:45:29Z","published":"2026-02-26T14:45:29Z","title":"RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection","summary":"Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.","authors":["Xin Wang","Burcu Ozek","Aruna Mohan","Amirhossein Ravari","Or Zilbershot","Fatemeh Afghah"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23050v1","updated":"2026-02-26T14:35:45Z","published":"2026-02-26T14:35:45Z","title":"Latent Matters: Learning Deep State-Space Models","summary":"Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by maximising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework significantly improves system identification and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE outperforms previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.","authors":["Alexej Klushyn","Richard Kurle","Maximilian Soelch","Botond Cseke","Patrick van der Smagt"],"pdf_url":"","comment":"Published at NeurIPS 2021"},{"id":"http://arxiv.org/abs/2510.26577v2","updated":"2026-02-26T14:19:55Z","published":"2025-10-30T15:04:36Z","title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models","summary":"Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5 % to 20%. The code is available at https://github.com/EAGLE-Research/sglang-eagle4.","authors":["Yinrong Hong","Zhiquan Tan","Kai Hu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23035v1","updated":"2026-02-26T14:17:49Z","published":"2026-02-26T14:17:49Z","title":"Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements","summary":"Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.","authors":["Viraj Patel","Marko Grujic","Philipp Aigner","Theodor Abart","Marcus Granegger","Deblina Bhattacharjee","Katharine Fraser"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23023v1","updated":"2026-02-26T14:03:55Z","published":"2026-02-26T14:03:55Z","title":"Low-degree Lower bounds for clustering in moderate dimension","summary":"We study the fundamental problem of clustering $n$ points into $K$ groups drawn from a mixture of isotropic Gaussians in $\\mathbb{R}^d$. Specifically, we investigate the requisite minimal distance $Δ$ between mean vectors to partially recover the underlying partition. While the minimax-optimal threshold for $Δ$ is well-established, a significant gap exists between this information-theoretic limit and the performance of known polynomial-time procedures. Although this gap was recently characterized in the high-dimensional regime ($n \\leq dK$), it remains largely unexplored in the moderate-dimensional regime ($n \\geq dK$). In this manuscript, we address this regime by establishing a new low-degree polynomial lower bound for the moderate-dimensional case when $d \\geq K$. We show that while the difficulty of clustering for $n \\leq dK$ is primarily driven by dimension reduction and spectral methods, the moderate-dimensional regime involves more delicate phenomena leading to a \"non-parametric rate\". We provide a novel non-spectral algorithm matching this rate, shedding new light on the computational limits of the clustering problem in moderate dimension.","authors":["Alexandra Carpentier","Nicolas Verzelen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.05790v6","updated":"2026-02-26T14:02:26Z","published":"2025-12-05T15:16:59Z","title":"Learnability Window in Gated Recurrent Neural Networks","summary":"We develop a statistical theory of temporal learnability in recurrent neural networks, showing how gating mechanisms determine the learnability window $\\mathcal{H}_N$, defined as the maximal temporal horizon over which gradient information remains recoverable at sample size $N$. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone does not guarantee recoverability. Instead, learnability is governed by the interaction between the decay geometry of the effective learning rate envelope $f(\\ell)=\\|μ_{t,\\ell}\\|_1$, derived from first-order expansions of gate-induced Jacobians in Backpropagation Through Time, and the statistical concentration properties of stochastic gradients. Under heavy-tailed ($α$-stable) gradient noise, empirical averages concentrate at rate $N^{-1/κ_α}$ with $κ_α=α/(α-1)$. We prove that this interaction yields explicit scaling laws for the growth of $\\mathcal{H}_N$, distinguishing logarithmic, polynomial, and exponential temporal learning regimes according to the attenuation of $f(\\ell)$. The theory reveals that gate-induced time-scale spectra are the dominant determinants of temporal learnability: broader spectra slow envelope decay and systematically expand $\\mathcal{H}_N$, whereas heavy-tailed noise uniformly compresses temporal horizons by weakening statistical concentration. Empirical results across multiple gated architectures confirm these structural scaling predictions.","authors":["Lorenzo Livi"],"pdf_url":"","comment":"Added results with LSTM and GRU. Improved discussions"},{"id":"http://arxiv.org/abs/2602.15457v2","updated":"2026-02-26T13:58:16Z","published":"2026-02-17T09:45:44Z","title":"Benchmarking IoT Time-Series AD with Event-Level Augmentations","summary":"Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.","authors":["Dmitry Zhevnenko","Ilya Makarov","Aleksandr Kovalenko","Fedor Meshchaninov","Anton Kozhukhov","Vladislav Travnikov","Makar Ippolitov","Kirill Yashunin","Iurii Katser"],"pdf_url":"","comment":"https://underline.io/events/521/sessions/21822/lecture/143905-benchmarking-iot-time-series-ad-with-event-level-augmentations?tab=poster"},{"id":"http://arxiv.org/abs/2602.23013v1","updated":"2026-02-26T13:52:57Z","published":"2026-02-26T13:52:57Z","title":"SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling","summary":"Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.","authors":["Camile Lendering","Erkut Akdag","Egor Bondarev"],"pdf_url":"","comment":"Accepted to CVPR 2026"},{"id":"http://arxiv.org/abs/2602.23012v1","updated":"2026-02-26T13:52:54Z","published":"2026-02-26T13:52:54Z","title":"Sequential Regression for Continuous Value Prediction using Residual Quantization","summary":"Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.\n  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.","authors":["Runpeng Cui","Zhipeng Sun","Chi Lu","Peng Jiang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.23008v1","updated":"2026-02-26T13:50:57Z","published":"2026-02-26T13:50:57Z","title":"Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization","summary":"Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.","authors":["Zeyuan Liu","Jeonghye Kim","Xufang Luo","Dongsheng Li","Yuqing Yang"],"pdf_url":"","comment":"Accepted to ICLR 2026"},{"id":"http://arxiv.org/abs/2602.23006v1","updated":"2026-02-26T13:50:28Z","published":"2026-02-26T13:50:28Z","title":"Regular Fourier Features for Nonstationary Gaussian Processes","summary":"Simulating a Gaussian process requires sampling from a high-dimensional Gaussian distribution, which scales cubically with the number of sample locations. Spectral methods address this challenge by exploiting the Fourier representation, treating the spectral density as a probability distribution for Monte Carlo approximation. Although this probabilistic interpretation works for stationary processes, it is overly restrictive for the nonstationary case, where spectral densities are generally not probability measures. We propose regular Fourier features for harmonizable processes that avoid this limitation. Our method discretizes the spectral representation directly, preserving the correlation structure among spectral weights without requiring probability assumptions. Under a finite spectral support assumption, this yields an efficient low-rank approximation that is positive semi-definite by construction. When the spectral density is unknown, the framework extends naturally to kernel learning from data. We demonstrate the method on locally stationary kernels and on harmonizable mixture kernels with complex-valued spectral densities.","authors":["Arsalan Jawaid","Abdullah Karatas","Jörg Seewig"],"pdf_url":"","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15190v3","updated":"2026-02-26T13:41:11Z","published":"2025-06-18T07:11:48Z","title":"Learning Task-Agnostic Motifs to Capture the Continuous Nature of Animal Behavior","summary":"Animals flexibly recombine a finite set of core motor motifs to meet diverse task demands, but existing behavior segmentation methods oversimplify this process by imposing discrete syllables under restrictive generative assumptions. To better capture the continuous structure of behavior generation, we introduce motif-based continuous dynamics (MCD) discovery, a framework that (1) uncovers interpretable motif sets as latent basis functions of behavior by leveraging representations of behavioral transition structure, and (2) models behavioral dynamics as continuously evolving mixtures of these motifs. We validate MCD on a multi-task gridworld, a labyrinth navigation task, and freely moving animal behavior. Across settings, it identifies reusable motif components, captures continuous compositional dynamics, and generates realistic trajectories beyond the capabilities of traditional discrete segmentation models. By providing a generative account of how complex animal behaviors emerge from dynamic combinations of fundamental motor motifs, our approach advances the quantitative study of natural behavior.","authors":["Jiyi Wang","Jingyang Ke","Bo Dai","Anqi Wu"],"pdf_url":"","comment":"8 pages and 4 figures for the main text"},{"id":"http://arxiv.org/abs/2602.02334v2","updated":"2026-02-26T13:40:03Z","published":"2026-02-02T16:58:17Z","title":"VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations","summary":"Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating codebook learning with contrastive learning and a novel information leakage loss to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.","authors":["Fatemeh Zargarbashi","Dhruv Agrawal","Jakob Buhmann","Martin Guay","Stelian Coros","Robert W. Sumner"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22988v1","updated":"2026-02-26T13:33:25Z","published":"2026-02-26T13:33:25Z","title":"Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability","summary":"Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the near-unit spectral mass, quantifies the fraction of modes concentrated near the unit circle, which captures instability risk. For predicting divergence across extensive configurations, this estimator achieves an AUROC of 0.995, outperforming the best gradient baseline. We further make this diagnostic actionable through Koopman Spectral Shaping (KSS), which reshapes spectra during training. We empirically validate that our method works in practice: RKSP predicts divergence at initialization, and when RKSP flags high risk, turning on KSS successfully prevents divergence. In the challenging high learning rate regime without normalization layers, KSS reduces the divergence rate from 66.7% to 12.5% and enables learning rates that are 50% to 150% higher. These findings generalize to WikiText-103 language modeling, vision transformers on CIFAR-10, and pretrained language models, including GPT-2 and LLaMA-2 up to 7B, as well as emerging architectures such as MoE, Mamba-style SSMs, and KAN.","authors":["Bum Jun Kim","Shohei Taniguchi","Makoto Kawano","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.22985v1","updated":"2026-02-26T13:29:12Z","published":"2026-02-26T13:29:12Z","title":"Kernel Integrated $R^2$: A Measure of Dependence","summary":"We introduce kernel integrated $R^2$, a new measure of statistical dependence that combines the local normalization principle of the recently introduced integrated $R^2$ with the flexibility of reproducing kernel Hilbert spaces (RKHSs). The proposed measure extends integrated $R^2$ from scalar responses to responses taking values on general spaces equipped with a characteristic kernel, allowing to measure dependence of multivariate, functional, and structured data, while remaining sensitive to tail behaviour and oscillatory dependence structures. We establish that (i) this new measure takes values in $[0,1]$, (ii) equals zero if and only if independence holds, and (iii) equals one if and only if the response is almost surely a measurable function of the covariates. Two estimators are proposed: a graph-based method using $K$-nearest neighbours and an RKHS-based method built on conditional mean embeddings. We prove consistency and derive convergence rates for the graph-based estimator, showing its adaptation to intrinsic dimensionality. Numerical experiments on simulated data and a real data experiment in the context of dependency testing for media annotations demonstrate competitive power against state-of-the-art dependence measures, particularly in settings involving non-linear and structured relationships.","authors":["Pouya Roudaki","Shakeel Gavioli-Akilagun","Florian Kalinke","Mona Azadkia","Zoltán Szabó"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.12125v2","updated":"2026-02-26T13:26:22Z","published":"2026-02-12T16:14:29Z","title":"Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation","summary":"On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.","authors":["Wenkai Yang","Weijie Liu","Ruobing Xie","Kai Yang","Saiyong Yang","Yankai Lin"],"pdf_url":"","comment":"v2, update results under stronger teachers with more RL training steps"},{"id":"http://arxiv.org/abs/2512.02700v4","updated":"2026-02-26T13:16:26Z","published":"2025-12-02T12:30:05Z","title":"VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm","summary":"Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.","authors":["Zhenkai Wu","Xiaowen Ma","Zhenliang Ni","Dengming Zhang","Han Shu","Xin Jiang","Xinghao Chen"],"pdf_url":"","comment":"Accepted by CVPR2026"},{"id":"http://arxiv.org/abs/2407.17120v3","updated":"2026-02-26T13:06:51Z","published":"2024-07-24T09:30:04Z","title":"Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective","summary":"Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown promise in adapting pre-trained models to sequential tasks while mitigating catastrophic forgetting problem. However, understanding the mechanisms that dictate continual performance in this paradigm remains elusive. To unravel this mystery, we undertake a rigorous analysis of PEFT-CL dynamics to derive relevant metrics for continual scenarios using Neural Tangent Kernel (NTK) theory. With the aid of NTK as a mathematical analysis tool, we recast the challenge of test-time forgetting into the quantifiable generalization gaps during training, identifying three key factors that influence these gaps and the performance of PEFT-CL: training sample size, task-level feature orthogonality, and regularization. To address these challenges, we introduce NTK-CL, a novel framework that eliminates task-specific parameter storage while adaptively generating task-relevant features. Aligning with theoretical guidance, NTK-CL triples the feature representation of each sample, theoretically and empirically reducing the magnitude of both task-interplay and task-specific generalization gaps. Grounded in NTK analysis, our framework imposes an adaptive exponential moving average mechanism and constraints on task-level feature orthogonality, maintaining intra-task NTK forms while attenuating inter-task NTK forms. Ultimately, by fine-tuning optimizable parameters with appropriate regularization, NTK-CL achieves state-of-the-art performance on established PEFT-CL benchmarks. This work provides a theoretical foundation for understanding and improving PEFT-CL models, offering insights into the interplay between feature representation, task orthogonality, and generalization, contributing to the development of more efficient continual learning systems.","authors":["Jingren Liu","Zhong Ji","YunLong Yu","Jiale Cao","Yanwei Pang","Jungong Han","Xuelong Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.22935v2","updated":"2026-02-26T13:04:00Z","published":"2025-09-26T21:09:54Z","title":"Compute-Optimal Quantization-Aware Training","summary":"Quantization-aware training (QAT) is a leading technique for improving the accuracy of quantized neural networks. Previous work has shown that decomposing training into a full-precision (FP) phase followed by a QAT phase yields superior accuracy compared to QAT alone. However, the optimal allocation of compute between the FP and QAT phases remains unclear. We conduct extensive experiments with various compute budgets, QAT bit widths, and model sizes from 86.0M to 2.2B to investigate how different QAT durations impact final performance. We demonstrate that, contrary to previous findings, the loss-optimal ratio of QAT to FP training increases with the total amount of compute. Moreover, the optimal fraction can be accurately predicted for a wide range of model sizes and quantization widths using the tokens-per-parameter-byte statistic. From experimental data, we derive a loss scaling law that predicts both optimal QAT ratios and final model performance across different QAT/FP compute allocation strategies and QAT bit widths. We use the scaling law to make further predictions, which we verify experimentally, including which QAT bit width is optimal under a given memory constraint and how QAT accuracy with different bit widths compares to full-precision model accuracy. Additionally, we propose a novel cooldown and QAT fusion approach that performs learning rate decay jointly with quantization-aware training, eliminating redundant full-precision model updates and achieving significant compute savings. These findings provide practical insights into efficient QAT planning and enable the training of higher-quality quantized models with the same compute budget.","authors":["Aleksandr Dremov","David Grangier","Angelos Katharopoulos","Awni Hannun"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2510.27480v2","updated":"2026-02-26T13:00:09Z","published":"2025-10-31T14:00:33Z","title":"Simplex-to-Euclidean Bijections for Categorical Flow Matching","summary":"We propose a method for learning and sampling from probability distributions supported on the simplex. Our approach maps the open simplex to Euclidean space via smooth bijections, leveraging the Aitchison geometry to define the mappings, and supports modeling categorical data by a Dirichlet interpolation that dequantizes discrete observations into continuous ones. This enables density modeling in Euclidean space through the bijection while still allowing exact recovery of the original discrete distribution. Compared to previous methods that operate on the simplex using Riemannian geometry or custom noise processes, our approach works in Euclidean space while respecting the Aitchison geometry, and achieves competitive performance on both synthetic and real-world data sets.","authors":["Bernardo Williams","Victor M. Yeom-Song","Marcelo Hartmann","Arto Klami"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22962v1","updated":"2026-02-26T12:57:38Z","published":"2026-02-26T12:57:38Z","title":"Scaling Laws of Global Weather Models","summary":"Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.","authors":["Yuejiang Yu","Langwen Huang","Alexandru Calotoiu","Torsten Hoefler"],"pdf_url":"","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2601.11670v2","updated":"2026-02-26T12:57:02Z","published":"2026-01-16T02:51:59Z","title":"A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning","summary":"Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)","authors":["Jinshi Liu","Pan Liu","Lei He"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.01031v2","updated":"2026-02-26T12:56:09Z","published":"2025-10-01T15:37:20Z","title":"Secure and reversible face anonymization with diffusion models","summary":"Face anonymization aims to protect sensitive identity information by altering faces while preserving visual realism and utility for downstream computer vision tasks. Current methods struggle to simultaneously ensure high image quality, strong security guarantees, and controlled reversibility for authorized identity recovery at a later time. To improve the image quality of generated anonymized faces, recent methods have adopted diffusion models. However, these new diffusion-based anonymization methods do not provide a mechanism to restrict de-anonymization to trusted parties, limiting their real-world applicability. In this paper, we present the first diffusion-based framework for secure, reversible face anonymization via secret-key conditioning. Our method injects the secret key directly into the diffusion process, enabling anonymization and authorized face reconstruction while preventing unauthorized de-anonymization. The use of deterministic forward and reverse diffusion steps guarantees exact identity recovery when the correct secret key is available. Experiments on CelebA-HQ and LFW demonstrate that our approach achieves better anonymization and de-anonymization capabilities than prior work. We also show that our method remains robust to incorrect or adversarial key de-anonymization. Our code will be made publicly available.","authors":["Pol Labarbarie","Vincent Itier","William Puech"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2509.26238v3","updated":"2026-02-26T12:49:39Z","published":"2025-09-30T13:32:59Z","title":"Beyond Linear Probes: Dynamic Safety Monitoring for Language Models","summary":"Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can \"buy\" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.","authors":["James Oldfield","Philip Torr","Ioannis Patras","Adel Bibi","Fazl Barez"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.20031v2","updated":"2026-02-26T12:43:02Z","published":"2026-02-23T16:39:42Z","title":"Latent Introspection: Models Can Detect Prior Concept Injections","summary":"We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.9%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.61 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.","authors":["Theia Pearson-Vogel","Martin Vanek","Raymond Douglas","Jan Kulveit"],"pdf_url":"","comment":"28 pages, 17 figures. Submitted to ICML 2026. Workshop version submitted to ICLR 2026 Workshop on Latent and Implicit Thinking"},{"id":"http://arxiv.org/abs/2602.10195v2","updated":"2026-02-26T12:37:43Z","published":"2026-02-10T19:00:02Z","title":"Versor: A Geometric Sequence Architecture","summary":"A novel sequence architecture is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of traditional linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders-of-magnitude fewer parameters ($200\\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (0.993 vs. 0.070 MCC for ViT); and featuring a Recursive Rotor Accumulator (RRA) for $O(L)$ linear temporal complexity in dynamical systems, and a Geometric Product Attention (GPA) mechanism for $O(L^{2})$ global relational modeling, allowing for task-specific architectural pruning or hybridization depending on the required scale. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve a cumulative over $100\\times$ speedup via bit-masked contraction and specialized Matrix Isomorphism kernels, reducing per-step latency to 1.05 ms and outperforming highly-optimized Transformer baselines.","authors":["Truong Minh Huy","Edward Hirst"],"pdf_url":"","comment":"19+28 pages, 5 figures"},{"id":"http://arxiv.org/abs/2509.21725v2","updated":"2026-02-26T12:32:59Z","published":"2025-09-26T00:52:14Z","title":"Information-Theoretic Bayesian Optimization for Bilevel Optimization Problems","summary":"A bilevel optimization problem consists of two optimization problems nested as an upper- and a lower-level problem, in which the optimality of the lower-level problem defines a constraint for the upper-level problem. This paper considers Bayesian optimization (BO) for the case that both the upper- and lower-levels involve expensive black-box functions. Because of its nested structure, bilevel optimization has a complex problem definition, by which bilevel BO has not been widely studied compared with other standard extensions of BO such as multi-objective or constraint problems. We propose an information-theoretic approach that considers the information gain of both the upper- and lower-optimal solutions and values. This enables us to define a unified criterion that measures the benefit for both level problems, simultaneously. Further, we also show a practical lower bound based approach to evaluating the information gain. We empirically demonstrate the effectiveness of our proposed method through several benchmark datasets.","authors":["Takuya Kanayama","Yuki Ito","Tomoyuki Tamura","Masayuki Karasuyama"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2503.05560v3","updated":"2026-02-26T12:29:08Z","published":"2025-03-07T16:38:41Z","title":"Global graph features unveiled by unsupervised geometric deep learning","summary":"Graphs provide a powerful framework for modeling complex systems, but their structural variability poses significant challenges for analysis and classification. To address these challenges, we introduce GAUDI (Graph Autoencoder Uncovering Descriptive Information), a novel unsupervised geometric deep learning framework designed to capture both local details and global structure. GAUDI employs an innovative hourglass architecture with hierarchical pooling and upsampling layers linked through skip connections, which preserve essential connectivity information throughout the encoding-decoding process. Even though identical or highly similar underlying parameters describing a system's state can lead to significant variability in graph realizations, GAUDI consistently maps them into nearby regions of a structured and continuous latent space, effectively disentangling invariant process-level features from stochastic noise. We demonstrate GAUDI's versatility across multiple applications, including small-world networks modeling, characterization of protein assemblies from super-resolution microscopy, analysis of collective motion in the Vicsek model, and identification of age-related changes in brain connectivity. Comparison with related approaches highlights GAUDI's superior performance in analyzing complex graphs, providing new insights into emergent phenomena across diverse scientific domains.","authors":["Mirja Granfors","Jesús Pineda","Blanca Zufiria Gerbolés","Joana B. Pereira","Carlo Manzo","Giovanni Volpe"],"pdf_url":"","comment":"28 pages, 6 figures"},{"id":"http://arxiv.org/abs/2602.22938v1","updated":"2026-02-26T12:27:06Z","published":"2026-02-26T12:27:06Z","title":"pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation","summary":"Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.","authors":["Shentong Mo","Xufang Luo","Dongsheng Li"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22937v1","updated":"2026-02-26T12:27:00Z","published":"2026-02-26T12:27:00Z","title":"MSINO: Curvature-Aware Sobolev Optimization for Manifold Neural Networks","summary":"We introduce Manifold Sobolev Informed Neural Optimization (MSINO), a curvature aware training framework for neural networks defined on Riemannian manifolds. The method replaces standard Euclidean derivative supervision with a covariant Sobolev loss that aligns gradients using parallel transport and improves stability via a Laplace Beltrami smoothness regularization term.\n  Building on classical results in Riemannian optimization and Sobolev theory on manifolds, we derive geometry dependent constants that yield (i) a Descent Lemma with a manifold Sobolev smoothness constant, (ii) a Sobolev Polyak Lojasiewicz inequality giving linear convergence guarantees for Riemannian gradient descent and stochastic gradient descent under explicit step size bounds, and (iii) a two step Newton Sobolev method with local quadratic contraction in curvature controlled neighborhoods.\n  Unlike prior Sobolev training in Euclidean space, MSINO provides training time guarantees that explicitly track curvature and transported Jacobians. Applications include surface imaging, physics informed learning settings, and robotics on Lie groups such as SO(3) and SE(3). The framework unifies value and gradient based learning with curvature aware convergence guarantees for neural training on manifolds.","authors":["Suresan Pareth"],"pdf_url":"","comment":"32 pages, 6 figures. Submitted for journal consideration"},{"id":"http://arxiv.org/abs/2602.22936v1","updated":"2026-02-26T12:26:32Z","published":"2026-02-26T12:26:32Z","title":"Generalization Bounds of Stochastic Gradient Descent in Homogeneous Neural Networks","summary":"Algorithmic stability is among the most potent techniques in generalization analysis. However, its derivation usually requires a stepsize $η_t = \\mathcal{O}(1/t)$ under non-convex training regimes, where $t$ denotes iterations. This rigid decay of the stepsize potentially impedes optimization and may not align with practical scenarios. In this paper, we derive the generalization bounds under the homogeneous neural network regimes, proving that this regime enables slower stepsize decay of order $Ω(1/\\sqrt{t})$ under mild assumptions. We further extend the theoretical results from several aspects, e.g., non-Lipschitz regimes. This finding is broadly applicable, as homogeneous neural networks encompass fully-connected and convolutional neural networks with ReLU and LeakyReLU activations.","authors":["Wenquan Ma","Yang Sui","Jiaye Teng","Bohan Wang","Jing Xu","Jingqin Yang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.15464v2","updated":"2026-02-26T12:24:34Z","published":"2025-10-17T09:20:17Z","title":"Learning to Answer from Correct Demonstrations","summary":"We study the problem of learning to generate an answer (or completion) to a question (or prompt), where there could be multiple correct answers, any one of which is acceptable at test time. Learning is based on demonstrations of some correct answer to each training question, as in Supervised Fine Tuning (SFT). We formalize the problem as imitation learning (i.e., apprenticeship learning) in contextual bandits, with offline demonstrations from some expert (optimal, or very good) policy, without explicitly observed rewards. In contrast to prior work, which assumes the demonstrator belongs to a bounded-complexity policy class, we propose relying only on the underlying reward model (i.e., specifying which answers are correct) being in a bounded-complexity class, which we argue is a strictly weaker assumption. We show that likelihood-maximization methods can fail in this setting, and instead present an approach that learns to answer nearly as well as the demonstrator, with sample complexity logarithmic in the cardinality of the reward class. Our method is similar to Syed and Schapire 2007, when adapted to a contextual bandit (i.e., single step) setup, but is a simple one-pass online approach that enjoys an \"optimistic rate\" (i.e., $1/\\varepsilon$ when the demonstrator is optimal, versus $1/\\varepsilon^2$ in Syed and Schapire), and works even with arbitrarily adaptive demonstrations.","authors":["Nirmit Joshi","Gene Li","Siddharth Bhandari","Shiva Prasad Kasiviswanathan","Cong Ma","Nathan Srebro"],"pdf_url":"","comment":"Generalized some results. Updated the presentation in light of an important related work of Syed and Schapire. Improved discussions. Comments are welcome"},{"id":"http://arxiv.org/abs/2505.08371v4","updated":"2026-02-26T12:20:29Z","published":"2025-05-13T09:18:41Z","title":"Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data","summary":"We address the problem of inferring the causal direction between a continuous variable $X$ and a discrete variable $Y$ from observational data. For the model $X \\to Y$, we adopt the threshold model used in prior work. For the model $Y \\to X$, we consider two cases: (1) the conditional distributions of $X$ given different values of $Y$ form a location-shift family, and (2) they are mixtures of generalized normal distributions with independently parameterized components. We establish identifiability of the causal direction through three theoretical results. First, we prove that under $X \\to Y$, the density ratio of $X$ conditioned on different values of $Y$ is monotonic. Second, we establish that under $Y \\to X$ with non-location-shift conditionals, monotonicity of the density ratio holds only on a set of Lebesgue measure zero in the parameter space. Third, we show that under $X \\to Y$, the conditional distributions forming a location-shift family requires a precise coordination between the causal mechanism and input distribution, which is non-generic under the principle of independent mechanisms. Together, these results imply that monotonicity of the density ratio characterizes the direction $X \\to Y$, whereas non-monotonicity or location-shift conditionals characterizes $Y \\to X$. Based on this, we propose Density Ratio-based Causal Discovery (DRCD), a method that determines causal direction by testing for location-shift conditionals and monotonicity of the estimated density ratio. Experiments on synthetic and real-world datasets demonstrate that DRCD outperforms existing methods.","authors":["Takashi Nicholas Maeda","Shohei Shimizu","Hidetoshi Matsui"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2512.14990v3","updated":"2026-02-26T12:19:29Z","published":"2025-12-17T00:50:58Z","title":"Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent","summary":"Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.","authors":["Mehil B Shah","Mohammad Masudur Rahman","Foutse Khomh"],"pdf_url":"","comment":"Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)"},{"id":"http://arxiv.org/abs/2602.22925v1","updated":"2026-02-26T12:15:11Z","published":"2026-02-26T12:15:11Z","title":"Beyond NNGP: Large Deviations and Feature Learning in Bayesian Neural Networks","summary":"We study wide Bayesian neural networks focusing on the rare but statistically dominant fluctuations that govern posterior concentration, beyond Gaussian-process limits. Large-deviation theory provides explicit variational objectives-rate functions-on predictors, providing an emerging notion of complexity and feature learning directly at the functional level. We show that the posterior output rate function is obtained by a joint optimization over predictors and internal kernels, in contrast with fixed-kernel (NNGP) theory. Numerical experiments demonstrate that the resulting predictions accurately describe finite-width behavior for moderately sized networks, capturing non-Gaussian tails, posterior deformation, and data-dependent kernel selection effects.","authors":["Katerina Papagiannouli","Dario Trevisan","Giuseppe Pio Zitto"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2510.20035v2","updated":"2026-02-26T12:12:35Z","published":"2025-10-22T21:26:18Z","title":"Throwing Vines at the Wall: Structure Learning via Random Search","summary":"Vine copulas offer flexible multivariate dependence modeling and have become widely used in machine learning. Yet, structure learning remains a key challenge. Early heuristics, such as Dissmann's greedy algorithm, are still considered the gold standard but are often suboptimal. We propose random search algorithms and a statistical framework based on model confidence sets, to improve structure selection, provide theoretical guarantees on selection probabilities, and serve as a foundation for ensembling. Empirical results on real-world data sets show that our methods consistently outperform state-of-the-art approaches.","authors":["Thibault Vatter","Thomas Nagler"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22913v1","updated":"2026-02-26T12:00:46Z","published":"2026-02-26T12:00:46Z","title":"SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress","summary":"With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.","authors":["Yang Yu","Lei Kou","Huaikuan Yi","Bin Chen","Yayu Cao","Lei Shen","Chao Zhang","Bing Wang","Xiaoyi Zeng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22911v1","updated":"2026-02-26T11:55:25Z","published":"2026-02-26T11:55:25Z","title":"NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion","summary":"Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.","authors":["Hung-Hsuan Chen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22903v1","updated":"2026-02-26T11:47:32Z","published":"2026-02-26T11:47:32Z","title":"PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA","summary":"Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.","authors":["Yunpeng Hong","Chenyang Bu","Jie Zhang","Yi He","Di Wu","Xindong Wu"],"pdf_url":"","comment":"2026 SIGKDD accept"},{"id":"http://arxiv.org/abs/2602.22902v1","updated":"2026-02-26T11:47:22Z","published":"2026-02-26T11:47:22Z","title":"A Data-Driven Approach to Support Clinical Renal Replacement Therapy","summary":"This study investigates a data-driven machine learning approach to predict membrane fouling in critically ill patients undergoing Continuous Renal Replacement Therapy (CRRT). Using time-series data from an ICU, 16 clinically selected features were identified to train predictive models. To ensure interpretability and enable reliable counterfactual analysis, the researchers adopted a tabular data approach rather than modeling temporal dependencies directly. Given the imbalance between fouling and non-fouling cases, the ADASYN oversampling technique was applied to improve minority class representation. Random Forest, XGBoost, and LightGBM models were tested, achieving balanced performance with 77.6% sensitivity and 96.3% specificity at a 10% rebalancing rate. Results remained robust across different forecasting horizons. Notably, the tabular approach outperformed LSTM recurrent neural networks, suggesting that explicit temporal modeling was not necessary for strong predictive performance. Feature selection further reduced the model to five key variables, improving simplicity and interpretability with minimal loss of accuracy. A Shapley value-based counterfactual analysis was applied to the best-performing model, successfully identifying minimal input changes capable of reversing fouling predictions. Overall, the findings support the viability of interpretable machine learning models for predicting membrane fouling during CRRT. The integration of prediction and counterfactual analysis offers practical clinical value, potentially guiding therapeutic adjustments to reduce fouling risk and improve patient management.","authors":["Alice Balboni","Luis Escobar","Andrea Manno","Fabrizio Rossi","Maria Cristina Ruffa","Gianluca Villa","Giordano D'Aloisio","Antonio Consolo"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2505.24403v3","updated":"2026-02-26T11:42:54Z","published":"2025-05-30T09:34:58Z","title":"On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets","summary":"The Lipschitz constant of a neural network is connected to several important prop- erties of the network such as its robustness and generalization. It is thus useful in many settings to estimate the Lipschitz constant of a model. Prior work has fo- cused mainly on estimating the Lipschitz constant of multi-layer perceptrons and convolutional neural networks. Here we focus on data modeled as sets or multi- sets of vectors and on neural networks that can handle such data. These models typically apply some permutation invariant aggregation function, such as the sum, mean or max operator, to the input multisets to produce a single vector for each input sample. In this paper, we investigate whether these aggregation functions, along with an attention-based aggregation function, are Lipschitz continuous with respect to three distance functions for unordered multisets, and we compute their Lipschitz constants. In the general case, we find that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions, while the attention-based function is not Lipschitz continuous with respect to any of them. Then, we build on these results to derive upper bounds on the Lipschitz constant of neural networks that can process multisets of vectors, while we also study their stability to perturbations and generalization under distribution shifts. To empirically verify our theoretical analysis, we conduct a series of experiments on datasets from different domains.","authors":["Giannis Nikolentzos","Konstantinos Skianis"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22897v1","updated":"2026-02-26T11:35:04Z","published":"2026-02-26T11:35:04Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Shijian Wang","Guanting Dong","Jiajie Jin","Hao Wang","Yinuo Wang","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22895v1","updated":"2026-02-26T11:33:21Z","published":"2026-02-26T11:33:21Z","title":"SPD Learn: A Geometric Deep Learning Python Library for Neural Decoding Through Trivialization","summary":"Implementations of symmetric positive definite (SPD) matrix-based neural networks for neural decoding remain fragmented across research codebases and Python packages. Existing implementations often employ ad hoc handling of manifold constraints and non-unified training setups, which hinders reproducibility and integration into modern deep-learning workflows. To address this gap, we introduce SPD Learn, a unified and modular Python package for geometric deep learning with SPD matrices. SPD Learn provides core SPD operators and neural-network layers, including numerically stable spectral operators, and enforces Stiefel/SPD constraints via trivialization-based parameterizations. This design enables standard backpropagation and optimization in unconstrained Euclidean spaces while producing manifold-constrained parameters by construction. The package also offers reference implementations of representative SPDNet-based models and interfaces with widely used brain computer interface/neuroimaging toolkits and modern machine-learning libraries (e.g., MOABB, Braindecode, Nilearn, and SKADA), facilitating reproducible benchmarking and practical deployment.","authors":["Bruno Aristimunha","Ce Ju","Antoine Collas","Florent Bouchard","Ammar Mian","Bertrand Thirion","Sylvain Chevallier","Reinmar Kobler"],"pdf_url":"","comment":"9 Pages"},{"id":"http://arxiv.org/abs/2410.12439v2","updated":"2026-02-26T11:26:59Z","published":"2024-10-16T10:34:11Z","title":"Beyond Attribution: Unified Concept-Level Explanations","summary":"There is an increasing need to integrate model-agnostic explanation techniques with concept-based approaches, as the former can explain models across different architectures while the latter makes explanations more faithful and understandable to end-users. However, existing concept-based model-agnostic explanation methods are limited in scope, mainly focusing on attribution-based explanations while neglecting diverse forms like sufficient conditions and counterfactuals, thus narrowing their utility. To bridge this gap, we propose a general framework UnCLE to elevate existing local model-agnostic techniques to provide concept-based explanations. Our key insight is that we can uniformly extend existing local model-agnostic methods to provide unified concept-based explanations with large pre-trained model perturbation. We have instantiated UnCLE to provide concept-based explanations in three forms: attributions, sufficient conditions, and counterfactuals, and applied it to popular text, image, and multimodal models. Our evaluation results demonstrate that UnCLE provides explanations more faithful than state-of-the-art concept-based explanation methods, and provides richer explanation forms that satisfy various user needs.","authors":["Junhao Liu","Haonan Yu","Xin Zhang"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22884v1","updated":"2026-02-26T11:22:46Z","published":"2026-02-26T11:22:46Z","title":"Unsupervised Continual Learning for Amortized Bayesian Inference","summary":"Amortized Bayesian Inference (ABI) enables efficient posterior estimation using generative neural networks trained on simulated data, but often suffers from performance degradation under model misspecification. While self-consistency (SC) training on unlabeled empirical data can enhance network robustness, current approaches are limited to static, single-task settings and fail to handle sequentially arriving data or distribution shifts. We propose a continual learning framework for ABI that decouples simulation-based pre-training from unsupervised sequential SC fine-tuning on real-world data. To address the challenge of catastrophic forgetting, we introduce two adaptation strategies: (1) SC with episodic replay, utilizing a memory buffer of past observations, and (2) SC with elastic weight consolidation, which regularizes updates to preserve task-critical parameters. Across three diverse case studies, our methods significantly mitigate forgetting and yield posterior estimates that outperform standard simulation-based training, achieving estimates closer to MCMC reference, providing a viable path for trustworthy ABI across a range of different tasks.","authors":["Aayush Mishra","Šimon Kucharský","Paul-Christian Bürkner"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22882v1","updated":"2026-02-26T11:22:08Z","published":"2026-02-26T11:22:08Z","title":"Fair feature attribution for multi-output prediction: a Shapley-based perspective","summary":"In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.","authors":["Umberto Biccari","Alain Ibáñez de Opakua","José María Mato","Óscar Millet","Roberto Morales","Enrique Zuazua"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.12108v3","updated":"2026-02-26T11:16:09Z","published":"2025-02-17T18:29:24Z","title":"Using the Path of Least Resistance to Explain Deep Networks","summary":"Integrated Gradients (IG), a widely used axiomatic path-based attribution method, assigns importance scores to input features by integrating model gradients along a straight path from a baseline to the input. While effective in some cases, we show that straight paths can lead to flawed attributions. In this paper, we identify the cause of these misattributions and propose an alternative approach that equips the input space with a model-induced Riemannian metric (derived from the explained model's Jacobian) and computes attributions by integrating gradients along geodesics under this metric. We call this method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we introduce two techniques: a k-Nearest Neighbours-based approach for smaller models and a Stochastic Variational Inference-based method for larger ones. Additionally, we propose a new axiom, No-Cancellation Completeness (NCC), which strengthens completeness by ruling out feature-wise cancellation. We prove that, for path-based attributions under the model-induced metric, NCC holds if and only if the integration path is a geodesic. Through experiments on both synthetic and real-world image classification data, we provide empirical evidence supporting our theoretical analysis and showing that GIG produces more faithful attributions than existing methods, including IG, on the benchmarks considered.","authors":["Sina Salek","Joseph Enguehard"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2411.11727v2","updated":"2026-02-26T11:11:12Z","published":"2024-11-18T16:57:41Z","title":"Aligning Few-Step Diffusion Models with Dense Reward Difference Learning","summary":"Few-step diffusion models enable efficient high-resolution image synthesis but struggle to align with specific downstream objectives due to limitations of existing reinforcement learning (RL) methods in low-step regimes with limited state spaces and suboptimal sample quality. To address this, we propose Stepwise Diffusion Policy Optimization (SDPO), a novel RL framework tailored for few-step diffusion models. SDPO introduces a dual-state trajectory sampling mechanism, tracking both noisy and predicted clean states at each step to provide dense reward feedback and enable low-variance, mixed-step optimization. For further efficiency, we develop a latent similarity-based dense reward prediction strategy to minimize costly dense reward queries. Leveraging these dense rewards, SDPO optimizes a dense reward difference learning objective that enables more frequent and granular policy updates. Additional refinements, including stepwise advantage estimates, temporal importance weighting, and step-shuffled gradient updates, further enhance long-term dependency, low-step priority, and gradient stability. Our experiments demonstrate that SDPO consistently delivers superior reward-aligned results across diverse few-step settings and tasks. Code is available at https://github.com/ZiyiZhang27/sdpo.","authors":["Ziyi Zhang","Li Shen","Sen Zhang","Deheng Ye","Yong Luo","Miaojing Shi","Dongjing Shan","Bo Du","Dacheng Tao"],"pdf_url":"","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2602.22850v1","updated":"2026-02-26T10:38:41Z","published":"2026-02-26T10:38:41Z","title":"MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction","summary":"Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its \"black-box\" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a \"sequence-structure synergy\" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.","authors":["Yi He","Yina Cao","Jixiu Zhai","Di Wang","Junxiao Kong","Tianchi Lu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22847v1","updated":"2026-02-26T10:37:23Z","published":"2026-02-26T10:37:23Z","title":"Decentralized Ranking Aggregation: Gossip Algorithms for Borda and Copeland Consensus","summary":"The concept of ranking aggregation plays a central role in preference analysis, and numerous algorithms for calculating median rankings, often originating in social choice theory, have been documented in the literature, offering theoretical guarantees in a centralized setting, i.e., when all the ranking data to be aggregated can be brought together in a single computing unit. For many technologies (e.g. peer-to-peer networks, IoT, multi-agent systems), extending the ability to calculate consensus rankings with guarantees in a decentralized setting, i.e., when preference data is initially distributed across a communicating network, remains a major methodological challenge. Indeed, in recent years, the literature on decentralized computation has mainly focused on computing or optimizing statistics such as arithmetic means using gossip algorithms. The purpose of this article is precisely to study how to achieve reliable consensus on collective rankings using classical rules (e.g. Borda, Copeland) in a decentralized setting, thereby raising new questions, robustness to corrupted nodes, and scalability through reduced communication costs in particular. The approach proposed and analyzed here relies on random gossip communication, allowing autonomous agents to compute global ranking consensus using only local interactions, without coordination or central authority.\n  We provide rigorous convergence guarantees, including explicit rate bounds, for the Borda and Copeland consensus methods. Beyond these rules, we also provide a decentralized implementation of consensus according to the median rank rule and local Kemenization. Extensive empirical evaluations on various network topologies and real and synthetic ranking datasets demonstrate that our algorithms converge quickly and reliably to the correct ranking aggregation.","authors":["Anna Van Elst","Kerrian Le Caillec","Igor Colin","Stephan Clémençon"],"pdf_url":"","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2602.01749v3","updated":"2026-02-26T10:33:17Z","published":"2026-02-02T07:34:30Z","title":"Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives","summary":"Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \\times$ increase in the number of discovered modes.","authors":["Lin Chen","Samuel Drapeau","Fanghao Shao","Xuekai Zhu","Bo Xue","Yunchong Song","Mathieu Laurière","Zhouhan Lin"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22831v1","updated":"2026-02-26T10:17:57Z","published":"2026-02-26T10:17:57Z","title":"Moral Preferences of LLMs Under Directed Contextual Influence","summary":"Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.","authors":["Phil Blandfort","Tushar Karayil","Urja Pawar","Robert Graham","Alex McKenzie","Dmitrii Krasheninnikov"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22827v1","updated":"2026-02-26T10:08:02Z","published":"2026-02-26T10:08:02Z","title":"TARAZ: Persian Short-Answer Question Benchmark for Cultural Evaluation of Language Models","summary":"This paper presents a comprehensive evaluation framework for assessing the cultural competence of large language models (LLMs) in Persian. Existing Persian cultural benchmarks rely predominantly on multiple-choice formats and English-centric metrics that fail to capture Persian's morphological complexity and semantic nuance. Our framework introduces a Persian-specific short-answer evaluation that combines rule-based morphological normalization with a hybrid syntactic and semantic similarity module, enabling robust soft-match scoring beyond exact string overlap. Through systematic evaluation of 15 state-of-the-art open- and closed-source models, we demonstrate that our hybrid evaluation improves scoring consistency by +10% compared to exact-match baselines by capturing meaning that surface-level methods cannot detect. We publicly release our evaluation framework, providing the first standardized benchmark for measuring cultural understanding in Persian and establishing a reproducible foundation for cross-cultural LLM evaluation research.","authors":["Reihaneh Iranmanesh","Saeedeh Davoudi","Pasha Abrishamchian","Ophir Frieder","Nazli Goharian"],"pdf_url":"","comment":"11 pages, 3 figures, Fifteenth biennial Language Resources and Evaluation Conference (LREC) 2026 (to appear)"},{"id":"http://arxiv.org/abs/2410.10922v3","updated":"2026-02-26T10:05:51Z","published":"2024-10-14T12:08:12Z","title":"Towards Privacy-Guaranteed Label Unlearning in Vertical Federated Learning: Few-Shot Forgetting without Disclosure","summary":"This paper addresses the critical challenge of unlearning in Vertical Federated Learning (VFL), a setting that has received far less attention than its horizontal counterpart. Specifically, we propose the first method tailored to \\textit{label unlearning} in VFL, where labels play a dual role as both essential inputs and sensitive information. To this end, we employ a representation-level manifold mixup mechanism to generate synthetic embeddings for both unlearned and retained samples. This is to provide richer signals for the subsequent gradient-based label forgetting and recovery steps. These augmented embeddings are then subjected to gradient-based label forgetting, effectively removing the associated label information from the model. To recover performance on the retained data, we introduce a recovery-phase optimization step that refines the remaining embeddings. This design achieves effective label unlearning while maintaining computational efficiency. We validate our method through extensive experiments on diverse datasets, including MNIST, CIFAR-10, CIFAR-100, ModelNet, Brain Tumor MRI, COVID-19 Radiography, and Yahoo Answers demonstrate strong efficacy and scalability. Overall, this work establishes a new direction for unlearning in VFL, showing that re-imagining mixup as an efficient mechanism can unlock practical and utility-preserving unlearning. The code is publicly available at \\href{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}{https://github.com/bryanhx/Towards-Privacy-Guaranteed-Label-Unlearning-in-Vertical-Federated-Learning}","authors":["Hanlin Gu","Hong Xi Tae","Chee Seng Chan","Lixin Fan"],"pdf_url":"","comment":"We introduce the first method for label unlearning in vertical federated learning (VFL), focused on preventing label leakage by the active party"},{"id":"http://arxiv.org/abs/2602.22823v1","updated":"2026-02-26T10:05:07Z","published":"2026-02-26T10:05:07Z","title":"Hypernetwork-based approach for grid-independent functional data clustering","summary":"Functional data clustering is concerned with grouping functions that share similar structure, yet most existing methods implicitly operate on sampled grids, causing cluster assignments to depend on resolution, sampling density, or preprocessing choices rather than on the underlying functions themselves. To address this limitation, we introduce a framework that maps discretized function observations -- at arbitrary resolution and on arbitrary grids -- into a fixed-dimensional vector space via an auto-encoding architecture. The encoder is a hypernetwork that maps coordinate-value pairs to the weight space of an implicit neural representation (INR), which serves as the decoder. Because INRs represent functions with very few parameters, this design yields compact representations that are decoupled from the sampling grid, while the hypernetwork amortizes weight prediction across the dataset. Clustering is then performed in this weight space using standard algorithms, making the approach agnostic to both the discretization and the choice of clustering method. By means of synthetic and real-world experiments in high-dimensional settings, we demonstrate competitive clustering performance that is robust to changes in sampling resolution -- including generalization to resolutions not seen during training.","authors":["Anirudh Thatipelli","Ali Siahkoohi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22822v1","updated":"2026-02-26T10:05:01Z","published":"2026-02-26T10:05:01Z","title":"FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics","summary":"The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.","authors":["Yunhua Zhong","Yixuan Tang","Yifan Li","Jie Yang","Pan Liu","Jun Xia"],"pdf_url":"","comment":"28 pages, preprint version"},{"id":"http://arxiv.org/abs/2502.11816v3","updated":"2026-02-26T10:04:01Z","published":"2025-02-17T14:06:36Z","title":"Mixing It Up: Exploring Mixer Networks for Irregular Multivariate Time Series Forecasting","summary":"Forecasting irregularly sampled multivariate time series with missing values (IMTS) is a fundamental challenge in domains such as healthcare, climate science, and biology. While recent advances in vision and time series forecasting have shown that lightweight MLP-based architectures (e.g., MLP-Mixer, TSMixer) can rival attention-based models in both accuracy and efficiency, their applicability to irregular and sparse time series remains unexplored. In this paper, we propose IMTS-Mixer, a novel architecture that adapts the principles of Mixer models to the IMTS setting. IMTS-Mixer introduces two key components: (1) ISCAM, a channel-wise encoder that transforms irregular observations into fixed-size vectors using simple MLPs, and (2) ConTP, a continuous time decoder that supports forecasting at arbitrary time points. In our experiments on established benchmark datasets we show that our model achieves state-of-the- art performance in both forecasting accuracy and inference time, while using fewer parameters compared to baselines.","authors":["Christian Klötergens","Tim Dernedde","Lars Schmidt-Thieme","Vijaya Krishna Yalavarthi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22817v1","updated":"2026-02-26T09:58:10Z","published":"2026-02-26T09:58:10Z","title":"Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks","summary":"Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.","authors":["Shuo He","Lang Feng","Qi Wei","Xin Cheng","Lei Feng","Bo An"],"pdf_url":"","comment":"Accepted at ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22812v1","updated":"2026-02-26T09:53:17Z","published":"2026-02-26T09:53:17Z","title":"Accelerating Local LLMs on Resource-Constrained Edge Devices via Distributed Prompt Caching","summary":"Since local LLM inference on resource-constrained edge devices imposes a severe performance bottleneck, this paper proposes distributed prompt caching to enhance inference performance by cooperatively sharing intermediate processing states across multiple low-end edge devices. To fully utilize prompt similarity, our distributed caching mechanism also supports partial matching. As this approach introduces communication overhead associated with state sharing over a wireless network, we introduce a Bloom-filter-based data structure, referred to as a catalog, to determine whether a remote server possesses the desired internal states, thereby suppressing unnecessary communication. Experiments using the Gemma-3 270M model and the MMLU dataset on the Raspberry Pi Zero 2W platform demonstrate that the proposed approach reduces TTFT (Time to First Token) and TTLT (Time to Last Token) by 93.12% and 50.07% on average, respectively.","authors":["Hiroki Matsutani","Naoki Matsuda","Naoto Sugiura"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.01932v6","updated":"2026-02-26T09:51:13Z","published":"2025-02-04T02:07:23Z","title":"VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play","summary":"Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative reinforcement learning (RL), multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy RL methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves 69.5% win rate against the strongest baseline in the 3 vs 3 task, demonstrating its potential for tackling the complex interplay between low-level control and high-level strategy. To highlight VolleyBots' sim-to-real potential, we further demonstrate the zero-shot deployment of a policy trained entirely in simulation on real-world drones.","authors":["Zelai Xu","Ruize Zhang","Chao Yu","Huining Yuan","Xiangmin Yi","Shilong Ji","Chuqi Wang","Wenhao Tang","Feng Gao","Wenbo Ding","Xinlei Chen","Yu Wang"],"pdf_url":"","comment":"Accepted by NeurIPS 2025"},{"id":"http://arxiv.org/abs/2602.22810v1","updated":"2026-02-26T09:50:15Z","published":"2026-02-26T09:50:15Z","title":"Multi-agent imitation learning with function approximation: Linear Markov games and beyond","summary":"In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level \"all policy deviation concentrability coefficient\" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.","authors":["Luca Viano","Till Freihaut","Emanuele Nevali","Volkan Cevher","Matthieu Geist","Giorgia Ramponi"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2506.18656v2","updated":"2026-02-26T09:40:40Z","published":"2025-06-23T13:56:43Z","title":"On the Interpolation Error of Nonlinear Attention versus Linear Regression","summary":"Attention has become the core building block of modern machine learning (ML) by efficiently capturing the long-range dependencies among input tokens. Its inherently parallelizable structure allows for efficient performance scaling with the rapidly increasing size of both data and model parameters. Despite its central role, the theoretical understanding of Attention, especially in the nonlinear setting, is progressing at a more modest pace.\n  This paper provides a precise characterization of the interpolation error for a nonlinear Attention, in the high-dimensional regime where the number of input tokens $n$ and the embedding dimension $p$ are both large and comparable. Under a signal-plus-noise data model and for fixed Attention weights, we derive explicit (limiting) expressions for the mean-squared interpolation error. Leveraging recent advances in random matrix theory, we show that nonlinear Attention generally incurs a larger interpolation error than linear regression on random inputs. However, this gap vanishes, and can even be reversed, when the input contains a structured signal, particularly if the Attention weights align with the signal direction. Our theoretical insights are supported by numerical experiments.","authors":["Zhenyu Liao","Jiaqing Liu","TianQi Hou","Difan Zou","Zenan Ling"],"pdf_url":"","comment":"37 pages, 7 figures"},{"id":"http://arxiv.org/abs/2602.22801v1","updated":"2026-02-26T09:37:38Z","published":"2026-02-26T09:37:38Z","title":"Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving","summary":"Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.","authors":["Yinan Zheng","Tianyi Tan","Bin Huang","Enguang Liu","Ruiming Liang","Jianlin Zhang","Jianwei Cui","Guang Chen","Kun Ma","Hangjun Ye","Long Chen","Ya-Qin Zhang","Xianyuan Zhan","Jingjing Liu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22794v1","updated":"2026-02-26T09:29:42Z","published":"2026-02-26T09:29:42Z","title":"Doubly Adaptive Channel and Spatial Attention for Semantic Image Communication by IoT Devices","summary":"Internet of Things (IoT) networks face significant challenges such as limited communication bandwidth, constrained computational and energy resources, and highly dynamic wireless channel conditions. Utilization of deep neural networks (DNNs) combined with semantic communication has emerged as a promising paradigm to address these limitations. Deep joint source-channel coding (DJSCC) has recently been proposed to enable semantic communication of images. Building upon the original DJSCC formulation, low-complexity attention-style architectures has been added to the DNNs for further performance enhancement. As a main hurdle, training these DNNs separately for various signal-to-noise ratios (SNRs) will amount to excessive storage or communication overhead, which can not be maintained by small IoT devices. SNR Adaptive DJSCC (ADJSCC), has been proposed to train the DNNs once but feed the current SNR as part of the data to the channel-wise attention mechanism. We improve upon ADJSCC by a simultaneous utilization of doubly adaptive channel-wise and spatial attention modules at both transmitter and receiver. These modules dynamically adjust to varying channel conditions and spatial feature importance, enabling robust and efficient feature extraction and semantic information recovery. Simulation results corroborate that our proposed doubly adaptive DJSCC (DA-DJSCC) significantly improves upon ADJSCC in several performance criteria, while incurring a mild increase in complexity. These facts render DA-DJSCC a desirable choice for semantic communication in performance demanding but low-complexity IoT networks.","authors":["Soroosh Miri","Sepehr Abolhasani","Shahrokh Farahmand","S. Mohammad Razavizadeh"],"pdf_url":"","comment":"6 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2509.21936v2","updated":"2026-02-26T09:23:45Z","published":"2025-09-26T06:21:30Z","title":"Statistical Advantage of Softmax Attention: Insights from Single-Location Regression","summary":"Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.","authors":["O. Duranthon","P. Marion","C. Boyer","B. Loureiro","L. Zdeborová"],"pdf_url":"","comment":"Accepted at the ICLR 2026"},{"id":"http://arxiv.org/abs/2602.22786v1","updated":"2026-02-26T09:20:46Z","published":"2026-02-26T09:20:46Z","title":"QSIM: Mitigating Overestimation in Multi-Agent Reinforcement Learning via Action Similarity Weighted Q-Learning","summary":"Value decomposition (VD) methods have achieved remarkable success in cooperative multi-agent reinforcement learning (MARL). However, their reliance on the max operator for temporal-difference (TD) target calculation leads to systematic Q-value overestimation. This issue is particularly severe in MARL due to the combinatorial explosion of the joint action space, which often results in unstable learning and suboptimal policies. To address this problem, we propose QSIM, a similarity weighted Q-learning framework that reconstructs the TD target using action similarity. Instead of using the greedy joint action directly, QSIM forms a similarity weighted expectation over a structured near-greedy joint action space. This formulation allows the target to integrate Q-values from diverse yet behaviorally related actions while assigning greater influence to those that are more similar to the greedy choice. By smoothing the target with structurally relevant alternatives, QSIM effectively mitigates overestimation and improves learning stability. Extensive experiments demonstrate that QSIM can be seamlessly integrated with various VD methods, consistently yielding superior performance and stability compared to the original algorithms. Furthermore, empirical analysis confirms that QSIM significantly mitigates the systematic value overestimation in MARL. Code is available at https://github.com/MaoMaoLYJ/pymarl-qsim.","authors":["Yuanjun Li","Bin Zhang","Hao Chen","Zhouyang Jiang","Dapeng Li","Zhiwei Xu"],"pdf_url":"","comment":"19 pages, 15 figures, 7tables. Accepted to the 36th International Conference on Automated Planning and Scheduling (ICAPS 2026)"},{"id":"http://arxiv.org/abs/2602.22777v1","updated":"2026-02-26T09:12:12Z","published":"2026-02-26T09:12:12Z","title":"KMLP: A Scalable Hybrid Architecture for Web-Scale Tabular Data Modeling","summary":"Predictive modeling on web-scale tabular data with billions of instances and hundreds of heterogeneous numerical features faces significant scalability challenges. These features exhibit anisotropy, heavy-tailed distributions, and non-stationarity, creating bottlenecks for models like Gradient Boosting Decision Trees and requiring laborious manual feature engineering. We introduce KMLP, a hybrid deep architecture integrating a shallow Kolmogorov-Arnold Network (KAN) front-end with a Gated Multilayer Perceptron (gMLP) backbone. The KAN front-end uses learnable activation functions to automatically model complex non-linear transformations for each feature, while the gMLP backbone captures high-order interactions. Experiments on public benchmarks and an industrial dataset with billions of samples show KMLP achieves state-of-the-art performance, with advantages over baselines like GBDTs increasing at larger scales, validating KMLP as a scalable deep learning paradigm for large-scale web tabular data.","authors":["Mingming Zhang","Pengfei Shi","Zhiqing Xiao","Feng Zhao","Guandong Sun","Yulin Kang","Ruizhe Gao","Ningtao Wang","Xing Fu","Weiqiang Wang","Junbo Zhao"],"pdf_url":"","comment":"Accepted by THE ACM WEB CONFERENCE 2026"},{"id":"http://arxiv.org/abs/2602.22769v1","updated":"2026-02-26T08:59:31Z","published":"2026-02-26T08:59:31Z","title":"AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications","summary":"Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.","authors":["Yujie Zhao","Boqin Yuan","Junbo Huang","Haocheng Yuan","Zhongming Yu","Haozhou Xu","Lanxiang Hu","Abhilash Shankarampeta","Zimeng Huang","Wentao Ni","Yuandong Tian","Jishen Zhao"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.19327v2","updated":"2026-02-26T08:54:51Z","published":"2026-02-22T20:21:00Z","title":"Soft Sequence Policy Optimization","summary":"A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. We introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights. We provide theoretical motivation for SSPO and investigate practical modifications to improve optimization behavior. Empirically, we show that SSPO improves training stability and performance in mathematical reasoning tasks.","authors":["Svetlana Glazyrina","Maksim Kryzhanovskiy","Roman Ischenko"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.23276v2","updated":"2026-02-26T08:51:13Z","published":"2026-01-30T18:47:54Z","title":"Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging","summary":"Astronomical imaging remains noise-limited under practical observing conditions. Standard calibration pipelines remove structured artifacts but largely leave stochastic noise unresolved. Although learning-based denoising has shown strong potential, progress is constrained by scarce paired training data and the requirement for physically interpretable models in scientific workflows. We propose a physics-based noise synthesis framework tailored to CCD noise formation in the telescope. The pipeline models photon shot noise, photo-response non-uniformity, dark-current noise, readout effects, and localized outliers arising from cosmic-ray hits and hot pixels. To obtain low-noise inputs for synthesis, we stack multiple unregistered exposures to produce high-SNR bases. Realistic noisy counterparts synthesized from these bases using our noise model enable the construction of abundant paired datasets for supervised learning. Extensive experiments on our real-world multi-band dataset curated from two ground-based telescopes demonstrate the effectiveness of our framework in both photometric and scientific accuracy.","authors":["Shuhong Liu","Xining Ge","Ziying Gu","Quanfeng Xu","Lin Gu","Ziteng Cui","Xuangeng Chu","Jun Liu","Dong Li","Tatsuya Harada"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.05435v2","updated":"2026-02-26T08:50:46Z","published":"2025-02-08T03:47:06Z","title":"Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning","summary":"Audio captioning systems face a fundamental challenge: teacher-forcing training creates exposure bias that leads to caption degeneration during inference. While contrastive methods have been proposed as solutions, they typically fail to capture the crucial temporal relationships between acoustic and linguistic modalities. We address this limitation by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel with rotary positional embedding, specifically designed to preserve temporal information across modalities. Our approach offers a practical advantage: the kernel enables efficient stochastic gradient optimization, making it computationally feasible for real-world applications. Building on this foundation, we develop a complete audio captioning framework that integrates stochastic decoding to further mitigate caption degeneration. Extensive experiments on AudioCaps and Clotho datasets demonstrate that our method significantly improves caption quality, lexical diversity, and text-to-audio retrieval accuracy. Furthermore, we demonstrate the generalizability of our USW-RBF kernel by applying it to audio reasoning tasks, where it enhances the reasoning capabilities of large audio language models on the CompA-R in terms of correctness and quality. Our kernel also improves the reasoning accuracy of the MMAU-test-mini benchmarks by $4\\%$. These results establish our approach as a powerful and generalizable solution for cross-modal alignment challenges in audio-language tasks.","authors":["Manh Luong","Khai Nguyen","Dinh Phung","Gholamreza Haffari","Lizhen Qu"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.00834v3","updated":"2026-02-26T08:46:11Z","published":"2026-01-31T17:57:53Z","title":"A Minimum Variance Path Principle for Accurate and Stable Score-Based Density Ratio Estimation","summary":"Score-based methods are powerful across machine learning, but they face a paradox: theoretically path-independent, yet practically path-dependent.\n  We resolve this by proving that practical training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the score function.\n  We propose the MVP (**M**imum **V**ariance **P**ath) Principle to minimize this path variance.\n  Our key contribution is deriving a closed-form expression for the variance, making optimization tractable.\n  By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns data-adaptive, low-variance paths without heuristic manual selection.\n  This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks and providing a general framework for optimizing score-based interpolation.","authors":["Wei Chen","Jiacheng Li","Shigui Li","Zhiqi Lin","Junmei Yang","John Paisley","Delu Zeng"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22747v1","updated":"2026-02-26T08:36:09Z","published":"2026-02-26T08:36:09Z","title":"Set-based v.s. Distribution-based Representations of Epistemic Uncertainty: A Comparative Study","summary":"Epistemic uncertainty in neural networks is commonly modeled using two second-order paradigms: distribution-based representations, which rely on posterior parameter distributions, and set-based representations based on credal sets (convex sets of probability distributions). These frameworks are often regarded as fundamentally non-comparable due to differing semantics, assumptions, and evaluation practices, leaving their relative merits unclear. Empirical comparisons are further confounded by variations in the underlying predictive models. To clarify this issue, we present a controlled comparative study enabling principled, like-for-like evaluation of the two paradigms. Both representations are constructed from the same finite collection of predictive distributions generated by a shared neural network, isolating representational effects from predictive accuracy. Our study evaluates each representation through the lens of 3 uncertainty measures across 8 benchmarks, including selective prediction and out-of-distribution detection, spanning 6 underlying predictive models and 10 independent runs per configuration. Our results show that meaningful comparison between these seemingly non-comparable frameworks is both feasible and informative, providing insights into how second-order representation choices impact practical uncertainty-aware performance.","authors":["Kaizheng Wang","Yunjia Wang","Fabio Cuzzolin","David Moens","Hans Hallez","Siu Lun Chau"],"pdf_url":"","comment":"29 pages"},{"id":"http://arxiv.org/abs/2602.09448v3","updated":"2026-02-26T08:33:54Z","published":"2026-02-10T06:33:10Z","title":"The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training","summary":"Prior synthetic query generation for dense retrieval produces one query per document, focusing on quality. We systematically study multi-query synthesis, discovering a quality-diversity trade-off: quality benefits in-domain, diversity benefits out-of-domain (OOD). Experiments on 31 datasets show diversity especially benefits multi-hop retrieval. Analysis reveals diversity benefit correlates with query complexity (r>=0.95), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides thresholds (CW>10: use diversity; CW<7: avoid it) and enables CW-weighted training that improves OOD even with single-query data.","authors":["Xincan Feng","Noriki Nishida","Yusuke Sakai","Yuji Matsumoto"],"pdf_url":"","comment":"Under review"},{"id":"http://arxiv.org/abs/2602.08470v2","updated":"2026-02-26T08:23:39Z","published":"2026-02-09T10:16:43Z","title":"Learning Credal Ensembles via Distributionally Robust Optimization","summary":"Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.","authors":["Kaizheng Wang","Ghifari Adam Faza","Fabio Cuzzolin","Siu Lun Chau","David Moens","Hans Hallez"],"pdf_url":"","comment":"32 pages"},{"id":"http://arxiv.org/abs/2602.22732v1","updated":"2026-02-26T08:15:26Z","published":"2026-02-26T08:15:26Z","title":"Generative Recommendation for Large-Scale Advertising","summary":"Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.","authors":["Ben Xue","Dan Liu","Lixiang Wang","Mingjie Sun","Peng Wang","Pengfei Zhang","Shaoyun Shi","Tianyu Xu","Yunhao Sha","Zhiqiang Liu","Bo Kong","Bo Wang","Hang Yang","Jieting Xue","Junhao Wang","Shengyu Wang","Shuping Hui","Wencai Ye","Xiao Lin","Yongzhi Li","Yuhang Chen","Zhihui Yin","Quan Chen","Shiyang Wen","Wenjin Wu","Han Li","Guorui Zhou","Changcheng Li","Peng Jiang"],"pdf_url":"","comment":"13 pages, 6 figures, under review"},{"id":"http://arxiv.org/abs/2509.21013v3","updated":"2026-02-26T08:09:24Z","published":"2025-09-25T11:20:38Z","title":"Predicting LLM Reasoning Performance with Small Proxy Model","summary":"Given the prohibitive cost of pre-training large language models, it is essential to leverage smaller proxy models to optimize datasets before scaling up. However, this approach becomes challenging for reasoning capabilities, which exhibit emergent behavior that only appear reliably at larger model sizes, often exceeding 7B parameters. To address this, we introduce rBridge, showing that small proxies ($\\leq$1B) can effectively predict large-model reasoning by aligning more closely with (1) the pre-training objective and (2) the target task. rBridge achieves this by weighting negative log-likelihood with task alignment, using reasoning traces from frontier models as gold labels. In our experiments, rBridge (i) reduces dataset ranking costs by over 100x relative to the best baseline, (ii) achieves the strongest correlation across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot transfers predictive relationships across pre-training datasets at 1B to 7B scale. These findings indicate that rBridge offers a practical path for exploring reasoning-oriented pre-training at lower cost.","authors":["Woosung Koh","Juyoung Suk","Sungjun Han","Se-Young Yun","Jamin Shin"],"pdf_url":"","comment":"ICLR 2026"},{"id":"http://arxiv.org/abs/2602.19241v2","updated":"2026-02-26T08:08:52Z","published":"2026-02-22T15:51:29Z","title":"Scaling Laws for Precision in High-Dimensional Linear Regression","summary":"Low-precision training is critical for optimizing the trade-off between model quality and training costs, necessitating the joint allocation of model size, dataset size, and numerical precision. While empirical scaling laws suggest that quantization impacts effective model and data capacities or acts as an additive error, the theoretical mechanisms governing these effects remain largely unexplored. In this work, we initiate a theoretical study of scaling laws for low-precision training within a high-dimensional sketched linear regression framework. By analyzing multiplicative (signal-dependent) and additive (signal-independent) quantization, we identify a critical dichotomy in their scaling behaviors. Our analysis reveals that while both schemes introduce an additive error and degrade the effective data size, they exhibit distinct effects on effective model size: multiplicative quantization maintains the full-precision model size, whereas additive quantization reduces the effective model size. Numerical experiments validate our theoretical findings. By rigorously characterizing the complex interplay among model scale, dataset size, and quantization error, our work provides a principled theoretical basis for optimizing training protocols under practical hardware constraints.","authors":["Dechen Zhang","Xuan Tang","Yingyu Liang","Difan Zou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.09789v2","updated":"2026-02-26T07:55:28Z","published":"2026-02-10T13:49:08Z","title":"When Less is More: The LLM Scaling Paradox in Context Compression","summary":"Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.","authors":["Ruishan Guo","Yibing Liu","Guoxin Ma","Yan Wang","Yueyang Zhang","Long Xia","Kecheng Chen","Zhiyuan Sun","Daiting Shi"],"pdf_url":"","comment":"10 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2602.22719v1","updated":"2026-02-26T07:46:42Z","published":"2026-02-26T07:46:42Z","title":"Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks","summary":"State-space models (SSMs) have emerged as an efficient strategy for building powerful language models, avoiding the quadratic complexity of computing attention in transformers. Despite their promise, the interpretability and steerability of modern SSMs remain relatively underexplored. We take a major step in this direction by identifying activation subspace bottlenecks in the Mamba family of SSM models using tools from mechanistic interpretability. We then introduce a test-time steering intervention that simply multiplies the activations of the identified bottlenecks by a scalar. Across 5 SSMs and 6 diverse benchmarks, this intervention improves performance by an average of 8.27%, without requiring any task-specific tuning. Finally, we validate that the identified bottlenecks are indeed hindering performance by modifying them to yield an architecture we call Stable-Mamba, which achieves long-context performance gains when retrained from scratch.","authors":["Vamshi Sunku Mohan","Kaustubh Gupta","Aneesha Das","Chandan Singh"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22703v1","updated":"2026-02-26T07:28:04Z","published":"2026-02-26T07:28:04Z","title":"Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning","summary":"Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\\%$ on in-domain data, $+8.0\\%$ on out-of-domain data, and $+39.0\\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive\n  to ensure reproducibility.","authors":["Hao Yu","Shuning Jia","Guanghao Li","Wenhao Jiang","Chun Yuan"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22699v1","updated":"2026-02-26T07:21:00Z","published":"2026-02-26T07:21:00Z","title":"DPSQL+: A Differentially Private SQL Library with a Minimum Frequency Rule","summary":"SQL is the de facto interface for exploratory data analysis; however, releasing exact query results can expose sensitive information through membership or attribute inference attacks. Differential privacy (DP) provides rigorous privacy guarantees, but in practice, DP alone may not satisfy governance requirements such as the \\emph{minimum frequency rule}, which requires each released group (cell) to include contributions from at least $k$ distinct individuals. In this paper, we present \\textbf{DPSQL+}, a privacy-preserving SQL library that simultaneously enforces user-level $(\\varepsilon,δ)$-DP and the minimum frequency rule. DPSQL+ adopts a modular architecture consisting of: (i) a \\emph{Validator} that statically restricts queries to a DP-safe subset of SQL; (ii) an \\emph{Accountant} that consistently tracks cumulative privacy loss across multiple queries; and (iii) a \\emph{Backend} that interfaces with various database engines, ensuring portability and extensibility. Experiments on the TPC-H benchmark demonstrate that DPSQL+ achieves practical accuracy across a wide range of analytical workloads -- from basic aggregates to quadratic statistics and join operations -- and allows substantially more queries under a fixed global privacy budget than prior libraries in our evaluation.","authors":["Tomoya Matsumoto","Shokichi Takakura","Shun Takagi","Satoshi Hasegawa"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.05535v2","updated":"2026-02-26T07:10:35Z","published":"2026-02-05T10:51:39Z","title":"Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification","summary":"%Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. %We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.","authors":["Tao Huang","Rui Wang","Xiaofei Liu","Yi Qin","Li Duan","Liping Jing"],"pdf_url":"","comment":"Accepted to ICLR 2026. Code is available at https://github.com/HT86159/EUQ"},{"id":"http://arxiv.org/abs/2602.22685v1","updated":"2026-02-26T07:03:19Z","published":"2026-02-26T07:03:19Z","title":"Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting","summary":"Intermittent demand, a pattern characterized by long sequences of zero sales punctuated by sporadic, non-zero values, poses a persistent challenge in retail and supply chain forecasting. Both traditional methods, such as ARIMA, exponential smoothing, or Croston variants, as well as modern neural architectures such as DeepAR and Transformer-based models often underperform on such data, as they treat demand as a single continuous process or become computationally expensive when scaled across many sparse series. To address these limitations, we introduce Switch-Hurdle: a new framework that integrates a Mixture-of-Experts (MoE) encoder with a Hurdle-based probabilistic decoder. The encoder uses a sparse Top-1 expert routing during the forward pass yet approximately dense in the backward pass via a straight-through estimator (STE). The decoder follows a cross-attention autoregressive design with a shared hurdle head that explicitly separates the forecasting task into two components: a binary classification component estimating the probability of a sale, and a conditional regression component, predicting the quantity given a sale. This structured separation enables the model to capture both occurrence and magnitude processes inherent to intermittent demand. Empirical results on the M5 benchmark and a large proprietary retail dataset show that Switch-Hurdle achieves state-of-the-art prediction performance while maintaining scalability.","authors":["Fabian Muşat","Simona Căbuz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2502.04758v2","updated":"2026-02-26T07:00:42Z","published":"2025-02-07T08:45:00Z","title":"Differential Privacy of Quantum and Quantum-Inspired Classical Recommendation Algorithms","summary":"We study the differential privacy (DP) of the quantum recommendation algorithm of Kerenidis--Prakash and its quantum-inspired classical counterpart. Under standard low-rank and incoherence assumptions on the preference matrix, we show that the randomness already present in the algorithms' measurement/$\\ell_2$-sampling steps can act as a privacy-curating mechanism, yielding $(\\varepsilon,δ)$-DP without injecting additional DP noise through the interface. Concretely, for a system with $m$ users and $n$ items and rank parameter $k$, we prove $\\varepsilon=\\mathcal O(\\sqrt{k/n})$ and $δ= \\mathcal O\\big(k^2/\\min^2\\{m,n\\}\\big)$; in the typical regime $k=\\mathrm{polylog}(m,n)$ this simplifies to $\\varepsilon=\\tilde{\\mathcal O}(1/\\sqrt n)$ and $δ=\\tilde{\\mathcal O}\\big(1/\\min^2\\{m,n\\}\\big)$. Our analysis introduces a perturbation technique for truncated SVD under a single-entry update, which tracks the induced change in the low-rank reconstruction while avoiding unstable singular-vector comparisons. Finally, we validate the scaling on real-world rating datasets and compare against classical DP recommender baselines.","authors":["Chenjian Li","Mingsheng Ying","Ji Guan"],"pdf_url":"","comment":"18 pages, 3 figures in total(including appendix)"},{"id":"http://arxiv.org/abs/2602.22681v1","updated":"2026-02-26T06:54:57Z","published":"2026-02-26T06:54:57Z","title":"Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement","summary":"Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.","authors":["Shuchen Zhu","Rizhen Hu","Mingze Wang","Mou Sun","Xue Wang","Kun Yuan","Zaiwen Wen"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2601.22669v2","updated":"2026-02-26T06:49:17Z","published":"2026-01-30T07:42:13Z","title":"Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning","summary":"Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework requires an average of 45/12 (skin lesion/blood cell) additional rounds to achieve over 12.3%/8.9% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an data-free early stopping framework for FL methods.","authors":["Youngjoon Lee","Hyukjoon Lee","Seungrok Jung","Andy Luo","Jinu Gong","Yang Cao","Joonhyuk Kang"],"pdf_url":"","comment":"Replaced with experimental results on AMD MI300X AI accelerators"},{"id":"http://arxiv.org/abs/2602.19805v2","updated":"2026-02-26T06:48:38Z","published":"2026-02-23T13:03:48Z","title":"Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing","summary":"Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.","authors":["Wall Kim","Chaeyoung Song","Hanul Kim"],"pdf_url":"","comment":"This work was intended as a replacement of arXiv:2408.10517 and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2408.10517v6","updated":"2026-02-26T06:45:16Z","published":"2024-08-20T03:35:28Z","title":"Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing","summary":"Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications. Code is available at https://github.com/too-z/decision-metamamba","authors":["Wall Kim","Chaeyoung Song","Hanul Kim"],"pdf_url":"","comment":"17 pages; Previously this version appeared as arXiv:2602.19805 which was submitted as a new work by accident. This is a revised version of the previously withdrawn manuscript, updated with new experiments and results"}],"Multimedia":[{"id":"http://arxiv.org/abs/2508.04228v2","updated":"2026-02-26T17:37:05Z","published":"2025-08-06T09:03:16Z","title":"LayerT2V: A Unified Multi-Layer Video Generation Framework","summary":"Text-to-video generation has advanced rapidly, but existing methods typically output only the final composited video and lack editable layered representations, limiting their use in professional workflows. We propose \\textbf{LayerT2V}, a unified multi-layer video generation framework that produces multiple semantically consistent outputs in a single inference pass: the full video, an independent background layer, and multiple foreground RGB layers with corresponding alpha mattes. Our key insight is that recent video generation backbones use high compression in both time and space, enabling us to serialize multiple layer representations along the temporal dimension and jointly model them on a shared generation trajectory. This turns cross-layer consistency into an intrinsic objective, improving semantic alignment and temporal coherence. To mitigate layer ambiguity and conditional leakage, we augment a shared DiT backbone with LayerAdaLN and layer-aware cross-attention modulation. LayerT2V is trained in three stages: alpha mask VAE adaptation, joint multi-layer learning, and multi-foreground extension. We also introduce \\textbf{VidLayer}, the first large-scale dataset for multi-layer video generation. Extensive experiments demonstrate that LayerT2V substantially outperforms prior methods in visual fidelity, temporal consistency, and cross-layer coherence.","authors":["Guangzhao Li","Kangrui Cen","Baixuan Zhao","Yi Xin","Siqi Luo","Guangtao Zhai","Lei Zhang","Xiaohong Liu"],"pdf_url":"","comment":"Project Page is https://layert2v.github.io/"},{"id":"http://arxiv.org/abs/2601.14510v3","updated":"2026-02-26T12:36:07Z","published":"2026-01-20T22:03:04Z","title":"Structured Image-based Coding for Efficient Gaussian Splatting Compression","summary":"Gaussian Splatting (GS) has recently emerged as a state-of-the-art representation for radiance fields, combining real-time rendering with high visual fidelity. However, GS models require storing millions of parameters, leading to large file sizes that impair their use in practical multimedia systems. To address this limitation, this paper introduces GS Image-based Compression (GSICO), a novel GS codec that efficiently compresses pre-trained GS models while preserving perceptual fidelity. The core contribution lies in a mapping procedure that arranges GS parameters into structured images, guided by a novel algorithm that enhances spatial coherence. These GS parameter images are then encoded using a conventional image codec. Experimental evaluations on Tanks and Temples, Deep Blending, and Mip-NeRF360 datasets show that GSICO achieves average compression factors of 20.2x with minimal loss in visual quality, as measured by PSNR, SSIM, and LPIPS. Compared with state-of-the-art GS compression methods, the proposed codec consistently yields superior rate-distortion (RD) trade-offs.","authors":["Pedro Martin","Antonio Rodrigues","Joao Ascenso","Maria Paula Queluz"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22944v1","updated":"2026-02-26T12:35:09Z","published":"2026-02-26T12:35:09Z","title":"MViR: Multi-View Visual-Semantic Representation for Fake News Detection","summary":"With the rise of online social networks, detecting fake news accurately is essential for a healthy online environment. While existing methods have advanced multimodal fake news detection, they often neglect the multi-view visual-semantic aspects of news, such as different text perspectives of the same image. To address this, we propose a Multi-View Visual-Semantic Representation (MViR) framework. Our approach includes a Multi-View Representation module using pyramid dilated convolution to capture multi-view visual-semantic features, a Multi-View Feature Fusion module to integrate these features with text, and multiple aggregators to extract multi-view semantic cues for detection. Experiments on benchmark datasets demonstrate the superiority of MViR. The source code of FedCoop is available at https://github.com/FlowerinZDF/FakeNews-MVIR.","authors":["Haochen Liang","Xinqi Su","Jun Wang","Chaomeng Chen","Zitong Yu"],"pdf_url":"","comment":"Accepted by ICASSP'26"},{"id":"http://arxiv.org/abs/2409.02108v3","updated":"2026-02-26T12:11:30Z","published":"2024-09-03T17:59:05Z","title":"Unveiling Deep Shadows: A Survey and Benchmark on Image and Video Shadow Detection, Removal, and Generation in the Deep Learning Era","summary":"Shadows, formed by the occlusion of light, play an essential role in visual perception and directly influence scene understanding, image quality, and visual realism. This paper presents a unified survey and benchmark of deep-learning-based shadow detection, removal, and generation across images and videos. We introduce consistent taxonomies for architectures, supervision strategies, and learning paradigms; review major datasets and evaluation protocols; and re-train representative methods under standardized settings to enable fair comparison. Our benchmark reveals key findings, including inconsistencies in prior reports, strong dependence on model design and resolution, and limited cross-dataset generalization due to dataset bias. By synthesizing insights across the three tasks, we highlight shared illumination cues and priors that connect detection, removal, and generation. We further outline future directions involving unified all-in-one frameworks, semantics- and geometry-aware reasoning, shadow-based AIGC authenticity analysis, and the integration of physics-guided priors into multimodal foundation models. Corrected datasets, trained models, and evaluation tools are released to support reproducible research.","authors":["Xiaowei Hu","Zhenghao Xing","Tianyu Wang","Chi-Wing Fu","Pheng-Ann Heng"],"pdf_url":"","comment":"Accepted by International Journal of Computer Vision (IJCV). Publicly available results, trained models, and evaluation metrics at https://github.com/xw-hu/Unveiling-Deep-Shadows"},{"id":"http://arxiv.org/abs/2602.22897v1","updated":"2026-02-26T11:35:04Z","published":"2026-02-26T11:35:04Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","authors":["Xiaoxi Li","Wenxiang Jiao","Jiarui Jin","Shijian Wang","Guanting Dong","Jiajie Jin","Hao Wang","Yinuo Wang","Ji-Rong Wen","Yuan Lu","Zhicheng Dou"],"pdf_url":"","comment":null},{"id":"http://arxiv.org/abs/2602.22659v1","updated":"2026-02-26T06:18:11Z","published":"2026-02-26T06:18:11Z","title":"Scaling Audio-Visual Quality Assessment Dataset via Crowdsourcing","summary":"Audio-visual quality assessment (AVQA) research has been stalled by limitations of existing datasets: they are typically small in scale, with insufficient diversity in content and quality, and annotated only with overall scores. These shortcomings provide limited support for model development and multimodal perception research. We propose a practical approach for AVQA dataset construction. First, we design a crowdsourced subjective experiment framework for AVQA, breaks the constraints of in-lab settings and achieves reliable annotation across varied environments. Second, a systematic data preparation strategy is further employed to ensure broad coverage of both quality levels and semantic scenarios. Third, we extend the dataset with additional annotations, enabling research on multimodal perception mechanisms and their relation to content. Finally, we validate this approach through YT-NTU-AVQ, the largest and most diverse AVQA dataset to date, consisting of 1,620 user-generated audio and video (A/V) sequences. The dataset and platform code are available at https://github.com/renyu12/YT-NTU-AVQ","authors":["Renyu Yang","Jian Jin","Lili Meng","Meiqin Liu","Yilin Wang","Balu Adsumilli","Weisi Lin"],"pdf_url":"","comment":"Accepted to ICASSP 2026. 5 pages (main paper) + 8 pages (supplementary material)"},{"id":"http://arxiv.org/abs/2403.15226v3","updated":"2026-02-26T03:53:55Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models","summary":"In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"","comment":null}]}}