{"2025-06-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.12014v1","updated":"2025-06-13T17:59:39Z","published":"2025-06-13T17:59:39Z","title":"code_transformed: The Influence of Large Language Models on Code","summary":"  Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.\n","authors":["Yuliang Xu","Siming Huang","Mingmeng Geng","Yao Wan","Xuanhua Shi","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2506.12014v1.pdf","comment":"We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code"},{"id":"http://arxiv.org/abs/2506.06266v3","updated":"2025-06-13T17:58:55Z","published":"2025-06-06T17:48:23Z","title":"Cartridges: Lightweight and general-purpose long context representations\n  via self-study","summary":"  Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.\n","authors":["Sabri Eyuboglu","Ryan Ehrlich","Simran Arora","Neel Guha","Dylan Zinsley","Emily Liu","Will Tennien","Atri Rudra","James Zou","Azalia Mirhoseini","Christopher Re"],"pdf_url":"https://arxiv.org/pdf/2506.06266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11999v1","updated":"2025-06-13T17:54:12Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v1.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"},{"id":"http://arxiv.org/abs/2506.11991v1","updated":"2025-06-13T17:47:43Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijiang Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.11986v1","updated":"2025-06-13T17:46:02Z","published":"2025-06-13T17:46:02Z","title":"Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task","summary":"  Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.\n","authors":["Wuzhenghong Wen","Su Pan","yuwei Sun"],"pdf_url":"https://arxiv.org/pdf/2506.11986v1.pdf","comment":"11 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2506.09026v2","updated":"2025-06-13T17:44:03Z","published":"2025-06-10T17:52:42Z","title":"e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs","summary":"  Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.\n","authors":["Amrith Setlur","Matthew Y. R. Yang","Charlie Snell","Jeremy Greer","Ian Wu","Virginia Smith","Max Simchowitz","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2506.09026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07833v2","updated":"2025-06-13T17:24:38Z","published":"2025-06-09T14:55:00Z","title":"Improving Large Language Models with Concept-Aware Fine-Tuning","summary":"  Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm\n","authors":["Michael K. Chen","Xikun Zhang","Jiaxing Huang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.07833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17076v3","updated":"2025-06-13T17:21:25Z","published":"2025-05-20T06:01:19Z","title":"Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and\n  English","summary":"  The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.\n","authors":["Haoyang Zhang","Hexin Liu","Xiangyu Zhang","Qiquan Zhang","Yuchen Hu","Junqi Zhao","Fei Tian","Xuerui Yang","Leibny Paola Garcia","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2505.17076v3.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.01220v5","updated":"2025-06-13T16:58:26Z","published":"2025-02-03T10:24:55Z","title":"Factual Knowledge in Language Models: Robustness and Anomalies under\n  Simple Temporal Context Variations","summary":"  This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\nLMs' ability to distinguish is analyzed along two dimensions: the distance of\nthe incorrect context from the validity period and the granularity of the\ncontext. To this end, a dataset called TimeStress is introduced, enabling the\nevaluation of 18 diverse LMs. Results reveal that the best LM achieves a\nperfect distinction for only 11% of the studied facts, with errors, certainly\nrare, but critical that humans would not make. This work highlights the\nlimitations of current LMs in temporal representation.\n","authors":["Hichem Ammar Khodja","Frédéric Béchet","Quentin Brabant","Alexis Nasr","Gwénolé Lecorvé"],"pdf_url":"https://arxiv.org/pdf/2502.01220v5.pdf","comment":"preprint v5, accepted for publication at ACL 2025 - L2M2 Workshop"},{"id":"http://arxiv.org/abs/2504.11190v2","updated":"2025-06-13T16:49:35Z","published":"2025-04-15T13:47:55Z","title":"Enhancing multimodal analogical reasoning with Logic Augmented\n  Generation","summary":"  Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.\n","authors":["Anna Sofia Lippolis","Andrea Giovanni Nuzzolese","Aldo Gangemi"],"pdf_url":"https://arxiv.org/pdf/2504.11190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21657v2","updated":"2025-06-13T16:43:15Z","published":"2025-05-27T18:32:38Z","title":"Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations","summary":"  Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.\n","authors":["Zeinab Dehghani","Mohammed Naveed Akram","Koorosh Aslansefat","Adil Khan"],"pdf_url":"https://arxiv.org/pdf/2505.21657v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.16277"},{"id":"http://arxiv.org/abs/2506.11938v1","updated":"2025-06-13T16:42:09Z","published":"2025-06-13T16:42:09Z","title":"Improving Large Language Model Safety with Contrastive Representation\n  Learning","summary":"  Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense\n","authors":["Samuel Simko","Mrinmaya Sachan","Bernhard Schölkopf","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2506.11938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11930v1","updated":"2025-06-13T16:31:51Z","published":"2025-06-13T16:31:51Z","title":"Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback","summary":"  Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.\n","authors":["Dongwei Jiang","Alvin Zhang","Andrew Wang","Nicholas Andrews","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2506.11930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11928v1","updated":"2025-06-13T16:29:09Z","published":"2025-06-13T16:29:09Z","title":"LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?","summary":"  Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.\n","authors":["Zihan Zheng","Zerui Cheng","Zeyu Shen","Shang Zhou","Kaiyuan Liu","Hansen He","Dongruixuan Li","Stanley Wei","Hangyi Hao","Jianzhu Yao","Peiyao Sheng","Zixuan Wang","Wenhao Chai","Aleksandra Korolova","Peter Henderson","Sanjeev Arora","Pramod Viswanath","Jingbo Shang","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2506.11928v1.pdf","comment":"Project Page at https://livecodebenchpro.com/"},{"id":"http://arxiv.org/abs/2501.11651v2","updated":"2025-06-13T16:15:45Z","published":"2025-01-20T18:33:33Z","title":"T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.\n","authors":["Zhenyu Hou","Xin Lv","Rui Lu","Jiajie Zhang","Yujiang Li","Zijun Yao","Juanzi Li","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.11651v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.11919v1","updated":"2025-06-13T16:11:04Z","published":"2025-06-13T16:11:04Z","title":"Effectiveness of Counter-Speech against Abusive Content: A\n  Multidimensional Annotation and Classification Study","summary":"  Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions.\n","authors":["Greta Damo","Elena Cabrio","Serena Villata"],"pdf_url":"https://arxiv.org/pdf/2506.11919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11903v1","updated":"2025-06-13T15:53:17Z","published":"2025-06-13T15:53:17Z","title":"GeistBERT: Breathing Life into German NLP","summary":"  Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license.\n","authors":["Raphael Scheible-Schmitt","Johann Frei"],"pdf_url":"https://arxiv.org/pdf/2506.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11902v1","updated":"2025-06-13T15:52:37Z","published":"2025-06-13T15:52:37Z","title":"TreeRL: LLM Reinforcement Learning with On-Policy Tree Search","summary":"  Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.\n","authors":["Zhenyu Hou","Ziniu Hu","Yujiang Li","Rui Lu","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.11902v1.pdf","comment":"Accepted to ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2501.18638v2","updated":"2025-06-13T15:44:43Z","published":"2025-01-28T17:10:20Z","title":"Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation","summary":"  As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety.\n","authors":["Daniel Schwartz","Dmitriy Bespalov","Zhe Wang","Ninad Kulkarni","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2501.18638v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11887v1","updated":"2025-06-13T15:36:22Z","published":"2025-06-13T15:36:22Z","title":"Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making","summary":"  Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.\n","authors":["Claudio Fanconi","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2506.11887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11886v1","updated":"2025-06-13T15:35:54Z","published":"2025-06-13T15:35:54Z","title":"Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache","summary":"  Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.\n","authors":["Xiaoran Liu","Siyang He","Qiqi Wang","Ruixiao Li","Yuerong Song","Zhigeng Liu","Linlin Li","Qun Liu","Zengfeng Huang","Qipeng Guo","Ziwei He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2506.11886v1.pdf","comment":"10 pages, 7 figures, work in progress"},{"id":"http://arxiv.org/abs/2506.11880v1","updated":"2025-06-13T15:29:43Z","published":"2025-06-13T15:29:43Z","title":"Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment","summary":"  The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.\n","authors":["Alejandro Peña","Julian Fierrez","Aythami Morales","Gonzalo Mancera","Miguel Lopez","Ruben Tolosana"],"pdf_url":"https://arxiv.org/pdf/2506.11880v1.pdf","comment":"Submitted to AIES 2025 (Under Review)"},{"id":"http://arxiv.org/abs/2506.07196v2","updated":"2025-06-13T15:23:25Z","published":"2025-06-08T15:30:04Z","title":"SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical\n  Action Planning","summary":"  Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.\n","authors":["Mengya Xu","Zhongzhen Huang","Dillan Imans","Yiru Ye","Xiaofan Zhang","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2506.07196v2.pdf","comment":"The authors could not reach a consensus on the final version of this\n  paper, necessitating its withdrawal"},{"id":"http://arxiv.org/abs/2504.13615v2","updated":"2025-06-13T15:09:26Z","published":"2025-04-18T10:43:21Z","title":"Long-context Non-factoid Question Answering in Indic Languages","summary":"  Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.\n","authors":["Ritwik Mishra","Rajiv Ratn Shah","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2504.13615v2.pdf","comment":"Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/"},{"id":"http://arxiv.org/abs/2503.09347v2","updated":"2025-06-13T15:07:08Z","published":"2025-03-12T12:49:02Z","title":"Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts","summary":"  Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.\n","authors":["Hongyu Chen","Seraphina Goldfarb-Tarrant"],"pdf_url":"https://arxiv.org/pdf/2503.09347v2.pdf","comment":"9 pages, ACL 2025"},{"id":"http://arxiv.org/abs/2506.11857v1","updated":"2025-06-13T15:04:01Z","published":"2025-06-13T15:04:01Z","title":"Post Persona Alignment for Multi-Session Dialogue Generation","summary":"  Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.\n","authors":["Yi-Pei Chen","Noriki Nishida","Hideki Nakayama","Yuji Matsumoto"],"pdf_url":"https://arxiv.org/pdf/2506.11857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00073v3","updated":"2025-06-13T15:02:02Z","published":"2025-05-29T17:41:39Z","title":"The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets","summary":"  AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.\n","authors":["Shenzhe Zhu","Jiao Sun","Yi Nian","Tobin South","Alex Pentland","Jiaxin Pei"],"pdf_url":"https://arxiv.org/pdf/2506.00073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09992v2","updated":"2025-06-13T15:01:28Z","published":"2025-06-11T17:59:33Z","title":"Large Language Models for Toxic Language Detection in Low-Resource\n  Balkan Languages","summary":"  Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.\n","authors":["Amel Muminovic","Amela Kadric Muminovic"],"pdf_url":"https://arxiv.org/pdf/2506.09992v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2506.11820v1","updated":"2025-06-13T14:23:38Z","published":"2025-06-13T14:23:38Z","title":"Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation","summary":"  Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.\n","authors":["Xintong Wang","Jingheng Pan","Yixiao Liu","Xiaohu Zhao","Chenyang Lyu","Minghao Wu","Chris Biemann","Longyue Wang","Linlong Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11812v1","updated":"2025-06-13T14:14:40Z","published":"2025-06-13T14:14:40Z","title":"On the Performance of LLMs for Real Estate Appraisal","summary":"  The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.\n","authors":["Margot Geerts","Manon Reusens","Bart Baesens","Seppe vanden Broucke","Jochen De Weerdt"],"pdf_url":"https://arxiv.org/pdf/2506.11812v1.pdf","comment":"Accepted at ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2506.01602v2","updated":"2025-06-13T14:11:43Z","published":"2025-06-02T12:40:46Z","title":"Word Sense Detection Leveraging Maximum Mean Discrepancy","summary":"  Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.\n","authors":["Kensuke Mitsuzawa"],"pdf_url":"https://arxiv.org/pdf/2506.01602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11807v1","updated":"2025-06-13T14:09:48Z","published":"2025-06-13T14:09:48Z","title":"Are Multimodal Large Language Models Pragmatically Competent Listeners\n  in Simple Reference Resolution Tasks?","summary":"  We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs.\n","authors":["Simeon Junker","Manar Ali","Larissa Koch","Sina Zarrieß","Hendrik Buschmeier"],"pdf_url":"https://arxiv.org/pdf/2506.11807v1.pdf","comment":"To appear in ACL Findings 2025"},{"id":"http://arxiv.org/abs/2506.11798v1","updated":"2025-06-13T14:02:21Z","published":"2025-06-13T14:02:21Z","title":"Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models","summary":"  Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.\n","authors":["Maximilian Kreutner","Marlene Lutz","Markus Strohmaier"],"pdf_url":"https://arxiv.org/pdf/2506.11798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19175v2","updated":"2025-06-13T13:50:34Z","published":"2025-02-26T14:31:43Z","title":"MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis","summary":"  Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process.\n","authors":["Daniel Rose","Chia-Chien Hung","Marco Lepri","Israa Alqassem","Kiril Gashteovski","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2502.19175v2.pdf","comment":"ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2501.03479v3","updated":"2025-06-13T13:42:41Z","published":"2025-01-07T02:47:59Z","title":"Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?","summary":"  Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage.\n","authors":["Sourabrata Mukherjee","Atharva Mehta","Soumya Teotia","Sougata Saha","Akhil Arora","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2501.03479v3.pdf","comment":"Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)"},{"id":"http://arxiv.org/abs/2506.11769v1","updated":"2025-06-13T13:25:39Z","published":"2025-06-13T13:25:39Z","title":"Long-Short Alignment for Effective Long-Context Modeling in LLMs","summary":"  Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.\n","authors":["Tianqi Du","Haotian Huang","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11769v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2502.20273v3","updated":"2025-06-13T13:24:27Z","published":"2025-02-27T17:01:23Z","title":"How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data","summary":"  Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.\n","authors":["Varshini Reddy","Craig W. Schmidt","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2502.20273v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11763v1","updated":"2025-06-13T13:17:32Z","published":"2025-06-13T13:17:32Z","title":"DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents","summary":"  Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.\n","authors":["Mingxuan Du","Benfeng Xu","Chiwei Zhu","Xiaorui Wang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2506.11763v1.pdf","comment":"31 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11752v1","updated":"2025-06-13T13:05:41Z","published":"2025-06-13T13:05:41Z","title":"DART: Distilling Autoregressive Reasoning to Silent Thought","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.\n","authors":["Nan Jiang","Ziming Wu","De-Chuan Zhan","Fuming Lai","Shaobing Lian"],"pdf_url":"https://arxiv.org/pdf/2506.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12415v2","updated":"2025-06-13T13:02:56Z","published":"2025-05-18T13:40:18Z","title":"Table-R1: Region-based Reinforcement Learning for Table Understanding","summary":"  Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.\n","authors":["Zhenhe Wu","Jian Yang","Jiaheng Liu","Xianjie Wu","Changzai Pan","Jie Zhang","Yu Zhao","Shuangyong Song","Yongxiang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2505.12415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11737v1","updated":"2025-06-13T12:48:39Z","published":"2025-06-13T12:48:39Z","title":"Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in\n  Interleaved Multi-Image Model","summary":"  This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.\n","authors":["Dinh Viet Cuong","Hoang-Bao Le","An Pham Ngoc Nguyen","Liting Zhou","Cathal Gurrin"],"pdf_url":"https://arxiv.org/pdf/2506.11737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07595v2","updated":"2025-06-13T12:47:42Z","published":"2024-11-12T07:09:44Z","title":"Entropy Controllable Direct Preference Optimization","summary":"  In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.\n","authors":["Motoki Omura","Yasuhiro Fujita","Toshiki Kataoka"],"pdf_url":"https://arxiv.org/pdf/2411.07595v2.pdf","comment":"ICML 2025 Workshop on Models of Human Feedback for AI Alignment"},{"id":"http://arxiv.org/abs/2506.01305v2","updated":"2025-06-13T12:40:58Z","published":"2025-06-02T04:32:15Z","title":"VM14K: First Vietnamese Medical Benchmark","summary":"  Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.\n","authors":["Thong Nguyen","Duc Nguyen","Minh Dang","Thai Dao","Long Nguyen","Quan H. Nguyen","Dat Nguyen","Kien Tran","Minh Tran"],"pdf_url":"https://arxiv.org/pdf/2506.01305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11728v1","updated":"2025-06-13T12:40:16Z","published":"2025-06-13T12:40:16Z","title":"The Cambrian Explosion of Mixed-Precision Matrix Multiplication for\n  Quantized Deep Learning Inference","summary":"  Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.\n","authors":["Héctor Martínez","Adrián Castelló","Francisco D. Igual","Enrique S. Quintana-Ortí"],"pdf_url":"https://arxiv.org/pdf/2506.11728v1.pdf","comment":"16 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.11042v3","updated":"2025-06-13T12:27:46Z","published":"2024-10-14T19:46:23Z","title":"Persistent Topological Features in Large Language Models","summary":"  Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework - zigzag persistence from topological\ndata analysis - with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective.\n","authors":["Yuri Gardinazzi","Karthik Viswanathan","Giada Panerai","Alessio Ansuini","Alberto Cazzaniga","Matteo Biagetti"],"pdf_url":"https://arxiv.org/pdf/2410.11042v3.pdf","comment":"10+17 pages, 17 figures, 3 tables. Accepted as poster at ICML 2025"},{"id":"http://arxiv.org/abs/2502.07855v2","updated":"2025-06-13T12:20:30Z","published":"2025-02-11T14:04:43Z","title":"Vision-Language Models for Edge Networks: A Comprehensive Survey","summary":"  Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.\n","authors":["Ahmed Sharshar","Latif U. Khan","Waseem Ullah","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2502.07855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11702v1","updated":"2025-06-13T12:17:38Z","published":"2025-06-13T12:17:38Z","title":"Configurable Preference Tuning with Rubric-Guided Synthetic Data","summary":"  Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning\n","authors":["Víctor Gallego"],"pdf_url":"https://arxiv.org/pdf/2506.11702v1.pdf","comment":"Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment"},{"id":"http://arxiv.org/abs/2506.11681v1","updated":"2025-06-13T11:19:27Z","published":"2025-06-13T11:19:27Z","title":"LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach","summary":"  This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.\n","authors":["Pratibha Zunjare","Michael Hsiao"],"pdf_url":"https://arxiv.org/pdf/2506.11681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11673v1","updated":"2025-06-13T11:07:14Z","published":"2025-06-13T11:07:14Z","title":"Improving Causal Interventions in Amnesic Probing with Mean Projection\n  or LEACE","summary":"  Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.\n","authors":["Alicja Dobrzeniecka","Antske Fokkens","Pia Sommerauer"],"pdf_url":"https://arxiv.org/pdf/2506.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21227v2","updated":"2025-06-13T11:04:13Z","published":"2025-03-27T07:36:11Z","title":"LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models","summary":"  Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.\n","authors":["Hengyuan Zhao","Ziqin Wang","Qixin Sun","Kaiyou Song","Yilin Li","Xiaolin Hu","Qingpei Guo","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2503.21227v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.16660v3","updated":"2025-06-13T10:57:16Z","published":"2025-05-22T13:24:52Z","title":"Can reasoning models comprehend mathematical problems in Chinese ancient\n  texts? An empirical study based on data from Suanjing Shishu","summary":"  This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.\n","authors":["Chang Liu","Dongbo Wang","Liu liu","Zhixiao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.16660v3.pdf","comment":"29pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.11666v1","updated":"2025-06-13T10:53:50Z","published":"2025-06-13T10:53:50Z","title":"Converting Annotated Clinical Cases into Structured Case Report Forms","summary":"  Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166\n","authors":["Pietro Ferrazzi","Alberto Lavelli","Bernardo Magnini"],"pdf_url":"https://arxiv.org/pdf/2506.11666v1.pdf","comment":"to be published in BioNLP 2025"},{"id":"http://arxiv.org/abs/2506.11638v1","updated":"2025-06-13T10:11:01Z","published":"2025-06-13T10:11:01Z","title":"LoRA-Gen: Specializing Large Language Model via Online LoRA Generation","summary":"  Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.\n","authors":["Yicheng Xiao","Lin Song","Rui Yang","Cheng Cheng","Yixiao Ge","Xiu Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2506.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08967v2","updated":"2025-06-13T10:07:42Z","published":"2025-06-10T16:37:39Z","title":"Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model","summary":"  Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.\n","authors":["Ailin Huang","Bingxin Li","Bruce Wang","Boyong Wu","Chao Yan","Chengli Feng","Heng Wang","Hongyu Zhou","Hongyuan Wang","Jingbei Li","Jianjian Sun","Joanna Wang","Mingrui Chen","Peng Liu","Ruihang Miao","Shilei Jiang","Tian Fei","Wang You","Xi Chen","Xuerui Yang","Yechang Huang","Yuxiang Zhang","Zheng Ge","Zheng Gong","Zhewei Huang","Zixin Zhang","Bin Wang","Bo Li","Buyun Ma","Changxin Miao","Changyi Wan","Chen Xu","Dapeng Shi","Dingyuan Hu","Enle Liu","Guanzhe Huang","Gulin Yan","Hanpeng Hu","Haonan Jia","Jiahao Gong","Jiaoren Wu","Jie Wu","Jie Yang","Junzhe Lin","Kaixiang Li","Lei Xia","Longlong Gu","Ming Li","Nie Hao","Ranchen Ming","Shaoliang Pang","Siqi Liu","Song Yuan","Tiancheng Cao","Wen Li","Wenqing He","Xu Zhao","Xuelin Zhang","Yanbo Yu","Yinmin Zhong","Yu Zhou","Yuanwei Liang","Yuanwei Lu","Yuxiang Yang","Zidong Yang","Zili Zhang","Binxing Jiao","Heung-Yeung Shum","Jiansheng Chen","Jing Li","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Daxin Jiang","Shuchang Zhou","Chen Hu"],"pdf_url":"https://arxiv.org/pdf/2506.08967v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.11631v1","updated":"2025-06-13T10:02:39Z","published":"2025-06-13T10:02:39Z","title":"SceneGram: Conceptualizing and Describing Tangrams in Scene Context","summary":"  Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.\n","authors":["Simeon Junker","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2506.11631v1.pdf","comment":"To appear in ACL Findings 2025"},{"id":"http://arxiv.org/abs/2406.02050v4","updated":"2025-06-13T09:44:56Z","published":"2024-06-04T07:31:06Z","title":"JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models","summary":"  With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data.\n","authors":["Hitomi Yanaka","Namgi Han","Ryoma Kumon","Jie Lu","Masashi Takeshita","Ryo Sekizawa","Taisei Kato","Hiromi Arai"],"pdf_url":"https://arxiv.org/pdf/2406.02050v4.pdf","comment":"Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025"},{"id":"http://arxiv.org/abs/2506.11620v1","updated":"2025-06-13T09:43:27Z","published":"2025-06-13T09:43:27Z","title":"(SimPhon Speech Test): A Data-Driven Method for In Silico Design and\n  Validation of a Phonetically Balanced Speech Test","summary":"  Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials.\n","authors":["Stefan Bleeck"],"pdf_url":"https://arxiv.org/pdf/2506.11620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v2","updated":"2025-06-13T09:32:19Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2506.11604v1","updated":"2025-06-13T09:20:41Z","published":"2025-06-13T09:20:41Z","title":"VLM@school -- Evaluation of AI image understanding on German middle\n  school knowledge","summary":"  This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems.\n","authors":["René Peinl","Vincent Tischler"],"pdf_url":"https://arxiv.org/pdf/2506.11604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11602v1","updated":"2025-06-13T09:17:08Z","published":"2025-06-13T09:17:08Z","title":"Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study","summary":"  We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.\n","authors":["Hawau Olamide Toyin","Samar M. Magdy","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2506.11602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04267v2","updated":"2025-06-13T09:03:31Z","published":"2024-09-06T13:24:22Z","title":"An overview of domain-specific foundation model: key technologies,\n  applications and challenges","summary":"  The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models (FMs), addresses the limitations of\ngeneral-purpose models, which may not fully capture the unique patterns and\nrequirements of domain-specific data. Despite its importance, there is a\nnotable lack of comprehensive overview papers on building domain-specific FMs,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific FMs. It introduces basic concepts, outlines the\ngeneral architecture, and surveys key methods for constructing domain-specific\nmodels. Furthermore, the article discusses various domains that can benefit\nfrom these specialized models and highlights the challenges ahead. Through this\noverview, we aim to offer valuable guidance and reference for researchers and\npractitioners from diverse fields to develop their own customized FMs.\n","authors":["Haolong Chen","Hanzhi Chen","Zijian Zhao","Kaifeng Han","Guangxu Zhu","Yichen Zhao","Ying Du","Wei Xu","Qingjiang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.04267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23252v2","updated":"2025-06-13T09:00:02Z","published":"2025-05-29T08:57:11Z","title":"Automatic Construction of Multiple Classification Dimensions for\n  Managing Approaches in Scientific Papers","summary":"  Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.\n","authors":["Bing Ma","Hai Zhuge"],"pdf_url":"https://arxiv.org/pdf/2505.23252v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.14218v3","updated":"2025-06-13T08:57:36Z","published":"2025-04-19T07:53:37Z","title":"Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective","summary":"  Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse. The source code of our work is publicly\navailable at: https://github.com/kaustpradalab/repeat-curse-llm\n","authors":["Junchi Yao","Shu Yang","Jianhua Xu","Lijie Hu","Mengdi Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14218v3.pdf","comment":"Accepted by ACL 2025, Findings, Long Paper"},{"id":"http://arxiv.org/abs/2405.04065v4","updated":"2025-06-13T08:32:26Z","published":"2024-05-07T07:14:38Z","title":"FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference","summary":"  Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.\n","authors":["Runheng Liu","Xingchen Xiao","Heyan Huang","Zewen Chi","Zhijing Wu"],"pdf_url":"https://arxiv.org/pdf/2405.04065v4.pdf","comment":"ACL 2025 Findings, 14 pages"},{"id":"http://arxiv.org/abs/2506.11558v1","updated":"2025-06-13T08:13:05Z","published":"2025-06-13T08:13:05Z","title":"DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs","summary":"  Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.\n","authors":["Bo-Cheng Chiu","Jen-Jee Chen","Yu-Chee Tseng","Feng-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11557v1","updated":"2025-06-13T08:12:52Z","published":"2025-06-13T08:12:52Z","title":"From Persona to Person: Enhancing the Naturalness with Multiple\n  Discourse Relations Graph Learning in Personalized Dialogue Generation","summary":"  In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges.\n","authors":["Chih-Hao Hsu","Ying-Jia Lin","Hung-Yu Kao"],"pdf_url":"https://arxiv.org/pdf/2506.11557v1.pdf","comment":"Accepted by PAKDD 2025"},{"id":"http://arxiv.org/abs/2506.11555v1","updated":"2025-06-13T08:06:49Z","published":"2025-06-13T08:06:49Z","title":"RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware\n  Reasoning","summary":"  The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs.\n","authors":["Yu Wang","Shiwan Zhao","Ming Fan","Zhihu Wang","Yubo Zhang","Xicheng Zhang","Zhengfan Wang","Heyuan Huang","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2506.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14023v4","updated":"2025-06-13T08:04:19Z","published":"2024-06-20T06:42:08Z","title":"Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective","summary":"  As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.\n","authors":["Yuchen Wen","Keping Bi","Wei Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.14023v4.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2410.20445v3","updated":"2025-06-13T08:02:21Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-based Agent Framework for Automated Trajectory\n  Modeling via Collaboration of Large and Small Models","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v3.pdf","comment":"the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2506.04078v2","updated":"2025-06-13T07:27:57Z","published":"2025-06-04T15:43:14Z","title":"LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation","summary":"  Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.\n","authors":["Ming Zhang","Yujiong Shen","Zelin Li","Huayu Sha","Binze Hu","Yuhui Wang","Chenhao Huang","Shichun Liu","Jingqi Tong","Changhao Jiang","Mingxu Chai","Zhiheng Xi","Shihan Dou","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2506.04078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06706v3","updated":"2025-06-13T07:21:17Z","published":"2025-03-09T17:43:30Z","title":"PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts","summary":"  Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.\n","authors":["Ming Zhang","Yuhui Wang","Yujiong Shen","Tingyi Yang","Changhao Jiang","Yilong Wu","Shihan Dou","Qinhao Chen","Zhiheng Xi","Zhihao Zhang","Yi Dong","Zhen Wang","Zhihui Fei","Mingyang Wan","Tao Liang","Guojun Ma","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.06706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11516v1","updated":"2025-06-13T07:17:41Z","published":"2025-06-13T07:17:41Z","title":"Brewing Knowledge in Context: Distillation Perspectives on In-Context\n  Learning","summary":"  In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection.\n","authors":["Chengye Li","Haiyun Liu","Yuanxi Li"],"pdf_url":"https://arxiv.org/pdf/2506.11516v1.pdf","comment":"10 main pages, 10 page appendix"},{"id":"http://arxiv.org/abs/2506.11515v1","updated":"2025-06-13T07:16:41Z","published":"2025-06-13T07:16:41Z","title":"Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs\n  and MLLMs","summary":"  Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.\n","authors":["Xiao Xu","Libo Qin","Wanxiang Che","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2506.11515v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266"},{"id":"http://arxiv.org/abs/2502.11020v2","updated":"2025-06-13T07:12:23Z","published":"2025-02-16T07:07:38Z","title":"TUMLU: A Unified and Native Language Understanding Benchmark for Turkic\n  Languages","summary":"  Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts.\n","authors":["Jafar Isbarov","Arofat Akhundjanova","Mammad Hajili","Kavsar Huseynova","Dmitry Gaynullin","Anar Rzayev","Osman Tursun","Aizirek Turdubaeva","Ilshat Saetov","Rinat Kharisov","Saule Belginova","Ariana Kenbayeva","Amina Alisheva","Abdullatif Köksal","Samir Rustamov","Duygu Ataman"],"pdf_url":"https://arxiv.org/pdf/2502.11020v2.pdf","comment":"Accepted to ACL 2025, Main Conference"},{"id":"http://arxiv.org/abs/2412.21015v2","updated":"2025-06-13T07:04:46Z","published":"2024-12-30T15:33:19Z","title":"MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets","summary":"  Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.\n","authors":["Mahir Labib Dihan","Mohammed Eunus Ali","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2412.21015v2.pdf","comment":"ACL 2025 (Demo)"},{"id":"http://arxiv.org/abs/2506.11499v1","updated":"2025-06-13T06:50:02Z","published":"2025-06-13T06:50:02Z","title":"On the Effectiveness of Integration Methods for Multimodal Dialogue\n  Response Retrieval","summary":"  Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities.\n","authors":["Seongbo Jang","Seonghyeon Lee","Dongha Lee","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11499v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.11498v1","updated":"2025-06-13T06:49:53Z","published":"2025-06-13T06:49:53Z","title":"Lag-Relative Sparse Attention In Long Context Training","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.\n","authors":["Manlai Liang","Wanyi Huang","Mandi Liu","Huaijun Li","Jinlong Li"],"pdf_url":"https://arxiv.org/pdf/2506.11498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11485v1","updated":"2025-06-13T06:20:03Z","published":"2025-06-13T06:20:03Z","title":"Relational Schemata in BERT Are Inducible, Not Emergent: A Study of\n  Performance vs. Competence in Language Models","summary":"  While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.\n","authors":["Cole Gawin"],"pdf_url":"https://arxiv.org/pdf/2506.11485v1.pdf","comment":"15 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.11478v1","updated":"2025-06-13T06:00:03Z","published":"2025-06-13T06:00:03Z","title":"ImmunoFOMO: Are Language Models missing what oncologists see?","summary":"  Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts.\n","authors":["Aman Sinha","Bogdan-Valentin Popescu","Xavier Coubez","Marianne Clausel","Mathieu Constant"],"pdf_url":"https://arxiv.org/pdf/2506.11478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18415v2","updated":"2025-06-13T05:55:26Z","published":"2025-04-25T15:17:52Z","title":"BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs","summary":"  Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.\n","authors":["Hongyu Wang","Shuming Ma","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.18415v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.11475v1","updated":"2025-06-13T05:39:28Z","published":"2025-06-13T05:39:28Z","title":"AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis\n  and Prediction","summary":"  This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.\n","authors":["Syeda Kisaa Fatima","Tehreem Zubair","Noman Ahmed","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2506.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11474v1","updated":"2025-06-13T05:36:30Z","published":"2025-06-13T05:36:30Z","title":"Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards","summary":"  Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/\n","authors":["Jaehoon Yun","Jiwoong Sohn","Jungwoo Park","Hyunjae Kim","Xiangru Tang","Yanjun Shao","Yonghoe Koo","Minhyeok Ko","Qingyu Chen","Mark Gerstein","Michael Moor","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2506.11474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04210v2","updated":"2025-06-13T05:29:27Z","published":"2025-06-04T17:55:09Z","title":"Does Thinking More always Help? Understanding Test-Time Scaling in\n  Reasoning Models","summary":"  Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.\n","authors":["Soumya Suvra Ghosal","Souradip Chakraborty","Avinash Reddy","Yifu Lu","Mengdi Wang","Dinesh Manocha","Furong Huang","Mohammad Ghavamzadeh","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2506.04210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19110v2","updated":"2025-06-13T04:49:28Z","published":"2025-02-26T13:01:49Z","title":"Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity","summary":"  Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity.\n","authors":["Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.19110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11467v1","updated":"2025-06-13T04:42:16Z","published":"2025-06-13T04:42:16Z","title":"A Gamified Evaluation and Recruitment Platform for Low Resource Language\n  Machine Translation Systems","summary":"  Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research.\n","authors":["Carlos Rafael Catalan"],"pdf_url":"https://arxiv.org/pdf/2506.11467v1.pdf","comment":"7 pages, 7 figures, presented at the HEAL Workshop at CHI"},{"id":"http://arxiv.org/abs/2506.10963v2","updated":"2025-06-13T04:39:54Z","published":"2025-06-12T17:58:09Z","title":"MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for\n  Text-to-Image Reasoning","summary":"  In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning -- a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits -- low entity fidelity,\nweak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only\n50.20, underscoring the benchmark's difficulty. To spur further progress, we\nrelease FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that\ncombines a reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.\n","authors":["Yuxuan Luo","Yuhui Yuan","Junwen Chen","Haonan Cai","Ziyi Yue","Yuwei Yang","Fatima Zohra Daha","Ji Li","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2506.10963v2.pdf","comment":"85 pages, 70 figures, code: https://github.com/MMMGBench/MMMG,\n  project page: https://mmmgbench.github.io/"},{"id":"http://arxiv.org/abs/2409.19243v2","updated":"2025-06-13T04:26:26Z","published":"2024-09-28T05:19:51Z","title":"Jointly modelling the evolution of social structure and language in\n  online communities","summary":"  Group interactions take place within a particular socio-temporal context,\nwhich should be taken into account when modelling interactions in online\ncommunities. We propose a method for jointly modelling community structure and\nlanguage over time. Our system produces dynamic word and user representations\nthat can be used to cluster users, investigate thematic interests of groups,\nand predict group membership. We apply and evaluate our method in the context\nof a set of misogynistic extremist groups. Our results indicate that this\napproach outperforms prior models which lacked one of these components (i.e.\nnot incorporating social structure, or using static word embeddings) when\nevaluated on clustering and embedding prediction tasks. Our method further\nenables novel types of analyses on online groups, including tracing their\nresponse to temporal events and quantifying their propensity for using violent\nlanguage, which is of particular importance in the context of extremist groups.\n","authors":["Christine de Kock"],"pdf_url":"https://arxiv.org/pdf/2409.19243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07044v4","updated":"2025-06-13T04:22:02Z","published":"2025-06-08T08:47:30Z","title":"Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...\n","authors":[" LASA Team","Weiwen Xu","Hou Pong Chan","Long Li","Mahani Aljunied","Ruifeng Yuan","Jianyu Wang","Chenghao Xiao","Guizhen Chen","Chaoqun Liu","Zhaodonghui Li","Yu Sun","Junao Shen","Chaojun Wang","Jie Tan","Deli Zhao","Tingyang Xu","Hao Zhang","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2506.07044v4.pdf","comment":"Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is\n  https://alibaba-damo-academy.github.io/lingshu/"},{"id":"http://arxiv.org/abs/2506.04518v2","updated":"2025-06-13T03:55:18Z","published":"2025-06-04T23:53:49Z","title":"Towards Efficient Speech-Text Jointly Decoding within One Speech\n  Language Model","summary":"  Speech language models (Speech LMs) enable end-to-end speech-text modelling\nwithin a single model, offering a promising direction for spoken dialogue\nsystems. The choice of speech-text jointly decoding paradigm plays a critical\nrole in performance, efficiency, and alignment quality. In this work, we\nsystematically compare representative joint speech-text decoding\nstrategies-including the interleaved, and parallel generation paradigms-under a\ncontrolled experimental setup using the same base language model, speech\ntokenizer and training data. Our results show that the interleaved approach\nachieves the best alignment. However it suffers from slow inference due to long\ntoken sequence length. To address this, we propose a novel early-stop\ninterleaved (ESI) pattern that not only significantly accelerates decoding but\nalso yields slightly better performance. Additionally, we curate high-quality\nquestion answering (QA) datasets to further improve speech QA performance.\n","authors":["Haibin Wu","Yuxuan Hu","Ruchao Fan","Xiaofei Wang","Kenichi Kumatani","Bo Ren","Jianwei Yu","Heng Lu","Lijuan Wang","Yao Qian","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2506.04518v2.pdf","comment":"Our company need to do internal review"},{"id":"http://arxiv.org/abs/2410.21027v2","updated":"2025-06-13T03:54:32Z","published":"2024-10-28T13:48:43Z","title":"Transferable Post-training via Inverse Value Learning","summary":"  As post-training processes utilize increasingly large datasets and base\nmodels continue to grow in size, the computational demands and implementation\nchallenges of existing algorithms are escalating significantly. In this paper,\nwe propose modeling the changes at the logits level during post-training using\na separate neural network (i.e., the value network). After training this\nnetwork on a small base model using demonstrations, this network can be\nseamlessly integrated with other pre-trained models during inference, enables\nthem to achieve similar capability enhancements. We systematically investigate\nthe best practices for this paradigm in terms of pre-training weights and\nconnection schemes. We demonstrate that the resulting value network has broad\ntransferability across pre-trained models of different parameter sizes within\nthe same family, models undergoing continuous pre-training within the same\nfamily, and models with different vocabularies across families. In certain\ncases, it can achieve performance comparable to full-parameter fine-tuning.\nFurthermore, we explore methods to enhance the transferability of the value\nmodel and prevent overfitting to the base model used during training.\n","authors":["Xinyu Lu","Xueru Wen","Yaojie Lu","Bowen Yu","Hongyu Lin","Haiyang Yu","Le Sun","Xianpei Han","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.21027v2.pdf","comment":"NAACL 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2506.11440v1","updated":"2025-06-13T03:38:29Z","published":"2025-06-13T03:38:29Z","title":"AbsenceBench: Language Models Can't Tell What's Missing","summary":"  Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).\n","authors":["Harvey Yiyun Fu","Aryan Shrivastava","Jared Moore","Peter West","Chenhao Tan","Ari Holtzman"],"pdf_url":"https://arxiv.org/pdf/2506.11440v1.pdf","comment":"23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench"},{"id":"http://arxiv.org/abs/2506.00637v2","updated":"2025-06-13T03:30:44Z","published":"2025-05-31T17:01:45Z","title":"Improving the Calibration of Confidence Scores in Text Generation Using\n  the Output Distribution's Characteristics","summary":"  Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets.\n","authors":["Lorenzo Jaime Yu Flores","Ori Ernst","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2506.00637v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.11432v1","updated":"2025-06-13T03:10:15Z","published":"2025-06-13T03:10:15Z","title":"KoGEC : Korean Grammatical Error Correction with Pre-trained Translation\n  Models","summary":"  This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks.\n","authors":["Taeeun Kim","Semin Jeong","Youngsook Song"],"pdf_url":"https://arxiv.org/pdf/2506.11432v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.07910v3","updated":"2025-06-13T03:03:21Z","published":"2024-02-27T02:00:28Z","title":"MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained\n  Identification of Expressions","summary":"  Media bias detection poses a complex, multifaceted problem traditionally\ntackled using single-task models and small in-domain datasets, consequently\nlacking generalizability. To address this, we introduce MAGPIE, the first\nlarge-scale multi-task pre-training approach explicitly tailored for media bias\ndetection. To enable pre-training at scale, we present Large Bias Mixture\n(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous\napproaches in media bias detection on the Bias Annotation By Experts (BABE)\ndataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs\nbetter than previous models on 5 out of 8 tasks in the Media Bias\nIdentification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%\nof finetuning steps compared to single-task approaches. Our evaluation shows,\nfor instance, that tasks like sentiment and emotionality boost all learning,\nall tasks enhance fake news detection, and scaling tasks leads to the best\nresults. MAGPIE confirms that MTL is a promising approach for addressing media\nbias detection, enhancing the accuracy and efficiency of existing models.\nFurthermore, LBM is the first available resource collection focused on media\nbias MTL.\n","authors":["Tomáš Horych","Martin Wessel","Jan Philip Wahle","Terry Ruas","Jerome Waßmuth","André Greiner-Petter","Akiko Aizawa","Bela Gipp","Timo Spinde"],"pdf_url":"https://arxiv.org/pdf/2403.07910v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15694v2","updated":"2025-06-13T03:01:26Z","published":"2024-11-24T03:17:37Z","title":"Deep Sparse Latent Feature Models for Knowledge Graph Completion","summary":"  Recent advances in knowledge graph completion (KGC) have emphasized\ntext-based approaches to navigate the inherent complexities of large-scale\nknowledge graphs (KGs). While these methods have achieved notable progress,\nthey frequently struggle to fully incorporate the global structural properties\nof the graph. Stochastic blockmodels (SBMs), especially the latent feature\nrelational model (LFRM), offer robust probabilistic frameworks for identifying\nlatent community structures and improving link prediction. This paper presents\na novel probabilistic KGC framework utilizing sparse latent feature models,\noptimized via a deep variational autoencoder (VAE). Our proposed method\ndynamically integrates global clustering information with local textual\nfeatures to effectively complete missing triples, while also providing enhanced\ninterpretability of the underlying latent structures. Extensive experiments on\nfour benchmark datasets with varying scales demonstrate the significant\nperformance gains achieved by our method.\n","authors":["Haotian Li","Rui Zhang","Lingzhi Wang","Bin Yu","Youwei Wang","Yuliang Wei","Kai Wang","Richard Yi Da Xu","Bailing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11425v1","updated":"2025-06-13T02:46:53Z","published":"2025-06-13T02:46:53Z","title":"Agent-RLVR: Training Software Engineering Agents via Guidance and\n  Environment Rewards","summary":"  Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle.\n","authors":["Jeff Da","Clinton Wang","Xiang Deng","Yuntao Ma","Nikhil Barhate","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2506.11425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11418v1","updated":"2025-06-13T02:36:15Z","published":"2025-06-13T02:36:15Z","title":"Efficient Long-Context LLM Inference via KV Cache Clustering","summary":"  Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.\n","authors":["Jie Hu","Shengnan Wang","Yutong He","Ping Gong","Jiawei Yi","Juncheng Zhang","Youhui Bai","Renhai Chen","Gong Zhang","Cheng Li","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2506.11418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20813v3","updated":"2025-06-13T02:33:26Z","published":"2025-05-27T07:22:00Z","title":"RSCF: Relation-Semantics Consistent Filter for Entity Embedding of\n  Knowledge Graph","summary":"  In knowledge graph embedding, leveraging relation specific entity\ntransformation has markedly enhanced performance. However, the consistency of\nembedding differences before and after transformation remains unaddressed,\nrisking the loss of valuable inductive bias inherent in the embeddings. This\ninconsistency stems from two problems. First, transformation representations\nare specified for relations in a disconnected manner, allowing dissimilar\ntransformations and corresponding entity embeddings for similar relations.\nSecond, a generalized plug-in approach as a SFBR (Semantic Filter Based on\nRelations) disrupts this consistency through excessive concentration of entity\nembeddings under entity-based regularization, generating indistinguishable\nscore distributions among relations. In this paper, we introduce a plug-in KGE\nmethod, Relation-Semantics Consistent Filter (RSCF). Its entity transformation\nhas three features for enhancing semantic consistency: 1) shared affine\ntransformation of relation embeddings across all relations, 2) rooted entity\ntransformation that adds an entity embedding to its change represented by the\ntransformed vector, and 3) normalization of the change to prevent scale\nreduction. To amplify the advantages of consistency that preserve semantics on\nembeddings, RSCF adds relation transformation and prediction modules for\nenhancing the semantics. In knowledge graph completion tasks with\ndistance-based and tensor decomposition models, RSCF significantly outperforms\nstate-of-the-art KGE methods, showing robustness across all relations and their\nfrequencies.\n","authors":["Junsik Kim","Jinwook Park","Kangil Kim"],"pdf_url":"https://arxiv.org/pdf/2505.20813v3.pdf","comment":"Accepted to ACL 2025, 17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.10521v2","updated":"2025-06-13T02:32:48Z","published":"2025-06-12T09:29:16Z","title":"Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning","summary":"  Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.\n","authors":["Yuhao Zhou","Yiheng Wang","Xuming He","Ruoyao Xiao","Zhiwei Li","Qiantai Feng","Zijie Guo","Yuejin Yang","Hao Wu","Wenxuan Huang","Jiaqi Wei","Dan Si","Xiuqi Yao","Jia Bu","Haiwen Huang","Tianfan Fu","Shixiang Tang","Ben Fei","Dongzhan Zhou","Fenghua Ling","Yan Lu","Siqi Sun","Chenhui Li","Guanjie Zheng","Jiancheng Lv","Wenlong Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2506.10521v2.pdf","comment":"82 pages"},{"id":"http://arxiv.org/abs/2506.10848v2","updated":"2025-06-13T02:28:47Z","published":"2025-06-12T16:08:28Z","title":"Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles","summary":"  Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.\n","authors":["Qingyan Wei","Yaojie Zhang","Zhiyuan Liu","Dongrui Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.10848v2.pdf","comment":"11 pages; 5 figures;"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2506.12015v1","updated":"2025-06-13T17:59:58Z","published":"2025-06-13T17:59:58Z","title":"EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction","summary":"  Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.\n","authors":["Hsi-Che Lin","Yu-Chu Yu","Kai-Po Chang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2506.12015v1.pdf","comment":"Under review. Project page: https://hsi-che-lin.github.io/EMLoC/"},{"id":"http://arxiv.org/abs/2506.12009v1","updated":"2025-06-13T17:57:18Z","published":"2025-06-13T17:57:18Z","title":"Affogato: Learning Open-Vocabulary Affordance Grounding with Automated\n  Data Generation at Scale","summary":"  Affordance grounding-localizing object regions based on natural language\ndescriptions of interactions-is a critical challenge for enabling intelligent\nagents to understand and interact with their environments. However, this task\nremains challenging due to the need for fine-grained part-level localization,\nthe ambiguity arising from multiple valid interaction regions, and the scarcity\nof large-scale datasets. In this work, we introduce Affogato, a large-scale\nbenchmark comprising 150K instances, annotated with open-vocabulary text\ndescriptions and corresponding 3D affordance heatmaps across a diverse set of\nobjects and interactions. Building on this benchmark, we develop simple yet\neffective vision-language models that leverage pretrained part-aware vision\nbackbones and a text-conditional heatmap decoder. Our models trained with the\nAffogato dataset achieve promising performance on the existing 2D and 3D\nbenchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain\ngeneralization. The Affogato dataset is shared in public:\nhttps://huggingface.co/datasets/project-affogato/affogato\n","authors":["Junha Lee","Eunha Park","Chunghyun Park","Dahyun Kang","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2506.12009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12007v1","updated":"2025-06-13T17:56:49Z","published":"2025-06-13T17:56:49Z","title":"SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution\n  Shifts","summary":"  Neural surrogates for Partial Differential Equations (PDEs) often suffer\nsignificant performance degradation when evaluated on unseen problem\nconfigurations, such as novel material types or structural dimensions.\nMeanwhile, Domain Adaptation (DA) techniques have been widely used in vision\nand language processing to generalize from limited information about unseen\nconfigurations. In this work, we address this gap through two focused\ncontributions. First, we introduce SIMSHIFT, a novel benchmark dataset and\nevaluation suite composed of four industrial simulation tasks: hot rolling,\nsheet metal forming, electric motor design and heatsink design. Second, we\nextend established domain adaptation methods to state of the art neural\nsurrogates and systematically evaluate them. These approaches use parametric\ndescriptions and ground truth simulations from multiple source configurations,\ntogether with only parametric descriptions from target configurations. The goal\nis to accurately predict target simulations without access to ground truth\nsimulation data. Extensive experiments on SIMSHIFT highlight the challenges of\nout of distribution neural surrogate modeling, demonstrate the potential of DA\nin simulation, and reveal open problems in achieving robust neural surrogates\nunder distribution shifts in industrially relevant scenarios. Our codebase is\navailable at https://github.com/psetinek/simshift\n","authors":["Paul Setinek","Gianluca Galletti","Thomas Gross","Dominik Schnürer","Johannes Brandstetter","Werner Zellinger"],"pdf_url":"https://arxiv.org/pdf/2506.12007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12006v1","updated":"2025-06-13T17:56:39Z","published":"2025-06-13T17:56:39Z","title":"crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation\n  Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to\n  2023","summary":"  The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated\nin 2021 in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI), focuses on unsupervised\ncross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and\ntransferring to T2 MRI. The task is an extreme example of domain shift chosen\nto serve as a meaningful and illustrative benchmark. From a clinical\napplication perspective, it aims to automate Vestibular Schwannoma (VS) and\ncochlea segmentation on T2 scans for more cost-effective VS management. Over\ntime, the challenge objectives have evolved to enhance its clinical relevance.\nThe challenge evolved from using single-institutional data and basic\nsegmentation in 2021 to incorporating multi-institutional data and Koos grading\nin 2022, and by 2023, it included heterogeneous routine data and\nsub-segmentation of intra- and extra-meatal tumour components. In this work, we\nreport the findings of the 2022 and 2023 editions and perform a retrospective\nanalysis of the challenge progression over the years. The observations from the\nsuccessive challenge contributions indicate that the number of outliers\ndecreases with an expanding dataset. This is notable since the diversity of\nscanning protocols of the datasets concurrently increased. The winning approach\nof the 2023 edition reduced the number of outliers on the 2021 and 2022 testing\ndata, demonstrating how increased data heterogeneity can enhance segmentation\nperformance even on homogeneous data. However, the cochlea Dice score declined\nin 2023, likely due to the added complexity from tumour sub-annotations\naffecting overall segmentation performance. While progress is still needed for\nclinically acceptable VS segmentation, the plateauing performance suggests that\na more challenging cross-modal task may better serve future benchmarking.\n","authors":["Navodini Wijethilake","Reuben Dorent","Marina Ivory","Aaron Kujawa","Stefan Cornelissen","Patrick Langenhuizen","Mohamed Okasha","Anna Oviedova","Hexin Dong","Bogyeong Kang","Guillaume Sallé","Luyi Han","Ziyuan Zhao","Han Liu","Tao Yang","Shahad Hardan","Hussain Alasmawi","Santosh Sanjeev","Yuzhou Zhuang","Satoshi Kondo","Maria Baldeon Calisto","Shaikh Muhammad Uzair Noman","Cancan Chen","Ipek Oguz","Rongguo Zhang","Mina Rezaei","Susana K. Lai-Yuen","Satoshi Kasai","Chih-Cheng Hung","Mohammad Yaqub","Lisheng Wang","Benoit M. Dawant","Cuntai Guan","Ritse Mann","Vincent Jaouen","Ji-Wung Han","Li Zhang","Jonathan Shapey","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2506.12006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11996v1","updated":"2025-06-13T17:51:14Z","published":"2025-06-13T17:51:14Z","title":"Improving Surgical Risk Prediction Through Integrating Automated Body\n  Composition Analysis: a Retrospective Trial on Colectomy Surgery","summary":"  Objective: To evaluate whether preoperative body composition metrics\nautomatically extracted from CT scans can predict postoperative outcomes after\ncolectomy, either alone or combined with clinical variables or existing risk\npredictors. Main outcomes and measures: The primary outcome was the predictive\nperformance for 1-year all-cause mortality following colectomy. A Cox\nproportional hazards model with 1-year follow-up was used, and performance was\nevaluated using the concordance index (C-index) and Integrated Brier Score\n(IBS). Secondary outcomes included postoperative complications, unplanned\nreadmission, blood transfusion, and severe infection, assessed using AUC and\nBrier Score from logistic regression. Odds ratios (OR) described associations\nbetween individual CT-derived body composition metrics and outcomes. Over 300\nfeatures were extracted from preoperative CTs across multiple vertebral levels,\nincluding skeletal muscle area, density, fat areas, and inter-tissue metrics.\nNSQIP scores were available for all surgeries after 2012.\n","authors":["Hanxue Gu","Yaqian Chen","isoo Lee","Diego Schaps","Regina Woody","Roy Colglazier","Maciej A. Mazurowski","Christopher Mantyh"],"pdf_url":"https://arxiv.org/pdf/2506.11996v1.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11991v1","updated":"2025-06-13T17:47:43Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijiang Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.11989v1","updated":"2025-06-13T17:46:14Z","published":"2025-06-13T17:46:14Z","title":"Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal","summary":"  Test-time scaling offers a promising way to improve the reasoning performance\nof vision-language large models (VLLMs) without additional training. In this\npaper, we explore a simple but effective approach for applying test-time\nscaling to radiology report generation. Specifically, we introduce a\nlightweight Thought Graph Traversal (TGT) framework that guides the model to\nreason through organ-specific findings in a medically coherent order. This\nframework integrates structured medical priors into the prompt, enabling deeper\nand more logical analysis with no changes to the underlying model. To further\nenhance reasoning depth, we apply a reasoning budget forcing strategy that\nadjusts the model's inference depth at test time by dynamically extending its\ngeneration process. This simple yet powerful combination allows a frozen\nradiology VLLM to self-correct and generate more accurate, consistent chest\nX-ray reports. Our method outperforms baseline prompting approaches on standard\nbenchmarks, and also reveals dataset biases through traceable reasoning paths.\nCode and prompts are open-sourced for reproducibility at\nhttps://github.com/glerium/Thought-Graph-Traversal.\n","authors":["Yue Yao","Zelin Wen","Yan Tong","Xinyu Tian","Xuqing Li","Xiao Ma","Dongliang Xu","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2506.11989v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.11209 by other authors"},{"id":"http://arxiv.org/abs/2506.11976v1","updated":"2025-06-13T17:34:05Z","published":"2025-06-13T17:34:05Z","title":"How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs","summary":"  Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning.\n","authors":["Constantin Venhoff","Ashkan Khakzar","Sonia Joseph","Philip Torr","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2506.11976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19407v8","updated":"2025-06-13T17:27:50Z","published":"2024-06-12T06:41:23Z","title":"YOLO advances to its genesis: a decadal and comprehensive review of the\n  You Only Look Once (YOLO) series","summary":"  This review systematically examines the progression of the You Only Look Once\n(YOLO) object detection algorithms from YOLOv1 to the recently unveiled\nYOLOv12. Employing a reverse chronological analysis, this study examines the\nadvancements introduced by YOLO algorithms, beginning with YOLOv12 and\nprogressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and\nsubsequent versions to explore each version's contributions to enhancing speed,\ndetection accuracy, and computational efficiency in real-time object detection.\nAdditionally, this study reviews the alternative versions derived from YOLO\narchitectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and\nGold-YOLO. Moreover, the study highlights the transformative impact of YOLO\nmodels across five critical application areas: autonomous vehicles and traffic\nsafety, healthcare and medical imaging, industrial manufacturing, surveillance\nand security, and agriculture. By detailing the incremental technological\nadvancements in subsequent YOLO versions, this review chronicles the evolution\nof YOLO, and discusses the challenges and limitations in each of the earlier\nversions. The evolution signifies a path towards integrating YOLO with\nmultimodal, context-aware, and Artificial General Intelligence (AGI) systems\nfor the next YOLO decade, promising significant implications for future\ndevelopments in AI-driven applications. YOLO Review, YOLO Advances, YOLOv13,\nYOLOv14, YOLOv15, YOLOv16, YOLOv17, YOLOv18, YOLOv19, YOLOv20, YOLO review,\nYOLO Object Detection\n","authors":["Ranjan Sapkota","Marco Flores Calero","Rizwan Qureshi","Chetan Badgujar","Upesh Nepal","Alwin Poulose","Peter Zeno","Uday Bhanu Prakash Vaddevolu","Sheheryar Khan","Maged Shoman","Hong Yan","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2406.19407v8.pdf","comment":"Published in Artificial Intelligence Review as\n  https://doi.org/10.1007/s10462-025-11253-3"},{"id":"http://arxiv.org/abs/2506.11967v1","updated":"2025-06-13T17:25:27Z","published":"2025-06-13T17:25:27Z","title":"Visual Pre-Training on Unlabeled Images using Reinforcement Learning","summary":"  In reinforcement learning (RL), value-based algorithms learn to associate\neach observation with the states and rewards that are likely to be reached from\nit. We observe that many self-supervised image pre-training methods bear\nsimilarity to this formulation: learning features that associate crops of\nimages with those of nearby views, e.g., by taking a different crop or color\naugmentation. In this paper, we complete this analogy and explore a method that\ndirectly casts pre-training on unlabeled image data like web crawls and video\nframes as an RL problem. We train a general value function in a dynamical\nsystem where an agent transforms an image by changing the view or adding image\naugmentations. Learning in this way resembles crop-consistency\nself-supervision, but through the reward function, offers a simple lever to\nshape feature learning using curated images or weakly labeled captions when\nthey exist. Our experiments demonstrate improved representations when training\non unlabeled images in the wild, including video data like EpicKitchens, scene\ndata like COCO, and web-crawl data like CC12M.\n","authors":["Dibya Ghosh","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2506.11967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03082v2","updated":"2025-06-13T17:00:16Z","published":"2025-06-03T17:02:38Z","title":"SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis","summary":"  Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities.\n","authors":["Ssharvien Kumar Sivakumar","Yannik Frisch","Ghazal Ghazaei","Anirban Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2506.03082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11932v1","updated":"2025-06-13T16:33:54Z","published":"2025-06-13T16:33:54Z","title":"Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A\n  Comparative Study of Appearance-Based and Infrared Eye Trackers","summary":"  This study evaluates a smartphone-based, deep-learning eye-tracking algorithm\nby comparing its performance against a commercial infrared-based eye tracker,\nthe Tobii Pro Nano. The aim is to investigate the feasibility of\nappearance-based gaze estimation under realistic mobile usage conditions. Key\nsensitivity factors, including age, gender, vision correction, lighting\nconditions, device type, and head position, were systematically analysed. The\nappearance-based algorithm integrates a lightweight convolutional neural\nnetwork (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to\npredict gaze coordinates from grayscale facial images. Gaze data were collected\nfrom 51 participants using dynamic visual stimuli, and accuracy was measured\nusing Euclidean distance. The deep learning model produced a mean error of\n17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy\ndifferences were small, the deep learning-based method was more sensitive to\nfactors such as lighting, vision correction, and age, with higher failure rates\nobserved under low-light conditions among participants using glasses and in\nolder age groups. Device-specific and positional factors also influenced\ntracking performance. These results highlight the potential of appearance-based\napproaches for mobile eye tracking and offer a reference framework for\nevaluating gaze estimation systems across varied usage conditions.\n","authors":["Nishan Gunawardena","Gough Yumu Lui","Jeewani Anupama Ginige","Bahman Javadi"],"pdf_url":"https://arxiv.org/pdf/2506.11932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11925v1","updated":"2025-06-13T16:24:28Z","published":"2025-06-13T16:24:28Z","title":"Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference","summary":"  Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.\n","authors":["M. Manzour","Catherine M. Elias","Omar M. Shehata","R. Izquierdo","M. A. Sotelo"],"pdf_url":"https://arxiv.org/pdf/2506.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11924v1","updated":"2025-06-13T16:19:00Z","published":"2025-06-13T16:19:00Z","title":"Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation","summary":"  We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.\n","authors":["Min-Seop Kwak","Junho Kim","Sangdoo Yun","Dongyoon Han","Taekyoung Kim","Seungryong Kim","Jin-Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2506.11924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20104v3","updated":"2025-06-13T16:17:20Z","published":"2025-02-27T13:58:44Z","title":"New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration","summary":"  Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. It serves as an essential testing ground for\nMultimodal Large Language Models (MLLMs). To advance this field, we introduced\na new REC dataset in our previous conference paper, characterized by two key\nfeatures. First, it is designed with controllable difficulty levels, requiring\nmulti-level fine-grained reasoning across object categories, attributes, and\nmulti-hop relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing and augmentation, explicitly testing a\nmodel's ability to reject scenarios where the target object is absent, an often\noverlooked yet critical challenge in existing datasets. In this extended work,\nwe propose two new methods to tackle the challenges of fine-grained REC by\ncombining the strengths of Specialist Models and MLLMs. The first method\nadaptively assigns simple cases to faster, lightweight models and reserves\ncomplex ones for powerful MLLMs, balancing accuracy and efficiency. The second\nmethod lets a specialist generate a set of possible object regions, and the\nMLLM selects the most plausible one using its reasoning ability. These\ncollaborative strategies lead to significant improvements on our dataset and\nother challenging benchmarks. Our results show that combining specialized and\ngeneral-purpose models offers a practical path toward solving complex\nreal-world vision-language tasks. Our dataset and code are available at\nhttps://github.com/sleepyshep/FineCops-Ref.\n","authors":["Xuzheng Yang","Junzhuo Liu","Peng Wang","Guoqing Wang","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.20104v3.pdf","comment":"Accepted by TPAMI 2025"},{"id":"http://arxiv.org/abs/2401.06122v3","updated":"2025-06-13T16:13:55Z","published":"2024-01-11T18:57:17Z","title":"Manipulating Feature Visualizations with Gradient Slingshots","summary":"  Feature Visualization (FV) is a widely used technique for interpreting the\nconcepts learned by Deep Neural Networks (DNNs), which synthesizes input\npatterns that maximally activate a given feature. Despite its popularity, the\ntrustworthiness of FV explanations has received limited attention. In this\npaper, we introduce a novel method, Gradient Slingshots, that enables\nmanipulation of FV without modifying the model architecture or significantly\ndegrading its performance. By shaping new trajectories in the off-distribution\nregions of the activation landscape of a feature, we coerce the optimization\nprocess to converge in a predefined visualization. We evaluate our approach on\nseveral DNN architectures, demonstrating its ability to replace faithfuls FV\nwith arbitrary targets. These results expose a critical vulnerability: auditors\nrelying solely on FV may accept entirely fabricated explanations. To mitigate\nthis risk, we propose a straightforward defense and quantitatively demonstrate\nits effectiveness.\n","authors":["Dilyara Bareeva","Marina M. -C. Höhne","Alexander Warnecke","Lukas Pirch","Klaus-Robert Müller","Konrad Rieck","Sebastian Lapuschkin","Kirill Bykov"],"pdf_url":"https://arxiv.org/pdf/2401.06122v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11913v1","updated":"2025-06-13T16:06:51Z","published":"2025-06-13T16:06:51Z","title":"O2Former:Direction-Aware and Multi-Scale Query Enhancement for SAR Ship\n  Instance Segmentation","summary":"  Instance segmentation of ships in synthetic aperture radar (SAR) imagery is\ncritical for applications such as maritime monitoring, environmental analysis,\nand national security. SAR ship images present challenges including scale\nvariation, object density, and fuzzy target boundary, which are often\noverlooked in existing methods, leading to suboptimal performance. In this\nwork, we propose O2Former, a tailored instance segmentation framework that\nextends Mask2Former by fully leveraging the structural characteristics of SAR\nimagery. We introduce two key components. The first is the Optimized Query\nGenerator(OQG). It enables multi-scale feature interaction by jointly encoding\nshallow positional cues and high-level semantic information. This improves\nquery quality and convergence efficiency. The second component is the\nOrientation-Aware Embedding Module(OAEM). It enhances directional sensitivity\nthrough direction-aware convolution and polar-coordinate encoding. This\neffectively addresses the challenge of uneven target orientations in SAR\nscenes. Together, these modules facilitate precise feature alignment from\nbackbone to decoder and strengthen the model's capacity to capture fine-grained\nstructural details. Extensive experiments demonstrate that O2Former outperforms\nstate of the art instance segmentation baselines, validating its effectiveness\nand generalization on SAR ship datasets.\n","authors":["F. Gao","Y Li","X He","J Sun","J Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11913v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.19037v2","updated":"2025-06-13T15:57:15Z","published":"2024-11-28T10:33:01Z","title":"3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for\n  High-Fidelity 3D Shapes","summary":"  Autoregressive (AR) models have achieved remarkable success in natural\nlanguage and image generation, but their application to 3D shape modeling\nremains largely unexplored. Unlike diffusion models, AR models enable more\nefficient and controllable generation with faster inference times, making them\nespecially suitable for data-intensive domains. Traditional 3D generative\nmodels using AR approaches often rely on ``next-token\" predictions at the voxel\nor point level. While effective for certain applications, these methods can be\nrestrictive and computationally expensive when dealing with large-scale 3D\ndata. To tackle these challenges, we introduce 3D-WAG, an AR model for 3D\nimplicit distance fields that can perform unconditional shape generation,\nclass-conditioned and also text-conditioned shape generation. Our key idea is\nto encode shapes as multi-scale wavelet token maps and use a Transformer to\npredict the ``next higher-resolution token map\" in an autoregressive manner. By\nredefining 3D AR generation task as ``next-scale\" prediction, we reduce the\ncomputational cost of generation compared to traditional ``next-token\"\nprediction models, while preserving essential geometric details of 3D shapes in\na more structured and hierarchical manner. We evaluate 3D-WAG to showcase its\nbenefit by quantitative and qualitative comparisons with state-of-the-art\nmethods on widely used benchmarks. Our results show 3D-WAG achieves superior\nperformance in key metrics like Coverage and MMD, generating high-fidelity 3D\nshapes that closely match the real data distribution.\n","authors":["Tejaswini Medi","Arianna Rampini","Pradyumna Reddy","Pradeep Kumar Jayaraman","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2411.19037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12695v3","updated":"2025-06-13T15:48:02Z","published":"2024-10-16T15:58:47Z","title":"Holstein-Friesian Re-Identification using Multiple Cameras and\n  Self-Supervision on a Working Farm","summary":"  We present MultiCamCows2024, a farm-scale image dataset filmed across\nmultiple cameras for the biometric identification of individual\nHolstein-Friesian cattle exploiting their unique black and white coat-patterns.\nCaptured by three ceiling-mounted visual sensors covering adjacent barn areas\nover seven days on a working dairy farm, the dataset comprises 101,329 images\nof 90 cows, plus underlying original CCTV footage. The dataset is provided with\nfull computer vision recognition baselines, that is both a supervised and\nself-supervised learning framework for individual cow identification trained on\ncattle tracklets. We report a performance above 96% single image identification\naccuracy from the dataset and demonstrate that combining data from multiple\ncameras during learning enhances self-supervised identification. We show that\nour framework enables automatic cattle identification, barring only the simple\nhuman verification of tracklet integrity during data collection. Crucially, our\nstudy highlights that multi-camera, supervised and self-supervised components\nin tandem not only deliver highly accurate individual cow identification, but\nalso achieve this efficiently with no labelling of cattle identities by humans.\nWe argue that this improvement in efficacy has practical implications for\nlivestock management, behaviour analysis, and agricultural monitoring. For\nreproducibility and practical ease of use, we publish all key software and code\nincluding re-identification components and the species detector with this\npaper, available at https://tinyurl.com/MultiCamCows2024.\n","authors":["Phoenix Yu","Tilo Burghardt","Andrew W Dowsey","Neill W Campbell"],"pdf_url":"https://arxiv.org/pdf/2410.12695v3.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.10496v2","updated":"2025-06-13T15:39:53Z","published":"2025-05-15T16:59:17Z","title":"CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs","summary":"  We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/\n","authors":["Raman Dutt","Pedro Sanchez","Yongchen Yao","Steven McDonagh","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2505.10496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11876v1","updated":"2025-06-13T15:27:29Z","published":"2025-06-13T15:27:29Z","title":"Methods for evaluating the resolution of 3D data derived from satellite\n  images","summary":"  3D data derived from satellite images is essential for scene modeling\napplications requiring large-scale coverage or involving locations not\naccessible by airborne lidar or cameras. Measuring the resolution of this data\nis important for determining mission utility and tracking improvements. In this\nwork, we consider methods to evaluate the resolution of point clouds, digital\nsurface models, and 3D mesh models. We describe 3D metric evaluation tools and\nworkflows that enable automated evaluation based on high-resolution reference\nairborne lidar, and we present results of analyses with data of varying\nquality.\n","authors":["Christina Selby","Holden Bindl","Tyler Feldman","Andrew Skow","Nicolas Norena Acosta","Shea Hagstrom","Myron Brown"],"pdf_url":"https://arxiv.org/pdf/2506.11876v1.pdf","comment":"11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.07196v2","updated":"2025-06-13T15:23:25Z","published":"2025-06-08T15:30:04Z","title":"SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical\n  Action Planning","summary":"  Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.\n","authors":["Mengya Xu","Zhongzhen Huang","Dillan Imans","Yiru Ye","Xiaofan Zhang","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2506.07196v2.pdf","comment":"The authors could not reach a consensus on the final version of this\n  paper, necessitating its withdrawal"},{"id":"http://arxiv.org/abs/2506.11863v1","updated":"2025-06-13T15:13:09Z","published":"2025-06-13T15:13:09Z","title":"SphereDrag: Spherical Geometry-Aware Panoramic Image Editing","summary":"  Image editing has made great progress on planar images, but panoramic image\nediting remains underexplored. Due to their spherical geometry and projection\ndistortions, panoramic images present three key challenges: boundary\ndiscontinuity, trajectory deformation, and uneven pixel density. To tackle\nthese issues, we propose SphereDrag, a novel panoramic editing framework\nutilizing spherical geometry knowledge for accurate and controllable editing.\nSpecifically, adaptive reprojection (AR) uses adaptive spherical rotation to\ndeal with discontinuity; great-circle trajectory adjustment (GCTA) tracks the\nmovement trajectory more accurate; spherical search region tracking (SSRT)\nadaptively scales the search range based on spherical location to address\nuneven pixel density. Also, we construct PanoBench, a panoramic editing\nbenchmark, including complex editing tasks involving multiple objects and\ndiverse styles, which provides a standardized evaluation framework. Experiments\nshow that SphereDrag gains a considerable improvement compared with existing\nmethods in geometric consistency and image quality, achieving up to 10.5%\nrelative improvement.\n","authors":["Zhiao Feng","Xuewei Li","Junjie Yang","Yuxin Peng","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2506.11863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11860v1","updated":"2025-06-13T15:09:15Z","published":"2025-06-13T15:09:15Z","title":"MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command\n  Line and Browser","summary":"  We developed MindGrab, a parameter- and memory-efficient deep\nfully-convolutional model for volumetric skull-stripping in head images of any\nmodality. Its architecture, informed by a spectral interpretation of dilated\nconvolutions, was trained exclusively on modality-agnostic synthetic data.\nMindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain\nscans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip\ndataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using\nDice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a\nmean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities,\nsignificantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05;\nBET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352),\nMindGrab delivered equivalent or superior performance in nearly half of the\ntested scenarios, with minor differences (<3% Dice) in the others. MindGrab\nutilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This\nefficiency yielded at least 2x faster inference, 50% lower memory usage on\nGPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x\nmemory reduction) and accessibility on a wider range of hardware, including\nsystems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with\ndramatically lower resource demands, supported in brainchop-cli\n(https://pypi.org/project/brainchop/) and at brainchop.org.\n","authors":["Armina Fani","Mike Doan","Isabelle Le","Alex Fedorov","Malte Hoffmann","Chris Rorden","Sergey Plis"],"pdf_url":"https://arxiv.org/pdf/2506.11860v1.pdf","comment":"12 pages, 1 table, 4 figures. 2 supplementary tables, 1 supplementary\n  figure. Brainchop-cli: https://pypi.org/project/brainchop/ . Brainchop web:\n  https://brainchop.org/"},{"id":"http://arxiv.org/abs/2506.11839v1","updated":"2025-06-13T14:40:12Z","published":"2025-06-13T14:40:12Z","title":"Vision-based Lifting of 2D Object Detections for Automated Driving","summary":"  Image-based 3D object detection is an inevitable part of autonomous driving\nbecause cheap onboard cameras are already available in most modern cars.\nBecause of the accurate depth information, currently, most state-of-the-art 3D\nobject detectors heavily rely on LiDAR data. In this paper, we propose a\npipeline which lifts the results of existing vision-based 2D algorithms to 3D\ndetections using only cameras as a cost-effective alternative to LiDAR. In\ncontrast to existing approaches, we focus not only on cars but on all types of\nroad users. To the best of our knowledge, we are the first using a 2D CNN to\nprocess the point cloud for each 2D detection to keep the computational effort\nas low as possible. Our evaluation on the challenging KITTI 3D object detection\nbenchmark shows results comparable to state-of-the-art image-based approaches\nwhile having a runtime of only a third.\n","authors":["Hendrik Königshof","Kun Li","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2506.11839v1.pdf","comment":"https://ieeexplore.ieee.org/document/9190325"},{"id":"http://arxiv.org/abs/2504.10750v2","updated":"2025-06-13T14:34:36Z","published":"2025-04-14T22:49:08Z","title":"Real-time Seafloor Segmentation and Mapping","summary":"  Posidonia oceanica meadows are a species of seagrass highly dependent on\nrocks for their survival and conservation. In recent years, there has been a\nconcerning global decline in this species, emphasizing the critical need for\nefficient monitoring and assessment tools. While deep learning-based semantic\nsegmentation and visual automated monitoring systems have shown promise in a\nvariety of applications, their performance in underwater environments remains\nchallenging due to complex water conditions and limited datasets. This paper\nintroduces a framework that combines machine learning and computer vision\ntechniques to enable an autonomous underwater vehicle (AUV) to inspect the\nboundaries of Posidonia oceanica meadows autonomously. The framework\nincorporates an image segmentation module using an existing Mask R-CNN model\nand a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a\nnew class dedicated to rocks is introduced to enhance the existing model,\naiming to contribute to a comprehensive monitoring approach and provide a\ndeeper understanding of the intricate interactions between the meadow and its\nsurrounding environment. The image segmentation model is validated using real\nunderwater images, while the overall inspection framework is evaluated in a\nrealistic simulation environment, replicating actual monitoring scenarios with\nreal underwater images. The results demonstrate that the proposed framework\nenables the AUV to autonomously accomplish the main tasks of underwater\ninspection and segmentation of rocks. Consequently, this work holds significant\npotential for the conservation and protection of marine environments, providing\nvaluable insights into the status of Posidonia oceanica meadows and supporting\ntargeted preservation efforts\n","authors":["Michele Grimaldi","Nouf Alkaabi","Francesco Ruscio","Sebastian Realpe Rua","Rafael Garcia","Nuno Gracias"],"pdf_url":"https://arxiv.org/pdf/2504.10750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11823v1","updated":"2025-06-13T14:29:40Z","published":"2025-06-13T14:29:40Z","title":"Structural Similarity-Inspired Unfolding for Lightweight Image\n  Super-Resolution","summary":"  Major efforts in data-driven image super-resolution (SR) primarily focus on\nexpanding the receptive field of the model to better capture contextual\ninformation. However, these methods are typically implemented by stacking\ndeeper networks or leveraging transformer-based attention mechanisms, which\nconsequently increases model complexity. In contrast, model-driven methods\nbased on the unfolding paradigm show promise in improving performance while\neffectively maintaining model compactness through sophisticated module design.\nBased on these insights, we propose a Structural Similarity-Inspired Unfolding\n(SSIU) method for efficient image SR. This method is designed through unfolding\nan SR optimization function constrained by structural similarity, aiming to\ncombine the strengths of both data-driven and model-driven approaches. Our\nmodel operates progressively following the unfolding paradigm. Each iteration\nconsists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse\nAttention Module (ESAM). The former implements comprehensive constraints on\nfeatures, including a structural similarity constraint, while the latter aims\nto achieve sparse activation. In addition, we design a Mixture-of-Experts-based\nFeature Selector (MoE-FS) that fully utilizes multi-level feature information\nby combining features from different steps. Extensive experiments validate the\nefficacy and efficiency of our unfolding-inspired network. Our model\noutperforms current state-of-the-art models, boasting lower parameter counts\nand reduced memory consumption. Our code will be available at:\nhttps://github.com/eezkni/SSIU\n","authors":["Zhangkai Ni","Yang Zhang","Wenhan Yang","Hanli Wang","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2506.11823v1.pdf","comment":"Accepted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2506.11821v1","updated":"2025-06-13T14:26:09Z","published":"2025-06-13T14:26:09Z","title":"Framework of a multiscale data-driven digital twin of the\n  muscle-skeletal system","summary":"  Musculoskeletal disorders (MSDs) are a leading cause of disability worldwide,\nrequiring advanced diagnostic and therapeutic tools for personalised assessment\nand treatment. Effective management of MSDs involves the interaction of\nheterogeneous data sources, making the Digital Twin (DT) paradigm a valuable\noption. This paper introduces the Musculoskeletal Digital Twin (MS-DT), a novel\nframework that integrates multiscale biomechanical data with computational\nmodelling to create a detailed, patient-specific representation of the\nmusculoskeletal system. By combining motion capture, ultrasound imaging,\nelectromyography, and medical imaging, the MS-DT enables the analysis of spinal\nkinematics, posture, and muscle function. An interactive visualisation platform\nprovides clinicians and researchers with an intuitive interface for exploring\nbiomechanical parameters and tracking patient-specific changes. Results\ndemonstrate the effectiveness of MS-DT in extracting precise kinematic and\ndynamic tissue features, offering a comprehensive tool for monitoring spine\nbiomechanics and rehabilitation. This framework provides high-fidelity\nmodelling and real-time visualization to improve patient-specific diagnosis and\nintervention planning.\n","authors":["Martina Paccini","Simone Cammarasana","Giuseppe Patanè"],"pdf_url":"https://arxiv.org/pdf/2506.11821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11820v1","updated":"2025-06-13T14:23:38Z","published":"2025-06-13T14:23:38Z","title":"Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation","summary":"  Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.\n","authors":["Xintong Wang","Jingheng Pan","Yixiao Liu","Xiaohu Zhao","Chenyang Lyu","Minghao Wu","Chris Biemann","Longyue Wang","Linlong Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01066v4","updated":"2025-06-13T14:07:47Z","published":"2024-05-02T07:47:49Z","title":"HandS3C: 3D Hand Mesh Reconstruction with State Space Spatial Channel\n  Attention from RGB images","summary":"  Reconstructing the hand mesh from one single RGB image is a challenging task\nbecause hands are often occluded by other objects. Most previous works attempt\nto explore more additional information and adopt attention mechanisms for\nimproving 3D reconstruction performance, while it would increase computational\ncomplexity simultaneously. To achieve a performance-reserving architecture with\nhigh computational efficiency, in this work, we propose a simple but effective\n3D hand mesh reconstruction network (i.e., HandS3C), which is the first time to\nincorporate state space model into the task of hand mesh reconstruction. In the\nnetwork, we design a novel state-space spatial-channel attention module that\nextends the effective receptive field, extracts hand features in the spatial\ndimension, and enhances regional features of hands in the channel dimension.\nThis helps to reconstruct a complete and detailed hand mesh. Extensive\nexperiments conducted on well-known datasets facing heavy occlusions (such as\nFREIHAND, DEXYCB, and HO3D) demonstrate that our proposed HandS3C achieves\nstate-of-the-art performance while maintaining a minimal parameters.\n","authors":["Zixun Jiao","Xihan Wang","Zhaoqiang Xia","Lianhe Shao","Quanli Gao"],"pdf_url":"https://arxiv.org/pdf/2405.01066v4.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.11804v1","updated":"2025-06-13T14:07:00Z","published":"2025-06-13T14:07:00Z","title":"Teleoperated Driving: a New Challenge for 3D Object Detection in\n  Compressed Point Clouds","summary":"  In recent years, the development of interconnected devices has expanded in\nmany fields, from infotainment to education and industrial applications. This\ntrend has been accelerated by the increased number of sensors and accessibility\nto powerful hardware and software. One area that significantly benefits from\nthese advancements is Teleoperated Driving (TD). In this scenario, a controller\ndrives safely a vehicle from remote leveraging sensors data generated onboard\nthe vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In\nthis work, we tackle the problem of detecting the presence of cars and\npedestrians from point cloud data to enable safe TD operations. More\nspecifically, we exploit the SELMA dataset, a multimodal, open-source,\nsynthetic dataset for autonomous driving, that we expanded by including the\nground-truth bounding boxes of 3D objects to support object detection. We\nanalyze the performance of state-of-the-art compression algorithms and object\ndetectors under several metrics, including compression efficiency,\n(de)compression and inference time, and detection accuracy. Moreover, we\nmeasure the impact of compression and detection on the V2X network in terms of\ndata rate and latency with respect to 3GPP requirements for TD applications.\n","authors":["Filippo Bragato","Michael Neri","Paolo Testolina","Marco Giordani","Federica Battisti"],"pdf_url":"https://arxiv.org/pdf/2506.11804v1.pdf","comment":"Submitted to IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2506.11796v1","updated":"2025-06-13T14:01:39Z","published":"2025-06-13T14:01:39Z","title":"Solving Inverse Problems in Stochastic Self-Organising Systems through\n  Invariant Representations","summary":"  Self-organising systems demonstrate how simple local rules can generate\ncomplex stochastic patterns. Many natural systems rely on such dynamics, making\nself-organisation central to understanding natural complexity. A fundamental\nchallenge in modelling such systems is solving the inverse problem: finding the\nunknown causal parameters from macroscopic observations. This task becomes\nparticularly difficult when observations have a strong stochastic component,\nyielding diverse yet equivalent patterns. Traditional inverse methods fail in\nthis setting, as pixel-wise metrics cannot capture feature similarities between\nvariable outcomes. In this work, we introduce a novel inverse modelling method\nspecifically designed to handle stochasticity in the observable space,\nleveraging the capacity of visual embeddings to produce robust representations\nthat capture perceptual invariances. By mapping the pattern representations\nonto an invariant embedding space, we can effectively recover unknown causal\nparameters without the need for handcrafted objective functions or heuristics.\nWe evaluate the method on two canonical models--a reaction-diffusion system and\nan agent-based model of social segregation--and show that it reliably recovers\nparameters despite stochasticity in the outcomes. We further apply the method\nto real biological patterns, highlighting its potential as a tool for both\ntheorists and experimentalists to investigate the dynamics underlying complex\nstochastic pattern formation.\n","authors":["Elias Najarro","Nicolas Bessone","Sebastian Risi"],"pdf_url":"https://arxiv.org/pdf/2506.11796v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.11784v1","updated":"2025-06-13T13:45:17Z","published":"2025-06-13T13:45:17Z","title":"GPLQ: A General, Practical, and Lightning QAT Method for Vision\n  Transformers","summary":"  Vision Transformers (ViTs) are essential in computer vision but are\ncomputationally intensive, too. Model quantization, particularly to low\nbit-widths like 4-bit, aims to alleviate this difficulty, yet existing\nPost-Training Quantization (PTQ) and Quantization-Aware Training (QAT) methods\nexhibit significant limitations. PTQ often incurs substantial accuracy drop,\nwhile QAT achieves high accuracy but suffers from prohibitive computational\ncosts, limited generalization to downstream tasks, training instability, and\nlacking of open-source codebase. To address these challenges, this paper\nintroduces General, Practical, and Lightning Quantization (GPLQ), a novel\nframework designed for efficient and effective ViT quantization. GPLQ is\nfounded on two key empirical insights: the paramount importance of activation\nquantization and the necessity of preserving the model's original optimization\n``basin'' to maintain generalization. Consequently, GPLQ employs a sequential\n``activation-first, weights-later'' strategy. Stage 1 keeps weights in FP32\nwhile quantizing activations with a feature mimicking loss in only 1 epoch to\nkeep it stay in the same ``basin'', thereby preserving generalization. Stage 2\nquantizes weights using a PTQ method. As a result, GPLQ is 100x faster than\nexisting QAT methods, lowers memory footprint to levels even below FP32\ntraining, and achieves 4-bit model performance that is highly competitive with\nFP32 models in terms of both accuracy on ImageNet and generalization to diverse\ndownstream tasks, including fine-grained visual classification and object\ndetection. We will release an easy-to-use open-source toolkit supporting\nmultiple vision tasks.\n","authors":["Guang Liang","Xinyao Liu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2506.11784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07612v2","updated":"2025-06-13T13:43:21Z","published":"2025-06-09T10:25:53Z","title":"Scaling Human Activity Recognition: A Comparative Evaluation of\n  Synthetic Data Generation and Augmentation Techniques","summary":"  Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.\n","authors":["Zikang Leng","Archith Iyer","Thomas Plötz"],"pdf_url":"https://arxiv.org/pdf/2506.07612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11777v1","updated":"2025-06-13T13:36:33Z","published":"2025-06-13T13:36:33Z","title":"Self-supervised Learning of Echocardiographic Video Representations via\n  Online Cluster Distillation","summary":"  Self-supervised learning (SSL) has achieved major advances in natural images\nand video understanding, but challenges remain in domains like echocardiography\n(heart ultrasound) due to subtle anatomical structures, complex temporal\ndynamics, and the current lack of domain-specific pre-trained models. Existing\nSSL approaches such as contrastive, masked modeling, and clustering-based\nmethods struggle with high intersample similarity, sensitivity to low PSNR\ninputs common in ultrasound, or aggressive augmentations that distort\nclinically relevant features. We present DISCOVR (Distilled Image Supervision\nfor Cross Modal Video Representation), a self-supervised dual branch framework\nfor cardiac ultrasound video representation learning. DISCOVR combines a\nclustering-based video encoder that models temporal dynamics with an online\nimage encoder that extracts fine-grained spatial semantics. These branches are\nconnected through a semantic cluster distillation loss that transfers\nanatomical knowledge from the evolving image encoder to the video encoder,\nenabling temporally coherent representations enriched with fine-grained\nsemantic understanding. Evaluated on six echocardiography datasets spanning\nfetal, pediatric, and adult populations, DISCOVR outperforms both specialized\nvideo anomaly detection methods and state-of-the-art video-SSL baselines in\nzero-shot and linear probing setups, and achieves superior segmentation\ntransfer.\n","authors":["Divyanshu Mishra","Mohammadreza Salehi","Pramit Saha","Olga Patey","Aris T. Papageorghiou","Yuki M. Asano","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2506.11777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11774v1","updated":"2025-06-13T13:33:59Z","published":"2025-06-13T13:33:59Z","title":"Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation","summary":"  Isometric exercises appeal to individuals seeking convenience, privacy, and\nminimal dependence on equipments. However, such fitness training is often\noverdependent on unreliable digital media content instead of expert\nsupervision, introducing serious risks, including incorrect posture, injury,\nand disengagement due to lack of corrective feedback. To address these\nchallenges, we present a real-time feedback system for assessing isometric\nposes. Our contributions include the release of the largest multiclass\nisometric exercise video dataset to date, comprising over 3,600 clips across\nsix poses with correct and incorrect variations. To support robust evaluation,\nwe benchmark state-of-the-art models-including graph-based networks-on this\ndataset and introduce a novel three-part metric that captures classification\naccuracy, mistake localization, and model confidence. Our results enhance the\nfeasibility of intelligent and personalized exercise training systems for home\nworkouts. This expert-level diagnosis, delivered directly to the users, also\nexpands the potential applications of these systems to rehabilitation,\nphysiotherapy, and various other fitness disciplines that involve physical\nmotion.\n","authors":["Abhishek Jaiswal","Armeet Singh Luthra","Purav Jangir","Bhavya Garg","Nisheeth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2506.11774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11773v1","updated":"2025-06-13T13:31:08Z","published":"2025-06-13T13:31:08Z","title":"AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated\n  Home Environments","summary":"  A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts.\n","authors":["Zikang Leng","Megha Thukral","Yaqi Liu","Hrudhai Rajasekhar","Shruthi K. Hiremath","Thomas Plötz"],"pdf_url":"https://arxiv.org/pdf/2506.11773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11772v1","updated":"2025-06-13T13:30:15Z","published":"2025-06-13T13:30:15Z","title":"CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection","summary":"  Anomaly detection is a complex problem due to the ambiguity in defining\nanomalies, the diversity of anomaly types (e.g., local and global defect), and\nthe scarcity of training data. As such, it necessitates a comprehensive model\ncapable of capturing both low-level and high-level features, even with limited\ndata. To address this, we propose CLIPFUSION, a method that leverages both\ndiscriminative and generative foundation models. Specifically, the CLIP-based\ndiscriminative model excels at capturing global features, while the\ndiffusion-based generative model effectively captures local details, creating a\nsynergistic and complementary approach. Notably, we introduce a methodology for\nutilizing cross-attention maps and feature maps extracted from diffusion models\nspecifically for anomaly detection. Experimental results on benchmark datasets\n(MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline\nmethods, achieving outstanding performance in both anomaly segmentation and\nclassification. We believe that our method underscores the effectiveness of\nmulti-modal and multi-model fusion in tackling the multifaceted challenges of\nanomaly detection, providing a scalable solution for real-world applications.\n","authors":["Byeongchan Lee","John Won","Seunghyun Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2506.11772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11768v1","updated":"2025-06-13T13:22:28Z","published":"2025-06-13T13:22:28Z","title":"MambaVSR: Content-Aware Scanning State Space Model for Video\n  Super-Resolution","summary":"  Video super-resolution (VSR) faces critical challenges in effectively\nmodeling non-local dependencies across misaligned frames while preserving\ncomputational efficiency. Existing VSR methods typically rely on optical flow\nstrategies or transformer architectures, which struggle with large motion\ndisplacements and long video sequences. To address this, we propose MambaVSR,\nthe first state-space model framework for VSR that incorporates an innovative\ncontent-aware scanning mechanism. Unlike rigid 1D sequential processing in\nconventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal\ninteractions through the Shared Compass Construction (SCC) and the\nContent-Aware Sequentialization (CAS). Specifically, the SCC module constructs\nintra-frame semantic connectivity graphs via efficient sparse attention and\ngenerates adaptive spatial scanning sequences through spectral clustering.\nBuilding upon SCC, the CAS module effectively aligns and aggregates non-local\nsimilar content across multiple frames by interleaving temporal features along\nthe learned spatial order. To bridge global dependencies with local details,\nthe Global-Local State Space Block (GLSSB) synergistically integrates window\nself-attention operations with SSM-based feature propagation, enabling\nhigh-frequency detail recovery under global dependency guidance. Extensive\nexperiments validate MambaVSR's superiority, outperforming the\nTransformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer\nparameters.\n","authors":["Linfeng He","Meiqin Liu","Qi Tang","Chao Yao","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.11768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11764v1","updated":"2025-06-13T13:18:09Z","published":"2025-06-13T13:18:09Z","title":"DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using\n  Diffusion Models","summary":"  This paper presents DiffFuSR, a modular pipeline for super-resolving all 12\nspectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling\ndistance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a\ndiffusion-based super-resolution (SR) model trained on high-resolution RGB\nimagery from the NAIP and WorldStrat datasets, harmonized to simulate\nSentinel-2 characteristics; and (ii) a learned fusion network that upscales the\nremaining multispectral bands using the super-resolved RGB image as a spatial\nprior. We introduce a robust degradation model and contrastive degradation\nencoder to support blind SR. Extensive evaluations of the proposed SR pipeline\non the OpenSR benchmark demonstrate that the proposed method outperforms\ncurrent SOTA baselines in terms of reflectance fidelity, spectral consistency,\nspatial alignment, and hallucination suppression. Furthermore, the fusion\nnetwork significantly outperforms classical pansharpening approaches, enabling\naccurate enhancement of Sentinel-2's 20 m and 60 m bands. This study\nunderscores the power of harmonized learning with generative priors and fusion\nstrategies to create a modular framework for Sentinel-2 SR. Our code and models\ncan be found at https://github.com/NorskRegnesentral/DiffFuSR.\n","authors":["Muhammad Sarmad","Arnt-Børre Salberg","Michael Kampffmeyer"],"pdf_url":"https://arxiv.org/pdf/2506.11764v1.pdf","comment":"preprint under review"},{"id":"http://arxiv.org/abs/2506.11753v1","updated":"2025-06-13T13:09:11Z","published":"2025-06-13T13:09:11Z","title":"Exploring the Effectiveness of Deep Features from Domain-Specific\n  Foundation Models in Retinal Image Synthesis","summary":"  The adoption of neural network models in medical imaging has been constrained\nby strict privacy regulations, limited data availability, high acquisition\ncosts, and demographic biases. Deep generative models offer a promising\nsolution by generating synthetic data that bypasses privacy concerns and\naddresses fairness by producing samples for under-represented groups. However,\nunlike natural images, medical imaging requires validation not only for\nfidelity (e.g., Fr\\'echet Inception Score) but also for morphological and\nclinical accuracy. This is particularly true for colour fundus retinal imaging,\nwhich requires precise replication of the retinal vascular network, including\nvessel topology, continuity, and thickness. In this study, we in-vestigated\nwhether a distance-based loss function based on deep activation layers of a\nlarge foundational model trained on large corpus of domain data, colour fundus\nimaging, offers advantages over a perceptual loss and edge-detection based loss\nfunctions. Our extensive validation pipeline, based on both domain-free and\ndomain specific tasks, suggests that domain-specific deep features do not\nimprove autoen-coder image generation. Conversely, our findings highlight the\neffectiveness of con-ventional edge detection filters in improving the\nsharpness of vascular structures in synthetic samples.\n","authors":["Zuzanna Skorniewska","Bartlomiej W. Papiez"],"pdf_url":"https://arxiv.org/pdf/2506.11753v1.pdf","comment":"To be published and presented at the MIUA 2025 conference"},{"id":"http://arxiv.org/abs/2506.11740v1","updated":"2025-06-13T12:52:46Z","published":"2025-06-13T12:52:46Z","title":"AgriPotential: A Novel Multi-Spectral and Multi-Temporal Remote Sensing\n  Dataset for Agricultural Potentials","summary":"  Remote sensing has emerged as a critical tool for large-scale Earth\nmonitoring and land management. In this paper, we introduce AgriPotential, a\nnovel benchmark dataset composed of Sentinel-2 satellite imagery spanning\nmultiple months. The dataset provides pixel-level annotations of agricultural\npotentials for three major crop types - viticulture, market gardening, and\nfield crops - across five ordinal classes. AgriPotential supports a broad range\nof machine learning tasks, including ordinal regression, multi-label\nclassification, and spatio-temporal modeling. The data covers diverse areas in\nSouthern France, offering rich spectral information. AgriPotential is the first\npublic dataset designed specifically for agricultural potential prediction,\naiming to improve data-driven approaches to sustainable land use planning. The\ndataset and the code are freely accessible at:\nhttps://zenodo.org/records/15556484\n","authors":["Mohammad El Sakka","Caroline De Pourtales","Lotfi Chaari","Josiane Mothe"],"pdf_url":"https://arxiv.org/pdf/2506.11740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11737v1","updated":"2025-06-13T12:48:39Z","published":"2025-06-13T12:48:39Z","title":"Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in\n  Interleaved Multi-Image Model","summary":"  This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.\n","authors":["Dinh Viet Cuong","Hoang-Bao Le","An Pham Ngoc Nguyen","Liting Zhou","Cathal Gurrin"],"pdf_url":"https://arxiv.org/pdf/2506.11737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04996v2","updated":"2025-06-13T12:48:32Z","published":"2025-06-05T13:05:23Z","title":"PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment","summary":"  Automated sports skill assessment requires capturing fundamental movement\npatterns that distinguish expert from novice performance, yet current video\nsampling methods disrupt the temporal continuity essential for proficiency\nevaluation. To this end, we introduce Proficiency-Aware Temporal Sampling\n(PATS), a novel sampling strategy that preserves complete fundamental movements\nwithin continuous temporal segments for multi-view skill assessment. PATS\nadaptively segments videos to ensure each analyzed portion contains full\nexecution of critical performance components, repeating this process across\nmultiple segments to maximize information coverage while maintaining temporal\ncoherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses\nthe state-of-the-art accuracy across all viewing configurations (+0.65% to\n+3.05%) and delivers substantial gains in challenging domains (+26.22%\nbouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that\nPATS successfully adapts to diverse activity characteristics-from\nhigh-frequency sampling for dynamic sports to fine-grained segmentation for\nsequential skills-demonstrating its effectiveness as an adaptive approach to\ntemporal sampling that advances automated skill assessment for real-world\napplications.\n","authors":["Edoardo Bianchi","Antonio Liotta"],"pdf_url":"https://arxiv.org/pdf/2506.04996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08665v2","updated":"2025-06-13T12:46:04Z","published":"2025-05-13T15:27:24Z","title":"SkillFormer: Unified Multi-View Video Understanding for Proficiency\n  Estimation","summary":"  Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.\n","authors":["Edoardo Bianchi","Antonio Liotta"],"pdf_url":"https://arxiv.org/pdf/2505.08665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07855v2","updated":"2025-06-13T12:20:30Z","published":"2025-02-11T14:04:43Z","title":"Vision-Language Models for Edge Networks: A Comprehensive Survey","summary":"  Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.\n","authors":["Ahmed Sharshar","Latif U. Khan","Waseem Ullah","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2502.07855v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09095v2","updated":"2025-06-13T12:07:06Z","published":"2025-06-10T12:14:05Z","title":"Foundation Models in Medical Imaging -- A Review and Outlook","summary":"  Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.\n","authors":["Vivien van Veldhuizen","Vanessa Botha","Chunyao Lu","Melis Erdal Cesur","Kevin Groot Lipman","Edwin D. de Jong","Hugo Horlings","Clárisa I. Sanchez","Cees G. M. Snoek","Lodewyk Wessels","Ritse Mann","Eric Marcus","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2506.09095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11691v1","updated":"2025-06-13T11:38:18Z","published":"2025-06-13T11:38:18Z","title":"DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete\n  Multi-Modal Medical Image Segmentation","summary":"  Incomplete multi-modal medical image segmentation faces critical challenges\nfrom modality imbalance, including imbalanced modality missing rates and\nheterogeneous modality contributions. Due to their reliance on idealized\nassumptions of complete modality availability, existing methods fail to\ndynamically balance contributions and neglect the structural relationships\nbetween modalities, resulting in suboptimal performance in real-world clinical\nscenarios. To address these limitations, we propose a novel model, named\nDynamic Modality-Aware Fusion Network (DMAF-Net). The DMAF-Net adopts three key\nideas. First, it introduces a Dynamic Modality-Aware Fusion (DMAF) module to\nsuppress missing-modality interference by combining transformer attention with\nadaptive masking and weight modality contributions dynamically through\nattention maps. Second, it designs a synergistic Relation Distillation and\nPrototype Distillation framework to enforce global-local feature alignment via\ncovariance consistency and masked graph attention, while ensuring semantic\nconsistency through cross-modal class-specific prototype alignment. Third, it\npresents a Dynamic Training Monitoring (DTM) strategy to stabilize optimization\nunder imbalanced missing rates by tracking distillation gaps in real-time, and\nto balance convergence speeds across modalities by adaptively reweighting\nlosses and scaling gradients. Extensive experiments on BraTS2020 and MyoPS2020\ndemonstrate that DMAF-Net outperforms existing methods for incomplete\nmulti-modal medical image segmentation. Extensive experiments on BraTS2020 and\nMyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete\nmulti-modal medical image segmentation. Our code is available at\nhttps://github.com/violet-42/DMAF-Net.\n","authors":["Libin Lan","Hongxing Li","Zunhui Xia","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11691v1.pdf","comment":"12 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.15969v2","updated":"2025-06-13T11:24:09Z","published":"2025-03-20T09:13:31Z","title":"Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation","summary":"  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, we introduce Llama3-MS-CLIP, the first vision-language\nmodel pre-trained with contrastive learning on a large-scale multispectral\ndataset and report on the performance gains due to the extended spectral range.\nFurthermore, we present the largest-to-date image-caption dataset for\nmultispectral data, consisting of one million Sentinel-2 samples and\ncorresponding textual descriptions generated using Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n+6.77% on average and retrieval performance by +4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. The image-caption dataset, code, and model weights\nare available at https://github.com/IBM/MS-CLIP.\n","authors":["Clive Tinashe Marimo","Benedikt Blumenstiel","Maximilian Nitsche","Johannes Jakubik","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2503.15969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11684v1","updated":"2025-06-13T11:21:00Z","published":"2025-06-13T11:21:00Z","title":"MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual\n  Space","summary":"  Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ninterpreting visual layouts and text. However, a significant challenge remains\nin their ability to interpret robustly and reason over multi-tabular data\npresented as images, a common occurrence in real-world scenarios like web pages\nand digital documents. Existing benchmarks typically address single tables or\nnon-visual data (text/structured). This leaves a critical gap: they don't\nassess the ability to parse diverse table images, correlate information across\nthem, and perform multi-hop reasoning on the combined visual data. We introduce\nMTabVQA, a novel benchmark specifically designed for multi-tabular visual\nquestion answering to bridge that gap. MTabVQA comprises 3,745 complex\nquestion-answer pairs that necessitate multi-hop reasoning across several\nvisually rendered table images. We provide extensive benchmark results for\nstate-of-the-art VLMs on MTabVQA, revealing significant performance\nlimitations. We further investigate post-training techniques to enhance these\nreasoning abilities and release MTabVQA-Instruct, a large-scale\ninstruction-tuning dataset. Our experiments show that fine-tuning VLMs with\nMTabVQA-Instruct substantially improves their performance on visual\nmulti-tabular reasoning. Code and dataset\n(https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval) are available online\n(https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E).\n","authors":["Anshul Singh","Chris Biemann","Jan Strich"],"pdf_url":"https://arxiv.org/pdf/2506.11684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17397v2","updated":"2025-06-13T11:18:58Z","published":"2025-04-24T09:37:02Z","title":"Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for\n  Geospatial Foundation Models","summary":"  Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.\n","authors":["Francesc Marti-Escofet","Benedikt Blumenstiel","Linus Scheibenreif","Paolo Fraccaro","Konrad Schindler"],"pdf_url":"https://arxiv.org/pdf/2504.17397v2.pdf","comment":"Code available at https://github.com/IBM/peft-geofm"},{"id":"http://arxiv.org/abs/2506.11678v1","updated":"2025-06-13T11:16:50Z","published":"2025-06-13T11:16:50Z","title":"Pose Matters: Evaluating Vision Transformers and CNNs for Human Action\n  Recognition on Small COCO Subsets","summary":"  This study explores human action recognition using a three-class subset of\nthe COCO image corpus, benchmarking models from simple fully connected networks\nto transformer architectures. The binary Vision Transformer (ViT) achieved 90%\nmean test accuracy, significantly exceeding multiclass classifiers such as\nconvolutional networks (approximately 35%) and CLIP-based models (approximately\n62-64%). A one-way ANOVA (F = 61.37, p < 0.001) confirmed these differences are\nstatistically significant. Qualitative analysis with SHAP explainer and LeGrad\nheatmaps indicated that the ViT localizes pose-specific regions (e.g., lower\nlimbs for walking or running), while simpler feed-forward models often focus on\nbackground textures, explaining their errors. These findings emphasize the data\nefficiency of transformer representations and the importance of explainability\ntechniques in diagnosing class-specific failures.\n","authors":["MingZe Tang","Madiha Kazi"],"pdf_url":"https://arxiv.org/pdf/2506.11678v1.pdf","comment":"7 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.11677v1","updated":"2025-06-13T11:16:23Z","published":"2025-06-13T11:16:23Z","title":"Predicting Patient Survival with Airway Biomarkers using\n  nn-Unet/Radiomics","summary":"  The primary objective of the AIIB 2023 competition is to evaluate the\npredictive significance of airway-related imaging biomarkers in determining the\nsurvival outcomes of patients with lung fibrosis.This study introduces a\ncomprehensive three-stage approach. Initially, a segmentation network, namely\nnn-Unet, is employed to delineate the airway's structural boundaries.\nSubsequently, key features are extracted from the radiomic images centered\naround the trachea and an enclosing bounding box around the airway. This step\nis motivated by the potential presence of critical survival-related insights\nwithin the tracheal region as well as pertinent information encoded in the\nstructure and dimensions of the airway. Lastly, radiomic features obtained from\nthe segmented areas are integrated into an SVM classifier. We could obtain an\noverall-score of 0.8601 for the segmentation in Task 1 while 0.7346 for the\nclassification in Task 2.\n","authors":["Zacharia Mesbah","Dhruv Jain","Tsiry Mayet","Romain Modzelewski","Romain Herault","Simon Bernard","Sebastien Thureau","Clement Chatelain"],"pdf_url":"https://arxiv.org/pdf/2506.11677v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2506.11674v1","updated":"2025-06-13T11:08:16Z","published":"2025-06-13T11:08:16Z","title":"Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised\n  Joint Learning from Medical Images and Reports","summary":"  Learning medical visual representations directly from paired images and\nreports through multimodal self-supervised learning has emerged as a novel and\nefficient approach to digital diagnosis in recent years. However, existing\nmodels suffer from several severe limitations. 1) neglecting the selection of\nnegative samples, resulting in the scarcity of hard negatives and the inclusion\nof false negatives; 2) focusing on global feature extraction, but overlooking\nthe fine-grained local details that are crucial for medical image recognition\ntasks; and 3) contrastive learning primarily targets high-level features but\nignoring low-level details which are essential for accurate medical analysis.\nMotivated by these critical issues, this paper presents a Cross-Modal\nCluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First,\nit extends the k-means clustering used for local text features in the\nsingle-modal domain to the multimodal domain through cross-modal attention.\nThis improvement increases the number of negative samples and boosts the model\nrepresentation capability. Second, it introduces a Cross-Modal Masked Image\nReconstruction (CM-MIR) module that leverages local text-to-image features\nobtained via cross-modal attention to reconstruct masked local image regions.\nThis module significantly strengthens the model's cross-modal information\ninteraction capabilities and retains low-level image features essential for\ndownstream tasks. By well handling the aforementioned limitations, the proposed\nCM-CGNS can learn effective and robust medical visual representations suitable\nfor various recognition tasks. Extensive experimental results on\nclassification, detection, and segmentation tasks across five downstream\ndatasets show that our method outperforms state-of-the-art approaches on\nmultiple metrics, verifying its superior performance.\n","authors":["Libin Lan","Hongxing Li","Zunhui Xia","Juan Zhou","Xiaofei Zhu","Yongmei Li","Yudong Zhang","Xin Luo"],"pdf_url":"https://arxiv.org/pdf/2506.11674v1.pdf","comment":"This work has been submitted to the IEEE TMI for possible\n  publication. Our code is available at https://github.com/violet-42/CM-CGNS"},{"id":"http://arxiv.org/abs/2506.11672v1","updated":"2025-06-13T11:03:46Z","published":"2025-06-13T11:03:46Z","title":"Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal\n  Instruction Tuning","summary":"  Continual multimodal instruction tuning is crucial for adapting Multimodal\nLarge Language Models (MLLMs) to evolving tasks. However, most existing methods\nadopt a fixed architecture, struggling with adapting to new tasks due to static\nmodel capacity. We propose to evolve the architecture under parameter budgets\nfor dynamic task adaptation, which remains unexplored and imposes two\nchallenges: 1) task architecture conflict, where different tasks require\nvarying layer-wise adaptations, and 2) modality imbalance, where different\ntasks rely unevenly on modalities, leading to unbalanced updates. To address\nthese challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts\n(D-MoLE) method, which automatically evolves MLLM's architecture with\ncontrolled parameter budgets to continually adapt to new tasks while retaining\npreviously learned knowledge. Specifically, we propose a dynamic layer-wise\nexpert allocator, which automatically allocates LoRA experts across layers to\nresolve architecture conflicts, and routes instructions layer-wisely to\nfacilitate knowledge sharing among experts. Then, we propose a gradient-based\ninter-modal continual curriculum, which adjusts the update ratio of each module\nin MLLM based on the difficulty of each modality within the task to alleviate\nthe modality imbalance problem. Extensive experiments show that D-MoLE\nsignificantly outperforms state-of-the-art baselines, achieving a 15% average\nimprovement over the best baseline. To the best of our knowledge, this is the\nfirst study of continual learning for MLLMs from an architectural perspective.\n","authors":["Chendi Ge","Xin Wang","Zeyang Zhang","Hong Chen","Jiapei Fan","Longtao Huang","Hui Xue","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.11672v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.11671v1","updated":"2025-06-13T11:03:11Z","published":"2025-06-13T11:03:11Z","title":"Brain Network Analysis Based on Fine-tuned Self-supervised Model for\n  Brain Disease Diagnosis","summary":"  Functional brain network analysis has become an indispensable tool for brain\ndisease analysis. It is profoundly impacted by deep learning methods, which can\ncharacterize complex connections between ROIs. However, the research on\nfoundation models of brain network is limited and constrained to a single\ndimension, which restricts their extensive application in neuroscience. In this\nstudy, we propose a fine-tuned brain network model for brain disease diagnosis.\nIt expands brain region representations across multiple dimensions based on the\noriginal brain network model, thereby enhancing its generalizability. Our model\nconsists of two key modules: (1)an adapter module that expands brain region\nfeatures across different dimensions. (2)a fine-tuned foundation brain network\nmodel, based on self-supervised learning and pre-trained on fMRI data from\nthousands of participants. Specifically, its transformer block is able to\neffectively extract brain region features and compute the inter-region\nassociations. Moreover, we derive a compact latent representation of the brain\nnetwork for brain disease diagnosis. Our downstream experiments in this study\ndemonstrate that the proposed model achieves superior performance in brain\ndisease diagnosis, which potentially offers a promising approach in brain\nnetwork analysis research.\n","authors":["Yifei Tang","Hongjie Jiang","Changhong Jing","Hieu Pham","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11671v1.pdf","comment":"13 pages, 3 figures, International Conference on Neural Computing for\n  Advanced Applications"},{"id":"http://arxiv.org/abs/2506.11661v1","updated":"2025-06-13T10:48:55Z","published":"2025-06-13T10:48:55Z","title":"Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling","summary":"  Instance segmentation of prohibited items in security X-ray images is a\ncritical yet challenging task. This is mainly caused by the significant\nappearance gap between prohibited items in X-ray images and natural objects, as\nwell as the severe overlapping among objects in X-ray images. To address these\nissues, we propose an occlusion-aware instance segmentation pipeline designed\nto identify prohibited items in X-ray images. Specifically, to bridge the\nrepresentation gap, we integrate the Segment Anything Model (SAM) into our\npipeline, taking advantage of its rich priors and zero-shot generalization\ncapabilities. To address the overlap between prohibited items, we design an\nocclusion-aware bilayer mask decoder module that explicitly models the\nocclusion relationships. To supervise occlusion estimation, we manually\nannotated occlusion areas of prohibited items in two large-scale X-ray image\nsegmentation datasets, PIDray and PIXray. We then reorganized these additional\nannotations together with the original information as two occlusion-annotated\ndatasets, PIDray-A and PIXray-A. Extensive experimental results on these\nocclusion-annotated datasets demonstrate the effectiveness of our proposed\nmethod. The datasets and codes are available at: https://github.com/Ryh1218/Occ\n","authors":["Yunhan Ren","Ruihuang Li","Lingbo Liu","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11661v1.pdf","comment":"Accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2506.11653v1","updated":"2025-06-13T10:29:03Z","published":"2025-06-13T10:29:03Z","title":"DISCO: Mitigating Bias in Deep Learning with Conditional Distance\n  Correlation","summary":"  During prediction tasks, models can use any signal they receive to come up\nwith the final answer - including signals that are causally irrelevant. When\npredicting objects from images, for example, the lighting conditions could be\ncorrelated to different targets through selection bias, and an oblivious model\nmight use these signals as shortcuts to discern between various objects. A\npredictor that uses lighting conditions instead of real object-specific details\nis obviously undesirable. To address this challenge, we introduce a standard\nanti-causal prediction model (SAM) that creates a causal framework for\nanalyzing the information pathways influencing our predictor in anti-causal\nsettings. We demonstrate that a classifier satisfying a specific conditional\nindependence criterion will focus solely on the direct causal path from label\nto image, being counterfactually invariant to the remaining variables. Finally,\nwe propose DISCO, a novel regularization strategy that uses conditional\ndistance correlation to optimize for conditional independence in regression\ntasks. We can show that DISCO achieves competitive results in different bias\nmitigation experiments, deeming it a valid alternative to classical\nkernel-based methods.\n","authors":["Emre Kavak","Tom Nuno Wolf","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2506.11653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11627v1","updated":"2025-06-13T09:54:01Z","published":"2025-06-13T09:54:01Z","title":"Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel\n  Technique using Tensor Data and Bayesian Regression","summary":"  Fairness is a critical component of Trustworthy AI. In this paper, we focus\non Machine Learning (ML) and the performance of model predictions when dealing\nwith skin color. Unlike other sensitive attributes, the nature of skin color\ndiffers significantly. In computer vision, skin color is represented as tensor\ndata rather than categorical values or single numerical points. However, much\nof the research on fairness across sensitive groups has focused on categorical\nfeatures such as gender and race. This paper introduces a new technique for\nevaluating fairness in ML for image classification tasks, specifically without\nthe use of annotation. To address the limitations of prior work, we handle\ntensor data, like skin color, without classifying it rigidly. Instead, we\nconvert it into probability distributions and apply statistical distance\nmeasures. This novel approach allows us to capture fine-grained nuances in\nfairness both within and across what would traditionally be considered distinct\ngroups. Additionally, we propose an innovative training method to mitigate the\nlatent biases present in conventional skin tone categorization. This method\nleverages color distance estimates calculated through Bayesian regression with\npolynomial functions, ensuring a more nuanced and equitable treatment of skin\ncolor in ML models.\n","authors":["Kuniko Paxton","Koorosh Aslansefat","Dhavalkumar Thakker","Yiannis Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2506.11627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11621v1","updated":"2025-06-13T09:44:42Z","published":"2025-06-13T09:44:42Z","title":"SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign\n  Language Generation","summary":"  Sign language generation aims to produce diverse sign representations based\non spoken language. However, achieving realistic and naturalistic generation\nremains a significant challenge due to the complexity of sign language, which\nencompasses intricate hand gestures, facial expressions, and body movements. In\nthis work, we introduce PHOENIX14T+, an extended version of the widely-used\nRWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations:\nPose, Hamer and Smplerx. We also propose a novel method, SignAligner, for\nrealistic sign language generation, consisting of three stages: text-driven\npose modalities co-generation, online collaborative correction of\nmultimodality, and realistic sign video synthesis. First, by incorporating text\nsemantics, we design a joint sign language generator to simultaneously produce\nposture coordinates, gesture actions, and body movements. The text encoder,\nbased on a Transformer architecture, extracts semantic features, while a\ncross-modal attention mechanism integrates these features to generate diverse\nsign language representations, ensuring accurate mapping and controlling the\ndiversity of modal features. Next, online collaborative correction is\nintroduced to refine the generated pose modalities using a dynamic loss\nweighting strategy and cross-modal attention, facilitating the complementarity\nof information across modalities, eliminating spatiotemporal conflicts, and\nensuring semantic coherence and action consistency. Finally, the corrected pose\nmodalities are fed into a pre-trained video generation network to produce\nhigh-fidelity sign language videos. Extensive experiments demonstrate that\nSignAligner significantly improves both the accuracy and expressiveness of the\ngenerated sign videos.\n","authors":["Xu Wang","Shengeng Tang","Lechao Cheng","Feng Li","Shuo Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2506.11621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11616v1","updated":"2025-06-13T09:38:57Z","published":"2025-06-13T09:38:57Z","title":"Wi-CBR: WiFi-based Cross-domain Behavior Recognition via Multimodal\n  Collaborative Awareness","summary":"  WiFi-based human behavior recognition aims to recognize gestures and\nactivities by analyzing wireless signal variations. However, existing methods\ntypically focus on a single type of data, neglecting the interaction and fusion\nof multiple features. To this end, we propose a novel multimodal collaborative\nawareness method. By leveraging phase data reflecting changes in dynamic path\nlength and Doppler Shift (DFS) data corresponding to frequency changes related\nto the speed of gesture movement, we enable efficient interaction and fusion of\nthese features to improve recognition accuracy. Specifically, we first\nintroduce a dual-branch self-attention module to capture spatial-temporal cues\nwithin each modality. Then, a group attention mechanism is applied to the\nconcatenated phase and DFS features to mine key group features critical for\nbehavior recognition. Through a gating mechanism, the combined features are\nfurther divided into PD-strengthen and PD-weaken branches, optimizing\ninformation entropy and promoting cross-modal collaborative awareness.\nExtensive in-domain and cross-domain experiments on two large publicly\navailable datasets, Widar3.0 and XRF55, demonstrate the superior performance of\nour method.\n","authors":["Ruobei Zhang","Shengeng Tang","Huan Yan","Xiang Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2506.11616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11604v1","updated":"2025-06-13T09:20:41Z","published":"2025-06-13T09:20:41Z","title":"VLM@school -- Evaluation of AI image understanding on German middle\n  school knowledge","summary":"  This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems.\n","authors":["René Peinl","Vincent Tischler"],"pdf_url":"https://arxiv.org/pdf/2506.11604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07713v2","updated":"2025-06-13T09:10:58Z","published":"2025-06-09T12:57:30Z","title":"Consistent Video Editing as Flow-Driven Image-to-Video Generation","summary":"  With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.\n","authors":["Ge Wang","Songlin Fan","Hangxu Liu","Quanjian Song","Hewei Wang","Jinfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2506.07713v2.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.11599v1","updated":"2025-06-13T09:07:47Z","published":"2025-06-13T09:07:47Z","title":"A$^2$LC: Active and Automated Label Correction for Semantic Segmentation","summary":"  Active Label Correction (ALC) has emerged as a promising solution to the high\ncost and error-prone nature of manual pixel-wise annotation in semantic\nsegmentation, by selectively identifying and correcting mislabeled data.\nAlthough recent work has improved correction efficiency by generating\npseudo-labels using foundation models, substantial inefficiencies still remain.\nIn this paper, we propose Active and Automated Label Correction for semantic\nsegmentation (A$^2$LC), a novel and efficient ALC framework that integrates an\nautomated correction stage into the conventional pipeline. Specifically, the\nautomated correction stage leverages annotator feedback to perform label\ncorrection beyond the queried samples, thereby maximizing cost efficiency. In\naddition, we further introduce an adaptively balanced acquisition function that\nemphasizes underrepresented tail classes and complements the automated\ncorrection mechanism. Extensive experiments on Cityscapes and PASCAL VOC 2012\ndemonstrate that A$^2$LC significantly outperforms previous state-of-the-art\nmethods. Notably, A$^2$LC achieves high efficiency by outperforming previous\nmethods using only 20% of their budget, and demonstrates strong effectiveness\nby yielding a 27.23% performance improvement under an equivalent budget\nconstraint on the Cityscapes dataset. The code will be released upon\nacceptance.\n","authors":["Youjin Jeon","Kyusik Cho","Suhan Woo","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2506.11599v1.pdf","comment":"Preprint. Under review. 22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.11595v1","updated":"2025-06-13T09:03:33Z","published":"2025-06-13T09:03:33Z","title":"EasyARC: Evaluating Vision Language Models on True Visual Reasoning","summary":"  Building on recent advances in language-based reasoning models, we explore\nmultimodal reasoning that integrates vision and text. Existing multimodal\nbenchmarks primarily test visual extraction combined with text-based reasoning,\nlacking true visual reasoning with more complex interactions between vision and\nlanguage. Inspired by the ARC challenge, we introduce EasyARC, a\nvision-language benchmark requiring multi-image, multi-step reasoning, and\nself-correction. EasyARC is procedurally generated, fully verifiable, and\nscalable, making it ideal for reinforcement learning (RL) pipelines. The\ngenerators incorporate progressive difficulty levels, enabling structured\nevaluation across task types and complexities. We benchmark state-of-the-art\nvision-language models and analyze their failure modes. We argue that EasyARC\nsets a new standard for evaluating true reasoning and test-time scaling\ncapabilities in vision-language models. We open-source our benchmark dataset\nand evaluation code.\n","authors":["Mert Unsal","Aylin Akkus"],"pdf_url":"https://arxiv.org/pdf/2506.11595v1.pdf","comment":"CVPR2025 Workshop on Test-time Scaling for Computer Vision"},{"id":"http://arxiv.org/abs/2506.10669v2","updated":"2025-06-13T08:57:35Z","published":"2025-06-12T12:58:43Z","title":"PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image\n  Analysis","summary":"  Background and Objective: Prototype-based methods improve interpretability by\nlearning fine-grained part-prototypes; however, their visualization in the\ninput pixel space is not always consistent with human-understandable\nbiomarkers. In addition, well-known prototype-based approaches typically learn\nextremely granular prototypes that are less interpretable in medical imaging,\nwhere both the presence and extent of biomarkers and lesions are critical.\n  Methods: To address these challenges, we propose PiPViT (Patch-based Visual\nInterpretable Prototypes), an inherently interpretable prototypical model for\nimage recognition. Leveraging a vision transformer (ViT), PiPViT captures\nlong-range dependencies among patches to learn robust, human-interpretable\nprototypes that approximate lesion extent only using image-level labels.\nAdditionally, PiPViT benefits from contrastive learning and multi-resolution\ninput processing, which enables effective localization of biomarkers across\nscales.\n  Results: We evaluated PiPViT on retinal OCT image classification across four\ndatasets, where it achieved competitive quantitative performance compared to\nstate-of-the-art methods while delivering more meaningful explanations.\nMoreover, quantitative evaluation on a hold-out test set confirms that the\nlearned prototypes are semantically and clinically relevant. We believe PiPViT\ncan transparently explain its decisions and assist clinicians in understanding\ndiagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT\n","authors":["Marzieh Oghbaie","Teresa Araújo","Hrvoje Bogunović"],"pdf_url":"https://arxiv.org/pdf/2506.10669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13203v2","updated":"2025-06-13T08:51:10Z","published":"2025-03-17T14:12:08Z","title":"Clustering is back: Reaching state-of-the-art LiDAR instance\n  segmentation without training","summary":"  Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\noutperforms state-of-the-art supervised methods on standard benchmarks\nincluding SemanticKITTI and nuScenes, and outperforms every publicly available\nmethod on SemanticKITTI as a drop-in instance head replacement, while running\nin real-time on a single-threaded CPU and requiring no instance labels. It is\nfully explainable, and requires no learning or parameter tuning. Alpine\ncombined with state-of-the-art semantic segmentation ranks first on the\nofficial panoptic segmentation leaderboard of SemanticKITTI. Code is available\nat https://github.com/valeoai/Alpine/\n","authors":["Corentin Sautier","Gilles Puy","Alexandre Boulch","Renaud Marlet","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2503.13203v2.pdf","comment":"Alpine ranks first in the leaderboard of SemanticKITTI's panoptic\n  segmentation"},{"id":"http://arxiv.org/abs/2506.11585v1","updated":"2025-06-13T08:49:23Z","published":"2025-06-13T08:49:23Z","title":"OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for\n  Robots","summary":"  We introduce OV-MAP, a novel approach to open-world 3D mapping for mobile\nrobots by integrating open-features into 3D maps to enhance object recognition\ncapabilities. A significant challenge arises when overlapping features from\nadjacent voxels reduce instance-level precision, as features spill over voxel\nboundaries, blending neighboring regions together. Our method overcomes this by\nemploying a class-agnostic segmentation model to project 2D masks into 3D\nspace, combined with a supplemented depth image created by merging raw and\nsynthetic depth from point clouds. This approach, along with a 3D mask voting\nmechanism, enables accurate zero-shot 3D instance segmentation without relying\non 3D supervised segmentation models. We assess the effectiveness of our method\nthrough comprehensive experiments on public datasets such as ScanNet200 and\nReplica, demonstrating superior zero-shot performance, robustness, and\nadaptability across diverse environments. Additionally, we conducted real-world\nexperiments to demonstrate our method's adaptability and robustness when\napplied to diverse real-world environments.\n","authors":["Juno Kim","Yesol Park","Hye-Jung Yoon","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11585v1.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2504.00812v2","updated":"2025-06-13T08:31:34Z","published":"2025-04-01T14:03:46Z","title":"Scaling Prompt Instructed Zero Shot Composed Image Retrieval with\n  Image-Only Data","summary":"  Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines.\n","authors":["Yiqun Duan","Sameera Ramasinghe","Stephen Gould","Ajanthan Thalaiyasingam"],"pdf_url":"https://arxiv.org/pdf/2504.00812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09565v2","updated":"2025-06-13T08:30:38Z","published":"2025-06-11T09:56:39Z","title":"SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware\n  Gaussian Fields","summary":"  Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.\n","authors":["Qijing Li","Jingxiang Sun","Liang An","Zhaoqi Su","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.09565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11574v1","updated":"2025-06-13T08:29:52Z","published":"2025-06-13T08:29:52Z","title":"Camera-based method for the detection of lifted truck axles using\n  convolutional neural networks","summary":"  The identification and classification of vehicles play a crucial role in\nvarious aspects of the control-sanction system. Current technologies such as\nweigh-in-motion (WIM) systems can classify most vehicle categories but they\nstruggle to accurately classify vehicles with lifted axles. Moreover, very few\ncommercial and technical methods exist for detecting lifted axles. In this\npaper, as part of the European project SETO (Smart Enforcement of Transport\nOperations), a method based on a convolutional neural network (CNN), namely\nYOLOv8s, was proposed for the detection of lifted truck axles in images of\ntrucks captured by cameras placed perpendicular to the direction of traffic.\nThe performance of the proposed method was assessed and it was found that it\nhad a precision of 87%, a recall of 91.7%, and an inference time of 1.4 ms,\nwhich makes it well-suited for real time implantations. These results suggest\nthat further improvements could be made, potentially by increasing the size of\nthe datasets and/or by using various image augmentation methods.\n","authors":["Bachir Tchana Tankeu","Mohamed Bouteldja","Nicolas Grignard","Bernard Jacob"],"pdf_url":"https://arxiv.org/pdf/2506.11574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10353v2","updated":"2025-06-13T08:28:20Z","published":"2025-06-12T05:21:43Z","title":"Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for\n  Human Motion Generation","summary":"  Recent advances in large language models, especially in natural language\nunderstanding and reasoning, have opened new possibilities for text-to-motion\ngeneration. Although existing approaches have made notable progress in semantic\nalignment and motion synthesis, they often rely on end-to-end mapping\nstrategies that fail to capture deep linguistic structures and logical\nreasoning. Consequently, generated motions tend to lack controllability,\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\na unified motion-language modeling framework that integrates a Chain-of-Thought\nmechanism. By explicitly decomposing complex textual instructions into\nlogically structured action paths, Motion-R1 provides high-level semantic\nguidance for motion generation, significantly enhancing the model's ability to\ninterpret and execute multi-step, long-horizon, and compositionally rich\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\nreinforcement learning algorithm designed for large models, which leverages\nmotion quality feedback to optimize reasoning chains and motion synthesis\njointly. Extensive experiments across multiple benchmark datasets demonstrate\nthat Motion-R1 achieves competitive or superior performance compared to\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\nunderstanding and long-term temporal coherence. The code, model and data will\nbe publicly available.\n","authors":["Runqi Ouyang","Haoyun Li","Zhenyuan Zhang","Xiaofeng Wang","Zheng Zhu","Guan Huang","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11571v1","updated":"2025-06-13T08:27:45Z","published":"2025-06-13T08:27:45Z","title":"VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather\n  than Previous Memories?","summary":"  Recent extensive works have demonstrated that by introducing long CoT, the\ncapabilities of MLLMs to solve complex problems can be effectively enhanced.\nHowever, the reasons for the effectiveness of such paradigms remain unclear. It\nis challenging to analysis with quantitative results how much the model's\nspecific extraction of visual cues and its subsequent so-called reasoning\nduring inference process contribute to the performance improvements. Therefore,\nevaluating the faithfulness of MLLMs' reasoning to visual information is\ncrucial. To address this issue, we first present a cue-driven automatic and\ncontrollable editing pipeline with the help of GPT-Image-1. It enables the\nautomatic and precise editing of specific visual cues based on the instruction.\nFurthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs'\nvisual reasoning capabilities and analyze the source of such capabilities with\nan emphasis on the visual faithfulness. Using the designed pipeline, we\nconstructed comparative question-answer pairs by altering the visual cues in\nimages that are crucial for solving the original reasoning problem, thereby\nchanging the question's answer. By testing similar questions with images that\nhave different details, the average accuracy reflects the model's visual\nreasoning ability, while the difference in accuracy before and after editing\nthe test set images effectively reveals the relationship between the model's\nreasoning ability and visual perception. We further designed specific metrics\nto expose this relationship. VFaith-Bench includes 755 entries divided into\nfive distinct subsets, along with an additional human-labeled perception task.\nWe conducted in-depth testing and analysis of existing mainstream flagship\nmodels and prominent open-source model series/reasoning models on VFaith-Bench,\nfurther investigating the underlying factors of their reasoning capabilities.\n","authors":["Jiachen Yu","Yufei Zhan","Ziheng Wu","Yousong Zhu","Jinqiao Wang","Minghui Qiu"],"pdf_url":"https://arxiv.org/pdf/2506.11571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19638v2","updated":"2025-06-13T08:22:59Z","published":"2025-05-26T07:55:49Z","title":"HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and\n  Semantic Alignment","summary":"  Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation.\n","authors":["Ming Meng","Qi Dong","Jiajie Li","Zhe Zhu","Xingyu Wang","Zhaoxin Fan","Wei Zhao","Wenjun Wu"],"pdf_url":"https://arxiv.org/pdf/2505.19638v2.pdf","comment":"After the publication of the paper, we discovered some significant\n  errors/omissions that need to be corrected and improved"},{"id":"http://arxiv.org/abs/2405.09933v4","updated":"2025-06-13T08:22:03Z","published":"2024-05-16T09:37:54Z","title":"MiniMaxAD: A Lightweight Autoencoder for Feature-Rich Anomaly Detection","summary":"  Previous industrial anomaly detection methods often struggle to handle the\nextensive diversity in training sets, particularly when they contain\nstylistically diverse and feature-rich samples, which we categorize as\nfeature-rich anomaly detection datasets (FRADs). This challenge is evident in\napplications such as multi-view and multi-class scenarios. To address this\nchallenge, we developed MiniMaxAD, a efficient autoencoder designed to\nefficiently compress and memorize extensive information from normal images. Our\nmodel employs a technique that enhances feature diversity, thereby increasing\nthe effective capacity of the network. It also utilizes large kernel\nconvolution to extract highly abstract patterns, which contribute to efficient\nand compact feature embedding. Moreover, we introduce an Adaptive Contraction\nHard Mining Loss (ADCLoss), specifically tailored to FRADs. In our methodology,\nany dataset can be unified under the framework of feature-rich anomaly\ndetection, in a way that the benefits far outweigh the drawbacks. Our approach\nhas achieved state-of-the-art performance in multiple challenging benchmarks.\nCode is available at:\n\\href{https://github.com/WangFengJiee/MiniMaxAD}{https://github.com/WangFengJiee/MiniMaxAD}\n","authors":["Fengjie Wang","Chengming Liu","Lei Shi","Pang Haibo"],"pdf_url":"https://arxiv.org/pdf/2405.09933v4.pdf","comment":"Accept by Computers in Industry"},{"id":"http://arxiv.org/abs/2503.21099v2","updated":"2025-06-13T08:15:04Z","published":"2025-03-27T02:37:05Z","title":"Learning Class Prototypes for Unified Sparse Supervised 3D Object\n  Detection","summary":"  Both indoor and outdoor scene perceptions are essential for embodied\nintelligence. However, current sparse supervised 3D object detection methods\nfocus solely on outdoor scenes without considering indoor settings. To this\nend, we propose a unified sparse supervised 3D object detection method for both\nindoor and outdoor scenes through learning class prototypes to effectively\nutilize unlabeled objects. Specifically, we first propose a prototype-based\nobject mining module that converts the unlabeled object mining into a matching\nproblem between class prototypes and unlabeled features. By using optimal\ntransport matching results, we assign prototype labels to high-confidence\nfeatures, thereby achieving the mining of unlabeled objects. We then present a\nmulti-label cooperative refinement module to effectively recover missed\ndetections through pseudo label quality control and prototype label\ncooperation. Experiments show that our method achieves state-of-the-art\nperformance under the one object per scene sparse supervised setting across\nindoor and outdoor datasets. With only one labeled object per scene, our method\nachieves about 78%, 90%, and 96% performance compared to the fully supervised\ndetector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting the\nscalability of our method. Code is available at\nhttps://github.com/zyrant/CPDet3D.\n","authors":["Yun Zhu","Le Hui","Hang Yang","Jianjun Qian","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2503.21099v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2506.11558v1","updated":"2025-06-13T08:13:05Z","published":"2025-06-13T08:13:05Z","title":"DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs","summary":"  Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.\n","authors":["Bo-Cheng Chiu","Jen-Jee Chen","Yu-Chee Tseng","Feng-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11549v1","updated":"2025-06-13T08:00:54Z","published":"2025-06-13T08:00:54Z","title":"EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video\n  Quality Assessment","summary":"  Free-energy-guided self-repair mechanisms have shown promising results in\nimage quality assessment (IQA), but remain under-explored in video quality\nassessment (VQA), where temporal dynamics and model constraints pose unique\nchallenges. Unlike static images, video content exhibits richer spatiotemporal\ncomplexity, making perceptual restoration more difficult. Moreover, VQA systems\noften rely on pre-trained backbones, which limits the direct integration of\nenhancement modules without affecting model stability. To address these issues,\nwe propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based\nself-repair. It adopts a dual-branch architecture, with an aesthetic branch for\nglobal perceptual evaluation and a technical branch for fine-grained structural\nand semantic analysis. Each branch integrates specialized enhancement modules\ntailored to distinct visual inputs-resized full-frame images and patch-based\nfragments-to simulate adaptive repair behaviors. We also explore a principled\nstrategy for incorporating high-level visual features without disrupting the\noriginal backbone. In addition, we design a biologically inspired prediction\nhead that models sweeping gaze dynamics to better fuse global and local\nrepresentations for quality prediction. Experiments on five public VQA\nbenchmarks demonstrate that EyeSimVQA achieves competitive or superior\nperformance compared to state-of-the-art methods, while offering improved\ninterpretability through its biologically grounded design.\n","authors":["Zhaoyang Wang","Wen Lu","Jie Li","Lihuo He","Maoguo Gong","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2506.11549v1.pdf","comment":"This work has been submitted to the IEEE TCSVT for possible\n  publication"},{"id":"http://arxiv.org/abs/2506.11547v1","updated":"2025-06-13T08:00:03Z","published":"2025-06-13T08:00:03Z","title":"Linearly Solving Robust Rotation Estimation","summary":"  Rotation estimation plays a fundamental role in computer vision and robot\ntasks, and extremely robust rotation estimation is significantly useful for\nsafety-critical applications. Typically, estimating a rotation is considered a\nnon-linear and non-convex optimization problem that requires careful design.\nHowever, in this paper, we provide some new perspectives that solving a\nrotation estimation problem can be reformulated as solving a linear model\nfitting problem without dropping any constraints and without introducing any\nsingularities. In addition, we explore the dual structure of a rotation motion,\nrevealing that it can be represented as a great circle on a quaternion sphere\nsurface. Accordingly, we propose an easily understandable voting-based method\nto solve rotation estimation. The proposed method exhibits exceptional\nrobustness to noise and outliers and can be computed in parallel with graphics\nprocessing units (GPUs) effortlessly. Particularly, leveraging the power of\nGPUs, the proposed method can obtain a satisfactory rotation solution for\nlarge-scale($10^6$) and severely corrupted (99$\\%$ outlier ratio) rotation\nestimation problems under 0.5 seconds. Furthermore, to validate our theoretical\nframework and demonstrate the superiority of our proposed method, we conduct\ncontrolled experiments and real-world dataset experiments. These experiments\nprovide compelling evidence supporting the effectiveness and robustness of our\napproach in solving rotation estimation problems.\n","authors":["Yinlong Liu","Tianyu Huang","Zhi-Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2506.11547v1.pdf","comment":"23 pages, 18 figures"},{"id":"http://arxiv.org/abs/2506.11546v1","updated":"2025-06-13T07:59:55Z","published":"2025-06-13T07:59:55Z","title":"CGVQM+D: Computer Graphics Video Quality Metric and Dataset","summary":"  While existing video and image quality datasets have extensively studied\nnatural videos and traditional distortions, the perception of synthetic content\nand modern rendering artifacts remains underexplored. We present a novel video\nquality dataset focused on distortions introduced by advanced rendering\ntechniques, including neural supersampling, novel-view synthesis, path tracing,\nneural denoising, frame interpolation, and variable rate shading. Our\nevaluations show that existing full-reference quality metrics perform\nsub-optimally on these distortions, with a maximum Pearson correlation of 0.78.\nAdditionally, we find that the feature space of pre-trained 3D CNNs aligns\nstrongly with human perception of visual quality. We propose CGVQM, a\nfull-reference video quality metric that significantly outperforms existing\nmetrics while generating both per-pixel error maps and global quality scores.\nOur dataset and metric implementation is available at\nhttps://github.com/IntelLabs/CGVQM.\n","authors":["Akshay Jindal","Nabil Sadaka","Manu Mathew Thomas","Anton Sochenov","Anton Kaplanyan"],"pdf_url":"https://arxiv.org/pdf/2506.11546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11545v1","updated":"2025-06-13T07:59:52Z","published":"2025-06-13T07:59:52Z","title":"FCA2: Frame Compression-Aware Autoencoder for Modular and Fast\n  Compressed Video Super-Resolution","summary":"  State-of-the-art (SOTA) compressed video super-resolution (CVSR) models face\npersistent challenges, including prolonged inference time, complex training\npipelines, and reliance on auxiliary information. As video frame rates continue\nto increase, the diminishing inter-frame differences further expose the\nlimitations of traditional frame-to-frame information exploitation methods,\nwhich are inadequate for addressing current video super-resolution (VSR)\ndemands. To overcome these challenges, we propose an efficient and scalable\nsolution inspired by the structural and statistical similarities between\nhyperspectral images (HSI) and video data. Our approach introduces a\ncompression-driven dimensionality reduction strategy that reduces computational\ncomplexity, accelerates inference, and enhances the extraction of temporal\ninformation across frames. The proposed modular architecture is designed for\nseamless integration with existing VSR frameworks, ensuring strong adaptability\nand transferability across diverse applications. Experimental results\ndemonstrate that our method achieves performance on par with, or surpassing,\nthe current SOTA models, while significantly reducing inference time. By\naddressing key bottlenecks in CVSR, our work offers a practical and efficient\npathway for advancing VSR technology. Our code will be publicly available at\nhttps://github.com/handsomewzy/FCA2.\n","authors":["Zhaoyang Wang","Jie Li","Wen Lu","Lihuo He","Maoguo Gong","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2506.11545v1.pdf","comment":"This work has been submitted to the IEEE TMM for possible publication"},{"id":"http://arxiv.org/abs/2506.11544v1","updated":"2025-06-13T07:58:20Z","published":"2025-06-13T07:58:20Z","title":"Leveraging Satellite Image Time Series for Accurate Extreme Event\n  Detection","summary":"  Climate change is leading to an increase in extreme weather events, causing\nsignificant environmental damage and loss of life. Early detection of such\nevents is essential for improving disaster response. In this work, we propose\nSITS-Extreme, a novel framework that leverages satellite image time series to\ndetect extreme events by incorporating multiple pre-disaster observations. This\napproach effectively filters out irrelevant changes while isolating\ndisaster-relevant signals, enabling more accurate detection. Extensive\nexperiments on both real-world and synthetic datasets validate the\neffectiveness of SITS-Extreme, demonstrating substantial improvements over\nwidely used strong bi-temporal baselines. Additionally, we examine the impact\nof incorporating more timesteps, analyze the contribution of key components in\nour framework, and evaluate its performance across different disaster types,\noffering valuable insights into its scalability and applicability for\nlarge-scale disaster monitoring.\n","authors":["Heng Fang","Hossein Azizpour"],"pdf_url":"https://arxiv.org/pdf/2506.11544v1.pdf","comment":"Accepted to the WACV 2025 Workshop on GeoCV. Code, datasets, and\n  model checkpoints available at:\n  https://github.com/hfangcat/SITS-ExtremeEvents"},{"id":"http://arxiv.org/abs/2506.11543v1","updated":"2025-06-13T07:57:38Z","published":"2025-06-13T07:57:38Z","title":"FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher\n  Information Matrix Approximation","summary":"  Post-training quantization (PTQ) has stood out as a cost-effective and\npromising model compression paradigm in recent years, as it avoids\ncomputationally intensive model retraining. Nevertheless, current PTQ methods\nfor Vision Transformers (ViTs) still suffer from significant accuracy\ndegradation, especially under low-bit quantization. To address these\nshortcomings, we analyze the prevailing Hessian-guided quantization loss, and\nuncover certain limitations of conventional Hessian approximations. By\nfollowing the block-wise reconstruction framework, we propose a novel PTQ\nmethod for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the\nconnection between KL divergence and FIM, which enables fast computation of the\nquantization loss during reconstruction. We further propose an efficient FIM\napproximation method, namely DPLR-FIM, by employing the diagonal plus low-rank\nprinciple, and formulate the ultimate quantization loss. Our extensive\nexperiments, conducted across various vision tasks with representative\nViT-based architectures on public datasets, demonstrate that our method\nsubstantially promotes the accuracy compared to the state-of-the-art\napproaches, especially in the case of low-bit quantization. The source code is\navailable at https://github.com/ShiheWang/FIMA-Q.\n","authors":["Zhuguanyu Wu","Shihe Wang","Jiayi Zhang","Jiaxin Chen","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11543v1.pdf","comment":"CVPR 2025 Highlight"},{"id":"http://arxiv.org/abs/2405.14343v2","updated":"2025-06-13T07:41:03Z","published":"2024-05-23T09:13:36Z","title":"Efficient Visual State Space Model for Image Deblurring","summary":"  Convolutional neural networks (CNNs) and Vision Transformers (ViTs) have\nachieved excellent performance in image restoration. While ViTs generally\noutperform CNNs by effectively capturing long-range dependencies and\ninput-specific characteristics, their computational complexity increases\nquadratically with image resolution. This limitation hampers their practical\napplication in high-resolution image restoration. In this paper, we propose a\nsimple yet effective visual state space model (EVSSM) for image deblurring,\nleveraging the benefits of state space models (SSMs) for visual data. In\ncontrast to existing methods that employ several fixed-direction scanning for\nfeature extraction, which significantly increases the computational cost, we\ndevelop an efficient visual scan block that applies various geometric\ntransformations before each SSM-based module, capturing useful non-local\ninformation and maintaining high efficiency. In addition, to more effectively\ncapture and represent local information, we propose an efficient discriminative\nfrequency domain-based feedforward network (EDFFN), which can effectively\nestimate useful frequency information for latent clear image restoration.\nExtensive experimental results show that the proposed EVSSM performs favorably\nagainst state-of-the-art methods on benchmark datasets and real-world images.\nThe code is available at https://github.com/kkkls/EVSSM.\n","authors":["Lingshun Kong","Jiangxin Dong","Jinhui Tang","Ming-Hsuan Yang","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2405.14343v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2506.11534v1","updated":"2025-06-13T07:37:25Z","published":"2025-06-13T07:37:25Z","title":"GNSS-inertial state initialization by distance residuals","summary":"  Initializing the state of a sensorized platform can be challenging, as a\nlimited set of initial measurements often carry limited information, leading to\npoor initial estimates that may converge to local minima during non-linear\noptimization. This paper proposes a novel GNSS-inertial initialization strategy\nthat delays the use of global GNSS measurements until sufficient information is\navailable to accurately estimate the transformation between the GNSS and\ninertial frames. Instead, the method initially relies on GNSS relative distance\nresiduals. To determine the optimal moment for switching to global\nmeasurements, we introduce a criterion based on the evolution of the Hessian\nmatrix singular values. Experiments on the EuRoC and GVINS datasets show that\nour approach consistently outperforms the naive strategy of using global GNSS\ndata from the start, yielding more accurate and robust initializations.\n","authors":["Samuel Cerezo","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2506.11534v1.pdf","comment":"8 pages, 8 figures, RA-L submission"},{"id":"http://arxiv.org/abs/2506.10488v2","updated":"2025-06-13T07:32:56Z","published":"2025-06-12T08:42:19Z","title":"Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation","summary":"  In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six\nhundred and eighty-five pages specifically designed to benchmark Optical Music\nRecognition (OMR) research. SMB encompasses a diverse array of musical\ntextures, including monophony, pianoform, quartet, and others, all encoded in\nCommon Western Modern Notation using the Humdrum **kern format. Alongside SMB,\nwe introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored\nexplicitly for evaluating OMR performance. OMR-NED builds upon the widely-used\nSymbol Error Rate (SER), offering a fine-grained and detailed error analysis\nthat covers individual musical elements such as note heads, beams, pitches,\naccidentals, and other critical notation features. The resulting numeric score\nprovided by OMR-NED facilitates clear comparisons, enabling researchers and\nend-users alike to identify optimal OMR approaches. Our work thus addresses a\nlong-standing gap in OMR evaluation, and we support our contributions with\nbaseline experiments using standardized SMB dataset splits for training and\nassessing state-of-the-art methods.\n","authors":["Juan C. Martinez-Sevilla","Joan Cerveto-Serrano","Noelia Luna","Greg Chapman","Craig Sapp","David Rizo","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2506.10488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11515v1","updated":"2025-06-13T07:16:41Z","published":"2025-06-13T07:16:41Z","title":"Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs\n  and MLLMs","summary":"  Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.\n","authors":["Xiao Xu","Libo Qin","Wanxiang Che","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2506.11515v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266"},{"id":"http://arxiv.org/abs/2506.11496v1","updated":"2025-06-13T06:45:05Z","published":"2025-06-13T06:45:05Z","title":"Taming Stable Diffusion for Computed Tomography Blind Super-Resolution","summary":"  High-resolution computed tomography (CT) imaging is essential for medical\ndiagnosis but requires increased radiation exposure, creating a critical\ntrade-off between image quality and patient safety. While deep learning methods\nhave shown promise in CT super-resolution, they face challenges with complex\ndegradations and limited medical training data. Meanwhile, large-scale\npre-trained diffusion models, particularly Stable Diffusion, have demonstrated\nremarkable capabilities in synthesizing fine details across various vision\ntasks. Motivated by this, we propose a novel framework that adapts Stable\nDiffusion for CT blind super-resolution. We employ a practical degradation\nmodel to synthesize realistic low-quality images and leverage a pre-trained\nvision-language model to generate corresponding descriptions. Subsequently, we\nperform super-resolution using Stable Diffusion with a specialized controlling\nstrategy, conditioned on both low-resolution inputs and the generated text\ndescriptions. Extensive experiments show that our method outperforms existing\napproaches, demonstrating its potential for achieving high-quality CT imaging\nat reduced radiation doses. Our code will be made publicly available.\n","authors":["Chunlei Li","Yilei Shi","Haoxi Hu","Jingliang Hu","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2506.11496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11493v1","updated":"2025-06-13T06:33:27Z","published":"2025-06-13T06:33:27Z","title":"Preserving Clusters in Prompt Learning for Unsupervised Domain\n  Adaptation","summary":"  Recent approaches leveraging multi-modal pre-trained models like CLIP for\nUnsupervised Domain Adaptation (UDA) have shown significant promise in bridging\ndomain gaps and improving generalization by utilizing rich semantic knowledge\nand robust visual representations learned through extensive pre-training on\ndiverse image-text datasets. While these methods achieve state-of-the-art\nperformance across benchmarks, much of the improvement stems from base\npseudo-labels (CLIP zero-shot predictions) and self-training mechanisms. Thus,\nthe training mechanism exhibits a key limitation wherein the visual embedding\ndistribution in target domains can deviate from the visual embedding\ndistribution in the pre-trained model, leading to misguided signals from class\ndescriptions. This work introduces a fresh solution to reinforce these\npseudo-labels and facilitate target-prompt learning, by exploiting the geometry\nof visual and text embeddings - an aspect that is overlooked by existing\nmethods. We first propose to directly leverage the reference predictions (from\nsource prompts) based on the relationship between source and target visual\nembeddings. We later show that there is a strong clustering behavior observed\nbetween visual and text embeddings in pre-trained multi-modal models. Building\non optimal transport theory, we transform this insight into a novel strategy to\nenforce the clustering property in text embeddings, further enhancing the\nalignment in the target domain. Our experiments and ablation studies validate\nthe effectiveness of the proposed approach, demonstrating superior performance\nand improved quality of target prompts in terms of representation.\n","authors":["Tung-Long Vuong","Hoang Phan","Vy Vo","Anh Bui","Thanh-Toan Do","Trung Le","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2506.11493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04066v2","updated":"2025-06-13T06:30:08Z","published":"2024-07-04T17:13:06Z","title":"E2MPL:An Enduring and Efficient Meta Prompt Learning Framework for\n  Few-shot Unsupervised Domain Adaptation","summary":"  Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount\nof labeled data from a source domain to enable accurate classification in an\nunlabeled target domain. Despite recent advancements, current approaches of\nFS-UDA continue to confront a major challenge: models often demonstrate\ninstability when adapted to new FS-UDA tasks and necessitate considerable time\ninvestment. To address these challenges, we put forward a novel framework\ncalled Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within\nthis framework, we utilize the pre-trained CLIP model as the backbone of\nfeature learning. Firstly, we design domain-shared prompts, consisting of\nvirtual tokens, which primarily capture meta-knowledge from a wide range of\nmeta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt\nlearning network that adaptively learns task-specific specific prompts with the\ngoal of achieving fast and stable task generalization. Thirdly, we formulate\nthe meta-prompt learning process as a bilevel optimization problem, consisting\nof (outer) meta-prompt learner and (inner) task-specific classifier and domain\nadapter. Also, the inner objective of each meta-task has the closed-form\nsolution, which enables efficient prompt learning and adaptation to new tasks\nin a single step. Extensive experimental studies demonstrate the promising\nperformance of our framework in a domain adaptation benchmark dataset\nDomainNet. Compared with state-of-the-art methods, our method has improved\naccuracy by at least 15.4% and reduced the time by 68.5% on average in 5-way\n1-shot tasks, and improved accuracy by 8.7% and reduced the time by 74.1% on\naverage in 5-way 5-shot tasks. Moreover, our approach exhibits more enduring\nperformance than the other methods, i.e., being more stable across 3600 test\ntasks.\n","authors":["Wanqi Yang","Haoran Wang","Lei Wang","Ge Song","Ming Yang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2407.04066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11490v1","updated":"2025-06-13T06:28:05Z","published":"2025-06-13T06:28:05Z","title":"Composite Data Augmentations for Synthetic Image Detection Against\n  Real-World Perturbations","summary":"  The advent of accessible Generative AI tools enables anyone to create and\nspread synthetic images on social media, often with the intention to mislead,\nthus posing a significant threat to online information integrity. Most existing\nSynthetic Image Detection (SID) solutions struggle on generated images sourced\nfrom the Internet, as these are often altered by compression and other\noperations. To address this, our research enhances SID by exploring data\naugmentation combinations, leveraging a genetic algorithm for optimal\naugmentation selection, and introducing a dual-criteria optimization approach.\nThese methods significantly improve model performance under real-world\nperturbations. Our findings provide valuable insights for developing detection\nmodels capable of identifying synthetic images across varying qualities and\ntransformations, with the best-performing model achieving a mean average\nprecision increase of +22.53% compared to models without augmentations. The\nimplementation is available at\ngithub.com/efthimia145/sid-composite-data-augmentation.\n","authors":["Efthymia Amarantidou","Christos Koutlis","Symeon Papadopoulos","Panagiotis C. Petrantonakis"],"pdf_url":"https://arxiv.org/pdf/2506.11490v1.pdf","comment":"EUSIPCO 2025 (33rd European Signal Processing Conference)"},{"id":"http://arxiv.org/abs/2506.11481v1","updated":"2025-06-13T06:09:43Z","published":"2025-06-13T06:09:43Z","title":"Environmental Change Detection: Toward a Practical Task of Scene Change\n  Detection","summary":"  Humans do not memorize everything. Thus, humans recognize scene changes by\nexploring the past images. However, available past (i.e., reference) images\ntypically represent nearby viewpoints of the present (i.e., query) scene,\nrather than the identical view. Despite this practical limitation, conventional\nScene Change Detection (SCD) has been formalized under an idealized setting in\nwhich reference images with matching viewpoints are available for every query.\nIn this paper, we push this problem toward a practical task and introduce\nEnvironmental Change Detection (ECD). A key aspect of ECD is to avoid\nunrealistically aligned query-reference pairs and rely solely on environmental\ncues. Inspired by real-world practices, we provide these cues through a\nlarge-scale database of uncurated images. To address this new task, we propose\na novel framework that jointly understands spatial environments and detects\nchanges. The main idea is that matching at the same spatial locations between a\nquery and a reference may lead to a suboptimal solution due to viewpoint\nmisalignment and limited field-of-view (FOV) coverage. We deal with this\nlimitation by leveraging multiple reference candidates and aggregating\nsemantically rich representations for change detection. We evaluate our\nframework on three standard benchmark sets reconstructed for ECD, and\nsignificantly outperform a naive combination of state-of-the-art methods while\nachieving comparable performance to the oracle setting. The code will be\nreleased upon acceptance.\n","authors":["Kyusik Cho","Suhan Woo","Hongje Seong","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2506.11481v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.15311v2","updated":"2025-06-13T06:02:48Z","published":"2025-02-21T09:05:29Z","title":"Fish feeding behavior recognition and intensity quantification methods\n  in aquaculture: From single modality analysis to multimodality fusion","summary":"  As a key part of aquaculture management, fish feeding behavior recognition\nand intensity quantification has been a hot area of great concern to\nresearchers, and it plays a crucial role in monitoring fish health, guiding\nbaiting work and improving aquaculture efficiency. In order to better carry out\nthe related work in the future, this paper firstly analyzes and compares the\nexisting reviews. Then reviews the research advances of fish feeding behavior\nrecognition and intensity quantification methods based on computer vision,\nacoustics and sensors in a single modality. Meanwhile, the application of the\ncurrent emerging multimodal fusion in fish feeding behavior recognition and\nintensity quantification methods is expounded. Finally, the advantages and\ndisadvantages of various techniques are compared and analyzed, and the future\nresearch directions are envisioned.\n","authors":["Shulong Zhang","Jiayin Zhao","Mingyuan Yao","Xiao Liu","Yukang Huo","Yingyi Chen","Haihua Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15311v2.pdf","comment":"24 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2506.11477v1","updated":"2025-06-13T05:47:09Z","published":"2025-06-13T05:47:09Z","title":"FAME: A Lightweight Spatio-Temporal Network for Model Attribution of\n  Face-Swap Deepfakes","summary":"  The widespread emergence of face-swap Deepfake videos poses growing risks to\ndigital security, privacy, and media integrity, necessitating effective\nforensic tools for identifying the source of such manipulations. Although most\nprior research has focused primarily on binary Deepfake detection, the task of\nmodel attribution -- determining which generative model produced a given\nDeepfake -- remains underexplored. In this paper, we introduce FAME (Fake\nAttribution via Multilevel Embeddings), a lightweight and efficient\nspatio-temporal framework designed to capture subtle generative artifacts\nspecific to different face-swap models. FAME integrates spatial and temporal\nattention mechanisms to improve attribution accuracy while remaining\ncomputationally efficient. We evaluate our model on three challenging and\ndiverse datasets: Deepfake Detection and Manipulation (DFDM), FaceForensics++,\nand FakeAVCeleb. Results show that FAME consistently outperforms existing\nmethods in both accuracy and runtime, highlighting its potential for deployment\nin real-world forensic and information security applications.\n","authors":["Wasim Ahmad","Yan-Tsung Peng","Yuan-Hao Chang"],"pdf_url":"https://arxiv.org/pdf/2506.11477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23461v3","updated":"2025-06-13T05:41:41Z","published":"2025-03-30T14:36:55Z","title":"TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes","summary":"  This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.\n","authors":["Nikai Du","Zhennan Chen","Zhizhou Chen","Shan Gao","Xi Chen","Zhengkai Jiang","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2503.23461v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11475v1","updated":"2025-06-13T05:39:28Z","published":"2025-06-13T05:39:28Z","title":"AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis\n  and Prediction","summary":"  This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.\n","authors":["Syeda Kisaa Fatima","Tehreem Zubair","Noman Ahmed","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2506.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08666v2","updated":"2025-06-13T05:32:59Z","published":"2025-06-10T10:27:52Z","title":"LLaVA-c: Continual Improved Visual Instruction Tuning","summary":"  Multimodal models like LLaVA-1.5 achieve state-of-the-art visual\nunderstanding through visual instruction tuning on multitask datasets, enabling\nstrong instruction-following and multimodal performance. However, multitask\nlearning faces challenges such as task balancing, requiring careful adjustment\nof data proportions, and expansion costs, where new tasks risk catastrophic\nforgetting and need costly retraining. Continual learning provides a promising\nalternative to acquiring new knowledge incrementally while preserving existing\ncapabilities. However, current methods prioritize task-specific performance,\nneglecting base model degradation from overfitting to specific instructions,\nwhich undermines general capabilities. In this work, we propose a simple but\neffective method with two modifications on LLaVA-1.5: spectral-aware\nconsolidation for improved task balance and unsupervised inquiry regularization\nto prevent base model degradation. We evaluate both general and task-specific\nperformance across continual pretraining and fine-tuning. Experiments\ndemonstrate that LLaVA-c consistently enhances standard benchmark performance\nand preserves general capabilities. For the first time, we show that\ntask-by-task continual learning can achieve results that match or surpass\nmultitask joint learning. The code will be publicly released.\n","authors":["Wenzhuo Liu","Fei Zhu","Haiyang Guo","Longhui Wei","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.08666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11472v1","updated":"2025-06-13T05:22:12Z","published":"2025-06-13T05:22:12Z","title":"On the Natural Robustness of Vision-Language Models Against Visual\n  Perception Attacks in Autonomous Driving","summary":"  Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical\ntasks such as traffic sign recognition (TSR), automated lane centering (ALC),\nand vehicle detection (VD). However, these models are vulnerable to attacks\nthat can cause misclassifications and compromise safety. Traditional defense\nmechanisms, including adversarial training, often degrade benign accuracy and\nfail to generalize against unseen attacks. In this work, we introduce Vehicle\nVision Language Models (V2LMs), fine-tuned vision-language models specialized\nfor AV perception. Our findings demonstrate that V2LMs inherently exhibit\nsuperior robustness against unseen attacks without requiring adversarial\ntraining, maintaining significantly higher accuracy than conventional DNNs\nunder adversarial conditions. We evaluate two deployment strategies: Solo Mode,\nwhere individual V2LMs handle specific perception tasks, and Tandem Mode, where\na single unified V2LM is fine-tuned for multiple tasks simultaneously.\nExperimental results reveal that DNNs suffer performance drops of 33% to 46%\nunder attacks, whereas V2LMs maintain adversarial accuracy with reductions of\nless than 8% on average. The Tandem Mode further offers a memory-efficient\nalternative while achieving comparable robustness to Solo Mode. We also explore\nintegrating V2LMs as parallel components to AV perception to enhance resilience\nagainst adversarial threats. Our results suggest that V2LMs offer a promising\npath toward more secure and resilient AV perception systems.\n","authors":["Pedram MohajerAnsari","Amir Salarpour","Michael Kühr","Siyu Huang","Mohammad Hamad","Sebastian Steinhorst","Habeeb Olufowobi","Mert D. Pesé"],"pdf_url":"https://arxiv.org/pdf/2506.11472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10096v2","updated":"2025-06-13T05:15:20Z","published":"2025-03-13T06:43:21Z","title":"A Self-supervised Motion Representation for Portrait Video Generation","summary":"  Recent advancements in portrait video generation have been noteworthy.\nHowever, existing methods rely heavily on human priors and pre-trained\ngenerative models, Motion representations based on human priors may introduce\nunrealistic motion, while methods relying on pre-trained generative models\noften suffer from inefficient inference. To address these challenges, we\npropose Semantic Latent Motion (SeMo), a compact and expressive motion\nrepresentation. Leveraging this representation, our approach achieve both\nhigh-quality visual results and efficient inference. SeMo follows an effective\nthree-step framework: Abstraction, Reasoning, and Generation. First, in the\nAbstraction step, we use a carefully designed Masked Motion Encoder, which\nleverages a self-supervised learning paradigm to compress the subject's motion\nstate into a compact and abstract latent motion (1D token). Second, in the\nReasoning step, we efficiently generate motion sequences based on the driving\naudio signal. Finally, in the Generation step, the motion dynamics serve as\nconditional information to guide the motion decoder in synthesizing realistic\ntransitions from reference frame to target video. Thanks to the compact and\nexpressive nature of Semantic Latent Motion, our method achieves efficient\nmotion representation and high-quality video generation. User studies\ndemonstrate that our approach surpasses state-of-the-art models with an 81% win\nrate in realism. Extensive experiments further highlight its strong compression\ncapability, reconstruction quality, and generative potential.\n","authors":["Qiyuan Zhang","Chenyu Wu","Wenzhang Sun","Huaize Liu","Donglin Di","Wei Chen","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2503.10096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05205v5","updated":"2025-06-13T04:41:06Z","published":"2025-01-09T12:55:55Z","title":"Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant\n  Learning","summary":"  Infants develop complex visual understanding rapidly, even preceding the\nacquisition of linguistic skills. As computer vision seeks to replicate the\nhuman vision system, understanding infant visual development may offer valuable\ninsights. In this paper, we present an interdisciplinary study exploring this\nquestion: can a computational model that imitates the infant learning process\ndevelop broader visual concepts that extend beyond the vocabulary it has heard,\nsimilar to how infants naturally learn? To investigate this, we analyze a\nrecently published model in Science by Vong et al., which is trained on\nlongitudinal, egocentric images of a single child paired with transcribed\nparental speech. We perform neuron labeling to identify visual concept neurons\nhidden in the model's internal representations. We then demonstrate that these\nneurons can recognize objects beyond the model's original vocabulary.\nFurthermore, we compare the differences in representation between infant models\nand those in modern computer vision models, such as CLIP and ImageNet\npre-trained model. Ultimately, our work bridges cognitive science and computer\nvision by analyzing the internal representations of a computational model\ntrained on an infant visual and linguistic inputs. Project page is available at\nhttps://kexueyi.github.io/webpage-discover-hidden-visual-concepts.\n","authors":["Xueyi Ke","Satoshi Tsutsui","Yayun Zhang","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2501.05205v5.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2506.11465v1","updated":"2025-06-13T04:39:58Z","published":"2025-06-13T04:39:58Z","title":"RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer","summary":"  Multimodal learning faces challenges in effectively fusing information from\ndiverse modalities, especially when modality quality varies across samples.\nDynamic fusion strategies, such as attention mechanism in Transformers, aim to\naddress such challenge by adaptively emphasizing modalities based on the\ncharacteristics of input data. However, through amounts of carefully designed\nexperiments, we surprisingly observed that the dynamic adaptability of\nwidely-used self-attention models diminishes. Model tends to prefer one\nmodality regardless of data characteristics. This bias triggers a\nself-reinforcing cycle that progressively overemphasizes the favored modality,\nwidening the distribution gap in attention keys across modalities and\ndeactivating attention mechanism's dynamic properties. To revive adaptability,\nwe propose a simple yet effective method Rolling Query (RollingQ), which\nbalances attention allocation by rotating the query to break the\nself-reinforcing cycle and mitigate the key distribution gap. Extensive\nexperiments on various multimodal scenarios validate the effectiveness of\nRollingQ and the restoration of cooperation dynamics is pivotal for enhancing\nthe broader capabilities of widely deployed multimodal Transformers. The source\ncode is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.\n","authors":["Haotian Ni","Yake Wei","Hang Liu","Gong Chen","Chong Peng","Hao Lin","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2506.11465v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.10963v2","updated":"2025-06-13T04:39:54Z","published":"2025-06-12T17:58:09Z","title":"MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for\n  Text-to-Image Reasoning","summary":"  In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning -- a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits -- low entity fidelity,\nweak relations, and clutter -- with GPT-4o achieving an MMMG-Score of only\n50.20, underscoring the benchmark's difficulty. To spur further progress, we\nrelease FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that\ncombines a reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.\n","authors":["Yuxuan Luo","Yuhui Yuan","Junwen Chen","Haonan Cai","Ziyi Yue","Yuwei Yang","Fatima Zohra Daha","Ji Li","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2506.10963v2.pdf","comment":"85 pages, 70 figures, code: https://github.com/MMMGBench/MMMG,\n  project page: https://mmmgbench.github.io/"},{"id":"http://arxiv.org/abs/2503.05107v2","updated":"2025-06-13T04:33:20Z","published":"2025-03-07T03:06:03Z","title":"We Care Each Pixel: Calibrating on Medical Segmentation Model","summary":"  Medical image segmentation is fundamental for computer-aided diagnostics,\nproviding accurate delineation of anatomical structures and pathological\nregions. While common metrics such as Accuracy, DSC, IoU, and HD primarily\nquantify spatial agreement between predictions and ground-truth labels, they do\nnot assess the calibration quality of segmentation models, which is crucial for\nclinical reliability. To address this limitation, we propose pixel-wise\nExpected Calibration Error (pECE), a novel metric that explicitly measures\nmiscalibration at the pixel level, thereby ensuring both spatial precision and\nconfidence reliability. We further introduce a morphological adaptation\nstrategy that applies morphological operations to ground-truth masks before\ncomputing calibration losses, particularly benefiting margin-based losses such\nas Margin SVLS and NACL. Additionally, we present the Signed Distance\nCalibration Loss (SDC), which aligns boundary geometry with calibration\nobjectives by penalizing discrepancies between predicted and ground-truth\nsigned distance functions (SDFs). Extensive experiments demonstrate that our\nmethod not only enhances segmentation performance but also improves calibration\nquality, yielding more trustworthy confidence estimates. Code is available at:\nhttps://github.com/EagleAdelaide/SDC-Loss.\n","authors":["Wenhao Liang","Wei Zhang","Lin Yue","Miao Xu","Olaf Maennel","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.05107v2.pdf","comment":"Under Reviewing"},{"id":"http://arxiv.org/abs/2506.07044v4","updated":"2025-06-13T04:22:02Z","published":"2025-06-08T08:47:30Z","title":"Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...\n","authors":[" LASA Team","Weiwen Xu","Hou Pong Chan","Long Li","Mahani Aljunied","Ruifeng Yuan","Jianyu Wang","Chenghao Xiao","Guizhen Chen","Chaoqun Liu","Zhaodonghui Li","Yu Sun","Junao Shen","Chaojun Wang","Jie Tan","Deli Zhao","Tingyang Xu","Hao Zhang","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2506.07044v4.pdf","comment":"Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is\n  https://alibaba-damo-academy.github.io/lingshu/"},{"id":"http://arxiv.org/abs/2506.11455v1","updated":"2025-06-13T04:14:38Z","published":"2025-06-13T04:14:38Z","title":"Voxel-Level Brain States Prediction Using Swin Transformer","summary":"  Understanding brain dynamics is important for neuroscience and mental health.\nFunctional magnetic resonance imaging (fMRI) enables the measurement of neural\nactivities through blood-oxygen-level-dependent (BOLD) signals, which represent\nbrain states. In this study, we aim to predict future human resting brain\nstates with fMRI. Due to the 3D voxel-wise spatial organization and temporal\ndependencies of the fMRI data, we propose a novel architecture which employs a\n4D Shifted Window (Swin) Transformer as encoder to efficiently learn\nspatio-temporal information and a convolutional decoder to enable brain state\nprediction at the same spatial and temporal resolution as the input fMRI data.\nWe used 100 unrelated subjects from the Human Connectome Project (HCP) for\nmodel training and testing. Our novel model has shown high accuracy when\npredicting 7.2s resting-state brain activities based on the prior 23.04s fMRI\ntime series. The predicted brain states highly resemble BOLD contrast and\ndynamics. This work shows promising evidence that the spatiotemporal\norganization of the human brain can be learned by a Swin Transformer model, at\nhigh resolution, which provides a potential for reducing the fMRI scan time and\nthe development of brain-computer interfaces in the future.\n","authors":["Yifei Sun","Daniel Chahine","Qinghao Wen","Tianming Liu","Xiang Li","Yixuan Yuan","Fernando Calamante","Jinglei Lv"],"pdf_url":"https://arxiv.org/pdf/2506.11455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11454v1","updated":"2025-06-13T04:10:48Z","published":"2025-06-13T04:10:48Z","title":"FAD-Net: Frequency-Domain Attention-Guided Diffusion Network for\n  Coronary Artery Segmentation using Invasive Coronary Angiography","summary":"  Background: Coronary artery disease (CAD) remains one of the leading causes\nof mortality worldwide. Precise segmentation of coronary arteries from invasive\ncoronary angiography (ICA) is critical for effective clinical decision-making.\nObjective: This study aims to propose a novel deep learning model based on\nfrequency-domain analysis to enhance the accuracy of coronary artery\nsegmentation and stenosis detection in ICA, thereby offering robust support for\nthe stenosis detection and treatment of CAD. Methods: We propose the\nFrequency-Domain Attention-Guided Diffusion Network (FAD-Net), which integrates\na frequency-domain-based attention mechanism and a cascading diffusion strategy\nto fully exploit frequency-domain information for improved segmentation\naccuracy. Specifically, FAD-Net employs a Multi-Level Self-Attention (MLSA)\nmechanism in the frequency domain, computing the similarity between queries and\nkeys across high- and low-frequency components in ICAs. Furthermore, a\nLow-Frequency Diffusion Module (LFDM) is incorporated to decompose ICAs into\nlow- and high-frequency components via multi-level wavelet transformation.\nSubsequently, it refines fine-grained arterial branches and edges by\nreintegrating high-frequency details via inverse fusion, enabling continuous\nenhancement of anatomical precision. Results and Conclusions: Extensive\nexperiments demonstrate that FAD-Net achieves a mean Dice coefficient of 0.8717\nin coronary artery segmentation, outperforming existing state-of-the-art\nmethods. In addition, it attains a true positive rate of 0.6140 and a positive\npredictive value of 0.6398 in stenosis detection, underscoring its clinical\napplicability. These findings suggest that FAD-Net holds significant potential\nto assist in the accurate diagnosis and treatment planning of CAD.\n","authors":["Nan Mu","Ruiqi Song","Xiaoning Li","Zhihui Xu","Jingfeng Jiang","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.11454v1.pdf","comment":"35 pages, 12 figures"},{"id":"http://arxiv.org/abs/2505.02471v3","updated":"2025-06-13T03:49:59Z","published":"2025-05-05T08:56:12Z","title":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction","summary":"  We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.\n","authors":["Inclusion AI","Biao Gong","Cheng Zou","Dandan Zheng","Hu Yu","Jingdong Chen","Jianxin Sun","Junbo Zhao","Jun Zhou","Kaixiang Ji","Lixiang Ru","Libin Wang","Qingpei Guo","Rui Liu","Weilong Chai","Xinyu Xiao","Ziyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02471v3.pdf","comment":"https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify"},{"id":"http://arxiv.org/abs/2506.11444v1","updated":"2025-06-13T03:45:15Z","published":"2025-06-13T03:45:15Z","title":"GaussMarker: Robust Dual-Domain Watermark for Diffusion Models","summary":"  As Diffusion Models (DM) generate increasingly realistic images, related\nissues such as copyright and misuse have become a growing concern. Watermarking\nis one of the promising solutions. Existing methods inject the watermark into\nthe single-domain of initial Gaussian noise for generation, which suffers from\nunsatisfactory robustness. This paper presents the first dual-domain DM\nwatermarking approach using a pipelined injector to consistently embed\nwatermarks in both the spatial and frequency domains. To further boost\nrobustness against certain image manipulations and advanced attacks, we\nintroduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine\nGaussian noise extracted from manipulated images and enhance detection\nrobustness by integrating the detection scores of both watermarks. GaussMarker\nefficiently achieves state-of-the-art performance under eight image distortions\nand four advanced attacks across three versions of Stable Diffusion with better\nrecall and lower false positive rates, as preferred in real applications.\n","authors":["Kecen Li","Zhicong Huang","Xinwen Hou","Cheng Hong"],"pdf_url":"https://arxiv.org/pdf/2506.11444v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.11439v1","updated":"2025-06-13T03:37:57Z","published":"2025-06-13T03:37:57Z","title":"Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in\n  Digital Pathology","summary":"  Machine-learning-assisted cancer subtyping is a promising avenue in digital\npathology. Cancer subtyping models, however, require careful training using\nexpert annotations so that they can be inferred with a degree of known\ncertainty (or uncertainty). To this end, we introduce the concept of\nuncertainty awareness into a self-supervised contrastive learning model. This\nis achieved by computing an evidence vector at every epoch, which assesses the\nmodel's confidence in its predictions. The derived uncertainty score is then\nutilized as a metric to selectively label the most crucial images that require\nfurther annotation, thus iteratively refining the training process. With just\n1-10% of strategically selected annotations, we attain state-of-the-art\nperformance in cancer subtyping on benchmark datasets. Our method not only\nstrategically guides the annotation process to minimize the need for extensive\nlabeled datasets, but also improves the precision and efficiency of\nclassifications. This development is particularly beneficial in settings where\nthe availability of labeled data is limited, offering a promising direction for\nfuture research and application in digital pathology.\n","authors":["Nirhoshan Sivaroopan","Chamuditha Jayanga Galappaththige","Chalani Ekanayake","Hasindri Watawana","Ranga Rodrigo","Chamira U. S. Edussooriya","Dushan N. Wadduwage"],"pdf_url":"https://arxiv.org/pdf/2506.11439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01881v3","updated":"2025-06-13T03:36:19Z","published":"2025-05-03T17:59:26Z","title":"PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in\n  Navigation Applications","summary":"  Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems.\n","authors":["Trisanth Srinivasan","Santosh Patapati"],"pdf_url":"https://arxiv.org/pdf/2505.01881v3.pdf","comment":"Accepted at IEEE/CVF Computer Society Conference on Computer Vision\n  and Pattern Recognition Workshops 2025 (CVPRW)"},{"id":"http://arxiv.org/abs/2506.10730v2","updated":"2025-06-13T03:32:12Z","published":"2025-06-12T14:23:06Z","title":"IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly\n  Detection in Medical Domain","summary":"  Recently, the rapid advancements of vision-language models, such as CLIP,\nleads to significant progress in zero-/few-shot anomaly detection (ZFSAD)\ntasks. However, most existing CLIP-based ZFSAD methods commonly assume prior\nknowledge of categories and rely on carefully crafted prompts tailored to\nspecific scenarios. While such meticulously designed text prompts effectively\ncapture semantic information in the textual space, they fall short of\ndistinguishing normal and anomalous instances within the joint embedding space.\nMoreover, these ZFSAD methods are predominantly explored in industrial\nscenarios, with few efforts conducted to medical tasks. To this end, we propose\nan innovative framework for ZFSAD tasks in medical domain, denoted as IQE-CLIP.\nWe reveal that query embeddings, which incorporate both textual and\ninstance-aware visual information, are better indicators for abnormalities.\nSpecifically, we first introduce class-based prompting tokens and learnable\nprompting tokens for better adaptation of CLIP to the medical domain. Then, we\ndesign an instance-aware query module (IQM) to extract region-level contextual\ninformation from both text prompts and visual features, enabling the generation\nof query embeddings that are more sensitive to anomalies. Extensive experiments\nconducted on six medical datasets demonstrate that IQE-CLIP achieves\nstate-of-the-art performance on both zero-shot and few-shot tasks. We release\nour code and data at https://github.com/hongh0/IQE-CLIP/.\n","authors":["Hong Huang","Weixiang Sun","Zhijian Wu","Jingwen Niu","Donghuan Lu","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2506.10730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05901v3","updated":"2025-06-13T03:27:00Z","published":"2024-08-12T02:48:00Z","title":"Efficient Visual Representation Learning with Heat Conduction Equation","summary":"  Foundation models, such as CNNs and ViTs, have powered the development of\nimage representation learning. However, general guidance to model architecture\ndesign is still missing. Inspired by the connection between image\nrepresentation learning and heat conduction, we model images by the heat\nconduction equation, where the essential idea is to conceptualize image\nfeatures as temperatures and model their information interaction as the\ndiffusion of thermal energy. Based on this idea, we find that many modern model\narchitectures, such as residual structures, SE block, and feed-forward\nnetworks, can be interpreted from the perspective of the heat conduction\nequation. Therefore, we leverage the heat equation to design new and more\ninterpretable models. As an example, we propose the Heat Conduction Layer and\nthe Refinement Approximation Layer inspired by solving the heat conduction\nequation using Finite Difference Method and Fourier series, respectively. The\nmain goal of this paper is to integrate the overall architectural design of\nneural networks into the theoretical framework of heat conduction.\nNevertheless, our Heat Conduction Network (HcNet) still shows competitive\nperformance, e.g., HcNet-T achieves 83.0% top-1 accuracy on ImageNet-1K while\nonly requiring 28M parameters and 4.1G MACs. The code is publicly available at:\nhttps://github.com/ZheminZhang1/HcNet.\n","authors":["Zhemin Zhang","Xun Gong"],"pdf_url":"https://arxiv.org/pdf/2408.05901v3.pdf","comment":"Accepted by IJCAI2025"},{"id":"http://arxiv.org/abs/2506.04830v2","updated":"2025-06-13T03:20:44Z","published":"2025-06-05T09:53:44Z","title":"DualX-VSR: Dual Axial Spatial$\\times$Temporal Transformer for Real-World\n  Video Super-Resolution without Motion Compensation","summary":"  Transformer-based models like ViViT and TimeSformer have advanced video\nunderstanding by effectively modeling spatiotemporal dependencies. Recent video\ngeneration models, such as Sora and Vidu, further highlight the power of\ntransformers in long-range feature extraction and holistic spatiotemporal\nmodeling. However, directly applying these models to real-world video\nsuper-resolution (VSR) is challenging, as VSR demands pixel-level precision,\nwhich can be compromised by tokenization and sequential attention mechanisms.\nWhile recent transformer-based VSR models attempt to address these issues using\nsmaller patches and local attention, they still face limitations such as\nrestricted receptive fields and dependence on optical flow-based alignment,\nwhich can introduce inaccuracies in real-world settings. To overcome these\nissues, we propose Dual Axial Spatial$\\times$Temporal Transformer for\nReal-World Video Super-Resolution (DualX-VSR), which introduces a novel dual\naxial spatial$\\times$temporal attention mechanism that integrates spatial and\ntemporal information along orthogonal directions. DualX-VSR eliminates the need\nfor motion compensation, offering a simplified structure that provides a\ncohesive representation of spatiotemporal information. As a result, DualX-VSR\nachieves high fidelity and superior performance in real-world VSR task.\n","authors":["Shuo Cao","Yihao Liu","Xiaohui Li","Yuanting Gao","Yu Zhou","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.04830v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.11436v1","updated":"2025-06-13T03:19:47Z","published":"2025-06-13T03:19:47Z","title":"TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models","summary":"  Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively\naligning audio and visual modalities. While recent approaches leverage\nfoundation models to address data scarcity, they often rely on single-modality\nknowledge or combine foundation models in an off-the-shelf manner, failing to\naddress the cross-modal alignment challenge. In this paper, we present TAViS, a\nnovel framework that \\textbf{couples} the knowledge of multimodal foundation\nmodels (ImageBind) for cross-modal alignment and a segmentation foundation\nmodel (SAM2) for precise segmentation. However, effectively combining these\nmodels poses two key challenges: the difficulty in transferring the knowledge\nbetween SAM2 and ImageBind due to their different feature spaces, and the\ninsufficiency of using only segmentation loss for supervision. To address these\nchallenges, we introduce a text-bridged design with two key components: (1) a\ntext-bridged hybrid prompting mechanism where pseudo text provides class\nprototype information while retaining modality-specific details from both audio\nand visual inputs, and (2) an alignment supervision strategy that leverages\ntext as a bridge to align shared semantic concepts within audio-visual\nmodalities. Our approach achieves superior performance on single-source,\nmulti-source, semantic datasets, and excels in zero-shot settings.\n","authors":["Ziyang Luo","Nian Liu","Xuguang Yang","Salman Khan","Rao Muhammad Anwer","Hisham Cholakkal","Fahad Shahbaz Khan","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2506.11436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11434v1","updated":"2025-06-13T03:16:16Z","published":"2025-06-13T03:16:16Z","title":"Auditing Data Provenance in Real-world Text-to-Image Diffusion Models\n  for Privacy and Copyright Protection","summary":"  Text-to-image diffusion model since its propose has significantly influenced\nthe content creation due to its impressive generation capability. However, this\ncapability depends on large-scale text-image datasets gathered from web\nplatforms like social media, posing substantial challenges in copyright\ncompliance and personal privacy leakage. Though there are some efforts devoted\nto explore approaches for auditing data provenance in text-to-image diffusion\nmodels, existing work has unrealistic assumptions that can obtain model\ninternal knowledge, e.g., intermediate results, or the evaluation is not\nreliable. To fill this gap, we propose a completely black-box auditing\nframework called Feature Semantic Consistency-based Auditing (FSCA). It\nutilizes two types of semantic connections within the text-to-image diffusion\nmodel for auditing, eliminating the need for access to internal knowledge. To\ndemonstrate the effectiveness of our FSCA framework, we perform extensive\nexperiments on LAION-mi dataset and COCO dataset, and compare with eight\nstate-of-the-art baseline approaches. The results show that FSCA surpasses\nprevious baseline approaches across various metrics and different data\ndistributions, showcasing the superiority of our FSCA. Moreover, we introduce a\nrecall balance strategy and a threshold adjustment strategy, which collectively\nallows FSCA to reach up a user-level accuracy of 90% in a real-world auditing\nscenario with only 10 samples/user, highlighting its strong auditing potential\nin real-world applications. Our code is made available at\nhttps://github.com/JiePKU/FSCA.\n","authors":["Jie Zhu","Leye Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11434v1.pdf","comment":"Under Review; A user-level accuracy of 90% in a real-world auditing\n  scenario"},{"id":"http://arxiv.org/abs/2412.03055v2","updated":"2025-06-13T03:09:37Z","published":"2024-12-04T06:20:36Z","title":"Real-Time AIoT for UAV Antenna Interference Detection via Edge-Cloud\n  Collaboration","summary":"  In the fifth-generation (5G) era, eliminating communication interference\nsources is crucial for maintaining network performance. Interference often\noriginates from unauthorized or malfunctioning antennas, and radio monitoring\nagencies must address numerous sources of such antennas annually. Unmanned\naerial vehicles (UAVs) can improve inspection efficiency. However, the data\ntransmission delay in the existing cloud-only (CO) artificial intelligence (AI)\nmode fails to meet the low latency requirements for real-time performance.\nTherefore, we propose a computer vision-based AI of Things (AIoT) system to\ndetect antenna interference sources for UAVs. The system adopts an optimized\nedge-cloud collaboration (ECC+) mode, combining a keyframe selection algorithm\n(KSA), focusing on reducing end-to-end latency (E2EL) and ensuring reliable\ndata transmission, which aligns with the core principles of ultra-reliable\nlow-latency communication (URLLC). At the core of our approach is an end-to-end\nantenna localization scheme based on the tracking-by-detection (TBD) paradigm,\nincluding a detector (EdgeAnt) and a tracker (AntSort). EdgeAnt achieves\nstate-of-the-art (SOTA) performance with a mean average precision (mAP) of\n42.1% on our custom antenna interference source dataset, requiring only 3\nmillion parameters and 14.7 GFLOPs. On the COCO dataset, EdgeAnt achieves 38.9%\nmAP with 5.4 GFLOPs. We deployed EdgeAnt on Jetson Xavier NX (TRT) and\nRaspberry Pi 4B (NCNN), achieving real-time inference speeds of 21.1 (1088) and\n4.8 (640) frames per second (FPS), respectively. Compared with CO mode, the\nECC+ mode reduces E2EL by 88.9%, increases accuracy by 28.2%. Additionally, the\nsystem offers excellent scalability for coordinated multiple UAVs inspections.\nThe detector code is publicly available at\nhttps://github.com/SCNU-RISLAB/EdgeAnt.\n","authors":["Jun Dong","Jintao Cheng","Jin Wu","Chengxi Zhang","Shunyi Zhao","Xiaoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2412.03055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11430v1","updated":"2025-06-13T03:06:52Z","published":"2025-06-13T03:06:52Z","title":"Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference\n  Optimization","summary":"  We introduce Auto-Connect, a novel approach for automatic rigging that\nexplicitly preserves skeletal connectivity through a connectivity-preserving\ntokenization scheme. Unlike previous methods that predict bone positions\nrepresented as two joints or first predict points before determining\nconnectivity, our method employs special tokens to define endpoints for each\njoint's children and for each hierarchical layer, effectively automating\nconnectivity relationships. This approach significantly enhances topological\naccuracy by integrating connectivity information directly into the prediction\nframework. To further guarantee high-quality topology, we implement a\ntopology-aware reward function that quantifies topological correctness, which\nis then utilized in a post-training phase through reward-guided Direct\nPreference Optimization. Additionally, we incorporate implicit geodesic\nfeatures for latent top-k bone selection, which substantially improves skinning\nquality. By leveraging geodesic distance information within the model's latent\nspace, our approach intelligently determines the most influential bones for\neach vertex, effectively mitigating common skinning artifacts. This combination\nof connectivity-preserving tokenization, reward-guided fine-tuning, and\ngeodesic-aware bone selection enables our model to consistently generate more\nanatomically plausible skeletal structures with superior deformation\nproperties.\n","authors":["Jingfeng Guo","Jian Liu","Jinnan Chen","Shiwei Mao","Changrong Hu","Puhua Jiang","Junlin Yu","Jing Xu","Qi Liu","Lixin Xu","Zhuo Chen","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2506.11430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02214v3","updated":"2025-06-13T02:45:51Z","published":"2025-04-03T02:08:22Z","title":"Geospatial Artificial Intelligence for Satellite-Based Flood Extent\n  Mapping: Concepts, Advances, and Future Perspectives","summary":"  Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent\nmapping systematically integrates artificial intelligence techniques with\nsatellite data to identify flood events and assess their impacts, for disaster\nmanagement and spatial decision-making. The primary output often includes flood\nextent maps, which delineate the affected areas, along with additional\nanalytical outputs such as uncertainty estimation and change detection.\n","authors":["Hyunho Lee","Wenwen Li"],"pdf_url":"https://arxiv.org/pdf/2504.02214v3.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11417v1","updated":"2025-06-13T02:35:03Z","published":"2025-06-13T02:35:03Z","title":"Stop learning it all to mitigate visual hallucination, Focus on the\n  hallucination target","summary":"  Multimodal Large Language Models (MLLMs) frequently suffer from hallucination\nissues, generating information about objects that are not present in input\nimages during vision-language tasks. These hallucinations particularly\nundermine model reliability in practical applications requiring accurate object\nidentification. To address this challenge, we propose \\mymethod,\\ a preference\nlearning approach that mitigates hallucinations by focusing on targeted areas\nwhere they occur. To implement this, we build a dataset containing hallucinated\nresponses, correct responses, and target information (i.e., objects present in\nthe images and the corresponding chunk positions in responses affected by\nhallucinations). By applying a preference learning method restricted to these\nspecific targets, the model can filter out irrelevant signals and focus on\ncorrecting hallucinations. This allows the model to produce more factual\nresponses by concentrating solely on relevant information. Experimental results\ndemonstrate that \\mymethod\\ effectively reduces hallucinations across multiple\nvision hallucination tasks, improving the reliability and performance of MLLMs\nwithout diminishing overall performance.\n","authors":["Dokyoon Yoon","Youngsook Song","Woomyong Park"],"pdf_url":"https://arxiv.org/pdf/2506.11417v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2411.04746v3","updated":"2025-06-13T02:29:47Z","published":"2024-11-07T14:29:02Z","title":"Taming Rectified Flow for Inversion and Editing","summary":"  Rectified-flow-based diffusion transformers like FLUX and OpenSora have\ndemonstrated outstanding performance in the field of image and video\ngeneration. Despite their robust generative capabilities, these models often\nstruggle with inversion inaccuracies, which could further limit their\neffectiveness in downstream tasks such as image and video editing. To address\nthis issue, we propose RF-Solver, a novel training-free sampler that\neffectively enhances inversion precision by mitigating the errors in the\nODE-solving process of rectified flow. Specifically, we derive the exact\nformulation of the rectified flow ODE and apply the high-order Taylor expansion\nto estimate its nonlinear components, significantly enhancing the precision of\nODE solutions at each timestep. Building upon RF-Solver, we further propose\nRF-Edit, a general feature-sharing-based framework for image and video editing.\nBy incorporating self-attention features from the inversion process into the\nediting process, RF-Edit effectively preserves the structural information of\nthe source image or video while achieving high-quality editing results. Our\napproach is compatible with any pre-trained rectified-flow-based models for\nimage and video tasks, requiring no additional training or optimization.\nExtensive experiments across generation, inversion, and editing tasks in both\nimage and video modalities demonstrate the superiority and versatility of our\nmethod. The source code is available at\nhttps://github.com/wangjiangshan0725/RF-Solver-Edit.\n","authors":["Jiangshan Wang","Junfu Pu","Zhongang Qi","Jiayi Guo","Yue Ma","Nisha Huang","Yuxin Chen","Xiu Li","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2411.04746v3.pdf","comment":"ICML 2025; GitHub:\n  https://github.com/wangjiangshan0725/RF-Solver-Edit"},{"id":"http://arxiv.org/abs/2503.16862v2","updated":"2025-06-13T02:00:40Z","published":"2025-03-21T05:24:48Z","title":"Improving Acoustic Scene Classification with City Features","summary":"  Acoustic scene recordings are often collected from a diverse range of cities.\nMost existing acoustic scene classification (ASC) approaches focus on\nidentifying common acoustic scene patterns across cities to enhance\ngeneralization. However, the potential acoustic differences introduced by\ncity-specific environmental and cultural factors are overlooked. In this paper,\nwe hypothesize that the city-specific acoustic features are beneficial for the\nASC task rather than being treated as noise or bias. To this end, we propose\nCity2Scene, a novel framework that leverages city features to improve ASC.\nUnlike conventional approaches that may discard or suppress city information,\nCity2Scene transfers the city-specific knowledge from pre-trained city\nclassification models to scene classification model using knowledge\ndistillation. We evaluate City2Scene on three datasets of DCASE Challenge Task\n1, which include both scene and city labels. Experimental results demonstrate\nthat city features provide valuable information for classifying scenes. By\ndistilling city-specific knowledge, City2Scene effectively improves accuracy\nacross a variety of lightweight CNN backbones, achieving competitive\nperformance to the top-ranked solutions of DCASE Challenge in recent years.\n","authors":["Yiqiang Cai","Yizhou Tan","Shengchen Li","Xi Shao","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2503.16862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11394v1","updated":"2025-06-13T01:27:45Z","published":"2025-06-13T01:27:45Z","title":"Dynamic Double Space Tower","summary":"  The Visual Question Answering (VQA) task requires the simultaneous\nunderstanding of image content and question semantics. However, existing\nmethods often have difficulty handling complex reasoning scenarios due to\ninsufficient cross-modal interaction and capturing the entity spatial\nrelationships in the\nimage.\\cite{huang2023adaptive}\\cite{liu2021comparing}\\cite{guibas2021adaptive}\\cite{zhang2022vsa}We\nstudied a brand-new approach to replace the attention mechanism in order to\nenhance the reasoning ability of the model and its understanding of spatial\nrelationships.Specifically, we propose a dynamic bidirectional spatial tower,\nwhich is divided into four layers to observe the image according to the\nprinciple of human gestalt vision. This naturally provides a powerful\nstructural prior for the spatial organization between entities, enabling the\nmodel to no longer blindly search for relationships between pixels but make\njudgments based on more meaningful perceptual units. Change from \"seeing\nimages\" to \"perceiving and organizing image content\".A large number of\nexperiments have shown that our module can be used in any other multimodal\nmodel and achieve advanced results, demonstrating its potential in spatial\nrelationship processing.Meanwhile, the multimodal visual question-answering\nmodel July trained by our method has achieved state-of-the-art results with\nonly 3B parameters, especially on the question-answering dataset of spatial\nrelations.\n","authors":["Weikai Sun","Shijie Song","Han Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11387v1","updated":"2025-06-13T01:09:44Z","published":"2025-06-13T01:09:44Z","title":"Control Architecture and Design for a Multi-robotic Visual Servoing\n  System in Automated Manufacturing Environment","summary":"  The use of robotic technology has drastically increased in manufacturing in\nthe 21st century. But by utilizing their sensory cues, humans still outperform\nmachines, especially in micro scale manufacturing, which requires\nhigh-precision robot manipulators. These sensory cues naturally compensate for\nhigh levels of uncertainties that exist in the manufacturing environment.\nUncertainties in performing manufacturing tasks may come from measurement\nnoise, model inaccuracy, joint compliance (e.g., elasticity), etc. Although\nadvanced metrology sensors and high precision microprocessors, which are\nutilized in modern robots, have compensated for many structural and dynamic\nerrors in robot positioning, a well-designed control algorithm still works as a\ncomparable and cheaper alternative to reduce uncertainties in automated\nmanufacturing. Our work illustrates that a multi-robot control system that\nsimulates the positioning process for fastening and unfastening applications\ncan reduce various uncertainties, which may occur in this process, to a great\nextent. In addition, most research papers in visual servoing mainly focus on\ndeveloping control and observation architectures in various scenarios, but few\nhave discussed the importance of the camera's location in the configuration. In\na manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. Therefore, in this paper, we also propose a novel\nalgorithm for the camera's moving policy so that it explores the camera\nworkspace and searches for the optimal location where the image noise level is\nminimized.\n","authors":["Rongfei Li"],"pdf_url":"https://arxiv.org/pdf/2506.11387v1.pdf","comment":"272 pages, 171 figures, PhD dissertation, University of California,\n  Davis, 2025. To be published in ProQuest ETD"},{"id":"http://arxiv.org/abs/2506.11380v1","updated":"2025-06-13T01:03:29Z","published":"2025-06-13T01:03:29Z","title":"Enhance Multimodal Consistency and Coherence for Text-Image Plan\n  Generation","summary":"  People get informed of a daily task plan through diverse media involving both\ntexts and images. However, most prior research only focuses on LLM's capability\nof textual plan generation. The potential of large-scale models in providing\ntext-image plans remains understudied. Generating high-quality text-image plans\nfaces two main challenges: ensuring consistent alignment between two modalities\nand keeping coherence among visual steps. To address these challenges, we\npropose a novel framework that generates and refines text-image plans\nstep-by-step. At each iteration, our framework (1) drafts the next textual step\nbased on the prediction history; (2) edits the last visual step to obtain the\nnext one; (3) extracts PDDL-like visual information; and (4) refines the draft\nwith the extracted visual information. The textual and visual step produced in\nstage (4) and (2) will then serve as inputs for the next iteration. Our\napproach offers a plug-and-play improvement to various backbone models, such as\nMistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our\napproach, we collect a new benchmark consisting of 1,100 tasks and their\ntext-image pair solutions covering 11 daily topics. We also design and validate\na new set of metrics to evaluate the multimodal consistency and coherence in\ntext-image plans. Extensive experiment results show the effectiveness of our\napproach on a range of backbone models against competitive baselines. Our code\nand data are available at https://github.com/psunlpgroup/MPlanner.\n","authors":["Xiaoxin Lu","Ranran Haoran Zhang","Yusen Zhang","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11380v1.pdf","comment":"18 pages, 10 figures; Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2208.07552v3","updated":"2025-06-13T00:19:30Z","published":"2022-08-16T05:57:24Z","title":"Self-supervised training of deep denoisers in multi-coil MRI considering\n  noise correlations","summary":"  Deep learning-based denoising methods have shown powerful results for\nimproving the signal-to-noise ratio of magnetic resonance (MR) images, mostly\nby leveraging supervised learning with clean ground truth. However, acquiring\nclean ground truth images is often expensive and time-consuming. Self\nsupervised methods have been widely investigated to mitigate the dependency on\nclean images, but mostly rely on the suboptimal splitting of K-space\nmeasurements of an image to yield input and target images for ensuring\nstatistical independence. In this study, we investigate an alternative\nself-supervised training method for deep denoisers in multi-coil MRI, dubbed\nCoil2Coil (C2C), that naturally split and combine the multi-coil data among\nphased array coils, generating two noise-corrupted images for training. This\nnovel approach allows exploiting multi-coil redundancy, but the images are\nstatistically correlated and may not have the same clean image. To mitigate\nthese issues, we propose the methods to pproximately decorrelate the\nstatistical dependence of these images and match the underlying clean images,\nthus enabling them to be used as the training pairs. For synthetic denoising\nexperiments, C2C yielded the best performance against prior self-supervised\nmethods, reporting outcome comparable even to supervised methods. For\nreal-world denoising cases, C2C yielded consistent performance as synthetic\ncases, removing only noise structures.\n","authors":["Juhyung Park","Dongwon Park","Sooyeon Ji","Hyeong-Geol Shin","Se Young Chun","Jongho Lee"],"pdf_url":"https://arxiv.org/pdf/2208.07552v3.pdf","comment":"9 pages, 5figures"},{"id":"http://arxiv.org/abs/2506.11371v1","updated":"2025-06-13T00:15:54Z","published":"2025-06-13T00:15:54Z","title":"A Watermark for Auto-Regressive Image Generation Models","summary":"  The rapid evolution of image generation models has revolutionized visual\ncontent creation, enabling the synthesis of highly realistic and contextually\naccurate images for diverse applications. However, the potential for misuse,\nsuch as deepfake generation, image based phishing attacks, and fabrication of\nmisleading visual evidence, underscores the need for robust authenticity\nverification mechanisms. While traditional statistical watermarking techniques\nhave proven effective for autoregressive language models, their direct\nadaptation to image generation models encounters significant challenges due to\na phenomenon we term retokenization mismatch, a disparity between original and\nretokenized sequences during the image generation process. To overcome this\nlimitation, we propose C-reweight, a novel, distortion-free watermarking method\nexplicitly designed for image generation models. By leveraging a\nclustering-based strategy that treats tokens within the same cluster\nequivalently, C-reweight mitigates retokenization mismatch while preserving\nimage fidelity. Extensive evaluations on leading image generation platforms\nreveal that C-reweight not only maintains the visual quality of generated\nimages but also improves detectability over existing distortion-free\nwatermarking techniques, setting a new standard for secure and trustworthy\nimage synthesis.\n","authors":["Yihan Wu","Xuehao Cui","Ruibo Chen","Georgios Milis","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2506.11371v1.pdf","comment":"Technical report"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.11999v1","updated":"2025-06-13T17:54:12Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v1.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"},{"id":"http://arxiv.org/abs/2405.19164v2","updated":"2025-06-13T14:26:31Z","published":"2024-05-29T15:08:55Z","title":"Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery","summary":"  Electronic Discovery (eDiscovery) requires identifying relevant documents\nfrom vast collections for legal production requests. While artificial\nintelligence (AI) and natural language processing (NLP) have improved document\nreview efficiency, current methods still struggle with legal entities,\ncitations, and complex legal artifacts. To address these challenges, we\nintroduce DISCOvery Graph (DISCOG), an emerging system that integrates\nknowledge graphs for enhanced document ranking and classification, augmented by\nLLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,\nprecision, and recall across both balanced and imbalanced datasets. In\nreal-world deployments, it has reduced litigation-related document review costs\nby approximately 98\\%, demonstrating significant business impact.\n","authors":["Sounak Lahiri","Sumit Pai","Tim Weninger","Sanmitra Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2405.19164v2.pdf","comment":"Updated with Camera Ready Copy for ACL 2025"},{"id":"http://arxiv.org/abs/2506.11763v1","updated":"2025-06-13T13:17:32Z","published":"2025-06-13T13:17:32Z","title":"DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents","summary":"  Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.\n","authors":["Mingxuan Du","Benfeng Xu","Chiwei Zhu","Xiaorui Wang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2506.11763v1.pdf","comment":"31 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.11727v1","updated":"2025-06-13T12:39:59Z","published":"2025-06-13T12:39:59Z","title":"Forgetful by Design? A Critical Audit of YouTube's Search API for\n  Academic Research","summary":"  This paper critically audits the search endpoint of YouTube's Data API (v3),\na common tool for academic research. Through systematic weekly searches over\nsix months using eleven queries, we identify major limitations regarding\ncompleteness, representativeness, consistency, and bias. Our findings reveal\nsubstantial differences between ranking parameters like relevance and date in\nterms of video recall and precision, with relevance often retrieving numerous\noff-topic videos. We also find severe temporal decay, as the number of findable\nvideos for a specific period dramatically decreases after just 20-60 days from\nthe publication date, potentially hampering many different research designs.\nFurthermore, search results lack consistency, with identical queries yielding\ndifferent video sets over time, compromising replicability. A case study on the\nEuropean Parliament elections highlights how these issues impact research\noutcomes. While the paper offers several mitigation strategies, it concludes\nthat the API's search function, potentially prioritizing \"freshness\" over\ncomprehensive retrieval, is not adequate for robust academic research,\nespecially concerning Digital Services Act requirements.\n","authors":["Bernhard Rieder","Adrian Padilla","Oscar Coromina"],"pdf_url":"https://arxiv.org/pdf/2506.11727v1.pdf","comment":"34 pages, 2 tables and 4 figures"},{"id":"http://arxiv.org/abs/2504.08754v4","updated":"2025-06-13T11:19:12Z","published":"2025-03-28T15:49:52Z","title":"Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action","summary":"  Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.\n","authors":["Tongyoung Kim","Jeongeun Lee","Soojin Yoon","Sunghwan Kim","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2504.08754v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11603v1","updated":"2025-06-13T09:17:36Z","published":"2025-06-13T09:17:36Z","title":"TongSearch-QR: Reinforced Query Reasoning for Retrieval","summary":"  Traditional information retrieval (IR) methods excel at textual and semantic\nmatching but struggle in reasoning-intensive retrieval tasks that require\nmulti-hop inference or complex semantic understanding between queries and\ndocuments. One promising solution is to explicitly rewrite or augment queries\nusing large language models (LLMs) to elicit reasoning-relevant content prior\nto retrieval. However, the widespread use of large-scale language models like\nGPT-4 or LLaMA3-70B remains impractical due to their high inference cost and\nlimited deployability in real-world systems. In this work, we introduce\nTongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of\nsmall-scale language models for query reasoning and rewriting in\nreasoning-intensive retrieval. With a novel semi-rule-based reward function, we\nemploy reinforcement learning approaches enabling smaller language models, e,g,\nQwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning\nperformance rivaling large-scale language models without their prohibitive\ninference costs. Experiment results on BRIGHT benchmark show that with BM25 as\nretrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly\noutperform existing baselines, including prompt-based query reasoners and some\nlatest dense retrievers trained for reasoning-intensive retrieval tasks,\noffering superior adaptability for real-world deployment.\n","authors":["Xubo Qin","Jun Bai","Jiaqi Li","Zixia Jia","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2506.11603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11600v1","updated":"2025-06-13T09:09:08Z","published":"2025-06-13T09:09:08Z","title":"GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news","summary":"  GraphRAG-Causal introduces an innovative framework that combines graph-based\nretrieval with large language models to enhance causal reasoning in news\nanalysis. Traditional NLP approaches often struggle with identifying complex,\nimplicit causal links, especially in low-data scenarios. Our approach addresses\nthese challenges by transforming annotated news headlines into structured\ncausal knowledge graphs. It then employs a hybrid retrieval system that merges\nsemantic embeddings with graph-based structural cues leveraging Neo4j to\naccurately match and retrieve relevant events. The framework is built on a\nthree-stage pipeline: First, during Data Preparation, news sentences are\nmeticulously annotated and converted into causal graphs capturing cause,\neffect, and trigger relationships. Next, the Graph Retrieval stage stores these\ngraphs along with their embeddings in a Neo4j database and utilizes hybrid\nCypher queries to efficiently identify events that share both semantic and\nstructural similarities with a given query. Finally, the LLM Inference stage\nutilizes these retrieved causal graphs in a few-shot learning setup with\nXML-based prompting, enabling robust classification and tagging of causal\nrelationships. Experimental evaluations demonstrate that GraphRAG-Causal\nachieves an impressive F1-score of 82.1% on causal classification using just 20\nfew-shot examples. This approach significantly boosts accuracy and consistency,\nmaking it highly suitable for real-time applications in news reliability\nassessment, misinformation detection, and policy analysis.\n","authors":["Abdul Haque","Umm e Hani","Ahmad Din","Muhammad Babar","Ali Abbas","Insaf Ullah"],"pdf_url":"https://arxiv.org/pdf/2506.11600v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.05161v2","updated":"2025-06-13T07:47:04Z","published":"2024-07-06T19:38:41Z","title":"A Survey of Datasets for Information Diffusion Tasks","summary":"  Information diffusion across various new media platforms gradually influences\nperceptions, decisions, and social behaviors of individual users. In\ncommunication studies, the famous Five W's of Communication model (5W Model)\nhas displayed the process of information diffusion clearly. At present,\nalthough plenty of studies and corresponding datasets about information\ndiffusion have emerged, a systematic categorization of tasks and an integration\nof datasets are still lacking. To address this gap, we survey a systematic\ntaxonomy of information diffusion tasks and datasets based on the \"5W Model\"\nframework. We first categorize the information diffusion tasks into ten\nsubtasks with definitions and datasets analysis, from three main tasks of\ninformation diffusion prediction, social bot detection, and misinformation\ndetection. We also collect the publicly available dataset repository of\ninformation diffusion tasks with the available links and compare them based on\nsix attributes affiliated to users and content: user information, social\nnetwork, bot label, propagation content, propagation network, and veracity\nlabel. In addition, we discuss the limitations and future directions of current\ndatasets and research topics to advance the future development of information\ndiffusion. The dataset repository can be accessed at our website\nhttps://github.com/fuxiaG/Information-Diffusion-Datasets.\n","authors":["Fuxia Guo","Xiaowen Wang","Yanwei Xie","Zehao Wang","Jingqiu Li","Lanjun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11538v1","updated":"2025-06-13T07:44:42Z","published":"2025-06-13T07:44:42Z","title":"Dual-View Disentangled Multi-Intent Learning for Enhanced Collaborative\n  Filtering","summary":"  Disentangling user intentions from implicit feedback has become a promising\nstrategy to enhance recommendation accuracy and interpretability. Prior methods\noften model intentions independently and lack explicit supervision, thus\nfailing to capture the joint semantics that drive user-item interactions. To\naddress these limitations, we propose DMICF, a unified framework that\nexplicitly models interaction-level intent alignment while leveraging\nstructural signals from both user and item perspectives. DMICF adopts a\ndual-view architecture that jointly encodes user-item interaction graphs from\nboth sides, enabling bidirectional information fusion. This design enhances\nrobustness under data sparsity by allowing the structural redundancy of one\nview to compensate for the limitations of the other. To model fine-grained\nuser-item compatibility, DMICF introduces an intent interaction encoder that\nperforms sub-intent alignment within each view, uncovering shared semantic\nstructures that underlie user decisions. This localized alignment enables\nadaptive refinement of intent embeddings based on interaction context, thus\nimproving the model's generalization and expressiveness, particularly in\nlong-tail scenarios. Furthermore, DMICF integrates an intent-aware scoring\nmechanism that aggregates compatibility signals from matched intent pairs\nacross user and item subspaces, enabling personalized prediction grounded in\nsemantic congruence rather than entangled representations. To facilitate\nsemantic disentanglement, we design a discriminative training signal via\nmulti-negative sampling and softmax normalization, which pulls together\nsemantically aligned intent pairs while pushing apart irrelevant or noisy ones.\nExtensive experiments demonstrate that DMICF consistently delivers robust\nperformance across datasets with diverse interaction distributions.\n","authors":["Shanfan Zhang","Yongyi Lin","Yuan Rao","Chenlong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11538v1.pdf","comment":"26 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.10488v2","updated":"2025-06-13T07:32:56Z","published":"2025-06-12T08:42:19Z","title":"Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation","summary":"  In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six\nhundred and eighty-five pages specifically designed to benchmark Optical Music\nRecognition (OMR) research. SMB encompasses a diverse array of musical\ntextures, including monophony, pianoform, quartet, and others, all encoded in\nCommon Western Modern Notation using the Humdrum **kern format. Alongside SMB,\nwe introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored\nexplicitly for evaluating OMR performance. OMR-NED builds upon the widely-used\nSymbol Error Rate (SER), offering a fine-grained and detailed error analysis\nthat covers individual musical elements such as note heads, beams, pitches,\naccidentals, and other critical notation features. The resulting numeric score\nprovided by OMR-NED facilitates clear comparisons, enabling researchers and\nend-users alike to identify optimal OMR approaches. Our work thus addresses a\nlong-standing gap in OMR evaluation, and we support our contributions with\nbaseline experiments using standardized SMB dataset splits for training and\nassessing state-of-the-art methods.\n","authors":["Juan C. Martinez-Sevilla","Joan Cerveto-Serrano","Noelia Luna","Greg Chapman","Craig Sapp","David Rizo","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2506.10488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11502v1","updated":"2025-06-13T06:58:25Z","published":"2025-06-13T06:58:25Z","title":"A Reference Model and Patterns for Production Event Data Enrichment","summary":"  With the advent of digital transformation, organisations are increasingly\ngenerating large volumes of data through the execution of various processes\nacross disparate systems. By integrating data from these heterogeneous sources,\nit becomes possible to derive new insights essential for tasks such as\nmonitoring and analysing process performance. Typically, this information is\nextracted during a data pre-processing or engineering phase. However, this step\nis often performed in an ad-hoc manner and is time-consuming and\nlabour-intensive. To streamline this process, we introduce a reference model\nand a collection of patterns designed to enrich production event data. The\nreference model provides a standard way for storing and extracting production\nevent data. The patterns describe common information extraction tasks and how\nsuch tasks can be automated effectively. The reference model is developed by\ncombining the ISA-95 industry standard with the Event Knowledge Graph\nformalism. The patterns are developed based on empirical observations from\nevent data sets originating in manufacturing processes and are formalised using\nthe reference model. We evaluate the relevance and applicability of these\npatterns by demonstrating their application to use cases.\n","authors":["Mark van der Pas","Remco Dijkman","Alp Akçay","Ivo Adan","John Walker"],"pdf_url":"https://arxiv.org/pdf/2506.11502v1.pdf","comment":"Extended version of the paper submitted to EDOC 2025"},{"id":"http://arxiv.org/abs/2411.06877v4","updated":"2025-06-13T06:52:31Z","published":"2024-11-11T11:17:35Z","title":"LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?","summary":"  Test collections are information-retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant manual-annotation effort, which often makes it very expensive and\ntime-consuming. Consequently, test collections can become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed using large language models (LLMs) to\ncompletely replace human assessors. However, while LLMs correlate to some\nextent with human judgments, their predictions are not perfect and often show\nbias. Thus, a complete replacement with LLMs is considered too risky and not\nfully reliable.\n  In this paper, we propose LLM-Assisted Relevance Assessments (LARA), an\neffective method to balance manual annotations with LLM annotations, helping\nbuild a rich and reliable test collection even under a low budget. We use the\nLLM's predicted relevance probabilities to select the most profitable documents\nfor manual annotation under a budget constraint. Guided by theoretical\nreasoning, LARA actively learns to calibrate the LLM's predicted relevance\nprobabilities, directing the human-annotation process. Then, using the\ncalibration model learned from the limited manual annotations, LARA debiases\nthe LLM predictions to annotate the remaining non-assessed data. Experiments on\nTREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and TREC-COVID datasets show\nthat LARA outperforms alternative solutions under almost any budget constraint.\nWhile the community debates humans versus LLMs in relevance assessments, we\ncontend that, given the same amount of human effort, it is reasonable to\nleverage LLMs.\n","authors":["Rikiya Takehi","Ellen M. Voorhees","Tetsuya Sakai","Ian Soboroff"],"pdf_url":"https://arxiv.org/pdf/2411.06877v4.pdf","comment":"11 pages. Accepted at SIGIR 2025 (48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval)"},{"id":"http://arxiv.org/abs/2506.11452v1","updated":"2025-06-13T04:03:09Z","published":"2025-06-13T04:03:09Z","title":"Leveraging Reference Documents for Zero-Shot Ranking via Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional performance in the\ntask of text ranking for information retrieval. While Pointwise ranking\napproaches offer computational efficiency by scoring documents independently,\nthey often yield biased relevance estimates due to the lack of inter-document\ncomparisons. In contrast, Pairwise methods improve ranking accuracy by\nexplicitly comparing document pairs, but suffer from substantial computational\noverhead with quadratic complexity ($O(n^2)$). To address this tradeoff, we\npropose \\textbf{RefRank}, a simple and effective comparative ranking method\nbased on a fixed reference document. Instead of comparing all document pairs,\nRefRank prompts the LLM to evaluate each candidate relative to a shared\nreference anchor. By selecting the reference anchor that encapsulates the core\nquery intent, RefRank implicitly captures relevance cues, enabling indirect\ncomparison between documents via this common anchor. This reduces computational\ncost to linear time ($O(n)$) while importantly, preserving the advantages of\ncomparative evaluation. To further enhance robustness, we aggregate multiple\nRefRank outputs using a weighted averaging scheme across different reference\nchoices. Experiments on several benchmark datasets and with various LLMs show\nthat RefRank significantly outperforms Pointwise baselines and could achieve\nperformance at least on par with Pairwise approaches with a significantly lower\ncomputational cost.\n","authors":["Jieran Li","Xiuyuan Hu","Yang Zhao","Shengyao Zhuang","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11421v1","updated":"2025-06-13T02:39:21Z","published":"2025-06-13T02:39:21Z","title":"Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems","summary":"  With the rapid growth of Internet services, recommendation systems play a\ncentral role in delivering personalized content. Faced with massive user\nrequests and complex model architectures, the key challenge for real-time\nrecommendation systems is how to reduce inference latency and increase system\nthroughput without sacrificing recommendation quality. This paper addresses the\nhigh computational cost and resource bottlenecks of deep learning models in\nreal-time settings by proposing a combined set of modeling- and system-level\nacceleration and optimization strategies. At the model level, we dramatically\nreduce parameter counts and compute requirements through lightweight network\ndesign, structured pruning, and weight quantization. At the system level, we\nintegrate multiple heterogeneous compute platforms and high-performance\ninference libraries, and we design elastic inference scheduling and\nload-balancing mechanisms based on real-time load characteristics. Experiments\nshow that, while maintaining the original recommendation accuracy, our methods\ncut latency to less than 30% of the baseline and more than double system\nthroughput, offering a practical solution for deploying large-scale online\nrecommendation services.\n","authors":["Junli Shao","Jing Dong","Dingzhou Wang","Kowei Shih","Dannier Li","Chengrui Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.11421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17844v3","updated":"2025-06-13T01:36:23Z","published":"2024-04-27T09:44:56Z","title":"Towards Robust Recommendation: A Review and an Adversarial Robustness\n  Evaluation Library","summary":"  Recently, recommender system has achieved significant success. However, due\nto the openness of recommender systems, they remain vulnerable to malicious\nattacks. Additionally, natural noise in training data and issues such as data\nsparsity can also degrade the performance of recommender systems. Therefore,\nenhancing the robustness of recommender systems has become an increasingly\nimportant research topic. In this survey, we provide a comprehensive overview\nof the robustness of recommender systems. Based on our investigation, we\ncategorize the robustness of recommender systems into adversarial robustness\nand non-adversarial robustness. In the adversarial robustness, we introduce the\nfundamental principles and classical methods of recommender system adversarial\nattacks and defenses. In the non-adversarial robustness, we analyze\nnon-adversarial robustness from the perspectives of data sparsity, natural\nnoise, and data imbalance. Additionally, we summarize commonly used datasets\nand evaluation metrics for evaluating the robustness of recommender systems.\nFinally, we also discuss the current challenges in the field of recommender\nsystem robustness and potential future research directions. Additionally, to\nfacilitate fair and efficient evaluation of attack and defense methods in\nadversarial robustness, we propose an adversarial robustness evaluation\nlibrary--ShillingREC, and we conduct evaluations of basic attack models and\nrecommendation models. ShillingREC project is released at\nhttps://github.com/chengleileilei/ShillingREC.\n","authors":["Lei Cheng","Xiaowen Huang","Jitao Sang","Jian Yu"],"pdf_url":"https://arxiv.org/pdf/2404.17844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13128v2","updated":"2025-06-13T00:25:54Z","published":"2025-04-17T17:44:06Z","title":"FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents","summary":"  We introduce FreshStack, a holistic framework for automatically building\ninformation retrieval (IR) evaluation benchmarks by incorporating challenging\nquestions and answers. FreshStack conducts the following steps: (1) automatic\ncorpus collection from code and technical documentation, (2) nugget generation\nfrom community-asked questions and answers, and (3) nugget-level support,\nretrieving documents using a fusion of retrieval techniques and hybrid\narchitectures. We use FreshStack to build five datasets on fast-growing,\nrecent, and niche topics to ensure the tasks are sufficiently challenging. On\nFreshStack, existing retrieval models, when applied out-of-the-box,\nsignificantly underperform oracle approaches on all five topics, denoting\nplenty of headroom to improve IR quality. In addition, we identify cases where\nrerankers do not improve first-stage retrieval accuracy (two out of five\ntopics) and oracle context helps an LLM generator generate a high-quality RAG\nanswer. We hope FreshStack will facilitate future work toward constructing\nrealistic, scalable, and uncontaminated IR and RAG evaluation benchmarks.\n","authors":["Nandan Thakur","Jimmy Lin","Sam Havens","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2504.13128v2.pdf","comment":"21 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.18251v2","updated":"2025-06-13T21:35:13Z","published":"2024-10-09T16:35:41Z","title":"Context-Augmented Code Generation Using Programming Knowledge Graphs","summary":"  Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly\nimproved code generation, but, they frequently face difficulties when dealing\nwith challenging and complex problems. Retrieval-Augmented Generation (RAG)\naddresses this issue by retrieving and integrating external knowledge at the\ninference time. However, retrieval models often fail to find most relevant\ncontext, and generation models, with limited context capacity, can hallucinate\nwhen given irrelevant data. We present a novel framework that leverages a\nProgramming Knowledge Graph (PKG) to semantically represent and retrieve code.\nThis approach enables fine-grained code retrieval by focusing on the most\nrelevant segments while reducing irrelevant context through a tree-pruning\ntechnique. PKG is coupled with a re-ranking mechanism to reduce even more\nhallucinations by selectively integrating non-RAG solutions. We propose two\nretrieval approaches-block-wise and function-wise-based on the PKG, optimizing\ncontext granularity. Evaluations on the HumanEval and MBPP benchmarks show our\nmethod improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art\nmodels by up to 34% on MBPP. Our contributions include PKG-based retrieval,\ntree pruning to enhance retrieval precision, a re-ranking method for robust\nsolution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic\ncode augmentation with relevant comments and docstrings.\n","authors":["Iman Saberi","Fatemeh Fard"],"pdf_url":"https://arxiv.org/pdf/2410.18251v2.pdf","comment":"20 pages, Conference"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2506.12015v1","updated":"2025-06-13T17:59:58Z","published":"2025-06-13T17:59:58Z","title":"EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction","summary":"  Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.\n","authors":["Hsi-Che Lin","Yu-Chu Yu","Kai-Po Chang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2506.12015v1.pdf","comment":"Under review. Project page: https://hsi-che-lin.github.io/EMLoC/"},{"id":"http://arxiv.org/abs/2506.12014v1","updated":"2025-06-13T17:59:39Z","published":"2025-06-13T17:59:39Z","title":"code_transformed: The Influence of Large Language Models on Code","summary":"  Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.\n","authors":["Yuliang Xu","Siming Huang","Mingmeng Geng","Yao Wan","Xuanhua Shi","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2506.12014v1.pdf","comment":"We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code"},{"id":"http://arxiv.org/abs/2506.06266v3","updated":"2025-06-13T17:58:55Z","published":"2025-06-06T17:48:23Z","title":"Cartridges: Lightweight and general-purpose long context representations\n  via self-study","summary":"  Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.\n","authors":["Sabri Eyuboglu","Ryan Ehrlich","Simran Arora","Neel Guha","Dylan Zinsley","Emily Liu","Will Tennien","Atri Rudra","James Zou","Azalia Mirhoseini","Christopher Re"],"pdf_url":"https://arxiv.org/pdf/2506.06266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12007v1","updated":"2025-06-13T17:56:49Z","published":"2025-06-13T17:56:49Z","title":"SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution\n  Shifts","summary":"  Neural surrogates for Partial Differential Equations (PDEs) often suffer\nsignificant performance degradation when evaluated on unseen problem\nconfigurations, such as novel material types or structural dimensions.\nMeanwhile, Domain Adaptation (DA) techniques have been widely used in vision\nand language processing to generalize from limited information about unseen\nconfigurations. In this work, we address this gap through two focused\ncontributions. First, we introduce SIMSHIFT, a novel benchmark dataset and\nevaluation suite composed of four industrial simulation tasks: hot rolling,\nsheet metal forming, electric motor design and heatsink design. Second, we\nextend established domain adaptation methods to state of the art neural\nsurrogates and systematically evaluate them. These approaches use parametric\ndescriptions and ground truth simulations from multiple source configurations,\ntogether with only parametric descriptions from target configurations. The goal\nis to accurately predict target simulations without access to ground truth\nsimulation data. Extensive experiments on SIMSHIFT highlight the challenges of\nout of distribution neural surrogate modeling, demonstrate the potential of DA\nin simulation, and reveal open problems in achieving robust neural surrogates\nunder distribution shifts in industrially relevant scenarios. Our codebase is\navailable at https://github.com/psetinek/simshift\n","authors":["Paul Setinek","Gianluca Galletti","Thomas Gross","Dominik Schnürer","Johannes Brandstetter","Werner Zellinger"],"pdf_url":"https://arxiv.org/pdf/2506.12007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12000v1","updated":"2025-06-13T17:54:42Z","published":"2025-06-13T17:54:42Z","title":"An Efficient Compression of Deep Neural Network Checkpoints Based on\n  Prediction and Context Modeling","summary":"  This paper is dedicated to an efficient compression of weights and optimizer\nstates (called checkpoints) obtained at different stages during a neural\nnetwork training process. First, we propose a prediction-based compression\napproach, where values from the previously saved checkpoint are used for\ncontext modeling in arithmetic coding. Second, in order to enhance the\ncompression performance, we also propose to apply pruning and quantization of\nthe checkpoint values. Experimental results show that our approach achieves\nsubstantial bit size reduction, while enabling near-lossless training recovery\nfrom restored checkpoints, preserving the model's performance and making it\nsuitable for storage-limited environments.\n","authors":["Yuriy Kim","Evgeny Belyaev"],"pdf_url":"https://arxiv.org/pdf/2506.12000v1.pdf","comment":"IEEE NW Russia Young Researchers in Electrical and Electronic\n  Engineering Conference (EIConRusNW)"},{"id":"http://arxiv.org/abs/2506.11997v1","updated":"2025-06-13T17:51:37Z","published":"2025-06-13T17:51:37Z","title":"pLSTM: parallelizable Linear Source Transition Mark networks","summary":"  Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.\n","authors":["Korbinian Pöppel","Richard Freinschlag","Thomas Schmied","Wei Lin","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2506.11997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11994v1","updated":"2025-06-13T17:49:25Z","published":"2025-06-13T17:49:25Z","title":"Spectral Estimation with Free Decompression","summary":"  Computing eigenvalues of very large matrices is a critical task in many\nmachine learning applications, including the evaluation of log-determinants,\nthe trace of matrix functions, and other important metrics. As datasets\ncontinue to grow in scale, the corresponding covariance and kernel matrices\nbecome increasingly large, often reaching magnitudes that make their direct\nformation impractical or impossible. Existing techniques typically rely on\nmatrix-vector products, which can provide efficient approximations, if the\nmatrix spectrum behaves well. However, in settings like distributed learning,\nor when the matrix is defined only indirectly, access to the full data set can\nbe restricted to only very small sub-matrices of the original matrix. In these\ncases, the matrix of nominal interest is not even available as an implicit\noperator, meaning that even matrix-vector products may not be available. In\nsuch settings, the matrix is \"impalpable,\" in the sense that we have access to\nonly masked snapshots of it. We draw on principles from free probability theory\nto introduce a novel method of \"free decompression\" to estimate the spectrum of\nsuch matrices. Our method can be used to extrapolate from the empirical\nspectral densities of small submatrices to infer the eigenspectrum of extremely\nlarge (impalpable) matrices (that we cannot form or even evaluate with full\nmatrix-vector products). We demonstrate the effectiveness of this approach\nthrough a series of examples, comparing its performance against known limiting\ndistributions from random matrix theory in synthetic settings, as well as\napplying it to submatrices of real-world datasets, matching them with their\nfull empirical eigenspectra.\n","authors":["Siavash Ameli","Chris van der Heide","Liam Hodgkinson","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2506.11994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11992v1","updated":"2025-06-13T17:48:50Z","published":"2025-06-13T17:48:50Z","title":"Compression Aware Certified Training","summary":"  Deep neural networks deployed in safety-critical, resource-constrained\nenvironments must balance efficiency and robustness. Existing methods treat\ncompression and certified robustness as separate goals, compromising either\nefficiency or safety. We propose CACTUS (Compression Aware Certified Training\nUsing network Sets), a general framework for unifying these objectives during\ntraining. CACTUS models maintain high certified accuracy even when compressed.\nWe apply CACTUS for both pruning and quantization and show that it effectively\ntrains models which can be efficiently compressed while maintaining high\naccuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy\nand certified performance for both pruning and quantization on a variety of\ndatasets and input specifications.\n","authors":["Changming Xu","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2506.11992v1.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.09026v2","updated":"2025-06-13T17:44:03Z","published":"2025-06-10T17:52:42Z","title":"e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs","summary":"  Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.\n","authors":["Amrith Setlur","Matthew Y. R. Yang","Charlie Snell","Jeremy Greer","Ian Wu","Virginia Smith","Max Simchowitz","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2506.09026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11982v1","updated":"2025-06-13T17:39:41Z","published":"2025-06-13T17:39:41Z","title":"Interpretable representation learning of quantum data enabled by\n  probabilistic variational autoencoders","summary":"  Interpretable machine learning is rapidly becoming a crucial tool for\nscientific discovery. Among existing approaches, variational autoencoders\n(VAEs) have shown promise in extracting the hidden physical features of some\ninput data, with no supervision nor prior knowledge of the system at study.\nYet, the ability of VAEs to create meaningful, interpretable representations\nrelies on their accurate approximation of the underlying probability\ndistribution of their input. When dealing with quantum data, VAEs must hence\naccount for its intrinsic randomness and complex correlations. While VAEs have\nbeen previously applied to quantum data, they have often neglected its\nprobabilistic nature, hindering the extraction of meaningful physical\ndescriptors. Here, we demonstrate that two key modifications enable VAEs to\nlearn physically meaningful latent representations: a decoder capable of\nfaithfully reproduce quantum states and a probabilistic loss tailored to this\ntask. Using benchmark quantum spin models, we identify regimes where standard\nmethods fail while the representations learned by our approach remain\nmeaningful and interpretable. Applied to experimental data from Rydberg atom\narrays, the model autonomously uncovers the phase structure without access to\nprior labels, Hamiltonian details, or knowledge of relevant order parameters,\nhighlighting its potential as an unsupervised and interpretable tool for the\nstudy of quantum systems.\n","authors":["Paulin de Schoulepnikoff","Gorka Muñoz-Gil","Hendrik Poulsen Nautrup","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2506.11982v1.pdf","comment":"Main text 10 pages, total document 16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.11981v1","updated":"2025-06-13T17:38:16Z","published":"2025-06-13T17:38:16Z","title":"Learning Before Filtering: Real-Time Hardware Learning at the Detector\n  Level","summary":"  Advances in sensor technology and automation have ushered in an era of data\nabundance, where the ability to identify and extract relevant information in\nreal time has become increasingly critical. Traditional filtering approaches,\nwhich depend on a priori knowledge, often struggle to adapt to dynamic or\nunanticipated data features. Machine learning offers a compelling\nalternative-particularly when training can occur directly at or near the\ndetector. This paper presents a digital hardware architecture designed for\nreal-time neural network training, specifically optimized for high-throughput\ndata ingestion. The design is described in an implementation-independent\nmanner, with detailed analysis of each architectural component and their\nperformance implications. Through system parameterization, the study explores\ntrade-offs between processing speed, model complexity, and hardware resource\nutilization. Practical examples illustrate how these parameters affect\napplicability across various use cases. A proof-of-concept implementation on an\nFPGA demonstrates in-situ training, confirming that computational accuracy is\npreserved relative to conventional software-based approaches. Moreover,\nresource estimates indicate that current-generation FPGAs can train networks of\napproximately 3,500 neurons per chip. The architecture is both scalable and\nadaptable, representing a significant advancement toward integrating learning\ndirectly within detector systems and enabling a new class of extreme-edge,\nreal-time information processing.\n","authors":["Boštjan Maček"],"pdf_url":"https://arxiv.org/pdf/2506.11981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11976v1","updated":"2025-06-13T17:34:05Z","published":"2025-06-13T17:34:05Z","title":"How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs","summary":"  Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning.\n","authors":["Constantin Venhoff","Ashkan Khakzar","Sonia Joseph","Philip Torr","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2506.11976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11973v1","updated":"2025-06-13T17:31:23Z","published":"2025-06-13T17:31:23Z","title":"Self-Regulating Cars: Automating Traffic Control in Free Flow Road\n  Networks","summary":"  Free-flow road networks, such as suburban highways, are increasingly\nexperiencing traffic congestion due to growing commuter inflow and limited\ninfrastructure. Traditional control mechanisms, such as traffic signals or\nlocal heuristics, are ineffective or infeasible in these high-speed,\nsignal-free environments. We introduce self-regulating cars, a reinforcement\nlearning-based traffic control protocol that dynamically modulates vehicle\nspeeds to optimize throughput and prevent congestion, without requiring new\nphysical infrastructure. Our approach integrates classical traffic flow theory,\ngap acceptance models, and microscopic simulation into a physics-informed RL\nframework. By abstracting roads into super-segments, the agent captures\nemergent flow dynamics and learns robust speed modulation policies from\ninstantaneous traffic observations. Evaluated in the high-fidelity PTV Vissim\nsimulator on a real-world highway network, our method improves total throughput\nby 5%, reduces average delay by 13%, and decreases total stops by 3% compared\nto the no-control setting. It also achieves smoother, congestion-resistant flow\nwhile generalizing across varied traffic patterns, demonstrating its potential\nfor scalable, ML-driven traffic management.\n","authors":["Ankit Bhardwaj","Rohail Asim","Sachin Chauhan","Yasir Zaki","Lakshminarayanan Subramanian"],"pdf_url":"https://arxiv.org/pdf/2506.11973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11967v1","updated":"2025-06-13T17:25:27Z","published":"2025-06-13T17:25:27Z","title":"Visual Pre-Training on Unlabeled Images using Reinforcement Learning","summary":"  In reinforcement learning (RL), value-based algorithms learn to associate\neach observation with the states and rewards that are likely to be reached from\nit. We observe that many self-supervised image pre-training methods bear\nsimilarity to this formulation: learning features that associate crops of\nimages with those of nearby views, e.g., by taking a different crop or color\naugmentation. In this paper, we complete this analogy and explore a method that\ndirectly casts pre-training on unlabeled image data like web crawls and video\nframes as an RL problem. We train a general value function in a dynamical\nsystem where an agent transforms an image by changing the view or adding image\naugmentations. Learning in this way resembles crop-consistency\nself-supervision, but through the reward function, offers a simple lever to\nshape feature learning using curated images or weakly labeled captions when\nthey exist. Our experiments demonstrate improved representations when training\non unlabeled images in the wild, including video data like EpicKitchens, scene\ndata like COCO, and web-crawl data like CC12M.\n","authors":["Dibya Ghosh","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2506.11967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04285v2","updated":"2025-06-13T17:24:41Z","published":"2024-10-05T21:11:32Z","title":"MindFlayer SGD: Efficient Parallel SGD in the Presence of Heterogeneous\n  and Random Worker Compute Times","summary":"  We investigate the problem of minimizing the expectation of smooth nonconvex\nfunctions in a distributed setting with multiple parallel workers that are able\nto compute stochastic gradients. A significant challenge in this context is the\npresence of arbitrarily heterogeneous and stochastic compute times among\nworkers, which can severely degrade the performance of existing parallel\nstochastic gradient descent (SGD) methods. While some parallel SGD algorithms\nachieve optimal performance under deterministic but heterogeneous delays, their\neffectiveness diminishes when compute times are random - a scenario not\nexplicitly addressed in their design. To bridge this gap, we introduce\nMindFlayer SGD, a novel parallel SGD method specifically designed to handle\nstochastic and heterogeneous compute times. Through theoretical analysis and\nempirical evaluation, we demonstrate that MindFlayer SGD consistently\noutperforms existing baselines, particularly in environments with heavy-tailed\nnoise. Our results highlight its robustness and scalability, making it a\ncompelling choice for large-scale distributed learning tasks.\n","authors":["Artavazd Maranjyan","Omar Shaikh Omar","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2410.04285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07833v2","updated":"2025-06-13T17:24:38Z","published":"2025-06-09T14:55:00Z","title":"Improving Large Language Models with Concept-Aware Fine-Tuning","summary":"  Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm\n","authors":["Michael K. Chen","Xikun Zhang","Jiaxing Huang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.07833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11957v1","updated":"2025-06-13T17:07:30Z","published":"2025-06-13T17:07:30Z","title":"Automated Treatment Planning for Interstitial HDR Brachytherapy for\n  Locally Advanced Cervical Cancer using Deep Reinforcement Learning","summary":"  High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of\nlocally advanced cervical cancer but remains highly dependent on manual\ntreatment planning expertise. The objective of this study is to develop a fully\nautomated HDR brachytherapy planning framework that integrates reinforcement\nlearning (RL) and dose-based optimization to generate clinically acceptable\ntreatment plans with improved consistency and efficiency. We propose a\nhierarchical two-stage autoplanning framework. In the first stage, a deep\nQ-network (DQN)-based RL agent iteratively selects treatment planning\nparameters (TPPs), which control the trade-offs between target coverage and\norgan-at-risk (OAR) sparing. The agent's state representation includes both\ndose-volume histogram (DVH) metrics and current TPP values, while its reward\nfunction incorporates clinical dose objectives and safety constraints,\nincluding D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder,\nrectum, sigmoid, small bowel, and large bowel). In the second stage, a\ncustomized Adam-based optimizer computes the corresponding dwell time\ndistribution for the selected TPPs using a clinically informed loss function.\nThe framework was evaluated on a cohort of patients with complex applicator\ngeometries. The proposed framework successfully learned clinically meaningful\nTPP adjustments across diverse patient anatomies. For the unseen test patients,\nthe RL-based automated planning method achieved an average score of 93.89%,\noutperforming the clinical plans which averaged 91.86%. These findings are\nnotable given that score improvements were achieved while maintaining full\ntarget coverage and reducing CTV hot spots in most cases.\n","authors":["Mohammadamin Moradi","Runyu Jiang","Yingzi Liu","Malvern Madondo","Tianming Wu","James J. Sohn","Xiaofeng Yang","Yasmin Hasan","Zhen Tian"],"pdf_url":"https://arxiv.org/pdf/2506.11957v1.pdf","comment":"12 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.01220v5","updated":"2025-06-13T16:58:26Z","published":"2025-02-03T10:24:55Z","title":"Factual Knowledge in Language Models: Robustness and Anomalies under\n  Simple Temporal Context Variations","summary":"  This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\nLMs' ability to distinguish is analyzed along two dimensions: the distance of\nthe incorrect context from the validity period and the granularity of the\ncontext. To this end, a dataset called TimeStress is introduced, enabling the\nevaluation of 18 diverse LMs. Results reveal that the best LM achieves a\nperfect distinction for only 11% of the studied facts, with errors, certainly\nrare, but critical that humans would not make. This work highlights the\nlimitations of current LMs in temporal representation.\n","authors":["Hichem Ammar Khodja","Frédéric Béchet","Quentin Brabant","Alexis Nasr","Gwénolé Lecorvé"],"pdf_url":"https://arxiv.org/pdf/2502.01220v5.pdf","comment":"preprint v5, accepted for publication at ACL 2025 - L2M2 Workshop"},{"id":"http://arxiv.org/abs/2407.08970v4","updated":"2025-06-13T16:53:16Z","published":"2024-07-12T03:40:13Z","title":"Self-interpreting Adversarial Images","summary":"  We introduce a new type of indirect, cross-modal injection attacks against\nvisual language models that enable creation of self-interpreting images. These\nimages contain hidden \"meta-instructions\" that control how models answer users'\nquestions about the image and steer models' outputs to express an\nadversary-chosen style, sentiment, or point of view.\n  Self-interpreting images act as soft prompts, conditioning the model to\nsatisfy the adversary's (meta-)objective while still producing answers based on\nthe image's visual content. Meta-instructions are thus a stronger form of\nprompt injection. Adversarial images look natural and the model's answers are\ncoherent and plausible, yet they also follow the adversary-chosen\ninterpretation, e.g., political spin, or even objectives that are not\nachievable with explicit text instructions.\n  We evaluate the efficacy of self-interpreting images for a variety of models,\ninterpretations, and user prompts. We describe how these attacks could cause\nharm by enabling creation of self-interpreting content that carries spam,\nmisinformation, or spin. Finally, we discuss defenses.\n","authors":["Tingwei Zhang","Collin Zhang","John X. Morris","Eugene Bagdasarian","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2407.08970v4.pdf","comment":"in USENIX Security 2025"},{"id":"http://arxiv.org/abs/2505.21657v2","updated":"2025-06-13T16:43:15Z","published":"2025-05-27T18:32:38Z","title":"Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations","summary":"  Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.\n","authors":["Zeinab Dehghani","Mohammed Naveed Akram","Koorosh Aslansefat","Adil Khan"],"pdf_url":"https://arxiv.org/pdf/2505.21657v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.16277"},{"id":"http://arxiv.org/abs/2506.11938v1","updated":"2025-06-13T16:42:09Z","published":"2025-06-13T16:42:09Z","title":"Improving Large Language Model Safety with Contrastive Representation\n  Learning","summary":"  Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense\n","authors":["Samuel Simko","Mrinmaya Sachan","Bernhard Schölkopf","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2506.11938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11936v1","updated":"2025-06-13T16:38:51Z","published":"2025-06-13T16:38:51Z","title":"Bubble Dynamics Transformer: Microrheology at Ultra-High Strain Rates","summary":"  Laser-induced inertial cavitation (LIC)-where microscale vapor bubbles\nnucleate due to a focused high-energy pulsed laser and then violently collapse\nunder surrounding high local pressures-offers a unique opportunity to\ninvestigate soft biological material mechanics at extremely high strain rates\n(>1000 1/s). Traditional rheological tools are often limited in these regimes\nby loading speed, resolution, or invasiveness. Here we introduce novel machine\nlearning (ML) based microrheological frameworks that leverage LIC to\ncharacterize the viscoelastic properties of biological materials at ultra-high\nstrain rates. We utilize ultra-high-speed imaging to capture time-resolved\nbubble radius dynamics during LIC events in various soft viscoelastic\nmaterials. These bubble radius versus time measurements are then analyzed using\na newly developed Bubble Dynamics Transformer (BDT), a neural network trained\non physics-based simulation data. The BDT accurately infers material\nviscoelastic parameters, eliminating the need for iterative fitting or complex\ninversion processes. This enables fast, accurate, and non-contact\ncharacterization of soft materials under extreme loading conditions, with\nsignificant implications for biomedical applications and materials science.\n","authors":["Lehu Bu","Zhaohan Yu","Shaoting Lin","Jan N. Fuhg","Jin Yang"],"pdf_url":"https://arxiv.org/pdf/2506.11936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11928v1","updated":"2025-06-13T16:29:09Z","published":"2025-06-13T16:29:09Z","title":"LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?","summary":"  Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.\n","authors":["Zihan Zheng","Zerui Cheng","Zeyu Shen","Shang Zhou","Kaiyuan Liu","Hansen He","Dongruixuan Li","Stanley Wei","Hangyi Hao","Jianzhu Yao","Peiyao Sheng","Zixuan Wang","Wenhao Chai","Aleksandra Korolova","Peter Henderson","Sanjeev Arora","Pramod Viswanath","Jingbo Shang","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2506.11928v1.pdf","comment":"Project Page at https://livecodebenchpro.com/"},{"id":"http://arxiv.org/abs/2506.11925v1","updated":"2025-06-13T16:24:28Z","published":"2025-06-13T16:24:28Z","title":"Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference","summary":"  Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.\n","authors":["M. Manzour","Catherine M. Elias","Omar M. Shehata","R. Izquierdo","M. A. Sotelo"],"pdf_url":"https://arxiv.org/pdf/2506.11925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07650v2","updated":"2025-06-13T16:17:34Z","published":"2025-02-11T15:39:47Z","title":"Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold","summary":"  Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to efficiently approximate the natural\ngradient of the KL divergence without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms. The code of the proposed algorithms can be found at\nhttps://github.com/anewgithubname/iNGD.\n","authors":["Song Liu","Leyang Wang","Yakun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07650v2.pdf","comment":"UAI2025"},{"id":"http://arxiv.org/abs/2501.11651v2","updated":"2025-06-13T16:15:45Z","published":"2025-01-20T18:33:33Z","title":"T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.\n","authors":["Zhenyu Hou","Xin Lv","Rui Lu","Jiajie Zhang","Yujiang Li","Zijun Yao","Juanzi Li","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2501.11651v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2401.06122v3","updated":"2025-06-13T16:13:55Z","published":"2024-01-11T18:57:17Z","title":"Manipulating Feature Visualizations with Gradient Slingshots","summary":"  Feature Visualization (FV) is a widely used technique for interpreting the\nconcepts learned by Deep Neural Networks (DNNs), which synthesizes input\npatterns that maximally activate a given feature. Despite its popularity, the\ntrustworthiness of FV explanations has received limited attention. In this\npaper, we introduce a novel method, Gradient Slingshots, that enables\nmanipulation of FV without modifying the model architecture or significantly\ndegrading its performance. By shaping new trajectories in the off-distribution\nregions of the activation landscape of a feature, we coerce the optimization\nprocess to converge in a predefined visualization. We evaluate our approach on\nseveral DNN architectures, demonstrating its ability to replace faithfuls FV\nwith arbitrary targets. These results expose a critical vulnerability: auditors\nrelying solely on FV may accept entirely fabricated explanations. To mitigate\nthis risk, we propose a straightforward defense and quantitatively demonstrate\nits effectiveness.\n","authors":["Dilyara Bareeva","Marina M. -C. Höhne","Alexander Warnecke","Lukas Pirch","Klaus-Robert Müller","Konrad Rieck","Sebastian Lapuschkin","Kirill Bykov"],"pdf_url":"https://arxiv.org/pdf/2401.06122v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11912v1","updated":"2025-06-13T16:06:47Z","published":"2025-06-13T16:06:47Z","title":"Breaking Habits: On the Role of the Advantage Function in Learning\n  Causal State Representations","summary":"  Recent work has shown that reinforcement learning agents can develop policies\nthat exploit spurious correlations between rewards and observations. This\nphenomenon, known as policy confounding, arises because the agent's policy\ninfluences both past and future observation variables, creating a feedback loop\nthat can hinder the agent's ability to generalize beyond its usual\ntrajectories. In this paper, we show that the advantage function, commonly used\nin policy gradient methods, not only reduces the variance of gradient estimates\nbut also mitigates the effects of policy confounding. By adjusting action\nvalues relative to the state representation, the advantage function downweights\nstate-action pairs that are more likely under the current policy, breaking\nspurious correlations and encouraging the agent to focus on causal factors. We\nprovide both analytical and empirical evidence demonstrating that training with\nthe advantage function leads to improved out-of-trajectory performance.\n","authors":["Miguel Suau"],"pdf_url":"https://arxiv.org/pdf/2506.11912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17834v2","updated":"2025-06-13T16:00:46Z","published":"2024-10-23T12:53:58Z","title":"Non-intrusive Speech Quality Assessment with Diffusion Models Trained on\n  Clean Speech","summary":"  Diffusion models have found great success in generating high quality, natural\nsamples of speech, but their potential for density estimation for speech has so\nfar remained largely unexplored. In this work, we leverage an unconditional\ndiffusion model trained only on clean speech for the assessment of speech\nquality. We show that the quality of a speech utterance can be assessed by\nestimating the likelihood of a corresponding sample in the terminating Gaussian\ndistribution, obtained via a deterministic noising process. The resulting\nmethod is purely unsupervised, trained only on clean speech, and therefore does\nnot rely on annotations. Our diffusion-based approach leverages clean speech\npriors to assess quality based on how the input relates to the learned\ndistribution of clean data. Our proposed log-likelihoods show promising\nresults, correlating well with intrusive speech quality metrics and showing the\nbest correlation with human scores in a listening experiment.\n","authors":["Danilo de Oliveira","Julius Richter","Jean-Marie Lemercier","Simon Welker","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2410.17834v2.pdf","comment":"Accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.11908v1","updated":"2025-06-13T15:58:05Z","published":"2025-06-13T15:58:05Z","title":"Spectra-to-Structure and Structure-to-Spectra Inference Across the\n  Periodic Table","summary":"  X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance.\n","authors":["Yufeng Wang","Peiyao Wang","Lu Ma","Yuewei Lin","Qun Liu","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2506.11908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11904v1","updated":"2025-06-13T15:53:17Z","published":"2025-06-13T15:53:17Z","title":"Convergence of Momentum-Based Optimization Algorithms with Time-Varying\n  Parameters","summary":"  In this paper, we present a unified algorithm for stochastic optimization\nthat makes use of a \"momentum\" term; in other words, the stochastic gradient\ndepends not only on the current true gradient of the objective function, but\nalso on the true gradient at the previous iteration. Our formulation includes\nthe Stochastic Heavy Ball (SHB) and the Stochastic Nesterov Accelerated\nGradient (SNAG) algorithms as special cases. In addition, in our formulation,\nthe momentum term is allowed to vary as a function of time (i.e., the iteration\ncounter). The assumptions on the stochastic gradient are the most general in\nthe literature, in that it can be biased, and have a conditional variance that\ngrows in an unbounded fashion as a function of time. This last feature is\ncrucial in order to make the theory applicable to \"zero-order\" methods, where\nthe gradient is estimated using just two function evaluations.\n  We present a set of sufficient conditions for the convergence of the unified\nalgorithm. These conditions are natural generalizations of the familiar\nRobbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochastic\ngradient descent. We also analyze another method from the literature for the\nSHB algorithm with a time-varying momentum parameter, and show that it is\nimpracticable.\n","authors":["Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2506.11904v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2506.11902v1","updated":"2025-06-13T15:52:37Z","published":"2025-06-13T15:52:37Z","title":"TreeRL: LLM Reinforcement Learning with On-Policy Tree Search","summary":"  Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.\n","authors":["Zhenyu Hou","Ziniu Hu","Yujiang Li","Rui Lu","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.11902v1.pdf","comment":"Accepted to ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2506.11901v1","updated":"2025-06-13T15:52:07Z","published":"2025-06-13T15:52:07Z","title":"A Neural Rejection System Against Universal Adversarial Perturbations in\n  Radio Signal Classification","summary":"  Advantages of deep learning over traditional methods have been demonstrated\nfor radio signal classification in the recent years. However, various\nresearchers have discovered that even a small but intentional feature\nperturbation known as adversarial examples can significantly deteriorate the\nperformance of the deep learning based radio signal classification. Among\nvarious kinds of adversarial examples, universal adversarial perturbation has\ngained considerable attention due to its feature of being data independent,\nhence as a practical strategy to fool the radio signal classification with a\nhigh success rate. Therefore, in this paper, we investigate a defense system\ncalled neural rejection system to propose against universal adversarial\nperturbations, and evaluate its performance by generating white-box universal\nadversarial perturbations. We show that the proposed neural rejection system is\nable to defend universal adversarial perturbations with significantly higher\naccuracy than the undefended deep neural network.\n","authors":["Lu Zhang","Sangarapillai Lambotharan","Gan Zheng","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2506.11901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09692v2","updated":"2025-06-13T15:49:13Z","published":"2025-02-13T17:58:07Z","title":"AB-UPT: Scaling Neural CFD Surrogates for High-Fidelity Automotive\n  Aerodynamics Simulations via Anchored-Branched Universal Physics Transformers","summary":"  Recent advances in neural surrogate modeling offer the potential for\ntransformative innovations in applications such as automotive aerodynamics.\nYet, industrial-scale problems often involve volumetric meshes with cell counts\nreaching the 100 millions, presenting major scalability challenges. Complex\ngeometries further complicate modeling through intricate surface-volume\ninteractions, while quantities such as vorticity are highly nonlinear and must\nsatisfy strict divergence-free constraints. To address these requirements, we\nintroduce AB-UPT as a novel modeling scheme for building neural surrogates for\nCFD simulations. AB-UPT is designed to: (i) decouple geometry encoding and\nprediction tasks via multi-branch operators; (ii) enable scalability to\nhigh-resolution outputs via neural simulation in a low-dimensional latent\nspace, coupled with anchored neural field decoders to predict high-fidelity\noutputs; (iii) enforce physics consistency by a novel divergence-free\nformulation. We show that AB-UPT yields state-of-the-art predictive accuracy of\nsurface and volume fields on automotive CFD simulations ranging from 33\nthousand up to 150 million mesh cells. Furthermore, our anchored neural field\narchitecture enables the enforcement of hard physical constraints on the\nphysics predictions without degradation in performance, exemplified by modeling\ndivergence-free vorticity fields. Notably, the proposed models can be trained\non a single GPU in less than a day and predict industry-standard surface and\nvolume fields within seconds. Additionally, we show that the flexible design of\nour method enables neural simulation from a CAD geometry alone, omitting the\nneed for costly CFD meshing procedures.\n","authors":["Benedikt Alkin","Maurits Bleeker","Richard Kurle","Tobias Kronlachner","Reinhard Sonnleitner","Matthias Dorfer","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.09692v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.08088v2","updated":"2025-06-13T15:48:03Z","published":"2025-05-12T21:46:36Z","title":"Graph-Based Floor Separation Using Node Embeddings and Clustering of\n  WiFi Trajectories","summary":"  Indoor positioning systems (IPSs) are increasingly vital for location-based\nservices in complex multi-storey environments. This study proposes a novel\ngraph-based approach for floor separation using Wi-Fi fingerprint trajectories,\naddressing the challenge of vertical localization in indoor settings. We\nconstruct a graph where nodes represent Wi-Fi fingerprints, and edges are\nweighted by signal similarity and contextual transitions. Node2Vec is employed\nto generate low-dimensional embeddings, which are subsequently clustered using\nK-means to identify distinct floors. Evaluated on the Huawei University\nChallenge 2021 dataset, our method outperforms traditional community detection\nalgorithms, achieving an accuracy of 68.97\\%, an F1-score of 61.99\\%, and an\nAdjusted Rand Index of 57.19\\%. By publicly releasing the preprocessed dataset\nand implementation code, this work contributes to advancing research in indoor\npositioning. The proposed approach demonstrates robustness to signal noise and\narchitectural complexities, offering a scalable solution for floor-level\nlocalization.\n","authors":["Rabia Yasa Kostas","Kahraman Kostas"],"pdf_url":"https://arxiv.org/pdf/2505.08088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11898v1","updated":"2025-06-13T15:44:14Z","published":"2025-06-13T15:44:14Z","title":"Scalable Generalized Bayesian Online Neural Network Training for\n  Sequential Decision Making","summary":"  We introduce scalable algorithms for online learning and generalized Bayesian\ninference of neural network parameters, designed for sequential decision making\ntasks. Our methods combine the strengths of frequentist and Bayesian filtering,\nwhich include fast low-rank updates via a block-diagonal approximation of the\nparameter error covariance, and a well-defined posterior predictive\ndistribution that we use for decision making. More precisely, our main method\nupdates a low-rank error covariance for the hidden layers parameters, and a\nfull-rank error covariance for the final layer parameters. Although this\ncharacterizes an improper posterior, we show that the resulting posterior\npredictive distribution is well-defined. Our methods update all network\nparameters online, with no need for replay buffers or offline retraining. We\nshow, empirically, that our methods achieve a competitive tradeoff between\nspeed and accuracy on (non-stationary) contextual bandit problems and Bayesian\noptimization problems.\n","authors":["Gerardo Duran-Martin","Leandro Sánchez-Betancourt","Álvaro Cartea","Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2506.11898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04613v3","updated":"2025-06-13T15:43:02Z","published":"2025-06-05T04:10:52Z","title":"DeePoly: A High-Order Accuracy Scientific Machine Learning Framework for\n  Function Approximation and Solving PDEs","summary":"  Recently, machine learning methods have gained significant traction in\nscientific computing, particularly for solving Partial Differential Equations\n(PDEs). However, methods based on deep neural networks (DNNs) often lack\nconvergence guarantees and computational efficiency compared to traditional\nnumerical schemes. This work introduces DeePoly, a novel framework that\ntransforms the solution paradigm from pure non-convex parameter optimization to\na two-stage approach: first employing a DNN to capture complex global features,\nfollowed by linear space optimization with combined DNN-extracted features\n(Spotter) and polynomial basis functions (Sniper). This strategic combination\nleverages the complementary strengths of both methods -- DNNs excel at\napproximating complex global features (i.e., high-gradient features) and\nstabilize the polynomial approximation while polynomial bases provide\nhigh-precision local corrections with convergence guarantees. Theoretical\nanalysis and numerical experiments demonstrate that this approach significantly\nenhances both high-order accuracy and efficiency across diverse problem types\nwhile maintaining mesh-free and scheme-free properties. This paper also serves\nas a theoretical exposition for the open-source project DeePoly.\n","authors":["Li Liu","Heng Yong"],"pdf_url":"https://arxiv.org/pdf/2506.04613v3.pdf","comment":"for associated mpeg file, see http://github.com/bfly123/DeePoly"},{"id":"http://arxiv.org/abs/2506.11893v1","updated":"2025-06-13T15:39:54Z","published":"2025-06-13T15:39:54Z","title":"Measurement-aligned Flow for Inverse Problem","summary":"  Diffusion models provide a powerful way to incorporate complex prior\ninformation for solving inverse problems. However, existing methods struggle to\ncorrectly incorporate guidance from conflicting signals in the prior and\nmeasurement, especially in the challenging setting of non-Gaussian or unknown\nnoise. To bridge these gaps, we propose Measurement-Aligned Sampling (MAS), a\nnovel framework for linear inverse problem solving that can more flexibly\nbalance prior and measurement information. MAS unifies and extends existing\napproaches like DDNM and DAPS, and offers a new optimization perspective. MAS\ncan generalize to handle known Gaussian noise, unknown or non-Gaussian noise\ntypes. Extensive experiments show that MAS consistently outperforms\nstate-of-the-art methods across a range of tasks.\n","authors":["Shaorong Zhang","Rob Brekelmans","Yunshu Wu","Greg Ver Steeg"],"pdf_url":"https://arxiv.org/pdf/2506.11893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11892v1","updated":"2025-06-13T15:39:01Z","published":"2025-06-13T15:39:01Z","title":"Attention-based Adversarial Robust Distillation in Radio Signal\n  Classifications for Low-Power IoT Devices","summary":"  Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples.\n","authors":["Lu Zhang","Sangarapillai Lambotharan","Gan Zheng","Guisheng Liao","Basil AsSadhan","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2506.11892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11891v1","updated":"2025-06-13T15:38:41Z","published":"2025-06-13T15:38:41Z","title":"Understanding Input Selectivity in Mamba: Impact on Approximation Power,\n  Memorization, and Associative Recall Capacity","summary":"  State-Space Models (SSMs), and particularly Mamba, have recently emerged as a\npromising alternative to Transformers. Mamba introduces input selectivity to\nits SSM layer (S6) and incorporates convolution and gating into its block\ndefinition. While these modifications do improve Mamba's performance over its\nSSM predecessors, it remains largely unclear how Mamba leverages the additional\nfunctionalities provided by input selectivity, and how these interact with the\nother operations in the Mamba architecture. In this work, we demystify the role\nof input selectivity in Mamba, investigating its impact on function\napproximation power, long-term memorization, and associative recall\ncapabilities. In particular: (i) we prove that the S6 layer of Mamba can\nrepresent projections onto Haar wavelets, providing an edge over its Diagonal\nSSM (S4D) predecessor in approximating discontinuous functions commonly arising\nin practice; (ii) we show how the S6 layer can dynamically counteract memory\ndecay; (iii) we provide analytical solutions to the MQAR associative recall\ntask using the Mamba architecture with different mixers -- Mamba, Mamba-2, and\nS4D. We demonstrate the tightness of our theoretical constructions with\nempirical results on concrete tasks. Our findings offer a mechanistic\nunderstanding of Mamba and reveal opportunities for improvement.\n","authors":["Ningyuan Huang","Miguel Sarabia","Abhinav Moudgil","Pau Rodriguez","Luca Zappella","Federico Danieli"],"pdf_url":"https://arxiv.org/pdf/2506.11891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00797v2","updated":"2025-06-13T15:38:03Z","published":"2023-02-01T23:06:23Z","title":"Combining Deep Reinforcement Learning and Search with Generative Models\n  for Game-Theoretic Opponent Modeling","summary":"  Opponent modeling methods typically involve two crucial steps: building a\nbelief distribution over opponents' strategies, and exploiting this opponent\nmodel by playing a best response. However, existing approaches typically\nrequire domain-specific heurstics to come up with such a model, and algorithms\nfor approximating best responses are hard to scale in large, imperfect\ninformation domains.\n  In this work, we introduce a scalable and generic multiagent training regime\nfor opponent modeling using deep game-theoretic reinforcement learning. We\nfirst propose Generative Best Respoonse (GenBR), a best response algorithm\nbased on Monte-Carlo Tree Search (MCTS) with a learned deep generative model\nthat samples world states during planning. This new method scales to large\nimperfect information domains and can be plug and play in a variety of\nmultiagent algorithms. We use this new method under the framework of Policy\nSpace Response Oracles (PSRO), to automate the generation of an \\emph{offline\nopponent model} via iterative game-theoretic reasoning and population-based\ntraining. We propose using solution concepts based on bargaining theory to\nbuild up an opponent mixture, which we find identifying profiles that are near\nthe Pareto frontier. Then GenBR keeps updating an \\emph{online opponent model}\nand reacts against it during gameplay. We conduct behavioral studies where\nhuman participants negotiate with our agents in Deal-or-No-Deal, a class of\nbilateral bargaining games. Search with generative modeling finds stronger\npolicies during both training time and test time, enables online Bayesian\nco-player prediction, and can produce agents that achieve comparable social\nwelfare and Nash bargaining score negotiating with humans as humans trading\namong themselves.\n","authors":["Zun Li","Marc Lanctot","Kevin R. McKee","Luke Marris","Ian Gemp","Daniel Hennes","Paul Muller","Kate Larson","Yoram Bachrach","Michael P. Wellman"],"pdf_url":"https://arxiv.org/pdf/2302.00797v2.pdf","comment":"Accepted by IJCAI'25 main track"},{"id":"http://arxiv.org/abs/2408.02509v2","updated":"2025-06-13T15:36:41Z","published":"2024-08-05T14:31:26Z","title":"Black-Box Adversarial Attacks on LLM-Based Code Completion","summary":"  Modern code completion engines, powered by large language models (LLMs),\nassist millions of developers with their strong capabilities to generate\nfunctionally correct code. Due to this popularity, it is crucial to investigate\nthe security implications of relying on LLM-based code completion. In this\nwork, we demonstrate that state-of-the-art black-box LLM-based code completion\nengines can be stealthily biased by adversaries to significantly increase their\nrate of insecure code generation. We present the first attack, named INSEC,\nthat achieves this goal. INSEC works by injecting an attack string as a short\ncomment in the completion input. The attack string is crafted through a\nquery-based optimization procedure starting from a set of carefully designed\ninitialization schemes. We demonstrate INSEC's broad applicability and\neffectiveness by evaluating it on various state-of-the-art open-source models\nand black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a\ndiverse set of security-critical test cases, covering 16 CWEs across 5\nprogramming languages, INSEC increases the rate of generated insecure code by\nmore than 50%, while maintaining the functional correctness of generated code.\nWe consider INSEC practical -- it requires low resources and costs less than 10\nUS dollars to develop on commodity hardware. Moreover, we showcase the attack's\nreal-world deployability, by developing an IDE plug-in that stealthily injects\nINSEC into the GitHub Copilot extension.\n","authors":["Slobodan Jenko","Niels Mündler","Jingxuan He","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2408.02509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11882v1","updated":"2025-06-13T15:32:52Z","published":"2025-06-13T15:32:52Z","title":"An Explainable AI Framework for Dynamic Resource Management in Vehicular\n  Network Slicing","summary":"  Effective resource management and network slicing are essential to meet the\ndiverse service demands of vehicular networks, including Enhanced Mobile\nBroadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC).\nThis paper introduces an Explainable Deep Reinforcement Learning (XRL)\nframework for dynamic network slicing and resource allocation in vehicular\nnetworks, built upon a near-real-time RAN intelligent controller. By\nintegrating a feature-based approach that leverages Shapley values and an\nattention mechanism, we interpret and refine the decisions of our\nreinforcementlearning agents, addressing key reliability challenges in\nvehicular communication systems. Simulation results demonstrate that our\napproach provides clear, real-time insights into the resource allocation\nprocess and achieves higher interpretability precision than a pure attention\nmechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC\nservices increased from 78.0% to 80.13%, while that for eMBB services improved\nfrom 71.44% to 73.21%.\n","authors":["Haochen Sun","Yifan Liu","Ahmed Al-Tahmeesschi","Swarna Chetty","Syed Ali Raza Zaidi","Avishek Nag","Hamed Ahmadi"],"pdf_url":"https://arxiv.org/pdf/2506.11882v1.pdf","comment":"To appear in Proceedings of IEEE PIMRC 2025. 6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.11879v1","updated":"2025-06-13T15:29:10Z","published":"2025-06-13T15:29:10Z","title":"Decadal sink-source shifts of forest aboveground carbon since 1988","summary":"  As enduring carbon sinks, forest ecosystems are vital to the terrestrial\ncarbon cycle and help moderate global warming. However, the long-term dynamics\nof aboveground carbon (AGC) in forests and their sink-source transitions remain\nhighly uncertain, owing to changing disturbance regimes and inconsistencies in\nobservations, data processing, and analysis methods. Here, we derive reliable,\nharmonized AGC stocks and fluxes in global forests from 1988 to 2021 at high\nspatial resolution by integrating multi-source satellite observations with\nprobabilistic deep learning models. Our approach simultaneously estimates AGC\nand associated uncertainties, showing high reliability across space and time.\nWe find that, although global forests remained an AGC sink of 6.2 PgC over 30\nyears, moist tropical forests shifted to a substantial AGC source between 2001\nand 2010 and, together with boreal forests, transitioned toward a source in the\n2011-2021 period. Temperate, dry tropical and subtropical forests generally\nexhibited increasing AGC stocks, although Europe and Australia became sources\nafter 2011. Regionally, pronounced sink-to-source transitions occurred in\ntropical forests over the past three decades. The interannual relationship\nbetween global atmospheric CO2 growth rates and tropical AGC flux variability\nbecame increasingly negative, reaching Pearson's r = -0.63 (p < 0.05) in the\nmost recent decade. In the Brazilian Amazon, the contribution of deforested\nregions to AGC losses declined from 60% in 1989-2000 to 13% in 2011-2021, while\nthe share from untouched areas increased from 33% to 76%. Our findings suggest\na growing role of tropical forest AGC in modulating variability in the\nterrestrial carbon cycle, with anthropogenic climate change potentially\ncontributing increasingly to AGC changes, particularly in previously untouched\nareas.\n","authors":["Zhen Qian","Sebastian Bathiany","Teng Liu","Lana L. Blaschke","Hoong Chen Teo","Niklas Boers"],"pdf_url":"https://arxiv.org/pdf/2506.11879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11877v1","updated":"2025-06-13T15:27:40Z","published":"2025-06-13T15:27:40Z","title":"Robust Molecular Property Prediction via Densifying Scarce Labeled Data","summary":"  A widely recognized limitation of molecular prediction models is their\nreliance on structures observed in the training data, resulting in poor\ngeneralization to out-of-distribution compounds. Yet in drug discovery, the\ncompounds most critical for advancing research often lie beyond the training\nset, making the bias toward the training data particularly problematic. This\nmismatch introduces substantial covariate shift, under which standard deep\nlearning models produce unstable and inaccurate predictions. Furthermore, the\nscarcity of labeled data, stemming from the onerous and costly nature of\nexperimental validation, further exacerbates the difficulty of achieving\nreliable generalization. To address these limitations, we propose a novel\nmeta-learning-based approach that leverages unlabeled data to interpolate\nbetween in-distribution (ID) and out-of-distribution (OOD) data, enabling the\nmodel to meta-learn how to generalize beyond the training distribution. We\ndemonstrate significant performance gains over state-of-the-art methods on\nchallenging real-world datasets that exhibit substantial covariate shift.\n","authors":["Jina Kim","Jeffrey Willette","Bruno Andreis","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.11877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11869v1","updated":"2025-06-13T15:19:28Z","published":"2025-06-13T15:19:28Z","title":"How do Probabilistic Graphical Models and Graph Neural Networks Look at\n  Network Data?","summary":"  Graphs are a powerful data structure for representing relational data and are\nwidely used to describe complex real-world systems. Probabilistic Graphical\nModels (PGMs) and Graph Neural Networks (GNNs) can both leverage\ngraph-structured data, but their inherent functioning is different. The\nquestion is how do they compare in capturing the information contained in\nnetworked datasets? We address this objective by solving a link prediction task\nand we conduct three main experiments, on both synthetic and real networks: one\nfocuses on how PGMs and GNNs handle input features, while the other two\ninvestigate their robustness to noisy features and increasing heterophily of\nthe graph. PGMs do not necessarily require features on nodes, while GNNs cannot\nexploit the network edges alone, and the choice of input features matters. We\nfind that GNNs are outperformed by PGMs when input features are low-dimensional\nor noisy, mimicking many real scenarios where node attributes might be scalar\nor noisy. Then, we find that PGMs are more robust than GNNs when the\nheterophily of the graph is increased. Finally, to assess performance beyond\nprediction tasks, we also compare the two frameworks in terms of their\ncomputational complexity and interpretability.\n","authors":["Michela Lapenna","Caterina De Bacco"],"pdf_url":"https://arxiv.org/pdf/2506.11869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09385v2","updated":"2025-06-13T15:13:53Z","published":"2025-04-13T00:40:17Z","title":"Expressivity of Quadratic Neural ODEs","summary":"  This work focuses on deriving quantitative approximation error bounds for\nneural ordinary differential equations having at most quadratic nonlinearities\nin the dynamics. The simple dynamics of this model form demonstrates how\nexpressivity can be derived primarily from iteratively composing many basic\nelementary operations, versus from the complexity of those elementary\noperations themselves. Like the analog differential analyzer and universal\npolynomial DAEs, the expressivity is derived instead primarily from the \"depth\"\nof the model. These results contribute to our understanding of what depth\nspecifically imparts to the capabilities of deep learning architectures.\n","authors":["Joshua Hanson","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2504.09385v2.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.13763v3","updated":"2025-06-13T15:00:00Z","published":"2024-05-22T15:47:35Z","title":"Banded Square Root Matrix Factorization for Differentially Private Model\n  Training","summary":"  Current state-of-the-art methods for differentially private model training\nare based on matrix factorization techniques. However, these methods suffer\nfrom high computational overhead because they require numerically solving a\ndemanding optimization problem to determine an approximately optimal\nfactorization prior to the actual model training. In this work, we present a\nnew matrix factorization approach, BSR, which overcomes this computational\nbottleneck. By exploiting properties of the standard matrix square root, BSR\nallows to efficiently handle also large-scale problems. For the key scenario of\nstochastic gradient descent with momentum and weight decay, we even derive\nanalytical expressions for BSR that render the computational overhead\nnegligible. We prove bounds on the approximation quality that hold both in the\ncentralized and in the federated learning setting. Our numerical experiments\ndemonstrate that models trained using BSR perform on par with the best existing\nmethods, while completely avoiding their computational overhead.\n","authors":["Nikita P. Kalinin","Christoph Lampert"],"pdf_url":"https://arxiv.org/pdf/2405.13763v3.pdf","comment":"Fixed typos in Lemma 8 and Theorem 8. Added a GitHub link to the\n  implementation"},{"id":"http://arxiv.org/abs/2505.22531v2","updated":"2025-06-13T14:59:23Z","published":"2025-05-28T16:18:21Z","title":"Training RL Agents for Multi-Objective Network Defense Tasks","summary":"  Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.\n","authors":["Andres Molina-Markham","Luis Robaina","Sean Steinle","Akash Trivedi","Derek Tsui","Nicholas Potteiger","Lauren Brandt","Ransom Winder","Ahmad Ridley"],"pdf_url":"https://arxiv.org/pdf/2505.22531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11850v1","updated":"2025-06-13T14:57:57Z","published":"2025-06-13T14:57:57Z","title":"Learning Overspecified Gaussian Mixtures Exponentially Fast with the EM\n  Algorithm","summary":"  We investigate the convergence properties of the EM algorithm when applied to\noverspecified Gaussian mixture models -- that is, when the number of components\nin the fitted model exceeds that of the true underlying distribution. Focusing\non a structured configuration where the component means are positioned at the\nvertices of a regular simplex and the mixture weights satisfy a non-degeneracy\ncondition, we demonstrate that the population EM algorithm converges\nexponentially fast in terms of the Kullback-Leibler (KL) distance. Our analysis\nleverages the strong convexity of the negative log-likelihood function in a\nneighborhood around the optimum and utilizes the Polyak-{\\L}ojasiewicz\ninequality to establish that an $\\epsilon$-accurate approximation is achievable\nin $O(\\log(1/\\epsilon))$ iterations. Furthermore, we extend these results to a\nfinite-sample setting by deriving explicit statistical convergence guarantees.\nNumerical experiments on synthetic datasets corroborate our theoretical\nfindings, highlighting the dramatic acceleration in convergence compared to\nconventional sublinear rates. This work not only deepens the understanding of\nEM's behavior in overspecified settings but also offers practical insights into\ninitialization strategies and model design for high-dimensional clustering and\ndensity estimation tasks.\n","authors":["Zhenisbek Assylbekov","Alan Legg","Artur Pak"],"pdf_url":"https://arxiv.org/pdf/2506.11850v1.pdf","comment":"ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2506.11849v1","updated":"2025-06-13T14:57:38Z","published":"2025-06-13T14:57:38Z","title":"Regression-adjusted Monte Carlo Estimators for Shapley Values and\n  Probabilistic Values","summary":"  With origins in game theory, probabilistic values like Shapley values,\nBanzhaf values, and semi-values have emerged as a central tool in explainable\nAI. They are used for feature attribution, data attribution, data valuation,\nand more. Since all of these values require exponential time to compute\nexactly, research has focused on efficient approximation methods using two\ntechniques: Monte Carlo sampling and linear regression formulations. In this\nwork, we present a new way of combining both of these techniques. Our approach\nis more flexible than prior algorithms, allowing for linear regression to be\nreplaced with any function family whose probabilistic values can be computed\nefficiently. This allows us to harness the accuracy of tree-based models like\nXGBoost, while still producing unbiased estimates. From experiments across\neight datasets, we find that our methods give state-of-the-art performance for\nestimating probabilistic values. For Shapley values, the error of our methods\ncan be $6.5\\times$ lower than Permutation SHAP (the most popular Monte Carlo\nmethod), $3.8\\times$ lower than Kernel SHAP (the most popular linear regression\nmethod), and $2.6\\times$ lower than Leverage SHAP (the prior state-of-the-art\nShapley value estimator). For more general probabilistic values, we can obtain\nerror $215\\times$ lower than the best estimator from prior work.\n","authors":["R. Teal Witter","Yurong Liu","Christopher Musco"],"pdf_url":"https://arxiv.org/pdf/2506.11849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11848v1","updated":"2025-06-13T14:57:19Z","published":"2025-06-13T14:57:19Z","title":"In Defense of Defensive Forecasting","summary":"  This tutorial provides a survey of algorithms for Defensive Forecasting,\nwhere predictions are derived not by prognostication but by correcting past\nmistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of\nprediction as a sequential game, and derives predictions to minimize metrics no\nmatter what outcomes occur. We present an elementary introduction to this\ngeneral theory and derive simple, near-optimal algorithms for online learning,\ncalibration, prediction with expert advice, and online conformal prediction.\n","authors":["Juan Carlos Perdomo","Benjamin Recht"],"pdf_url":"https://arxiv.org/pdf/2506.11848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19645v2","updated":"2025-06-13T14:54:40Z","published":"2025-05-26T08:01:45Z","title":"MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE","summary":"  Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.\n","authors":["Zongle Huang","Lei Zhu","Zongyuan Zhan","Ting Hu","Weikai Mao","Xianzhi Yu","Yongpan Liu","Tianyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.19645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07756v2","updated":"2025-06-13T14:51:05Z","published":"2025-06-09T13:37:47Z","title":"Agent Semantics, Semantic Spacetime, and Graphical Reasoning","summary":"  Some formal aspects of the Semantic Spacetime graph model are presented, with\nreference to its use for directed knowledge representations and process\nmodelling. A finite $\\gamma(3,4)$ representation is defined to form a closed\nset of operations that can scale to any degree of semantic complexity. The\nSemantic Spacetime postulates bring predictability with minimal constraints to\npathways in graphs. The ubiquitous appearance of absorbing states in any\npartial graph means that a graph process leaks information. The issue is\nclosely associated with the issue of division by zero, which signals a loss of\nclosure and the need for manual injection of remedial information. The Semantic\nSpacetime model (and its Promise Theory) origins help to clarify how such\nabsorbing states are associated with boundary information where intentionality\ncan enter.\n","authors":["Mark Burgess"],"pdf_url":"https://arxiv.org/pdf/2506.07756v2.pdf","comment":"Some typos corrected"},{"id":"http://arxiv.org/abs/2506.11844v1","updated":"2025-06-13T14:48:01Z","published":"2025-06-13T14:48:01Z","title":"TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text,\n  and Structure Attacks","summary":"  Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field.\n","authors":["Qihai Zhang","Xinyue Sheng","Yuanfu Sun","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2506.11844v1.pdf","comment":"12 pages, 5 figures, in KDD 2025"},{"id":"http://arxiv.org/abs/2505.11343v2","updated":"2025-06-13T14:45:52Z","published":"2025-05-16T15:10:58Z","title":"Revisiting Stochastic Approximation and Stochastic Gradient Descent","summary":"  In this paper, we introduce a new approach to proving the convergence of the\nStochastic Approximation (SA) and the Stochastic Gradient Descent (SGD)\nalgorithms. The new approach is based on a concept called GSLLN (Generalized\nStrong Law of Large Numbers), which extends the traditional SLLN. Using this\nconcept, we provide sufficient conditions for convergence, which effectively\ndecouple the properties of the function whose zero we are trying to find, from\nthe properties of the measurement errors (noise sequence). The new approach\nprovides an alternative to the two widely used approaches, namely the ODE\napproach and the martingale approach, and also permits a wider class of noise\nsignals than either of the two known approaches. In particular, the ``noise''\nor measurement error \\textit{need not} have a finite second moment, and under\nsuitable conditions, not even a finite mean. By adapting this method of proof,\nwe also derive sufficient conditions for the convergence of zero-order SGD,\nwherein the stochastic gradient is computed using $2d$ function evaluations,\nbut no gradient computations. The sufficient conditions derived here are the\nweakest to date, thus leading to a considerable expansion of the applicability\nof SA and SGD theory.\n","authors":["Rajeeva Laxman Karandikar","Bhamidi Visweswara Rao","Mathukumalli Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2505.11343v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2505.12586v4","updated":"2025-06-13T14:43:47Z","published":"2025-05-19T00:48:53Z","title":"A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead\n  Adversarial Example Detection","summary":"  Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy. The code is available here:\nhttps://github.com/c0510gy/AFLS-AED.\n","authors":["Sanggeon Yun","Ryozo Masukawa","Hyunwoo Oh","Nathaniel D. Bastian","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2505.12586v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11839v1","updated":"2025-06-13T14:40:12Z","published":"2025-06-13T14:40:12Z","title":"Vision-based Lifting of 2D Object Detections for Automated Driving","summary":"  Image-based 3D object detection is an inevitable part of autonomous driving\nbecause cheap onboard cameras are already available in most modern cars.\nBecause of the accurate depth information, currently, most state-of-the-art 3D\nobject detectors heavily rely on LiDAR data. In this paper, we propose a\npipeline which lifts the results of existing vision-based 2D algorithms to 3D\ndetections using only cameras as a cost-effective alternative to LiDAR. In\ncontrast to existing approaches, we focus not only on cars but on all types of\nroad users. To the best of our knowledge, we are the first using a 2D CNN to\nprocess the point cloud for each 2D detection to keep the computational effort\nas low as possible. Our evaluation on the challenging KITTI 3D object detection\nbenchmark shows results comparable to state-of-the-art image-based approaches\nwhile having a runtime of only a third.\n","authors":["Hendrik Königshof","Kun Li","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2506.11839v1.pdf","comment":"https://ieeexplore.ieee.org/document/9190325"},{"id":"http://arxiv.org/abs/2503.08388v2","updated":"2025-06-13T14:38:12Z","published":"2025-03-11T12:53:24Z","title":"V-Max: A Reinforcement Learning Framework for Autonomous Driving","summary":"  Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets.\n","authors":["Valentin Charraut","Thomas Tournaire","Waël Doulazmi","Thibault Buhet"],"pdf_url":"https://arxiv.org/pdf/2503.08388v2.pdf","comment":"Accepted to RLC 25"},{"id":"http://arxiv.org/abs/2506.11831v1","updated":"2025-06-13T14:35:39Z","published":"2025-06-13T14:35:39Z","title":"Bayesian Optimization with Inexact Acquisition: Is Random Grid Search\n  Sufficient?","summary":"  Bayesian optimization (BO) is a widely used iterative algorithm for\noptimizing black-box functions. Each iteration requires maximizing an\nacquisition function, such as the upper confidence bound (UCB) or a sample path\nfrom the Gaussian process (GP) posterior, as in Thompson sampling (TS).\nHowever, finding an exact solution to these maximization problems is often\nintractable and computationally expensive. Reflecting such realistic\nsituations, in this paper, we delve into the effect of inexact maximizers of\nthe acquisition functions. Defining a measure of inaccuracy in acquisition\nsolutions, we establish cumulative regret bounds for both GP-UCB and GP-TS\nwithout requiring exact solutions of acquisition function maximization. Our\nresults show that under appropriate conditions on accumulated inaccuracy,\ninexact BO algorithms can still achieve sublinear cumulative regret. Motivated\nby such findings, we provide both theoretical justification and numerical\nvalidation for random grid search as an effective and computationally efficient\nacquisition function solver.\n","authors":["Hwanwoo Kim","Chong Liu","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11831v1.pdf","comment":"This paper is accepted to UAI 2025"},{"id":"http://arxiv.org/abs/2506.11830v1","updated":"2025-06-13T14:34:29Z","published":"2025-06-13T14:34:29Z","title":"CLEAN-MI: A Scalable and Efficient Pipeline for Constructing\n  High-Quality Neurodata in Motor Imagery Paradigm","summary":"  The construction of large-scale, high-quality datasets is a fundamental\nprerequisite for developing robust and generalizable foundation models in motor\nimagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals\ncollected from different subjects and devices are often plagued by low\nsignal-to-noise ratio, heterogeneity in electrode configurations, and\nsubstantial inter-subject variability, posing significant challenges for\neffective model training. In this paper, we propose CLEAN-MI, a scalable and\nsystematic data construction pipeline for constructing large-scale, efficient,\nand accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band\nfiltering, channel template selection, subject screening, and marginal\ndistribution alignment to systematically filter out irrelevant or low-quality\ndata and standardize multi-source EEG datasets. We demonstrate the\neffectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent\nimprovements in data quality and classification performance.\n","authors":["Dingkun Liu","Zhu Chen","Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2506.11830v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.11815v1","updated":"2025-06-13T14:19:04Z","published":"2025-06-13T14:19:04Z","title":"Diffusion-Based Electrocardiography Noise Quantification via Anomaly\n  Detection","summary":"  Electrocardiography (ECG) signals are often degraded by noise, which\ncomplicates diagnosis in clinical and wearable settings. This study proposes a\ndiffusion-based framework for ECG noise quantification via reconstruction-based\nanomaly detection, addressing annotation inconsistencies and the limited\ngeneralizability of conventional methods. We introduce a distributional\nevaluation using the Wasserstein-1 distance ($W_1$), comparing the\nreconstruction error distributions between clean and noisy ECGs to mitigate\ninconsistent annotations. Our final model achieved robust noise quantification\nusing only three reverse diffusion steps. The model recorded a macro-average\n$W_1$ score of 1.308 across the benchmarks, outperforming the next-best method\nby over 48%. External validations demonstrated strong generalizability,\nsupporting the exclusion of low-quality segments to enhance diagnostic accuracy\nand enable timely clinical responses to signal degradation. The proposed method\nenhances clinical decision-making, diagnostic accuracy, and real-time ECG\nmonitoring capabilities, supporting future advancements in clinical and\nwearable ECG applications.\n","authors":["Tae-Seong Han","Jae-Wook Heo","Hakseung Kim","Cheol-Hui Lee","Hyub Huh","Eue-Keun Choi","Dong-Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2506.11815v1.pdf","comment":"This manuscript contains 17 pages, 10 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2506.11812v1","updated":"2025-06-13T14:14:40Z","published":"2025-06-13T14:14:40Z","title":"On the Performance of LLMs for Real Estate Appraisal","summary":"  The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.\n","authors":["Margot Geerts","Manon Reusens","Bart Baesens","Seppe vanden Broucke","Jochen De Weerdt"],"pdf_url":"https://arxiv.org/pdf/2506.11812v1.pdf","comment":"Accepted at ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2502.13825v2","updated":"2025-06-13T14:12:48Z","published":"2025-02-19T15:39:14Z","title":"Mixup Regularization: A Probabilistic Perspective","summary":"  In recent years, mixup regularization has gained popularity as an effective\nway to improve the generalization performance of deep learning models by\ntraining on convex combinations of training data. While many mixup variants\nhave been explored, the proper adoption of the technique to conditional density\nestimation and probabilistic machine learning remains relatively unexplored.\nThis work introduces a novel framework for mixup regularization based on\nprobabilistic fusion that is better suited for conditional density estimation\ntasks. For data distributed according to a member of the exponential family, we\nshow that likelihood functions can be analytically fused using log-linear\npooling. We further propose an extension of probabilistic mixup, which allows\nfor fusion of inputs at an arbitrary intermediate layer of the neural network.\nWe provide a theoretical analysis comparing our approach to standard mixup\nvariants. Empirical results on synthetic and real datasets demonstrate the\nbenefits of our proposed framework compared to existing mixup variants.\n","authors":["Yousef El-Laham","Niccolò Dalmasso","Svitlana Vyetrenko","Vamsi K. Potluru","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2502.13825v2.pdf","comment":"Accepted at UAI 2025, 28 figures, 9 tables"},{"id":"http://arxiv.org/abs/2506.01602v2","updated":"2025-06-13T14:11:43Z","published":"2025-06-02T12:40:46Z","title":"Word Sense Detection Leveraging Maximum Mean Discrepancy","summary":"  Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.\n","authors":["Kensuke Mitsuzawa"],"pdf_url":"https://arxiv.org/pdf/2506.01602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06114v3","updated":"2025-06-13T14:11:37Z","published":"2025-06-06T14:24:41Z","title":"Scalable unsupervised feature selection via weight stability","summary":"  Unsupervised feature selection is critical for improving clustering\nperformance in high-dimensional data, where irrelevant features can obscure\nmeaningful structure. In this work, we introduce the Minkowski weighted\n$k$-means++, a novel initialisation strategy for the Minkowski Weighted\n$k$-means. Our initialisation selects centroids probabilistically using feature\nrelevance estimates derived from the data itself. Building on this, we propose\ntwo new feature selection algorithms, FS-MWK++, which aggregates feature\nweights across a range of Minkowski exponents to identify stable and\ninformative features, and SFS-MWK++, a scalable variant based on subsampling.\nWe support our approach with a theoretical guarantee under mild assumptions and\nextensive experiments showing that our methods consistently outperform existing\nalternatives. Our software can be found at\nhttps://github.com/xzhang4-ops1/FSMWK.\n","authors":["Xudong Zhang","Renato Cordeiro de Amorim"],"pdf_url":"https://arxiv.org/pdf/2506.06114v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11578v4","updated":"2025-06-13T14:02:43Z","published":"2025-05-16T14:40:56Z","title":"Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with\n  Physics-informed Fine-tuning","summary":"  This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation.\n","authors":["Peimian Du","Jiabin Liu","Xiaowei Jin","Wangmeng Zuo","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2505.11578v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11798v1","updated":"2025-06-13T14:02:21Z","published":"2025-06-13T14:02:21Z","title":"Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models","summary":"  Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.\n","authors":["Maximilian Kreutner","Marlene Lutz","Markus Strohmaier"],"pdf_url":"https://arxiv.org/pdf/2506.11798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11796v1","updated":"2025-06-13T14:01:39Z","published":"2025-06-13T14:01:39Z","title":"Solving Inverse Problems in Stochastic Self-Organising Systems through\n  Invariant Representations","summary":"  Self-organising systems demonstrate how simple local rules can generate\ncomplex stochastic patterns. Many natural systems rely on such dynamics, making\nself-organisation central to understanding natural complexity. A fundamental\nchallenge in modelling such systems is solving the inverse problem: finding the\nunknown causal parameters from macroscopic observations. This task becomes\nparticularly difficult when observations have a strong stochastic component,\nyielding diverse yet equivalent patterns. Traditional inverse methods fail in\nthis setting, as pixel-wise metrics cannot capture feature similarities between\nvariable outcomes. In this work, we introduce a novel inverse modelling method\nspecifically designed to handle stochasticity in the observable space,\nleveraging the capacity of visual embeddings to produce robust representations\nthat capture perceptual invariances. By mapping the pattern representations\nonto an invariant embedding space, we can effectively recover unknown causal\nparameters without the need for handcrafted objective functions or heuristics.\nWe evaluate the method on two canonical models--a reaction-diffusion system and\nan agent-based model of social segregation--and show that it reliably recovers\nparameters despite stochasticity in the outcomes. We further apply the method\nto real biological patterns, highlighting its potential as a tool for both\ntheorists and experimentalists to investigate the dynamics underlying complex\nstochastic pattern formation.\n","authors":["Elias Najarro","Nicolas Bessone","Sebastian Risi"],"pdf_url":"https://arxiv.org/pdf/2506.11796v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2411.00635v2","updated":"2025-06-13T13:59:20Z","published":"2024-11-01T14:46:17Z","title":"Variational Neural Stochastic Differential Equations with Change Points","summary":"  In this work, we explore modeling change points in time-series data using\nneural stochastic differential equations (neural SDEs). We propose a novel\nmodel formulation and training procedure based on the variational autoencoder\n(VAE) framework for modeling time-series as a neural SDE. Unlike existing\nalgorithms training neural SDEs as VAEs, our proposed algorithm only\nnecessitates a Gaussian prior of the initial state of the latent stochastic\nprocess, rather than a Wiener process prior on the entire latent stochastic\nprocess. We develop two methodologies for modeling and estimating change points\nin time-series data with distribution shifts. Our iterative algorithm\nalternates between updating neural SDE parameters and updating the change\npoints based on either a maximum likelihood-based approach or a change point\ndetection algorithm using the sequential likelihood ratio test. We provide a\ntheoretical analysis of this proposed change point detection scheme. Finally,\nwe present an empirical evaluation that demonstrates the expressive power of\nour proposed model, showing that it can effectively model both classical\nparametric SDEs and some real datasets with distribution shifts.\n","authors":["Yousef El-Laham","Zhongchang Sun","Haibei Zhu","Tucker Balch","Svitlana Vyetrenko"],"pdf_url":"https://arxiv.org/pdf/2411.00635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03664v2","updated":"2025-06-13T13:54:42Z","published":"2025-03-15T08:48:38Z","title":"PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices","summary":"  The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory.\n","authors":["Yangyijian Liu","Jun Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2504.03664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11791v1","updated":"2025-06-13T13:54:30Z","published":"2025-06-13T13:54:30Z","title":"SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks","summary":"  Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.\n","authors":["Hwiwon Lee","Ziqi Zhang","Hanxiao Lu","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11790v1","updated":"2025-06-13T13:52:32Z","published":"2025-06-13T13:52:32Z","title":"Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature\n  Attributions? A Synthetic Data Investigation","summary":"  Evaluating feature attribution methods represents a critical challenge in\nexplainable AI (XAI), as researchers typically rely on perturbation-based\nmetrics when ground truth is unavailable. However, recent work demonstrates\nthat these evaluation metrics can show different performance across predicted\nclasses within the same dataset. These \"class-dependent evaluation effects\"\nraise questions about whether perturbation analysis reliably measures\nattribution quality, with direct implications for XAI method development and\nthe trustworthiness of evaluation techniques. We investigate under which\nconditions these class-dependent effects arise by conducting controlled\nexperiments with synthetic time series data where ground truth feature\nlocations are known. We systematically vary feature types and class contrasts\nacross binary classification tasks, then compare perturbation-based degradation\nscores with ground truth-based precision-recall metrics using multiple\nattribution methods. Our experiments demonstrate that class-dependent effects\nemerge with both evaluation approaches even in simple scenarios with temporally\nlocalized features, triggered by basic variations in feature amplitude or\ntemporal extent between classes. Most critically, we find that\nperturbation-based and ground truth metrics frequently yield contradictory\nassessments of attribution quality across classes, with weak correlations\nbetween evaluation approaches. These findings suggest that researchers should\ninterpret perturbation-based metrics with care, as they may not always align\nwith whether attributions correctly identify discriminating features. These\nfindings reveal opportunities to reconsider what attribution evaluation\nactually measures and to develop more comprehensive evaluation frameworks that\ncapture multiple dimensions of attribution quality.\n","authors":["Gregor Baer","Isel Grau","Chao Zhang","Pieter Van Gorp"],"pdf_url":"https://arxiv.org/pdf/2506.11790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11786v1","updated":"2025-06-13T13:47:27Z","published":"2025-06-13T13:47:27Z","title":"SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics\n  Estimation","summary":"  Accurate real-time estimation of human movement dynamics, including internal\njoint moments and muscle forces, is essential for applications in clinical\ndiagnostics and sports performance monitoring. Inertial measurement units\n(IMUs) provide a minimally intrusive solution for capturing motion data,\nparticularly when used in sparse sensor configurations. However, current\nreal-time methods rely on supervised learning, where a ground truth dataset\nneeds to be measured with laboratory measurement systems, such as optical\nmotion capture. These systems are known to introduce measurement and processing\nerrors and often fail to generalize to real-world or previously unseen\nmovements, necessitating new data collection efforts that are time-consuming\nand impractical. To overcome these limitations, we propose SSPINNpose, a\nself-supervised, physics-informed neural network that estimates joint\nkinematics and kinetics directly from IMU data, without requiring ground truth\nlabels for training. We run the network output through a physics model of the\nhuman body to optimize physical plausibility and generate virtual measurement\ndata. Using this virtual sensor data, the network is trained directly on the\nmeasured sensor data instead of a ground truth. When compared to optical motion\ncapture, SSPINNpose is able to accurately estimate joint angles and joint\nmoments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and\nrunning at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the\nframework demonstrates robustness across sparse sensor configurations and can\ninfer the anatomical locations of the sensors. These results underscore the\npotential of SSPINNpose as a scalable and adaptable solution for real-time\nbiomechanical analysis in both laboratory and field environments.\n","authors":["Markus Gambietz","Eva Dorschky","Altan Akat","Marcel Schöckel","Jörg Miehling","Anne D. Koelewijn"],"pdf_url":"https://arxiv.org/pdf/2506.11786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11777v1","updated":"2025-06-13T13:36:33Z","published":"2025-06-13T13:36:33Z","title":"Self-supervised Learning of Echocardiographic Video Representations via\n  Online Cluster Distillation","summary":"  Self-supervised learning (SSL) has achieved major advances in natural images\nand video understanding, but challenges remain in domains like echocardiography\n(heart ultrasound) due to subtle anatomical structures, complex temporal\ndynamics, and the current lack of domain-specific pre-trained models. Existing\nSSL approaches such as contrastive, masked modeling, and clustering-based\nmethods struggle with high intersample similarity, sensitivity to low PSNR\ninputs common in ultrasound, or aggressive augmentations that distort\nclinically relevant features. We present DISCOVR (Distilled Image Supervision\nfor Cross Modal Video Representation), a self-supervised dual branch framework\nfor cardiac ultrasound video representation learning. DISCOVR combines a\nclustering-based video encoder that models temporal dynamics with an online\nimage encoder that extracts fine-grained spatial semantics. These branches are\nconnected through a semantic cluster distillation loss that transfers\nanatomical knowledge from the evolving image encoder to the video encoder,\nenabling temporally coherent representations enriched with fine-grained\nsemantic understanding. Evaluated on six echocardiography datasets spanning\nfetal, pediatric, and adult populations, DISCOVR outperforms both specialized\nvideo anomaly detection methods and state-of-the-art video-SSL baselines in\nzero-shot and linear probing setups, and achieves superior segmentation\ntransfer.\n","authors":["Divyanshu Mishra","Mohammadreza Salehi","Pramit Saha","Olga Patey","Aris T. Papageorghiou","Yuki M. Asano","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2506.11777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11772v1","updated":"2025-06-13T13:30:15Z","published":"2025-06-13T13:30:15Z","title":"CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection","summary":"  Anomaly detection is a complex problem due to the ambiguity in defining\nanomalies, the diversity of anomaly types (e.g., local and global defect), and\nthe scarcity of training data. As such, it necessitates a comprehensive model\ncapable of capturing both low-level and high-level features, even with limited\ndata. To address this, we propose CLIPFUSION, a method that leverages both\ndiscriminative and generative foundation models. Specifically, the CLIP-based\ndiscriminative model excels at capturing global features, while the\ndiffusion-based generative model effectively captures local details, creating a\nsynergistic and complementary approach. Notably, we introduce a methodology for\nutilizing cross-attention maps and feature maps extracted from diffusion models\nspecifically for anomaly detection. Experimental results on benchmark datasets\n(MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline\nmethods, achieving outstanding performance in both anomaly segmentation and\nclassification. We believe that our method underscores the effectiveness of\nmulti-modal and multi-model fusion in tackling the multifaceted challenges of\nanomaly detection, providing a scalable solution for real-world applications.\n","authors":["Byeongchan Lee","John Won","Seunghyun Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2506.11772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11769v1","updated":"2025-06-13T13:25:39Z","published":"2025-06-13T13:25:39Z","title":"Long-Short Alignment for Effective Long-Context Modeling in LLMs","summary":"  Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.\n","authors":["Tianqi Du","Haotian Huang","Yifei Wang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11769v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.11761v1","updated":"2025-06-13T13:16:09Z","published":"2025-06-13T13:16:09Z","title":"Using Deep Operators to Create Spatio-temporal Surrogates for Dynamical\n  Systems under Uncertainty","summary":"  Spatio-temporal data, which consists of responses or measurements gathered at\ndifferent times and positions, is ubiquitous across diverse applications of\ncivil infrastructure. While SciML methods have made significant progress in\ntackling the issue of response prediction for individual time histories,\ncreating a full spatial-temporal surrogate remains a challenge. This study\nproposes a novel variant of deep operator networks (DeepONets), namely the\nfull-field Extended DeepONet (FExD), to serve as a spatial-temporal surrogate\nthat provides multi-output response predictions for dynamical systems. The\nproposed FExD surrogate model effectively learns the full solution operator\nacross multiple degrees of freedom by enhancing the expressiveness of the\nbranch network and expanding the predictive capabilities of the trunk network.\nThe proposed FExD surrogate is deployed to simultaneously capture the dynamics\nat several sensing locations along a testbed model of a cable-stayed bridge\nsubjected to stochastic ground motions. The ensuing response predictions from\nthe FExD are comprehensively compared against both a vanilla DeepONet and a\nmodified spatio-temporal Extended DeepONet. The results demonstrate the\nproposed FExD can achieve both superior accuracy and computational efficiency,\nrepresenting a significant advancement in operator learning for structural\ndynamics applications.\n","authors":["Jichuan Tang","Patrick T. Brewick","Ryan G. McClarren","Christopher Sweet"],"pdf_url":"https://arxiv.org/pdf/2506.11761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16282v2","updated":"2025-06-13T13:11:45Z","published":"2025-02-22T16:27:31Z","title":"Understanding the Emergence of Multimodal Representation Alignment","summary":"  Multimodal representation learning is fundamentally about transforming\nincomparable modalities into comparable representations. While prior research\nprimarily focused on explicitly aligning these representations through targeted\nlearning objectives and model architectures, a recent line of work has found\nthat independently trained unimodal models of increasing scale and performance\ncan become implicitly aligned with each other. These findings raise fundamental\nquestions regarding the emergence of aligned representations in multimodal\nlearning. Specifically: (1) when and why does alignment emerge implicitly? and\n(2) is alignment a reliable indicator of performance? Through a comprehensive\nempirical investigation, we demonstrate that both the emergence of alignment\nand its relationship with task performance depend on several critical data\ncharacteristics. These include, but are not necessarily limited to, the degree\nof similarity between the modalities and the balance between redundant and\nunique information they provide for the task. Our findings suggest that\nalignment may not be universally beneficial; rather, its impact on performance\nvaries depending on the dataset and task. These insights can help practitioners\ndetermine whether increasing alignment between modalities is advantageous or,\nin some cases, detrimental to achieving optimal performance. Code is released\nat https://github.com/MeganTj/multimodal_alignment.\n","authors":["Megan Tjandrasuwita","Chanakya Ekbote","Liu Ziyin","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2502.16282v2.pdf","comment":"To appear as a poster in ICML 2025. 21 pages, 22 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.11756v1","updated":"2025-06-13T13:11:37Z","published":"2025-06-13T13:11:37Z","title":"Causal Effect Identification in Heterogeneous Environments from\n  Higher-Order Moments","summary":"  We investigate the estimation of the causal effect of a treatment variable on\nan outcome in the presence of a latent confounder. We first show that the\ncausal effect is identifiable under certain conditions when data is available\nfrom multiple environments, provided that the target causal effect remains\ninvariant across these environments. Secondly, we propose a moment-based\nalgorithm for estimating the causal effect as long as only a single parameter\nof the data-generating mechanism varies across environments -- whether it be\nthe exogenous noise distribution or the causal relationship between two\nvariables. Conversely, we prove that identifiability is lost if both exogenous\nnoise distributions of both the latent and treatment variables vary across\nenvironments. Finally, we propose a procedure to identify which parameter of\nthe data-generating mechanism has varied across the environments and evaluate\nthe performance of our proposed methods through experiments on synthetic data.\n","authors":["Yaroslav Kivva","Sina Akbari","Saber Salehkaleybar","Negar Kiyavash"],"pdf_url":"https://arxiv.org/pdf/2506.11756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08950v2","updated":"2025-06-13T13:11:34Z","published":"2025-01-15T16:52:21Z","title":"Approximating Fixpoints of Approximated Functions","summary":"  Fixpoints are ubiquitous in computer science and when dealing with\nquantitative semantics and verification one often considers least fixpoints of\n(higher-dimensional) functions over the non-negative reals. We show how to\napproximate the least fixpoint of such functions, focusing on the case in which\nthey are not known precisely, but represented by a sequence of approximating\nfunctions that converge to them. We concentrate on monotone and non-expansive\nfunctions, for which uniqueness of fixpoints is not guaranteed and standard\nfixpoint iteration schemes might get stuck at a fixpoint that is not the least.\nOur main contribution is the identification of an iteration scheme, a variation\nof Mann iteration with a dampening factor, which, under suitable conditions, is\nshown to guarantee convergence to the least fixpoint of the function of\ninterest. We then argue that these results are relevant in the context of\nmodel-based reinforcement learning for Markov decision processes, showing how\nthe proposed iteration scheme instantiates and allows us to derive convergence\nto the optimal expected return. More generally, we show that our results can be\nused to iterate to the least fixpoint almost surely for systems where the\nfunction of interest can be approximated with given probabilistic error bounds,\nas it happens for probabilistic systems, such as simple stochastic games, which\ncan be explored via sampling.\n","authors":["Paolo Baldan","Sebastian Gurke","Barbara König","Tommaso Padoan","Florian Wittbold"],"pdf_url":"https://arxiv.org/pdf/2501.08950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04650v2","updated":"2025-06-13T13:10:26Z","published":"2025-06-05T05:42:27Z","title":"Neural Network Reprogrammability: A Unified Theme on Model\n  Reprogramming, Prompt Tuning, and Prompt Instruction","summary":"  As large-scale pre-trained foundation models continue to expand in size and\ncapability, efficiently adapting them to specific downstream tasks has become\nincreasingly critical. Despite substantial progress, existing adaptation\napproaches have evolved largely in isolation, without a clear understanding of\ntheir interrelationships. This survey introduces neural network\nreprogrammability as a unifying framework that bridges mainstream model\nadaptation techniques--model reprogramming, prompt tuning, and prompt\ninstruction--previously fragmented research areas yet converges on a shared\nprinciple: repurposing a pre-trained model by manipulating information at the\ninterfaces while keeping the model parameters frozen. These methods exploit\nneural networks' sensitivity to manipulation on different interfaces, be it\nthrough perturbing inputs, inserting tokens into intermediate layers, or\nproviding task-specific examples in context, to redirect model behaviors\ntowards desired outcomes. We then present a taxonomy that categorizes such\ninformation manipulation-based adaptation approaches across four key\ndimensions: manipulation format (fixed or learnable), location (interfaces\nwhere manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream\ntasks). Notably, this framework applies consistently across data modalities,\nindependent of specific model architectures. Moreover, viewing established\ntechniques like in-context learning and chain-of-thought prompting through this\nlens reveals both their theoretical connections and practical distinctions. We\nfurther analyze remaining technical challenges and ethical considerations,\npositioning neural network reprogrammability as a fundamental paradigm for\nefficient model adaptation. We lastly identify promising research directions\nemerging from this integrative viewpoint.\n","authors":["Zesheng Ye","Chengyi Cai","Ruijiang Dong","Jianzhong Qi","Lei Feng","Pin-Yu Chen","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2506.04650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07417v2","updated":"2025-06-13T13:08:22Z","published":"2025-06-09T04:34:46Z","title":"Evidential Spectrum-Aware Contrastive Learning for OOD Detection in\n  Dynamic Graphs","summary":"  Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims\nto identify whether incoming data deviates from the distribution of the\nin-distribution (ID) training set, has garnered considerable attention in\nsecurity-sensitive fields. Current OOD detection paradigms primarily focus on\nstatic graphs and confront two critical challenges: i) high bias and high\nvariance caused by single-point estimation, which makes the predictions\nsensitive to randomness in the data; ii) score homogenization resulting from\nthe lack of OOD training data, where the model only learns ID-specific\npatterns, resulting in overall low OOD scores and a narrow score gap between ID\nand OOD data. To tackle these issues, we first investigate OOD detection in\ndynamic graphs through the lens of Evidential Deep Learning (EDL).\nSpecifically, we propose EviSEC, an innovative and effective OOD detector via\nEvidential Spectrum-awarE Contrastive Learning. We design an evidential neural\nnetwork to redefine the output as the posterior Dirichlet distribution,\nexplaining the randomness of inputs through the uncertainty of distribution,\nwhich is overlooked by single-point estimation. Moreover, spectrum-aware\naugmentation module generates OOD approximations to identify patterns with high\nOOD scores, thereby widening the score gap between ID and OOD data and\nmitigating score homogenization. Extensive experiments on real-world datasets\ndemonstrate that EviSAC effectively detects OOD samples in dynamic graphs.\n","authors":["Nan Sun","Xixun Lin","Zhiheng Zhou","Yanmin Shang","Zhenlin Cheng","Yanan Cao"],"pdf_url":"https://arxiv.org/pdf/2506.07417v2.pdf","comment":"Accepted by ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2506.11751v1","updated":"2025-06-13T13:04:29Z","published":"2025-06-13T13:04:29Z","title":"Bias and Identifiability in the Bounded Confidence Model","summary":"  Opinion dynamics models such as the bounded confidence models (BCMs) describe\nhow a population can reach consensus, fragmentation, or polarization, depending\non a few parameters. Connecting such models to real-world data could help\nunderstanding such phenomena, testing model assumptions. To this end,\nestimation of model parameters is a key aspect, and maximum likelihood\nestimation provides a principled way to tackle it. Here, our goal is to outline\nthe properties of statistical estimators of the two key BCM parameters: the\nconfidence bound and the convergence rate. We find that their maximum\nlikelihood estimators present different characteristics: the one for the\nconfidence bound presents a small-sample bias but is consistent, while the\nestimator of the convergence rate shows a persistent bias. Moreover, the joint\nparameter estimation is affected by identifiability issues for specific regions\nof the parameter space, as several local maxima are present in the likelihood\nfunction. Our results show how the analysis of the likelihood function is a\nfruitful approach for better understanding the pitfalls and possibilities of\nestimating the parameters of opinion dynamics models, and more in general,\nagent-based models, and for offering formal guarantees for their calibration.\n","authors":["Claudio Borile","Jacopo Lenti","Valentina Ghidini","Corrado Monti","Gianmarco De Francisci Morales"],"pdf_url":"https://arxiv.org/pdf/2506.11751v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.11747v1","updated":"2025-06-13T13:00:57Z","published":"2025-06-13T13:00:57Z","title":"Enabling automatic transcription of child-centered audio recordings from\n  real-world environments","summary":"  Longform audio recordings obtained with microphones worn by children-also\nknown as child-centered daylong recordings-have become a standard method for\nstudying children's language experiences and their impact on subsequent\nlanguage development. Transcripts of longform speech audio would enable rich\nanalyses at various linguistic levels, yet the massive scale of typical\nlongform corpora prohibits comprehensive manual annotation. At the same time,\nautomatic speech recognition (ASR)-based transcription faces significant\nchallenges due to the noisy, unconstrained nature of real-world audio, and no\nexisting study has successfully applied ASR to transcribe such data. However,\nprevious attempts have assumed that ASR must process each longform recording in\nits entirety. In this work, we present an approach to automatically detect\nthose utterances in longform audio that can be reliably transcribed with modern\nASR systems, allowing automatic and relatively accurate transcription of a\nnotable proportion of all speech in typical longform data. We validate the\napproach on four English longform audio corpora, showing that it achieves a\nmedian word error rate (WER) of 0% and a mean WER of 18% when transcribing 13%\nof the total speech in the dataset. In contrast, transcribing all speech\nwithout any filtering yields a median WER of 52% and a mean WER of 51%. We also\ncompare word log-frequencies derived from the automatic transcripts with those\nfrom manual annotations and show that the frequencies correlate at r = 0.92\n(Pearson) for all transcribed words and r = 0.98 for words that appear at least\nfive times in the automatic transcripts. Overall, the work provides a concrete\nstep toward increasingly detailed automated linguistic analyses of\nchild-centered longform audio.\n","authors":["Daniil Kocharov","Okko Räsänen"],"pdf_url":"https://arxiv.org/pdf/2506.11747v1.pdf","comment":"pre-print"},{"id":"http://arxiv.org/abs/2506.11743v1","updated":"2025-06-13T12:55:03Z","published":"2025-06-13T12:55:03Z","title":"Taxonomy of reduction matrices for Graph Coarsening","summary":"  Graph coarsening aims to diminish the size of a graph to lighten its memory\nfootprint, and has numerous applications in graph signal processing and machine\nlearning. It is usually defined using a reduction matrix and a lifting matrix,\nwhich, respectively, allows to project a graph signal from the original graph\nto the coarsened one and back. This results in a loss of information measured\nby the so-called Restricted Spectral Approximation (RSA). Most coarsening\nframeworks impose a fixed relationship between the reduction and lifting\nmatrices, generally as pseudo-inverses of each other, and seek to define a\ncoarsening that minimizes the RSA. In this paper, we remark that the roles of\nthese two matrices are not entirely symmetric: indeed, putting constraints on\nthe lifting matrix alone ensures the existence of important objects such as the\ncoarsened graph's adjacency matrix or Laplacian. In light of this, in this\npaper, we introduce a more general notion of reduction matrix, that is not\nnecessarily the pseudo-inverse of the lifting matrix. We establish a taxonomy\nof ``admissible'' families of reduction matrices, discuss the different\nproperties that they must satisfy and whether they admit a closed-form\ndescription or not. We show that, for a fixed coarsening represented by a fixed\nlifting matrix, the RSA can be further reduced simply by modifying the\nreduction matrix. We explore different examples, including some based on a\nconstrained optimization process of the RSA. Since this criterion has also been\nlinked to the performance of Graph Neural Networks, we also illustrate the\nimpact of this choices on different node classification tasks on coarsened\ngraphs.\n","authors":["Antonin Joly","Nicolas Keriven","Aline Roumy"],"pdf_url":"https://arxiv.org/pdf/2506.11743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12921v5","updated":"2025-06-13T12:53:35Z","published":"2024-02-20T11:15:13Z","title":"Right on Time: Revising Time Series Models by Constraining their\n  Explanations","summary":"  Deep time series models often suffer from reliability issues due to their\ntendency to rely on spurious correlations, leading to incorrect predictions. To\nmitigate such shortcuts and prevent \"Clever-Hans\" moments in time series\nmodels, we introduce Right on Time (RioT), a novel method that enables\ninteracting with model explanations across both the time and frequency domains.\nBy incorporating feedback on explanations in both domains, RioT constrains the\nmodel, steering it away from annotated spurious correlations. This dual-domain\ninteraction strategy is crucial for effectively addressing shortcuts in time\nseries datasets. We empirically demonstrate the effectiveness of RioT in\nguiding models toward more reliable decision-making across popular time series\nclassification and forecasting datasets, as well as our newly recorded dataset\nwith naturally occuring shortcuts, P2S, collected from a real mechanical\nproduction line.\n","authors":["Maurice Kraus","David Steinmann","Antonia Wüst","Andre Kokozinski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.12921v5.pdf","comment":"to be published in ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2302.02173v6","updated":"2025-06-13T12:48:19Z","published":"2023-02-04T14:33:07Z","title":"A Survey on Deep Learning based Time Series Analysis with Frequency\n  Transformation","summary":"  Recently, frequency transformation (FT) has been increasingly incorporated\ninto deep learning models to significantly enhance state-of-the-art accuracy\nand efficiency in time series analysis. The advantages of FT, such as high\nefficiency and a global view, have been rapidly explored and exploited in\nvarious time series tasks and applications, demonstrating the promising\npotential of FT as a new deep learning paradigm for time series analysis.\nDespite the growing attention and the proliferation of research in this\nemerging field, there is currently a lack of a systematic review and in-depth\nanalysis of deep learning-based time series models with FT. It is also unclear\nwhy FT can enhance time series analysis and what its limitations are in the\nfield. To address these gaps, we present a comprehensive review that\nsystematically investigates and summarizes the recent research advancements in\ndeep learning-based time series analysis with FT. Specifically, we explore the\nprimary approaches used in current models that incorporate FT, the types of\nneural networks that leverage FT, and the representative FT-equipped models in\ndeep time series analysis. We propose a novel taxonomy to categorize the\nexisting methods in this field, providing a structured overview of the diverse\napproaches employed in incorporating FT into deep learning models for time\nseries analysis. Finally, we highlight the advantages and limitations of FT for\ntime series modeling and identify potential future research directions that can\nfurther contribute to the community of time series analysis.\n","authors":["Kun Yi","Qi Zhang","Wei Fan","Longbing Cao","Shoujin Wang","Guodong Long","Liang Hu","Hui He","Qingsong Wen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2302.02173v6.pdf","comment":"Accepted By KDD 2025"},{"id":"http://arxiv.org/abs/2412.07514v3","updated":"2025-06-13T12:48:12Z","published":"2024-12-10T13:51:48Z","title":"Modelling Mosquito Population Dynamics using PINN-derived Empirical\n  Parameters","summary":"  Vector-borne diseases continue to pose a significant health threat globally\nwith more than 3 billion people at risk each year. Despite some limitations,\nmechanistic dynamic models are a popular approach to representing biological\nprocesses using ordinary differential equations where the parameters describe\nthe different development and survival rates. Recent advances in population\nmodelling have seen the combination of these mechanistic models with machine\nlearning. One approach is physics-informed neural networks (PINNs) whereby the\nmachine learning framework embeds physical, biological, or chemical laws into\nneural networks trained on observed or measured data. This enables forward\nsimulations, predicting system behaviour from given parameters and inputs, and\ninverse modelling, improving parameterisation of existing parameters and\nestimating unknown or latent variables. In this paper, we focus on improving\nthe parameterisation of biological processes in mechanistic models using PINNs\nto determine inverse parameters. In comparing mechanistic and PINN models, our\nexperiments offer important insights into the strengths and weaknesses of both\napproaches but demonstrated that the PINN approach generally outperforms the\ndynamic model. For a deeper understanding of the performance of PINN models, a\nfinal validation was used to investigate how modifications to PINN\narchitectures affect the performance of the framework. By varying only a single\ncomponent at a time and keeping all other factors constant, we are able to\nobserve the effect of each change.\n","authors":["Branislava Lalic","Dinh Viet Cuong","Mina Petric","Vladimir Pavlovic","Ana Firanj Sremac","Mark Roantree"],"pdf_url":"https://arxiv.org/pdf/2412.07514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07595v2","updated":"2025-06-13T12:47:42Z","published":"2024-11-12T07:09:44Z","title":"Entropy Controllable Direct Preference Optimization","summary":"  In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.\n","authors":["Motoki Omura","Yasuhiro Fujita","Toshiki Kataoka"],"pdf_url":"https://arxiv.org/pdf/2411.07595v2.pdf","comment":"ICML 2025 Workshop on Models of Human Feedback for AI Alignment"},{"id":"http://arxiv.org/abs/2506.11732v1","updated":"2025-06-13T12:44:32Z","published":"2025-06-13T12:44:32Z","title":"Data-driven approaches to inverse problems","summary":"  Inverse problems are concerned with the reconstruction of unknown physical\nquantities using indirect measurements and are fundamental across diverse\nfields such as medical imaging, remote sensing, and material sciences. These\nproblems serve as critical tools for visualizing internal structures beyond\nwhat is visible to the naked eye, enabling quantification, diagnosis,\nprediction, and discovery. However, most inverse problems are ill-posed,\nnecessitating robust mathematical treatment to yield meaningful solutions.\nWhile classical approaches provide mathematically rigorous and computationally\nstable solutions, they are constrained by the ability to accurately model\nsolution properties and implement them efficiently.\n  A more recent paradigm considers deriving solutions to inverse problems in a\ndata-driven manner. Instead of relying on classical mathematical modeling, this\napproach utilizes highly over-parameterized models, typically deep neural\nnetworks, which are adapted to specific inverse problems using carefully\nselected training data. Current approaches that follow this new paradigm\ndistinguish themselves through solution accuracy paired with computational\nefficiency that was previously inconceivable.\n  These notes offer an introduction to this data-driven paradigm for inverse\nproblems. The first part of these notes will provide an introduction to inverse\nproblems, discuss classical solution strategies, and present some applications.\nThe second part will delve into modern data-driven approaches, with a\nparticular focus on adversarial regularization and provably convergent linear\nplug-and-play denoisers. Throughout the presentation of these methodologies,\ntheir theoretical properties will be discussed, and numerical examples will be\nprovided. The lecture series will conclude with a discussion of open problems\nand future perspectives in the field.\n","authors":["Carola-Bibiane Schönlieb","Zakhar Shumaylov"],"pdf_url":"https://arxiv.org/pdf/2506.11732v1.pdf","comment":"Notes from Machine Learning: From Data to Mathematical Understanding\n  (CIME 2023)"},{"id":"http://arxiv.org/abs/2506.11730v1","updated":"2025-06-13T12:43:24Z","published":"2025-06-13T12:43:24Z","title":"Quantum Learning and Estimation for Distribution Networks and Energy\n  Communities Coordination","summary":"  Price signals from distribution networks (DNs) guide energy communities (ECs)\nto adjust energy usage, enabling effective coordination for reliable power\nsystem operation. However, this coordination faces significant challenges due\nto the limited availability of information (i.e., only the aggregated energy\nusage of ECs is available to DNs), and the high computational burden of\naccounting for uncertainties and the associated risks through numerous\nscenarios. To address these challenges, we propose a quantum learning and\nestimation approach to enhance coordination between DNs and ECs. Specifically,\nleveraging advanced quantum properties such as quantum superposition and\nentanglement, we develop a hybrid quantum temporal convolutional network-long\nshort-term memory (Q-TCN-LSTM) model to establish an end-to-end mapping between\nECs' responses and the price incentives from DNs. Moreover, we develop a\nquantum estimation method based on quantum amplitude estimation (QAE) and two\nphase-rotation circuits to significantly accelerate the optimization process\nunder numerous uncertainty scenarios. Numerical experiments demonstrate that,\ncompared to classical neural networks, the proposed Q-TCN-LSTM model improves\nthe mapping accuracy by 69.2% while reducing the model size by 99.75% and the\ncomputation time by 93.9%. Compared to classical Monte Carlo simulation, QAE\nachieves comparable accuracy with a dramatic reduction in computational time\n(up to 99.99%) and requires significantly fewer computational resources.\n","authors":["Yingrui Zhuang","Lin Cheng","Yuji Cao","Tongxin Li","Ning Qi","Yan Xu","Yue Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11730v1.pdf","comment":"This is a manuscript submitted to PROTECTION AND CONTROL OF MODERN\n  POWER SYSTEMS"},{"id":"http://arxiv.org/abs/2506.11721v1","updated":"2025-06-13T12:35:56Z","published":"2025-06-13T12:35:56Z","title":"Relational GNNs Cannot Learn $C_2$ Features for Planning","summary":"  Relational Graph Neural Networks (R-GNNs) are a GNN-based approach for\nlearning value functions that can generalise to unseen problems from a given\nplanning domain. R-GNNs were theoretically motivated by the well known\nconnection between the expressive power of GNNs and $C_2$, first-order logic\nwith two variables and counting. In the context of planning, $C_2$ features\nrefer to the set of formulae in $C_2$ with relations defined by the unary and\nbinary predicates of a planning domain. Some planning domains exhibit optimal\nvalue functions that can be decomposed as arithmetic expressions of $C_2$\nfeatures. We show that, contrary to empirical results, R-GNNs cannot learn\nvalue functions defined by $C_2$ features. We also identify prior GNN\narchitectures for planning that may better learn value functions defined by\n$C_2$ features.\n","authors":["Dillon Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19039v2","updated":"2025-06-13T12:27:48Z","published":"2025-02-26T10:48:59Z","title":"Stationary distribution of node2vec random walks on household models","summary":"  The node2vec random walk has proven to be a key tool in network embedding\nalgorithms. These random walks are tuneable, and their transition probabilities\ndepend on the previous visited node and on the triangles containing the current\nand the previously visited node. Even though these walks are widely used in\npractice, most mathematical properties of node2vec walks are largely\nunexplored, including their stationary distribution. We study the node2vec\nrandom walk on community-structured household model graphs. We prove an\nexplicit description of the stationary distribution of node2vec walks in terms\nof the walk parameters. We then show that by tuning the walk parameters, the\nstationary distribution can interpolate between uniform, size-biased, or the\nsimple random walk stationary distributions, demonstrating the wide range of\npossible walks. We further explore these effects on some specific graph\nsettings.\n","authors":["Lars Schroeder","Clara Stegehuis"],"pdf_url":"https://arxiv.org/pdf/2502.19039v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.11042v3","updated":"2025-06-13T12:27:46Z","published":"2024-10-14T19:46:23Z","title":"Persistent Topological Features in Large Language Models","summary":"  Understanding the decision-making processes of large language models is\ncritical given their widespread applications. To achieve this, we aim to\nconnect a formal mathematical framework - zigzag persistence from topological\ndata analysis - with practical and easily applicable algorithms. Zigzag\npersistence is particularly effective for characterizing data as it dynamically\ntransforms across model layers. Within this framework, we introduce topological\ndescriptors that measure how topological features, $p$-dimensional holes,\npersist and evolve throughout the layers. Unlike methods that assess each layer\nindividually and then aggregate the results, our approach directly tracks the\nfull evolutionary path of these features. This offers a statistical perspective\non how prompts are rearranged and their relative positions changed in the\nrepresentation space, providing insights into the system's operation as an\nintegrated whole. To demonstrate the expressivity and applicability of our\nframework, we highlight how sensitive these descriptors are to different models\nand a variety of datasets. As a showcase application to a downstream task, we\nuse zigzag persistence to establish a criterion for layer pruning, achieving\nresults comparable to state-of-the-art methods while preserving the\nsystem-level perspective.\n","authors":["Yuri Gardinazzi","Karthik Viswanathan","Giada Panerai","Alessio Ansuini","Alberto Cazzaniga","Matteo Biagetti"],"pdf_url":"https://arxiv.org/pdf/2410.11042v3.pdf","comment":"10+17 pages, 17 figures, 3 tables. Accepted as poster at ICML 2025"},{"id":"http://arxiv.org/abs/2502.05437v2","updated":"2025-06-13T12:22:09Z","published":"2025-02-08T03:58:55Z","title":"Approximating the total variation distance between spin systems","summary":"  Spin systems form an important class of undirected graphical models. For two\nGibbs distributions $\\mu$ and $\\nu$ induced by two spin systems on the same\ngraph $G = (V, E)$, we study the problem of approximating the total variation\ndistance $d_{TV}(\\mu,\\nu)$ with an $\\epsilon$-relative error. We propose a new\nreduction that connects the problem of approximating the TV-distance to\nsampling and approximate counting. Our applications include the hardcore model\nand the antiferromagnetic Ising model in the uniqueness regime, the\nferromagnetic Ising model, and the general Ising model satisfying the spectral\ncondition.\n  Additionally, we explore the computational complexity of approximating the\ntotal variation distance $d_{TV}(\\mu_S,\\nu_S)$ between two marginal\ndistributions on an arbitrary subset $S \\subseteq V$. We prove that this\nproblem remains hard even when both $\\mu$ and $\\nu$ admit polynomial-time\nsampling and approximate counting algorithms.\n","authors":["Weiming Feng","Hongyang Liu","Minji Yang"],"pdf_url":"https://arxiv.org/pdf/2502.05437v2.pdf","comment":"Accepted by COLT 2025; fix typos; minor edit"},{"id":"http://arxiv.org/abs/2410.04466v4","updated":"2025-06-13T12:20:54Z","published":"2024-10-06T12:42:04Z","title":"Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html.\n","authors":["Jinhao Li","Jiaming Xu","Shan Huang","Yonghua Chen","Wen Li","Jun Liu","Yaoxiu Lian","Jiayi Pan","Li Ding","Hao Zhou","Yu Wang","Guohao Dai"],"pdf_url":"https://arxiv.org/pdf/2410.04466v4.pdf","comment":"Collect and update results in recent half year. 54 pages. Github\n  link: https://github.com/Kimho666/LLM_Hardware_Survey"},{"id":"http://arxiv.org/abs/2506.11706v1","updated":"2025-06-13T12:20:16Z","published":"2025-06-13T12:20:16Z","title":"Growing with Experience: Growing Neural Networks in Deep Reinforcement\n  Learning","summary":"  While increasingly large models have revolutionized much of the machine\nlearning landscape, training even mid-sized networks for Reinforcement Learning\n(RL) is still proving to be a struggle. This, however, severely limits the\ncomplexity of policies we are able to learn. To enable increased network\ncapacity while maintaining network trainability, we propose GrowNN, a simple\nyet effective method that utilizes progressive network growth during training.\nWe start training a small network to learn an initial policy. Then we add\nlayers without changing the encoded function. Subsequent updates can utilize\nthe added layers to learn a more expressive policy, adding capacity as the\npolicy's complexity increases. GrowNN can be seamlessly integrated into most\nexisting RL agents. Our experiments on MiniHack and Mujoco show improved agent\nperformance, with incrementally GrowNN-deeper networks outperforming their\nrespective static counterparts of the same size by up to 48% on MiniHack Room\nand 72% on Ant.\n","authors":["Lukas Fehring","Marius Lindauer","Theresa Eimer"],"pdf_url":"https://arxiv.org/pdf/2506.11706v1.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2506.11700v1","updated":"2025-06-13T12:01:46Z","published":"2025-06-13T12:01:46Z","title":"Geometry-Aware Edge Pooling for Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have shown significant success for graph-based\ntasks. Motivated by the prevalence of large datasets in real-world\napplications, pooling layers are crucial components of GNNs. By reducing the\nsize of input graphs, pooling enables faster training and potentially better\ngeneralisation. However, existing pooling operations often optimise for the\nlearning task at the expense of fundamental graph structures and\ninterpretability. This leads to unreliable performance across varying dataset\ntypes, downstream tasks and pooling ratios. Addressing these concerns, we\npropose novel graph pooling layers for structure aware pooling via edge\ncollapses. Our methods leverage diffusion geometry and iteratively reduce a\ngraph's size while preserving both its metric structure and structural\ndiversity. We guide pooling using magnitude, an isometry-invariant diversity\nmeasure, which permits us to control the fidelity of the pooling process.\nFurther, we use the spread of a metric space as a faster and more stable\nalternative ensuring computational efficiency. Empirical results demonstrate\nthat our methods (i) achieve superior performance compared to alternative\npooling layers across a range of diverse graph classification tasks, (ii)\npreserve key spectral properties of the input graphs, and (iii) retain high\naccuracy across varying pooling ratios.\n","authors":["Katharina Limbeck","Lydia Mezrag","Guy Wolf","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2506.11700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07633v3","updated":"2025-06-13T11:57:05Z","published":"2025-04-10T10:27:43Z","title":"Kernel Logistic Regression Learning for High-Capacity Hopfield Networks","summary":"  Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron\nratio around 0.14). We propose Kernel Logistic Regression (KLR) learning.\nUnlike linear methods, KLR uses kernels to implicitly map patterns to\nhigh-dimensional feature space, enhancing separability. By learning dual\nvariables, KLR dramatically improves storage capacity, achieving perfect recall\neven when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and\nenhances noise robustness. KLR demonstrably outperforms Hebbian and linear\nlogistic regression approaches.\n","authors":["Akira Tamamori"],"pdf_url":"https://arxiv.org/pdf/2504.07633v3.pdf","comment":"Accepted by IEICE Transactions on Information and Systems"},{"id":"http://arxiv.org/abs/2411.03263v3","updated":"2025-06-13T11:54:15Z","published":"2024-11-05T17:02:29Z","title":"Proxy-informed Bayesian transfer learning with unknown sources","summary":"  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (may not have the ability to \"fine-tune\"), and instead makes\nuse of proxy (indirect) information. Our theoretical results show that the\nthreat of negative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n","authors":["Sabina J. Sloman","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.03263v3.pdf","comment":"Accepted for UAI 2025"},{"id":"http://arxiv.org/abs/2502.06911v2","updated":"2025-06-13T11:54:07Z","published":"2025-02-10T05:01:08Z","title":"Foundation Models for Anomaly Detection: Vision and Challenges","summary":"  As data continues to grow in volume and complexity across domains such as\nfinance, manufacturing, and healthcare, effective anomaly detection is\nessential for identifying irregular patterns that may signal critical issues.\nRecently, foundation models (FMs) have emerged as a powerful tool for advancing\nanomaly detection. They have demonstrated unprecedented capabilities in\nenhancing anomaly identification, generating detailed data descriptions, and\nproviding visual explanations. This survey presents the first comprehensive\nreview of recent advancements in FM-based anomaly detection. We propose a novel\ntaxonomy that classifies FMs into three categories based on their roles in\nanomaly detection tasks, i.e., as encoders, detectors, or interpreters. We\nprovide a systematic analysis of state-of-the-art methods and discuss key\nchallenges in leveraging FMs for improved anomaly detection. We also outline\nfuture research directions in this rapidly evolving field.\n","authors":["Jing Ren","Tao Tang","Hong Jia","Ziqi Xu","Haytham Fayek","Xiaodong Li","Suyu Ma","Xiwei Xu","Feng Xia"],"pdf_url":"https://arxiv.org/pdf/2502.06911v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00573v4","updated":"2025-06-13T11:46:16Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  In the context of over-parameterization, there is a line of work\ndemonstrating that randomly initialized (stochastic) gradient descent (GD)\nconverges to a globally optimal solution at a linear convergence rate for the\nquadratic loss function. However, the learning rate of GD for training\ntwo-layer neural networks exhibits poor dependence on the sample size and the\nGram matrix, leading to a slow training process. In this paper, we show that\nfor training two-layer $\\text{ReLU}^3$ Physics-Informed Neural Networks\n(PINNs), the learning rate can be improved from $\\mathcal{O}(\\lambda_0)$ to\n$\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, implying that GD actually enjoys a\nfaster convergence rate. Despite such improvements, the convergence rate is\nstill tied to the least eigenvalue of the Gram matrix, leading to slow\nconvergence. We then develop the positive definiteness of Gram matrices with\ngeneral smooth activation functions and provide the convergence analysis of\nnatural gradient descent (NGD) in training two-layer PINNs, demonstrating that\nthe learning rate can be $\\mathcal{O}(1)$ and at this rate, the convergence\nrate is independent of the Gram matrix. In particular, for smooth activation\nfunctions, the convergence rate of NGD is quadratic. Numerical experiments are\nconducted to verify our theoretical results.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Bin Shan","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11687v1","updated":"2025-06-13T11:30:35Z","published":"2025-06-13T11:30:35Z","title":"Differential Privacy in Machine Learning: From Symbolic AI to LLMs","summary":"  Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.\n","authors":["Francisco Aguilera-Martínez","Fernando Berzal"],"pdf_url":"https://arxiv.org/pdf/2506.11687v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.00654 by other authors"},{"id":"http://arxiv.org/abs/2506.11683v1","updated":"2025-06-13T11:20:49Z","published":"2025-06-13T11:20:49Z","title":"On the performance of multi-fidelity and reduced-dimensional neural\n  emulators for inference of physiologic boundary conditions","summary":"  Solving inverse problems in cardiovascular modeling is particularly\nchallenging due to the high computational cost of running high-fidelity\nsimulations. In this work, we focus on Bayesian parameter estimation and\nexplore different methods to reduce the computational cost of sampling from the\nposterior distribution by leveraging low-fidelity approximations. A common\napproach is to construct a surrogate model for the high-fidelity simulation\nitself. Another is to build a surrogate for the discrepancy between high- and\nlow-fidelity models. This discrepancy, which is often easier to approximate, is\nmodeled with either a fully connected neural network or a nonlinear\ndimensionality reduction technique that enables surrogate construction in a\nlower-dimensional space. A third possible approach is to treat the discrepancy\nbetween the high-fidelity and surrogate models as random noise and estimate its\ndistribution using normalizing flows. This allows us to incorporate the\napproximation error into the Bayesian inverse problem by modifying the\nlikelihood function. We validate five different methods which are variations of\nthe above on analytical test cases by comparing them to posterior distributions\nderived solely from high-fidelity models, assessing both accuracy and\ncomputational cost. Finally, we demonstrate our approaches on two\ncardiovascular examples of increasing complexity: a lumped-parameter Windkessel\nmodel and a patient-specific three-dimensional anatomy.\n","authors":["Chloe H. Choi","Andrea Zanoni","Daniele E. Schiavazzi","Alison L. Marsden"],"pdf_url":"https://arxiv.org/pdf/2506.11683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11677v1","updated":"2025-06-13T11:16:23Z","published":"2025-06-13T11:16:23Z","title":"Predicting Patient Survival with Airway Biomarkers using\n  nn-Unet/Radiomics","summary":"  The primary objective of the AIIB 2023 competition is to evaluate the\npredictive significance of airway-related imaging biomarkers in determining the\nsurvival outcomes of patients with lung fibrosis.This study introduces a\ncomprehensive three-stage approach. Initially, a segmentation network, namely\nnn-Unet, is employed to delineate the airway's structural boundaries.\nSubsequently, key features are extracted from the radiomic images centered\naround the trachea and an enclosing bounding box around the airway. This step\nis motivated by the potential presence of critical survival-related insights\nwithin the tracheal region as well as pertinent information encoded in the\nstructure and dimensions of the airway. Lastly, radiomic features obtained from\nthe segmented areas are integrated into an SVM classifier. We could obtain an\noverall-score of 0.8601 for the segmentation in Task 1 while 0.7346 for the\nclassification in Task 2.\n","authors":["Zacharia Mesbah","Dhruv Jain","Tsiry Mayet","Romain Modzelewski","Romain Herault","Simon Bernard","Sebastien Thureau","Clement Chatelain"],"pdf_url":"https://arxiv.org/pdf/2506.11677v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2506.09096v2","updated":"2025-06-13T11:11:52Z","published":"2025-06-10T12:59:14Z","title":"Intra-Trajectory Consistency for Reward Modeling","summary":"  Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.\n","authors":["Chaoyang Zhou","Shunyu Liu","Zengmao Wang","Di Wang","Rong-Cheng Tu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.09096v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2506.10419v2","updated":"2025-06-13T10:56:49Z","published":"2025-06-12T07:18:05Z","title":"Data-Driven Soil Organic Carbon Sampling: Integrating Spectral\n  Clustering with Conditioned Latin Hypercube Optimization","summary":"  Soil organic carbon (SOC) monitoring often relies on selecting representative\nfield sampling locations based on environmental covariates. We propose a novel\nhybrid methodology that integrates spectral clustering - an unsupervised\nmachine learning technique with conditioned Latin hypercube sampling (cLHS) to\nenhance the representativeness of SOC sampling. In our approach, spectral\nclustering partitions the study area into $K$ homogeneous zones using\nmultivariate covariate data, and cLHS is then applied within each zone to\nselect sampling locations that collectively capture the full diversity of\nenvironmental conditions. This hybrid spectral-cLHS method ensures that even\nminor but important environmental clusters are sampled, addressing a key\nlimitation of vanilla cLHS which can overlook such areas. We demonstrate on a\nreal SOC mapping dataset that spectral-cLHS provides more uniform coverage of\ncovariate feature space and spatial heterogeneity than standard cLHS. This\nimproved sampling design has the potential to yield more accurate SOC\npredictions by providing better-balanced training data for machine learning\nmodels.\n","authors":["Weiying Zhao","Aleksei Unagaev","Natalia Efremova"],"pdf_url":"https://arxiv.org/pdf/2506.10419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07619v2","updated":"2025-06-13T10:55:58Z","published":"2025-04-10T10:07:05Z","title":"Beating Transformers using Synthetic Cognition","summary":"  The road to Artificial General Intelligence goes through the generation of\ncontext-aware reactive behaviors, where the Transformer architecture has been\nproven to be the state-of-the-art. However, they still fail to develop\nreasoning. Recently, a novel approach for developing cognitive architectures,\ncalled Synthetic Cognition, has been proposed and implemented to develop\ninstantaneous reactive behavior. In this study, we aim to explore the use of\nSynthetic Cognition to develop context-aware reactive behaviors. We propose a\nmechanism to deal with sequences for the recent implementation of Synthetic\nCognition, and test it against DNA foundation models in DNA sequence\nclassification tasks. In our experiments, our proposal clearly outperforms the\nDNA foundation models, obtaining the best score on more benchmark tasks than\nthe alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to\ndeal with sequences, and beating the Transformer architecture for sequence\nclassification.\n","authors":["Alfredo Ibias","Miguel Rodriguez-Galindo","Hector Antona","Guillem Ramirez-Miranda","Enric Guinovart"],"pdf_url":"https://arxiv.org/pdf/2504.07619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11653v1","updated":"2025-06-13T10:29:03Z","published":"2025-06-13T10:29:03Z","title":"DISCO: Mitigating Bias in Deep Learning with Conditional Distance\n  Correlation","summary":"  During prediction tasks, models can use any signal they receive to come up\nwith the final answer - including signals that are causally irrelevant. When\npredicting objects from images, for example, the lighting conditions could be\ncorrelated to different targets through selection bias, and an oblivious model\nmight use these signals as shortcuts to discern between various objects. A\npredictor that uses lighting conditions instead of real object-specific details\nis obviously undesirable. To address this challenge, we introduce a standard\nanti-causal prediction model (SAM) that creates a causal framework for\nanalyzing the information pathways influencing our predictor in anti-causal\nsettings. We demonstrate that a classifier satisfying a specific conditional\nindependence criterion will focus solely on the direct causal path from label\nto image, being counterfactually invariant to the remaining variables. Finally,\nwe propose DISCO, a novel regularization strategy that uses conditional\ndistance correlation to optimize for conditional independence in regression\ntasks. We can show that DISCO achieves competitive results in different bias\nmitigation experiments, deeming it a valid alternative to classical\nkernel-based methods.\n","authors":["Emre Kavak","Tom Nuno Wolf","Christian Wachinger"],"pdf_url":"https://arxiv.org/pdf/2506.11653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01591v2","updated":"2025-06-13T10:22:12Z","published":"2025-05-02T21:26:21Z","title":"Machine Learning Fairness in House Price Prediction: A Case Study of\n  America's Expanding Metropolises","summary":"  As a basic human need, housing plays a key role in enhancing health,\nwell-being, and educational outcome in society, and the housing market is a\nmajor factor for promoting quality of life and ensuring social equity. To\nimprove the housing conditions, there has been extensive research on building\nMachine Learning (ML)-driven house price prediction solutions to accurately\nforecast the future conditions, and help inform actions and policies in the\nfield. In spite of their success in developing high-accuracy models, there is a\ngap in our understanding of the extent to which various ML-driven house price\nprediction approaches show ethnic and/or racial bias, which in turn is\nessential for the responsible use of ML, and ensuring that the ML-driven\nsolutions do not exacerbate inequity. To fill this gap, this paper develops\nseveral ML models from a combination of structural and neighborhood-level\nattributes, and conducts comprehensive assessments on the fairness of ML models\nunder various definitions of privileged groups. As a result, it finds that the\nML-driven house price prediction models show various levels of bias towards\nprotected attributes (i.e., race and ethnicity in this study). Then, it\ninvestigates the performance of different bias mitigation solutions, and the\nexperimental results show their various levels of effectiveness on different\nML-driven methods. However, in general, the in-processing bias mitigation\napproach tends to be more effective than the pre-processing one in this problem\ndomain. Our code is available at https://github.com/wahab1412/housing_fairness.\n","authors":["Abdalwahab Almajed","Maryam Tabar","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2505.01591v2.pdf","comment":"Accepted at ACM-COMPASS2025"},{"id":"http://arxiv.org/abs/2506.11641v1","updated":"2025-06-13T10:12:34Z","published":"2025-06-13T10:12:34Z","title":"Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective","summary":"  Deep autoencoders have become a fundamental tool in various machine learning\napplications, ranging from dimensionality reduction and reduced order modeling\nof partial differential equations to anomaly detection and neural machine\ntranslation. Despite their empirical success, a solid theoretical foundation\nfor their expressiveness remains elusive, particularly when compared to\nclassical projection-based techniques. In this work, we aim to take a step\nforward in this direction by presenting a comprehensive analysis of what we\nrefer to as symmetric autoencoders, a broad class of deep learning\narchitectures ubiquitous in the literature. Specifically, we introduce a formal\ndistinction between different classes of symmetric architectures, analyzing\ntheir strengths and limitations from a mathematical perspective. For instance,\nwe show that the reconstruction error of symmetric autoencoders with\northonormality constraints can be understood by leveraging the well-renowned\nEckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up\ndeveloping the EYS initialization strategy for symmetric autoencoders, which is\nbased on an iterated application of the Singular Value Decomposition (SVD). To\nvalidate our findings, we conduct a series of numerical experiments where we\nbenchmark our proposal against conventional deep autoencoders, discussing the\nimportance of model design and initialization.\n","authors":["Simone Brivio","Nicola Rares Franco"],"pdf_url":"https://arxiv.org/pdf/2506.11641v1.pdf","comment":"28 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.11639v1","updated":"2025-06-13T10:11:32Z","published":"2025-06-13T10:11:32Z","title":"Recursive KalmanNet: Deep Learning-Augmented Kalman Filtering for State\n  Estimation with Consistent Uncertainty Quantification","summary":"  State estimation in stochastic dynamical systems with noisy measurements is a\nchallenge. While the Kalman filter is optimal for linear systems with\nindependent Gaussian white noise, real-world conditions often deviate from\nthese assumptions, prompting the rise of data-driven filtering techniques. This\npaper introduces Recursive KalmanNet, a Kalman-filter-informed recurrent neural\nnetwork designed for accurate state estimation with consistent error covariance\nquantification. Our approach propagates error covariance using the recursive\nJoseph's formula and optimizes the Gaussian negative log-likelihood.\nExperiments with non-Gaussian measurement white noise demonstrate that our\nmodel outperforms both the conventional Kalman filter and an existing\nstate-of-the-art deep learning based estimator.\n","authors":["Hassan Mortada","Cyril Falcon","Yanis Kahil","Mathéo Clavaud","Jean-Philippe Michel"],"pdf_url":"https://arxiv.org/pdf/2506.11639v1.pdf","comment":"5 pages, 3 figures. Accepted for publication in EUSIPCO 2025\n  proceedings"},{"id":"http://arxiv.org/abs/2506.11627v1","updated":"2025-06-13T09:54:01Z","published":"2025-06-13T09:54:01Z","title":"Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel\n  Technique using Tensor Data and Bayesian Regression","summary":"  Fairness is a critical component of Trustworthy AI. In this paper, we focus\non Machine Learning (ML) and the performance of model predictions when dealing\nwith skin color. Unlike other sensitive attributes, the nature of skin color\ndiffers significantly. In computer vision, skin color is represented as tensor\ndata rather than categorical values or single numerical points. However, much\nof the research on fairness across sensitive groups has focused on categorical\nfeatures such as gender and race. This paper introduces a new technique for\nevaluating fairness in ML for image classification tasks, specifically without\nthe use of annotation. To address the limitations of prior work, we handle\ntensor data, like skin color, without classifying it rigidly. Instead, we\nconvert it into probability distributions and apply statistical distance\nmeasures. This novel approach allows us to capture fine-grained nuances in\nfairness both within and across what would traditionally be considered distinct\ngroups. Additionally, we propose an innovative training method to mitigate the\nlatent biases present in conventional skin tone categorization. This method\nleverages color distance estimates calculated through Bayesian regression with\npolynomial functions, ensuring a more nuanced and equitable treatment of skin\ncolor in ML models.\n","authors":["Kuniko Paxton","Koorosh Aslansefat","Dhavalkumar Thakker","Yiannis Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2506.11627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11625v1","updated":"2025-06-13T09:52:49Z","published":"2025-06-13T09:52:49Z","title":"Physically-informed change-point kernels for structural dynamics","summary":"  The relative balance between physics and data within any physics-informed\nmachine learner is an important modelling consideration to ensure that the\nbenefits of both physics and data-based approaches are maximised. An over\nreliance on physical knowledge can be detrimental, particularly when the\nphysics-based component of a model may not accurately represent the true\nunderlying system. An underutilisation of physical knowledge potentially wastes\na valuable resource, along with benefits in model interpretability and reduced\ndemand for expensive data collection. Achieving an optimal physics-data balance\nis a challenging aspect of model design, particularly if the level varies\nthrough time; for example, one might have a physical approximation, only valid\nwithin particular regimes, or a physical phenomenon may be known to only occur\nwhen given conditions are met (e.g. at high temperatures). This paper develops\nnovel, physically-informed, change-point kernels for Gaussian processes,\ncapable of dynamically varying the reliance upon available physical knowledge.\nA high level of control is granted to a user, allowing for the definition of\nconditions in which they believe a phenomena should occur and the rate at which\nthe knowledge should be phased in and out of a model. In circumstances where\nusers may be less certain, the switching reliance upon physical knowledge may\nbe automatically learned and recovered from the model in an interpretable and\nintuitive manner. Variation of the modelled noise based on the physical\nphenomena occurring is also implemented to provide a more representative\ncapture of uncertainty alongside predictions. The capabilities of the new\nkernel structures are explored through the use of two engineering case studies:\nthe directional wind loading of a cable-stayed bridge and the prediction of\naircraft wing strain during in-flight manoeuvring.\n","authors":["Daniel James Pitchforth","Matthew Rhys Jones","Samuel John Gibson","Elizabeth Jane Cross"],"pdf_url":"https://arxiv.org/pdf/2506.11625v1.pdf","comment":"26 pages, 14 figures, 2 tables, 38 references"},{"id":"http://arxiv.org/abs/2506.11618v1","updated":"2025-06-13T09:39:54Z","published":"2025-06-13T09:39:54Z","title":"Convergent Linear Representations of Emergent Misalignment","summary":"  Fine-tuning large language models on narrow datasets can cause them to\ndevelop broadly misaligned behaviours: a phenomena known as emergent\nmisalignment. However, the mechanisms underlying this misalignment, and why it\ngeneralizes beyond the training domain, are poorly understood, demonstrating\ncritical gaps in our knowledge of model alignment. In this work, we train and\nstudy a minimal model organism which uses just 9 rank-1 adapters to emergently\nmisalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently\nmisaligned models converge to similar representations of misalignment. We\ndemonstrate this convergence by extracting a 'misalignment direction' from one\nfine-tuned model's activations, and using it to effectively ablate misaligned\nbehaviour from fine-tunes using higher dimensional LoRAs and different\ndatasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further\npresent a set of experiments for directly interpreting the fine-tuning\nadapters, showing that six contribute to general misalignment, while two\nspecialise for misalignment in just the fine-tuning domain. Emergent\nmisalignment is a particularly salient example of undesirable and unexpected\nmodel behaviour and by advancing our understanding of the mechanisms behind it,\nwe hope to move towards being able to better understand and mitigate\nmisalignment more generally.\n","authors":["Anna Soligo","Edward Turner","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2506.11618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11615v1","updated":"2025-06-13T09:37:11Z","published":"2025-06-13T09:37:11Z","title":"Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and\n  Neuron Pruning in Noisy Environments","summary":"  Deep neural networks (DNNs) have achieved remarkable success across diverse\ndomains, but their performance can be severely degraded by noisy or corrupted\ntraining data. Conventional noise mitigation methods often rely on explicit\nassumptions about noise distributions or require extensive retraining, which\ncan be impractical for large-scale models. Inspired by the principles of\nmachine unlearning, we propose a novel framework that integrates\nattribution-guided data partitioning, discriminative neuron pruning, and\ntargeted fine-tuning to mitigate the impact of noisy samples. Our approach\nemploys gradient-based attribution to probabilistically distinguish\nhigh-quality examples from potentially corrupted ones without imposing\nrestrictive assumptions on the noise. It then applies regression-based\nsensitivity analysis to identify and prune neurons that are most vulnerable to\nnoise. Finally, the resulting network is fine-tuned on the high-quality data\nsubset to efficiently recover and enhance its generalization performance. This\nintegrated unlearning-inspired framework provides several advantages over\nconventional noise-robust learning approaches. Notably, it combines data-level\nunlearning with model-level adaptation, thereby avoiding the need for full\nmodel retraining or explicit noise modeling. We evaluate our method on\nrepresentative tasks (e.g., CIFAR-10 image classification and speech\nrecognition) under various noise levels and observe substantial gains in both\naccuracy and efficiency. For example, our framework achieves approximately a\n10% absolute accuracy improvement over standard retraining on CIFAR-10 with\ninjected label noise, while reducing retraining time by up to 47% in some\nsettings. These results demonstrate the effectiveness and scalability of the\nproposed approach for achieving robust generalization in noisy environments.\n","authors":["Deliang Jin","Gang Chen","Shuo Feng","Yufeng Ling","Haoran Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.11615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11613v1","updated":"2025-06-13T09:34:25Z","published":"2025-06-13T09:34:25Z","title":"Model Organisms for Emergent Misalignment","summary":"  Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs.\n","authors":["Edward Turner","Anna Soligo","Mia Taylor","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2506.11613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11611v1","updated":"2025-06-13T09:33:41Z","published":"2025-06-13T09:33:41Z","title":"KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel\n  Complexity","summary":"  Graph Neural Networks (GNNs) have achieved impressive success across a wide\nrange of graph-based tasks, yet they remain highly vulnerable to small,\nimperceptible perturbations and adversarial attacks. Although numerous defense\nmethods have been proposed to address these vulnerabilities, many rely on\nheuristic metrics, overfit to specific attack patterns, and suffer from high\ncomputational complexity. In this paper, we propose Kernel Complexity-Based\nEdge Sanitization (KCES), a training-free, model-agnostic defense framework.\nKCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the\ngraph's Gram matrix that characterizes GNN generalization via its test error\nbound. Building on GKC, we define a KC score for each edge, measuring the\nchange in GKC when the edge is removed. Edges with high KC scores, typically\nintroduced by adversarial perturbations, are pruned to mitigate their harmful\neffects, thereby enhancing GNNs' robustness. KCES can also be seamlessly\nintegrated with existing defense strategies as a plug-and-play module without\nrequiring training. Theoretical analysis and extensive experiments demonstrate\nthat KCES consistently enhances GNN robustness, outperforms state-of-the-art\nbaselines, and amplifies the effectiveness of existing defenses, offering a\nprincipled and efficient solution for securing GNNs.\n","authors":["Yaning Jia","Shenyang Deng","Chiyu Ma","Yaoqing Yang","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2506.11611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11812v2","updated":"2025-06-13T09:32:19Z","published":"2025-02-17T13:59:41Z","title":"Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis","summary":"  Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.\n","authors":["Xu Wang","Yan Hu","Wenyu Du","Reynold Cheng","Benyou Wang","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2502.11812v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2501.18528v2","updated":"2025-06-13T09:20:42Z","published":"2025-01-30T17:46:17Z","title":"Joint Learning of Energy-based Models and their Partition Function","summary":"  Energy-based models (EBMs) offer a flexible framework for parameterizing\nprobability distributions using neural networks. However, learning EBMs by\nexact maximum likelihood estimation (MLE) is generally intractable, due to the\nneed to compute the partition function (normalization constant). In this paper,\nwe propose a novel formulation for approximately learning probabilistic EBMs in\ncombinatorially-large discrete spaces, such as sets or permutations. Our key\nidea is to jointly learn both an energy model and its log-partition, both\nparameterized as a neural network. Our approach not only provides a novel\ntractable objective criterion to learn EBMs by stochastic gradient descent\n(without relying on MCMC), but also a novel means to estimate the log-partition\nfunction on unseen data points. On the theoretical side, we show that our\napproach recovers the optimal MLE solution when optimizing in the space of\ncontinuous functions. Furthermore, we show that our approach naturally extends\nto the broader family of Fenchel-Young losses, allowing us to obtain the first\ntractable method for optimizing the sparsemax loss in combinatorially-large\nspaces. We demonstrate our approach on multilabel classification and label\nranking.\n","authors":["Michael E. Sander","Vincent Roulet","Tianlin Liu","Mathieu Blondel"],"pdf_url":"https://arxiv.org/pdf/2501.18528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11521v3","updated":"2025-06-13T09:11:46Z","published":"2024-11-18T12:31:22Z","title":"Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions","summary":"  Interactions with online Large Language Models raise privacy issues where\nproviders can gather sensitive information about users and their companies from\nthe prompts. While textual prompts can be sanitized using Differential Privacy,\nwe show that it is difficult to anticipate the performance of an LLM on such\nsanitized prompt. Poor performance has clear monetary consequences for LLM\nservices charging on a pay-per-use model as well as great amount of computing\nresources wasted. To this end, we propose a middleware architecture leveraging\na Small Language Model to predict the utility of a given sanitized prompt\nbefore it is sent to the LLM. We experimented on a summarization task and a\ntranslation task to show that our architecture helps prevent such resource\nwaste for up to 20% of the prompts. During our study, we also reproduced\nexperiments from one of the most cited paper on text sanitization using DP and\nshow that a potential performance-driven implementation choice dramatically\nchanges the output while not being explicitly acknowledged in the paper.\n","authors":["Robin Carpentier","Benjamin Zi Hao Zhao","Hassan Jameel Asghar","Dali Kaafar"],"pdf_url":"https://arxiv.org/pdf/2411.11521v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10805v2","updated":"2025-06-13T09:04:48Z","published":"2025-06-12T15:20:33Z","title":"Detecting High-Stakes Interactions with Activation Probes","summary":"  Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.\n","authors":["Alex McKenzie","Urja Pawar","Phil Blandfort","William Bankes","David Krueger","Ekdeep Singh Lubana","Dmitrii Krasheninnikov"],"pdf_url":"https://arxiv.org/pdf/2506.10805v2.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2506.11595v1","updated":"2025-06-13T09:03:33Z","published":"2025-06-13T09:03:33Z","title":"EasyARC: Evaluating Vision Language Models on True Visual Reasoning","summary":"  Building on recent advances in language-based reasoning models, we explore\nmultimodal reasoning that integrates vision and text. Existing multimodal\nbenchmarks primarily test visual extraction combined with text-based reasoning,\nlacking true visual reasoning with more complex interactions between vision and\nlanguage. Inspired by the ARC challenge, we introduce EasyARC, a\nvision-language benchmark requiring multi-image, multi-step reasoning, and\nself-correction. EasyARC is procedurally generated, fully verifiable, and\nscalable, making it ideal for reinforcement learning (RL) pipelines. The\ngenerators incorporate progressive difficulty levels, enabling structured\nevaluation across task types and complexities. We benchmark state-of-the-art\nvision-language models and analyze their failure modes. We argue that EasyARC\nsets a new standard for evaluating true reasoning and test-time scaling\ncapabilities in vision-language models. We open-source our benchmark dataset\nand evaluation code.\n","authors":["Mert Unsal","Aylin Akkus"],"pdf_url":"https://arxiv.org/pdf/2506.11595v1.pdf","comment":"CVPR2025 Workshop on Test-time Scaling for Computer Vision"},{"id":"http://arxiv.org/abs/2506.09093v2","updated":"2025-06-13T09:02:50Z","published":"2025-06-10T11:34:23Z","title":"Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD\n  Data","summary":"  Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.\n","authors":["Bingjie Zhang","Hongkang Li","Changlong Shi","Guowei Rong","He Zhao","Dongsheng Wang","Dandan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2506.09093v2.pdf","comment":"Minor formatting adjustments; no changes to content"},{"id":"http://arxiv.org/abs/2405.15013v3","updated":"2025-06-13T09:00:28Z","published":"2024-05-23T19:36:10Z","title":"Fast Inference with Kronecker-Sparse Matrices","summary":"  Kronecker-sparse (KS) matrices -- whose supports are Kronecker products of\nidentity and all-ones blocks -- underpin the structure of Butterfly and Monarch\nmatrices and offer the promise of more efficient models. However, existing GPU\nkernels for KS matrix multiplication suffer from high data movement costs, with\nup to 50% of time spent on memory-bound tensor permutations. We propose a\nfused, output-stationary GPU kernel that eliminates these overheads, reducing\nglobal memory traffic threefold. Across 600 KS patterns, our kernel achieves in\nFP32 a median speedup of x1.4 and lowers energy consumption by 15%. A simple\nheuristic based on KS pattern parameters predicts when our method outperforms\nexisting ones. We release all code at github.com/PascalCarrivain/ksmm,\nincluding a PyTorch-compatible KSLinear layer, and demonstrate in FP32\nend-to-end latency reductions of up to 22% in ViT-S/16 and 16% in GPT-2 medium.\n","authors":["Antoine Gonon","Léon Zheng","Pascal Carrivain","Quoc-Tung Le"],"pdf_url":"https://arxiv.org/pdf/2405.15013v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11586v1","updated":"2025-06-13T08:49:39Z","published":"2025-06-13T08:49:39Z","title":"SecONNds: Secure Outsourced Neural Network Inference on ImageNet","summary":"  The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds.\n","authors":["Shashank Balla"],"pdf_url":"https://arxiv.org/pdf/2506.11586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11584v1","updated":"2025-06-13T08:47:04Z","published":"2025-06-13T08:47:04Z","title":"A Comparative Analysis of Influence Signals for Data Debugging","summary":"  Improving the quality of training samples is crucial for improving the\nreliability and performance of ML models. In this paper, we conduct a\ncomparative evaluation of influence-based signals for debugging training data.\nThese signals can potentially identify both mislabeled and anomalous samples\nfrom a potentially noisy training set as we build the models and hence\nalleviate the need for dedicated glitch detectors. Although several\ninfluence-based signals (e.g., Self-Influence, Average Absolute Influence,\nMarginal Influence, GD-class) have been recently proposed in the literature,\nthere are no experimental studies for assessing their power in detecting\ndifferent glitch types (e.g., mislabeled and anomalous samples) under a common\ninfluence estimator (e.g., TraceIn) for different data modalities (image and\ntabular), and deep learning models (trained from scratch or foundation).\nThrough extensive experiments, we show that signals like Self-Influence\neffectively detect mislabeled samples, but none of the existing signals can\ndetect anomalies. Existing signals do not take into account the training\ndynamics, i.e., how the samples' influence on the model changes during\ntraining, while some signals fall into influence cancellation effects, i.e.,\ninfluence score is zero due to unsigned scores accumulation, resulting in\nmisleading influence attribution.\n","authors":["Nikolaos Myrtakis","Ioannis Tsamardinos","Vassilis Christophides"],"pdf_url":"https://arxiv.org/pdf/2506.11584v1.pdf","comment":"Accepted and presented at the Data-centric Machine Learning Research\n  (DMLR) Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2405.15006v3","updated":"2025-06-13T08:43:26Z","published":"2024-05-23T19:23:09Z","title":"A Rescaling-Invariant Lipschitz Bound Based on Path-Metrics for Modern\n  ReLU Network Parameterizations","summary":"  Robustness with respect to weight perturbations underpins guarantees for\ngeneralization, pruning and quantization. Existing guarantees rely on Lipschitz\nbounds in parameter space, cover only plain feed-forward MLPs, and break under\nthe ubiquitous neuron-wise rescaling symmetry of ReLU networks. We prove a new\nLipschitz inequality expressed through the $\\ell^1$-path-metric of the weights.\nThe bound is (i) rescaling-invariant by construction and (ii) applies to any\nReLU-DAG architecture with any combination of convolutions, skip connections,\npooling, and frozen (inference-time) batch-normalization -- thus encompassing\nResNets, U-Nets, VGG-style CNNs, and more. By respecting the network's natural\nsymmetries, the new bound strictly sharpens prior parameter-space bounds and\ncan be computed in two forward passes. To illustrate its utility, we derive\nfrom it a symmetry-aware pruning criterion and show -- through a\nproof-of-concept experiment on a ResNet-18 trained on ImageNet -- that its\npruning performance matches that of classical magnitude pruning, while becoming\ntotally immune to arbitrary neuron-wise rescalings.\n","authors":["Antoine Gonon","Nicolas Brisebarre","Elisa Riccietti","Rémi Gribonval"],"pdf_url":"https://arxiv.org/pdf/2405.15006v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18023v2","updated":"2025-06-13T08:43:13Z","published":"2025-05-23T15:28:00Z","title":"Time to Spike? Understanding the Representational Power of Spiking\n  Neural Networks in Discrete Time","summary":"  Recent years have seen significant progress in developing spiking neural\nnetworks (SNNs) as a potential solution to the energy challenges posed by\nconventional artificial neural networks (ANNs). However, our theoretical\nunderstanding of SNNs remains relatively limited compared to the ever-growing\nbody of literature on ANNs. In this paper, we study a discrete-time model of\nSNNs based on leaky integrate-and-fire (LIF) neurons, referred to as\ndiscrete-time LIF-SNNs, a widely used framework that still lacks solid\ntheoretical foundations. We demonstrate that discrete-time LIF-SNNs with static\ninputs and outputs realize piecewise constant functions defined on polyhedral\nregions, and more importantly, we quantify the network size required to\napproximate continuous functions. Moreover, we investigate the impact of\nlatency (number of time steps) and depth (number of layers) on the complexity\nof the input space partitioning induced by discrete-time LIF-SNNs. Our analysis\nhighlights the importance of latency and contrasts these networks with ANNs\nemploying piecewise linear activation functions. Finally, we present numerical\nexperiments to support our theoretical findings.\n","authors":["Duc Anh Nguyen","Ernesto Araya","Adalbert Fono","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2505.18023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10816v4","updated":"2025-06-13T08:32:24Z","published":"2025-02-15T14:42:42Z","title":"BalanceBenchmark: A Survey for Multimodal Imbalance Learning","summary":"  Multimodal learning has gained attention for its capacity to integrate\ninformation from different modalities. However, it is often hindered by the\nmultimodal imbalance problem, where certain modality dominates while others\nremain underutilized. Although recent studies have proposed various methods to\nalleviate this problem, they lack comprehensive and fair comparisons. In this\npaper, we systematically categorize various mainstream multimodal imbalance\nalgorithms into four groups based on the strategies they employ to mitigate\nimbalance. To facilitate a comprehensive evaluation of these methods, we\nintroduce BalanceBenchmark, a benchmark including multiple widely used\nmultidimensional datasets and evaluation metrics from three perspectives:\nperformance, imbalance degree, and complexity. To ensure fair comparisons, we\nhave developed a modular and extensible toolkit that standardizes the\nexperimental workflow across different methods. Based on the experiments using\nBalanceBenchmark, we have identified several key insights into the\ncharacteristics and advantages of different method groups in terms of\nperformance, balance degree and computational complexity. We expect such\nanalysis could inspire more efficient approaches to address the imbalance\nproblem in the future, as well as foundation models. The code of the toolkit is\navailable at https://github.com/GeWu-Lab/BalanceBenchmark.\n","authors":["Shaoxuan Xu","Menglu Cui","Chengxiang Huang","Hongfa Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2502.10816v4.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.11565v1","updated":"2025-06-13T08:21:06Z","published":"2025-06-13T08:21:06Z","title":"Gradients of unitary optical neural networks using parameter-shift rule","summary":"  This paper explores the application of the parameter-shift rule (PSR) for\ncomputing gradients in unitary optical neural networks (UONNs). While\nbackpropagation has been fundamental to training conventional neural networks,\nits implementation in optical neural networks faces significant challenges due\nto the physical constraints of optical systems. We demonstrate how PSR, which\ncalculates gradients by evaluating functions at shifted parameter values, can\nbe effectively adapted for training UONNs constructed from Mach-Zehnder\ninterferometer meshes. The method leverages the inherent Fourier series nature\nof optical interference in these systems to compute exact analytical gradients\ndirectly from hardware measurements. This approach offers a promising\nalternative to traditional in silico training methods and circumvents the\nlimitations of both finite difference approximations and all-optical\nbackpropagation implementations. We present the theoretical framework and\npractical methodology for applying PSR to optimize phase parameters in optical\nneural networks, potentially advancing the development of efficient\nhardware-based training strategies for optical computing systems.\n","authors":["Jinzhe Jiang","Yaqian Zhao","Xin Zhang","Chen Li","Yunlong Yu","Hailing Liu"],"pdf_url":"https://arxiv.org/pdf/2506.11565v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.00744v2","updated":"2025-06-13T08:18:12Z","published":"2025-02-02T10:32:55Z","title":"CoNNect: Connectivity-Based Regularization for Structural Pruning","summary":"  Pruning encompasses a range of techniques aimed at increasing the sparsity of\nneural networks (NNs). These techniques can generally be framed as minimizing a\nloss function subject to an $L_0$ norm constraint. This paper introduces\nCoNNect, a novel differentiable regularizer for sparse NN training that ensures\nconnectivity between input and output layers. We prove that CoNNect\napproximates $L_0$ regularization, guaranteeing maximally connected network\nstructures while avoiding issues like layer collapse. Moreover, CoNNect is\neasily integrated with established structural pruning strategies. Numerical\nexperiments demonstrate that CoNNect can improve classical pruning strategies\nand enhance state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner.\n","authors":["Christian Franssen","Jinyang Jiang","Yijie Peng","Bernd Heidergott"],"pdf_url":"https://arxiv.org/pdf/2502.00744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11563v1","updated":"2025-06-13T08:17:07Z","published":"2025-06-13T08:17:07Z","title":"Learn to Preserve Personality: Federated Foundation Models in\n  Recommendations","summary":"  A core learning challenge for existed Foundation Models (FM) is striking the\ntradeoff between generalization with personalization, which is a dilemma that\nhas been highlighted by various parameter-efficient adaptation techniques.\nFederated foundation models (FFM) provide a structural means to decouple shared\nknowledge from individual specific adaptations via decentralized processes.\nRecommendation systems offer a perfect testbed for FFMs, given their reliance\non rich implicit feedback reflecting unique user characteristics. This position\npaper discusses a novel learning paradigm where FFMs not only harness their\ngeneralization capabilities but are specifically designed to preserve the\nintegrity of user personality, illustrated thoroughly within the recommendation\ncontexts. We envision future personal agents, powered by personalized adaptive\nFMs, guiding user decisions on content. Such an architecture promises a user\ncentric, decentralized system where individuals maintain control over their\npersonalized agents.\n","authors":["Zhiwei Li","Guodong Long","Chunxu Zhang","Honglei Zhang","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11563v1.pdf","comment":"14 pages, 3 figures, conference, position paper"},{"id":"http://arxiv.org/abs/2506.10680v2","updated":"2025-06-13T08:14:34Z","published":"2025-06-12T13:18:26Z","title":"Saturation Self-Organizing Map","summary":"  Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.\n","authors":["Igor Urbanik","Paweł Gajewski"],"pdf_url":"https://arxiv.org/pdf/2506.10680v2.pdf","comment":"github repository: https://github.com/Radinyn/satsom"},{"id":"http://arxiv.org/abs/2506.11552v1","updated":"2025-06-13T08:02:37Z","published":"2025-06-13T08:02:37Z","title":"Learning Encodings by Maximizing State Distinguishability: Variational\n  Quantum Error Correction","summary":"  Quantum error correction is crucial for protecting quantum information\nagainst decoherence. Traditional codes like the surface code require\nsubstantial overhead, making them impractical for near-term, early\nfault-tolerant devices. We propose a novel objective function for tailoring\nerror correction codes to specific noise structures by maximizing the\ndistinguishability between quantum states after a noise channel, ensuring\nefficient recovery operations. We formalize this concept with the\ndistinguishability loss function, serving as a machine learning objective to\ndiscover resource-efficient encoding circuits optimized for given noise\ncharacteristics. We implement this methodology using variational techniques,\ntermed variational quantum error correction (VarQEC). Our approach yields codes\nwith desirable theoretical and practical properties and outperforms standard\ncodes in various scenarios. We also provide proof-of-concept demonstrations on\nIBM and IQM hardware devices, highlighting the practical relevance of our\nprocedure.\n","authors":["Nico Meyer","Christopher Mutschler","Andreas Maier","Daniel D. Scherer"],"pdf_url":"https://arxiv.org/pdf/2506.11552v1.pdf","comment":"50 pages, 24 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.20445v3","updated":"2025-06-13T08:02:21Z","published":"2024-10-27T13:51:09Z","title":"TrajAgent: An LLM-based Agent Framework for Automated Trajectory\n  Modeling via Collaboration of Large and Small Models","summary":"  Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods.\n","authors":["Yuwei Du","Jie Feng","Jie Zhao","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.20445v3.pdf","comment":"the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent"},{"id":"http://arxiv.org/abs/2506.11550v1","updated":"2025-06-13T08:01:29Z","published":"2025-06-13T08:01:29Z","title":"Improving Multimodal Learning Balance and Sufficiency through Data\n  Remixing","summary":"  Different modalities hold considerable gaps in optimization trajectories,\nincluding speeds and paths, which lead to modality laziness and modality clash\nwhen jointly training multimodal models, resulting in insufficient and\nimbalanced multimodal learning. Existing methods focus on enforcing the weak\nmodality by adding modality-specific optimization objectives, aligning their\noptimization speeds, or decomposing multimodal learning to enhance unimodal\nlearning. These methods fail to achieve both unimodal sufficiency and\nmultimodal balance. In this paper, we, for the first time, address both\nconcerns by proposing multimodal Data Remixing, including decoupling multimodal\ndata and filtering hard samples for each modality to mitigate modality\nimbalance; and then batch-level reassembling to align the gradient directions\nand avoid cross-modal interference, thus enhancing unimodal learning\nsufficiency. Experimental results demonstrate that our method can be seamlessly\nintegrated with existing approaches, improving accuracy by approximately\n6.50%$\\uparrow$ on CREMAD and 3.41%$\\uparrow$ on Kinetic-Sounds, without\ntraining set expansion or additional computational overhead during inference.\nThe source code is available at\n\\href{https://github.com/MatthewMaxy/Remix_ICML2025}{Data Remixing}.\n","authors":["Xiaoyu Ma","Hao Chen","Yongjian Deng"],"pdf_url":"https://arxiv.org/pdf/2506.11550v1.pdf","comment":"ICML2025"},{"id":"http://arxiv.org/abs/2506.11543v1","updated":"2025-06-13T07:57:38Z","published":"2025-06-13T07:57:38Z","title":"FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher\n  Information Matrix Approximation","summary":"  Post-training quantization (PTQ) has stood out as a cost-effective and\npromising model compression paradigm in recent years, as it avoids\ncomputationally intensive model retraining. Nevertheless, current PTQ methods\nfor Vision Transformers (ViTs) still suffer from significant accuracy\ndegradation, especially under low-bit quantization. To address these\nshortcomings, we analyze the prevailing Hessian-guided quantization loss, and\nuncover certain limitations of conventional Hessian approximations. By\nfollowing the block-wise reconstruction framework, we propose a novel PTQ\nmethod for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the\nconnection between KL divergence and FIM, which enables fast computation of the\nquantization loss during reconstruction. We further propose an efficient FIM\napproximation method, namely DPLR-FIM, by employing the diagonal plus low-rank\nprinciple, and formulate the ultimate quantization loss. Our extensive\nexperiments, conducted across various vision tasks with representative\nViT-based architectures on public datasets, demonstrate that our method\nsubstantially promotes the accuracy compared to the state-of-the-art\napproaches, especially in the case of low-bit quantization. The source code is\navailable at https://github.com/ShiheWang/FIMA-Q.\n","authors":["Zhuguanyu Wu","Shihe Wang","Jiayi Zhang","Jiaxin Chen","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11543v1.pdf","comment":"CVPR 2025 Highlight"},{"id":"http://arxiv.org/abs/2411.15111v4","updated":"2025-06-13T07:52:33Z","published":"2024-11-22T18:25:13Z","title":"Learnable Activation Functions in Physics-Informed Neural Networks for\n  Solving Partial Differential Equations","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a promising approach\nfor solving Partial Differential Equations (PDEs). However, they face\nchallenges related to spectral bias (the tendency to learn low-frequency\ncomponents while struggling with high-frequency features) and unstable\nconvergence dynamics (mainly stemming from the multi-objective nature of the\nPINN loss function). These limitations impact their accuracy for problems\ninvolving rapid oscillations, sharp gradients, and complex boundary behaviors.\nWe systematically investigate learnable activation functions as a solution to\nthese challenges, comparing Multilayer Perceptrons (MLPs) using fixed and\nlearnable activation functions against Kolmogorov-Arnold Networks (KANs) that\nemploy learnable basis functions. Our evaluation spans diverse PDE types,\nincluding linear and non-linear wave problems, mixed-physics systems, and fluid\ndynamics. Using empirical Neural Tangent Kernel (NTK) analysis and Hessian\neigenvalue decomposition, we assess spectral bias and convergence stability of\nthe models. Our results reveal a trade-off between expressivity and training\nconvergence stability. While learnable activation functions work well in\nsimpler architectures, they encounter scalability issues in complex networks\ndue to the higher functional dimensionality. Counterintuitively, we find that\nlow spectral bias alone does not guarantee better accuracy, as functions with\nbroader NTK eigenvalue spectra may exhibit convergence instability. We\ndemonstrate that activation function selection remains inherently\nproblem-specific, with different bases showing distinct advantages for\nparticular PDE characteristics. We believe these insights will help in the\ndesign of more robust neural PDE solvers.\n","authors":["Afrah Farea","Mustafa Serdar Celebi"],"pdf_url":"https://arxiv.org/pdf/2411.15111v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05126v2","updated":"2025-06-13T07:46:35Z","published":"2025-05-08T10:57:28Z","title":"Taming OOD Actions for Offline Reinforcement Learning: An\n  Advantage-Based Approach","summary":"  Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks.\n","authors":["Xuyang Chen","Keyu Yan","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.05126v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19002v2","updated":"2025-06-13T07:42:25Z","published":"2025-02-26T10:06:37Z","title":"The Sharpness Disparity Principle in Transformers for Accelerating\n  Language Model Pre-Training","summary":"  Transformers consist of diverse building blocks, such as embedding layers,\nnormalization layers, self-attention mechanisms, and point-wise feedforward\nnetworks. Thus, understanding the differences and interactions among these\nblocks is important. In this paper, we uncover a clear Sharpness Disparity\nacross these blocks, which emerges early in training and intriguingly persists\nthroughout the training process. Motivated by this finding, we propose\nBlockwise Learning Rate (LR), a strategy that tailors the LR to each block's\nsharpness, accelerating large language model (LLM) pre-training. By integrating\nBlockwise LR into AdamW, we consistently achieve lower terminal loss and nearly\n$2\\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration\nacross GPT-2 and LLaMA, with model sizes ranging from 0.12B to 2B and datasets\nof OpenWebText, MiniPile, and C4. Finally, we incorporate Blockwise LR into\nAdam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of\nAdam, achieving a combined $2\\times$ speedup and $2\\times$ memory saving. These\nresults underscore the potential of exploiting the sharpness disparity to\nimprove LLM training.\n","authors":["Jinbo Wang","Mingze Wang","Zhanpeng Zhou","Junchi Yan","Weinan E","Lei Wu"],"pdf_url":"https://arxiv.org/pdf/2502.19002v2.pdf","comment":"21 pages, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.11530v1","updated":"2025-06-13T07:30:35Z","published":"2025-06-13T07:30:35Z","title":"Robust Filtering -- Novel Statistical Learning and Inference Algorithms\n  with Applications","summary":"  State estimation or filtering serves as a fundamental task to enable\nintelligent decision-making in applications such as autonomous vehicles,\nrobotics, healthcare monitoring, smart grids, intelligent transportation, and\npredictive maintenance. Standard filtering assumes prior knowledge of noise\nstatistics to extract latent system states from noisy sensor data. However,\nreal-world scenarios involve abnormalities like outliers, biases, drifts, and\nmissing observations with unknown or partially known statistics, limiting\nconventional approaches. This thesis presents novel robust nonlinear filtering\nmethods to mitigate these challenges. Based on insights from our filtering\nproposals, we extend the formulations to offline estimation/learning setups and\npropose smoothing extensions. Our methods leverage Bayesian inference\nframeworks, employing both deterministic and stochastic approximation\ntechniques including Variational Inference (VI) and Particle Filters/Sequential\nMonte Carlo (SMC). We also study theoretical estimation limits using Bayesian\nCram\\'er-Rao bounds (BCRBs) in the context of measurement abnormalities. To\nvalidate the performance gains of the proposed methods, we perform simulations\nand experiments in scenarios including target tracking, indoor localization, 3D\npoint cloud registration, mesh registration, and pose graph optimization. The\nfundamental nature of the work makes it useful in diverse applications, with\npossible future extensions toward developing outlier-robust machine learning\npipelines, learning system dynamics from anomalous data, and addressing\nchallenges in generative AI where standard diffusion models struggle with\noutliers, imbalanced datasets, and mode collapse.\n","authors":["Aamir Hussain Chughtai"],"pdf_url":"https://arxiv.org/pdf/2506.11530v1.pdf","comment":"PhD Thesis"},{"id":"http://arxiv.org/abs/2506.11528v1","updated":"2025-06-13T07:30:12Z","published":"2025-06-13T07:30:12Z","title":"Delayformer: spatiotemporal transformation for predicting\n  high-dimensional dynamics","summary":"  Predicting time-series is of great importance in various scientific and\nengineering fields. However, in the context of limited and noisy data,\naccurately predicting dynamics of all variables in a high-dimensional system is\na challenging task due to their nonlinearity and also complex interactions.\nCurrent methods including deep learning approaches often perform poorly for\nreal-world systems under such circumstances. This study introduces the\nDelayformer framework for simultaneously predicting dynamics of all variables,\nby developing a novel multivariate spatiotemporal information (mvSTI)\ntransformation that makes each observed variable into a delay-embedded state\n(vector) and further cross-learns those states from different variables. From\ndynamical systems viewpoint, Delayformer predicts system states rather than\nindividual variables, thus theoretically and computationally overcoming such\nnonlinearity and cross-interaction problems. Specifically, it first utilizes a\nsingle shared Visual Transformer (ViT) encoder to cross-represent dynamical\nstates from observed variables in a delay embedded form and then employs\ndistinct linear decoders for predicting next states, i.e. equivalently\npredicting all original variables parallelly. By leveraging the theoretical\nfoundations of delay embedding theory and the representational capabilities of\nTransformers, Delayformer outperforms current state-of-the-art methods in\nforecasting tasks on both synthetic and real-world datasets. Furthermore, the\npotential of Delayformer as a foundational time-series model is demonstrated\nthrough cross-domain forecasting tasks, highlighting its broad applicability\nacross various scenarios.\n","authors":["Zijian Wang","Peng Tao","Luonan Chen"],"pdf_url":"https://arxiv.org/pdf/2506.11528v1.pdf","comment":"This paper is currently under review"},{"id":"http://arxiv.org/abs/2503.06706v3","updated":"2025-06-13T07:21:17Z","published":"2025-03-09T17:43:30Z","title":"PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on\n  UML Flowcharts","summary":"  Process-driven dialogue systems, which operate under strict predefined\nprocess constraints, are essential in customer service and equipment\nmaintenance scenarios. Although Large Language Models (LLMs) have shown\nremarkable progress in dialogue and reasoning, they still struggle to solve\nthese strictly constrained dialogue tasks. To address this challenge, we\nconstruct Process Flow Dialogue (PFDial) dataset, which contains 12,705\nhigh-quality Chinese dialogue instructions derived from 440 flowcharts\ncontaining 5,055 process nodes. Based on PlantUML specification, each UML\nflowchart is converted into atomic dialogue units i.e., structured five-tuples.\nExperimental results demonstrate that a 7B model trained with merely 800\nsamples, and a 0.5B model trained on total data both can surpass 90% accuracy.\nAdditionally, the 8B model can surpass GPT-4o up to 43.88% with an average of\n11.00%. We further evaluate models' performance on challenging backward\ntransitions in process flows and conduct an in-depth analysis of various\ndataset formats to reveal their impact on model performance in handling\ndecision and sequential branches. The data is released in\nhttps://github.com/KongLongGeFDU/PFDial.\n","authors":["Ming Zhang","Yuhui Wang","Yujiong Shen","Tingyi Yang","Changhao Jiang","Yilong Wu","Shihan Dou","Qinhao Chen","Zhiheng Xi","Zhihao Zhang","Yi Dong","Zhen Wang","Zhihui Fei","Mingyang Wan","Tao Liang","Guojun Ma","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.06706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13055v2","updated":"2025-06-13T07:21:01Z","published":"2025-05-19T12:43:33Z","title":"Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio\n  Channels","summary":"  We introduce the Sparse pretrained Radio Transformer (SpaRTran), an\nunsupervised representation learning approach based on the concept of\ncompressed sensing for radio channels. Our approach learns embeddings that\nfocus on the physical properties of radio propagation, to create the optimal\nbasis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparse\ngated autoencoder that induces a simplicity bias to the learned\nrepresentations, resembling the sparse nature of radio propagation. For signal\nreconstruction, it learns a dictionary that holds atomic features, which\nincreases flexibility across signal waveforms and spatiotemporal signal\npatterns. Our experiments show that SpaRTran reduces errors by up to 85 %\ncompared to state-of-the-art methods when fine-tuned on radio fingerprinting, a\nchallenging downstream task. In addition, our method requires less pretraining\neffort and offers greater flexibility, as we train it solely on individual\nradio signals. SpaRTran serves as an excellent base model that can be\nfine-tuned for various radio-based downstream tasks, effectively reducing the\ncost for labeling. In addition, it is significantly more versatile than\nexisting methods and demonstrates superior generalization.\n","authors":["Jonathan Ott","Maximilian Stahlke","Tobias Feigl","Bjoern M. Eskofier","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2505.13055v2.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.11516v1","updated":"2025-06-13T07:17:41Z","published":"2025-06-13T07:17:41Z","title":"Brewing Knowledge in Context: Distillation Perspectives on In-Context\n  Learning","summary":"  In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection.\n","authors":["Chengye Li","Haiyun Liu","Yuanxi Li"],"pdf_url":"https://arxiv.org/pdf/2506.11516v1.pdf","comment":"10 main pages, 10 page appendix"},{"id":"http://arxiv.org/abs/2506.11515v1","updated":"2025-06-13T07:16:41Z","published":"2025-06-13T07:16:41Z","title":"Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs\n  and MLLMs","summary":"  Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.\n","authors":["Xiao Xu","Libo Qin","Wanxiang Che","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2506.11515v1.pdf","comment":"Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266"},{"id":"http://arxiv.org/abs/2506.11512v1","updated":"2025-06-13T07:13:05Z","published":"2025-06-13T07:13:05Z","title":"Prioritizing Alignment Paradigms over Task-Specific Model Customization\n  in Time-Series LLMs","summary":"  Recent advances in Large Language Models (LLMs) have enabled unprecedented\ncapabilities for time-series reasoning in diverse real-world applications,\nincluding medical, financial, and spatio-temporal domains. However, existing\napproaches typically focus on task-specific model customization, such as\nforecasting and anomaly detection, while overlooking the data itself, referred\nto as time-series primitives, which are essential for in-depth reasoning. This\nposition paper advocates a fundamental shift in approaching time-series\nreasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic\nprimitives of time series data over task-specific model customization. This\nrealignment addresses the core limitations of current time-series reasoning\napproaches, which are often costly, inflexible, and inefficient, by\nsystematically accounting for intrinsic structure of data before task\nengineering. To this end, we propose three alignment paradigms: Injective\nAlignment, Bridging Alignment, and Internal Alignment, which are emphasized by\nprioritizing different aspects of time-series primitives: domain,\ncharacteristic, and representation, respectively, to activate time-series\nreasoning capabilities of LLMs to enable economical, flexible, and efficient\nreasoning. We further recommend that practitioners adopt an alignment-oriented\nmethod to avail this instruction to select an appropriate alignment paradigm.\nAdditionally, we categorize relevant literature into these alignment paradigms\nand outline promising research directions.\n","authors":["Wei Li","Yunyao Cheng","Xinli Hao","Chaohong Ma","Yuxuan Liang","Bin Yang","Christian S. Jensen","Xiaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2506.11512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11511v1","updated":"2025-06-13T07:12:49Z","published":"2025-06-13T07:12:49Z","title":"Task-Driven Discrete Representation Learning","summary":"  In recent years, deep discrete representation learning (DRL) has achieved\nsignificant success across various domains. Most DRL frameworks (e.g., the\nwidely used VQ-VAE and its variants) have primarily focused on generative\nsettings, where the quality of a representation is implicitly gauged by the\nfidelity of its generation. In fact, the goodness of a discrete representation\nremain ambiguously defined across the literature. In this work, we adopt a\npractical approach that examines DRL from a task-driven perspective. We propose\na unified framework that explores the usefulness of discrete features in\nrelation to downstream tasks, with generation naturally viewed as one possible\napplication. In this context, the properties of discrete representations as\nwell as the way they benefit certain tasks are also relatively understudied. We\ntherefore provide an additional theoretical analysis of the trade-off between\nrepresentational capacity and sample complexity, shedding light on how discrete\nrepresentation utilization impacts task performance. Finally, we demonstrate\nthe flexibility and effectiveness of our framework across diverse applications.\n","authors":["Tung-Long Vuong"],"pdf_url":"https://arxiv.org/pdf/2506.11511v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.11934v1","updated":"2025-06-13T16:36:44Z","published":"2025-06-13T16:36:44Z","title":"Temporal Dynamics of Emotions in Italian Online Soccer Fandoms","summary":"  This study investigates the emotional dynamics of Italian soccer fandoms\nthrough computational analysis of user-generated content from official\nInstagram accounts of 83 teams across Serie A, Serie B, and Lega Pro during the\n2023-24 season. By applying sentiment analysis to fan comments, we extract\ntemporal emotional patterns and identify distinct clusters of fan bases with\nsimilar preseason expectations. Drawing from complex systems theory, we\ncharacterize joy as displaying anti-bursty temporal distributions, while anger\nis marked by pronounced bursty patterns. Our analysis reveals significant\ncorrelations between these emotional signals, preseason expectations,\nsocioeconomic factors, and final league rankings. In particular, the burstiness\nmetric emerges as a meaningful correlate of team performance; statistical\nmodels excluding this parameter show a decrease in the coefficient of\ndetermination of 32%. These findings offer novel insights into the relationship\nbetween fan emotional expression and team outcomes, suggesting potential\navenues for research in sports analytics, social media dynamics, and fan\nengagement studies.\n","authors":["Salvatore Citraro","Giovanni Mauro","Emanuele Ferragina"],"pdf_url":"https://arxiv.org/pdf/2506.11934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11737v1","updated":"2025-06-13T12:48:39Z","published":"2025-06-13T12:48:39Z","title":"Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in\n  Interleaved Multi-Image Model","summary":"  This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.\n","authors":["Dinh Viet Cuong","Hoang-Bao Le","An Pham Ngoc Nguyen","Liting Zhou","Cathal Gurrin"],"pdf_url":"https://arxiv.org/pdf/2506.11737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05987v2","updated":"2025-06-13T12:25:00Z","published":"2025-06-06T11:16:40Z","title":"The JPEG XL Image Coding System: History, Features, Coding Tools, Design\n  Rationale, and Future","summary":"  JPEG XL is a new image coding system offering state-of-the-art compression\nperformance, lossless JPEG recompression, and advanced features. It aims to\nreplace JPEG, PNG, GIF, and other formats with a single universal codec. This\narticle provides an overview of JPEG XL, including its history, design\nrationale, coding tools, and future potential. It can be used as a companion\ndocument to the standard (ISO/IEC 18181), or as a standalone article to better\nunderstand JPEG XL, either at a high level or in considerable technical detail.\n","authors":["Jon Sneyers","Jyrki Alakuijala","Luca Versari","Zoltán Szabadka","Sami Boukortt","Amnon Cohen-Tidhar","Moritz Firsching","Evgenii Kliuchnikov","Tal Lev-Ami","Eric Portis","Thomas Richter","Osamu Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.05987v2.pdf","comment":"73 pages, 62 figures"},{"id":"http://arxiv.org/abs/2504.00812v2","updated":"2025-06-13T08:31:34Z","published":"2025-04-01T14:03:46Z","title":"Scaling Prompt Instructed Zero Shot Composed Image Retrieval with\n  Image-Only Data","summary":"  Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines.\n","authors":["Yiqun Duan","Sameera Ramasinghe","Stephen Gould","Ajanthan Thalaiyasingam"],"pdf_url":"https://arxiv.org/pdf/2504.00812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11521v1","updated":"2025-06-13T07:22:36Z","published":"2025-06-13T07:22:36Z","title":"Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks:\n  A Comprehensive Survey Emphasizing Multimodal Models","summary":"  Multimodal large language models (MLLMs), which bridge the gap between\naudio-visual and natural language processing, achieve state-of-the-art\nperformance on several audio-visual tasks. Despite the superior performance of\nMLLMs, the scarcity of high-quality audio-visual training data and\ncomputational resources necessitates the utilization of third-party data and\nopen-source MLLMs, a trend that is increasingly observed in contemporary\nresearch. This prosperity masks significant security risks. Empirical studies\ndemonstrate that the latest MLLMs can be manipulated to produce malicious or\nharmful content. This manipulation is facilitated exclusively through\ninstructions or inputs, including adversarial perturbations and malevolent\nqueries, effectively bypassing the internal security mechanisms embedded within\nthe models. To gain a deeper comprehension of the inherent security\nvulnerabilities associated with audio-visual-based multimodal models, a series\nof surveys investigates various types of attacks, including adversarial and\nbackdoor attacks. While existing surveys on audio-visual attacks provide a\ncomprehensive overview, they are limited to specific types of attacks, which\nlack a unified review of various types of attacks. To address this issue and\ngain insights into the latest trends in the field, this paper presents a\ncomprehensive and systematic review of audio-visual attacks, which include\nadversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this\npaper also reviews various types of attacks in the latest audio-visual-based\nMLLMs, a dimension notably absent in existing surveys. Drawing upon\ncomprehensive insights from a substantial review, this paper delineates both\nchallenges and emergent trends for future research on audio-visual attacks and\ndefense.\n","authors":["Jinming Wen","Xinyi Wu","Shuai Zhao","Yanhao Jia","Yuwen Li"],"pdf_url":"https://arxiv.org/pdf/2506.11521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01881v3","updated":"2025-06-13T03:36:19Z","published":"2025-05-03T17:59:26Z","title":"PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in\n  Navigation Applications","summary":"  Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems.\n","authors":["Trisanth Srinivasan","Santosh Patapati"],"pdf_url":"https://arxiv.org/pdf/2505.01881v3.pdf","comment":"Accepted at IEEE/CVF Computer Society Conference on Computer Vision\n  and Pattern Recognition Workshops 2025 (CVPRW)"},{"id":"http://arxiv.org/abs/2506.10009v2","updated":"2025-06-13T00:54:34Z","published":"2025-04-21T21:02:51Z","title":"The Iris File Extension","summary":"  A modern digital pathology vendor-agnostic binary slide format specifically\ntargeting the unmet need of efficient real-time transfer and display has not\nyet been established. The growing adoption of digital pathology only\nintensifies the need for an intermediary digital slide format that emphasizes\nperformance for use between slide servers and image management software. The\nDICOM standard is a well-established format widely used for the long-term\nstorage of both images and associated critical metadata. However, it was\ninherently designed for radiology rather than digital pathology, a discipline\nthat imposes a unique set of performance requirements due to high-speed\nmulti-pyramidal rendering within whole slide viewer applications. Here we\nintroduce the Iris file extension, a binary container specification explicitly\ndesigned for performance-oriented whole slide image viewer systems. The Iris\nfile extension specification is explicit and straightforward, adding modern\ncompression support, a dynamic structure with fully optional metadata features,\ncomputationally trivial deep file validation, corruption recovery capabilities,\nand slide annotations. In addition to the file specification document, we\nprovide source code to allow for (de)serialization and validation of a binary\nstream against the standard. We also provide corresponding binary builds with\nC++, Python, and JavaScript language support. Finally, we provide full encoder\nand decoder implementation source code, as well as binary builds (part of the\nseparate Iris Codec Community module), with language bindings for C++ and\nPython, allowing for easy integration with existing WSI solutions. We provide\nthe Iris File Extension specification openly to the community in the form of a\nCreative Commons Attribution-No Derivative 4.0 International license.\n","authors":["Ryan Erik Landvater","Michael David Olp","Mustafa Yousif","Ulysses Balis"],"pdf_url":"https://arxiv.org/pdf/2506.10009v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.12269v1","updated":"2025-06-13T22:46:27Z","published":"2025-06-13T22:46:27Z","title":"ICME 2025 Grand Challenge on Video Super-Resolution for Video\n  Conferencing","summary":"  Super-Resolution (SR) is a critical task in computer vision, focusing on\nreconstructing high-resolution (HR) images from low-resolution (LR) inputs. The\nfield has seen significant progress through various challenges, particularly in\nsingle-image SR. Video Super-Resolution (VSR) extends this to the temporal\ndomain, aiming to enhance video quality using methods like local, uni-,\nbi-directional propagation, or traditional upscaling followed by restoration.\nThis challenge addresses VSR for conferencing, where LR videos are encoded with\nH.265 at fixed QPs. The goal is to upscale videos by a specific factor,\nproviding HR outputs with enhanced perceptual quality under a low-delay\nscenario using causal models. The challenge included three tracks:\ngeneral-purpose videos, talking head videos, and screen content videos, with\nseparate datasets provided by the organizers for training, validation, and\ntesting. We open-sourced a new screen content dataset for the SR task in this\nchallenge. Submissions were evaluated through subjective tests using a\ncrowdsourced implementation of the ITU-T Rec P.910.\n","authors":["Babak Naderi","Ross Cutler","Juhee Cho","Nabakumar Khongbantabam","Dejan Ivkovic"],"pdf_url":"https://arxiv.org/pdf/2506.12269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10016v2","updated":"2025-06-13T20:56:42Z","published":"2025-05-29T12:29:39Z","title":"A Survey of Generative Categories and Techniques in Multimodal Large\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text\ngeneration, now spanning diverse output modalities including images, music,\nvideo, human motion, and 3D objects, by integrating language with other sensory\nmodalities under unified architectures. This survey categorises six primary\ngenerative modalities and examines how foundational techniques, namely\nSelf-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement\nLearning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,\nenable cross-modal capabilities. We analyze key models, architectural trends,\nand emergent cross-modal synergies, while highlighting transferable techniques\nand unresolved challenges. Architectural innovations like transformers and\ndiffusion models underpin this convergence, enabling cross-modal transfer and\nmodular specialization. We highlight emerging patterns of synergy, and identify\nopen challenges in evaluation, modularity, and structured reasoning. This\nsurvey offers a unified perspective on MLLM development and identifies critical\npaths toward more general-purpose, adaptive, and interpretable multimodal\nsystems.\n","authors":["Longzhen Han","Awes Mubarak","Almas Baimagambetov","Nikolaos Polatidis","Thar Baker"],"pdf_url":"https://arxiv.org/pdf/2506.10016v2.pdf","comment":null}]},"2025-06-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.13752v1","updated":"2025-06-16T17:57:05Z","published":"2025-06-16T17:57:05Z","title":"Steering LLM Thinking with Budget Guidance","summary":"  Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.\n","authors":["Junyan Li","Wenshuo Zhao","Yang Zhang","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2506.13752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13743v1","updated":"2025-06-16T17:53:18Z","published":"2025-06-16T17:53:18Z","title":"LTRR: Learning To Rank Retrievers for LLMs","summary":"  Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.\n","authors":["To Eun Kim","Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2506.13743v1.pdf","comment":"SIGIR 2025 LiveRAG Spotlight"},{"id":"http://arxiv.org/abs/2506.13734v1","updated":"2025-06-16T17:42:35Z","published":"2025-06-16T17:42:35Z","title":"Instruction Following by Boosting Attention of Large Language Models","summary":"  Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.\n","authors":["Vitoria Guardieiro","Adam Stein","Avishree Khare","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2506.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13727v1","updated":"2025-06-16T17:38:36Z","published":"2025-06-16T17:38:36Z","title":"Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs","summary":"  Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2506.13727v1.pdf","comment":"Work in progress (10 pages manuscript, 3 pages references, 12 pages\n  appendix)"},{"id":"http://arxiv.org/abs/2506.05606v2","updated":"2025-06-16T17:32:08Z","published":"2025-06-05T21:37:49Z","title":"OPeRA: A Dataset of Observation, Persona, Rationale, and Action for\n  Evaluating LLMs on Human Online Shopping Behavior Simulation","summary":"  Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.\n","authors":["Ziyi Wang","Yuxuan Lu","Wenbo Li","Amirali Amini","Bo Sun","Yakov Bart","Weimin Lyu","Jiri Gesi","Tian Wang","Jing Huang","Yu Su","Upol Ehsan","Malihe Alikhani","Toby Jia-Jun Li","Lydia Chilton","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2506.05606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13692v1","updated":"2025-06-16T16:54:03Z","published":"2025-06-16T16:54:03Z","title":"Balancing Knowledge Delivery and Emotional Comfort in Healthcare\n  Conversational Systems","summary":"  With the advancement of large language models, many dialogue systems are now\ncapable of providing reasonable and informative responses to patients' medical\nconditions. However, when patients consult their doctor, they may experience\nnegative emotions due to the severity and urgency of their situation. If the\nmodel can provide appropriate comfort and empathy based on the patient's\nnegative emotions while answering medical questions, it will likely offer a\nmore reassuring experience during the medical consultation process. To address\nthis issue, our paper explores the balance between knowledge sharing and\nemotional support in the healthcare dialogue process. We utilize a large\nlanguage model to rewrite a real-world interactive medical dialogue dataset,\ngenerating patient queries with negative emotions and corresponding medical\nresponses aimed at soothing the patient's emotions while addressing their\nconcerns. The modified data serves to refine the latest large language models\nwith various fine-tuning methods, enabling them to accurately provide sentences\nwith both emotional reassurance and constructive suggestions in response to\npatients' questions. Compared to the original LLM model, our experimental\nresults demonstrate that our methodology significantly enhances the model's\nability to generate emotional responses while maintaining its original\ncapability to provide accurate knowledge-based answers.\n","authors":["Shang-Chi Tsai","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13692v1.pdf","comment":"IWSDS 2025 Oral Paper"},{"id":"http://arxiv.org/abs/2503.23077v2","updated":"2025-06-16T16:51:48Z","published":"2025-03-29T13:27:46Z","title":"Efficient Inference for Large Reasoning Models: A Survey","summary":"  Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.\n","authors":["Yue Liu","Jiaying Wu","Yufei He","Hongcheng Gao","Hongyu Chen","Baolong Bi","Ruihan Gong","Jiaheng Zhang","Zhiqi Huang","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2503.23077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20273v4","updated":"2025-06-16T16:51:10Z","published":"2025-02-27T17:01:23Z","title":"How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data","summary":"  Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.\n","authors":["Varshini Reddy","Craig W. Schmidt","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2502.20273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13681v1","updated":"2025-06-16T16:38:04Z","published":"2025-06-16T16:38:04Z","title":"Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models","summary":"  Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.\n","authors":["Rylan Schaeffer","Joshua Kazdan","Yegor Denisov-Blanch"],"pdf_url":"https://arxiv.org/pdf/2506.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13674v1","updated":"2025-06-16T16:30:26Z","published":"2025-06-16T16:30:26Z","title":"Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent\n  Prefix Data","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.\n","authors":["Haonan Wang","Brian Chen","Li Siquan","Liang Xinhe","Tianyang Hu","Hwee Kuan Lee","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2506.13674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03781v2","updated":"2025-06-16T16:25:05Z","published":"2025-06-04T09:42:17Z","title":"Unifying Uniform and Binary-coding Quantization for Accurate Compression\n  of Large Language Models","summary":"  How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.\n","authors":["Seungcheol Park","Jeongin Bae","Beomseok Kwon","Minjun Kim","Byeongwook Kim","Se Jung Kwon","U Kang","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2506.03781v2.pdf","comment":"ACL 2025 Main Track"},{"id":"http://arxiv.org/abs/2408.14568v2","updated":"2025-06-16T16:24:42Z","published":"2024-08-26T18:39:31Z","title":"Improving Clinical Note Generation from Complex Doctor-Patient\n  Conversation","summary":"  Writing clinical notes and documenting medical exams is a critical task for\nhealthcare professionals, serving as a vital component of patient care\ndocumentation. However, manually writing these notes is time-consuming and can\nimpact the amount of time clinicians can spend on direct patient interaction\nand other tasks. Consequently, the development of automated clinical note\ngeneration systems has emerged as a clinically meaningful area of research\nwithin AI for health. In this paper, we present three key contributions to the\nfield of clinical note generation using large language models (LLMs). First, we\nintroduce CliniKnote, a comprehensive dataset consisting of 1,200 complex\ndoctor-patient conversations paired with their full clinical notes. This\ndataset, created and curated by medical experts with the help of modern neural\nnetworks, provides a valuable resource for training and evaluating models in\nclinical note generation tasks. Second, we propose the K-SOAP (Keyword,\nSubjective, Objective, Assessment, and Plan) note format, which enhances\ntraditional SOAP~\\cite{podder2023soap} (Subjective, Objective, Assessment, and\nPlan) notes by adding a keyword section at the top, allowing for quick\nidentification of essential information. Third, we develop an automatic\npipeline to generate K-SOAP notes from doctor-patient conversations and\nbenchmark various modern LLMs using various metrics. Our results demonstrate\nsignificant improvements in efficiency and performance compared to standard LLM\nfinetuning methods.\n","authors":["Yizhan Li","Sifan Wu","Christopher Smith","Thomas Lo","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.14568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05317v2","updated":"2025-06-16T16:22:39Z","published":"2025-02-21T09:43:18Z","title":"On Synthesizing Data for Context Attribution in Question Answering","summary":"  Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.\n","authors":["Gorjan Radevski","Kiril Gashteovski","Shahbaz Syed","Christopher Malon","Sebastien Nicolas","Chia-Chien Hung","Timo Sztyler","Verena Heußer","Wiem Ben Rim","Masafumi Enomoto","Kunihiro Takeoka","Masafumi Oyamada","Goran Glavaš","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2504.05317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13642v1","updated":"2025-06-16T16:06:45Z","published":"2025-06-16T16:06:45Z","title":"Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model","summary":"  The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.\n","authors":["Shaolei Zhang","Shoutao Guo","Qingkai Fang","Yan Zhou","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2506.13642v1.pdf","comment":"Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b"},{"id":"http://arxiv.org/abs/2506.13641v1","updated":"2025-06-16T16:05:17Z","published":"2025-06-16T16:05:17Z","title":"EvolvTrip: Enhancing Literary Character Understanding with Temporal\n  Theory-of-Mind Graphs","summary":"  A compelling portrayal of characters is essential to the success of narrative\nwriting. For readers, appreciating a character's traits requires the ability to\ninfer their evolving beliefs, desires, and intentions over the course of a\ncomplex storyline, a cognitive skill known as Theory-of-Mind (ToM). Performing\nToM reasoning in prolonged narratives requires readers to integrate historical\ncontext with current narrative information, a task at which humans excel but\nLarge Language Models (LLMs) often struggle. To systematically evaluate LLMs'\nToM reasoning capability in long narratives, we construct LitCharToM, a\nbenchmark of character-centric questions across four ToM dimensions from\nclassic literature. Further, we introduce EvolvTrip, a perspective-aware\ntemporal knowledge graph that tracks psychological development throughout\nnarratives. Our experiments demonstrate that EvolvTrip consistently enhances\nperformance of LLMs across varying scales, even in challenging extended-context\nscenarios. EvolvTrip proves to be particularly valuable for smaller models,\npartially bridging the performance gap with larger LLMs and showing great\ncompatibility with lengthy narratives. Our findings highlight the importance of\nexplicit representation of temporal character mental states in narrative\ncomprehension and offer a foundation for more sophisticated character\nunderstanding. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/EvolvTrip.\n","authors":["Bohao Yang","Hainiu Xu","Jinhua Du","Ze Li","Yulan He","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2506.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13639v1","updated":"2025-06-16T16:04:43Z","published":"2025-06-16T16:04:43Z","title":"An Empirical Study of LLM-as-a-Judge: How Design Choices Impact\n  Evaluation Reliability","summary":"  As large language models (LLMs) continue to advance, reliable evaluation\nmethods are essential particularly for open-ended, instruction-following tasks.\nLLM-as-a-Judge enables automatic evaluation using LLMs as evaluators, but its\nreliability remains uncertain. In this work, we analyze key factors affecting\nits trustworthiness, focusing on alignment with human judgments and evaluation\nconsistency. Using BIGGENBench and EvalBiasBench, we study the effects of\nevaluation design, decoding strategies, and Chain-of-Tought (CoT) reasoning in\nevaluation. Our results show that evaluation criteria are critical for\nreliability, non-deterministic sampling improves alignment with human\npreferences over deterministic evaluation, and CoT reasoning offers minimal\ngains when clear evaluation criteria are present.\n","authors":["Yusuke Yamauchi","Taro Yano","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2506.13639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11130v2","updated":"2025-06-16T15:47:41Z","published":"2025-06-10T17:30:32Z","title":"A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data","summary":"  We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.\n","authors":["Cheng-Kang Chou","Chan-Jan Hsu","Ho-Lam Chung","Liang-Hsuan Tseng","Hsi-Chun Cheng","Yu-Kuan Fu","Kuan Po Huang","Hung-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2506.11130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13610v1","updated":"2025-06-16T15:38:39Z","published":"2025-06-16T15:38:39Z","title":"A Structured Bangla Dataset of Disease-Symptom Associations to Improve\n  Diagnostic Accuracy","summary":"  Disease-symptom datasets are significant and in demand for medical research,\ndisease diagnosis, clinical decision-making, and AI-driven health management\napplications. These datasets help identify symptom patterns associated with\nspecific diseases, thus improving diagnostic accuracy and enabling early\ndetection. The dataset presented in this study systematically compiles\ndisease-symptom relationships from various online sources, medical literature,\nand publicly available health databases. The data was gathered through\nanalyzing peer-reviewed medical articles, clinical case studies, and\ndisease-symptom association reports. Only the verified medical sources were\nincluded in the dataset, while those from non-peer-reviewed and anecdotal\nsources were excluded. The dataset is structured in a tabular format, where the\nfirst column represents diseases, and the remaining columns represent symptoms.\nEach symptom cell contains a binary value (1 or 0), indicating whether a\nsymptom is associated with a disease (1 for presence, 0 for absence). Thereby,\nthis structured representation makes the dataset very useful for a wide range\nof applications, including machine learning-based disease prediction, clinical\ndecision support systems, and epidemiological studies. Although there are some\nadvancements in the field of disease-symptom datasets, there is a significant\ngap in structured datasets for the Bangla language. This dataset aims to bridge\nthat gap by facilitating the development of multilingual medical informatics\ntools and improving disease prediction models for underrepresented linguistic\ncommunities. Further developments should include region-specific diseases and\nfurther fine-tuning of symptom associations for better diagnostic performance\n","authors":["Abdullah Al Shafi","Rowzatul Zannat","Abdul Muntakim","Mahmudul Hasan"],"pdf_url":"https://arxiv.org/pdf/2506.13610v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2501.02039v2","updated":"2025-06-16T15:37:06Z","published":"2025-01-03T14:35:32Z","title":"An Investigation into Value Misalignment in LLM-Generated Texts for\n  Cultural Heritage","summary":"  As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.\n","authors":["Fan Bu","Zheng Wang","Siyi Wang","Ziyao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.02039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00942v2","updated":"2025-06-16T15:32:22Z","published":"2025-04-01T16:28:38Z","title":"Experiential Semantic Information and Brain Alignment: Are Multimodal\n  Models Better than Language Models?","summary":"  A common assumption in Computational Linguistics is that text representations\nlearnt by multimodal models are richer and more human-like than those by\nlanguage-only models, as they are grounded in images or audio -- similar to how\nhuman language is grounded in real-world experiences. However, empirical\nstudies checking whether this is true are largely lacking. We address this gap\nby comparing word representations from contrastive multimodal models vs.\nlanguage-only ones in the extent to which they capture experiential information\n-- as defined by an existing norm-based 'experiential model' -- and align with\nhuman fMRI responses. Our results indicate that, surprisingly, language-only\nmodels are superior to multimodal ones in both respects. Additionally, they\nlearn more unique brain-relevant semantic information beyond that shared with\nthe experiential model. Overall, our study highlights the need to develop\ncomputational models that better integrate the complementary semantic\ninformation provided by multimodal data sources.\n","authors":["Anna Bavaresco","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2504.00942v2.pdf","comment":"Accepted to CoNLL 2025"},{"id":"http://arxiv.org/abs/2502.12150v2","updated":"2025-06-16T15:27:25Z","published":"2025-02-17T18:59:02Z","title":"Idiosyncrasies in Large Language Models","summary":"  In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning text embedding models on LLM-generated texts yields\nexcellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, including training on synthetic data,\ninferring model similarity, and robust evaluation of LLMs. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.\n","authors":["Mingjie Sun","Yida Yin","Zhiqiu Xu","J. Zico Kolter","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12150v2.pdf","comment":"Published in ICML 2025. Website at\n  https://eric-mingjie.github.io/llm-idiosyncrasies/index.html"},{"id":"http://arxiv.org/abs/2506.13599v1","updated":"2025-06-16T15:24:07Z","published":"2025-06-16T15:24:07Z","title":"CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation","summary":"  Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.\n","authors":["Yuwei Du","Jie Feng","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2506.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13596v1","updated":"2025-06-16T15:23:07Z","published":"2025-06-16T15:23:07Z","title":"Qwen vs. Gemma Integration with Whisper: A Comparative Study in\n  Multilingual SpeechLLM Systems","summary":"  This paper presents our system for the MLC-SLM Challenge 2025, focusing on\nmultilingual speech recognition and language modeling with large language\nmodels (LLMs). Our approach combines a fine-tuned Whisper-large-v3 encoder with\nefficient projector architectures and various decoder configurations. We employ\na three-stage training methodology that progressively optimizes the encoder,\nprojector, and LLM components. Our system achieves competitive performance with\na private test average WER/CER result of 16.63% using the Gemma3-12B and 18.6%\nusing the Qwen2.5-7B as decoder-only language model.\n","authors":["Tuan Nguyen","Long-Vu Hoang","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.13596v1.pdf","comment":"Technical report for Interspeech 2025 MLC-SLM Challenge"},{"id":"http://arxiv.org/abs/2506.13585v1","updated":"2025-06-16T15:08:02Z","published":"2025-06-16T15:08:02Z","title":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention","summary":"  We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.\n","authors":[" MiniMax"," :","Aili Chen","Aonian Li","Bangwei Gong","Binyang Jiang","Bo Fei","Bo Yang","Boji Shan","Changqing Yu","Chao Wang","Cheng Zhu","Chengjun Xiao","Chengyu Du","Chi Zhang","Chu Qiao","Chunhao Zhang","Chunhui Du","Congchao Guo","Da Chen","Deming Ding","Dianjun Sun","Dong Li","Enwei Jiao","Haigang Zhou","Haimo Zhang","Han Ding","Haohai Sun","Haoyu Feng","Huaiguang Cai","Haichao Zhu","Jian Sun","Jiaqi Zhuang","Jiaren Cai","Jiayuan Song","Jin Zhu","Jingyang Li","Jinhao Tian","Jinli Liu","Junhao Xu","Junjie Yan","Junteng Liu","Junxian He","Kaiyi Feng","Ke Yang","Kecheng Xiao","Le Han","Leyang Wang","Lianfei Yu","Liheng Feng","Lin Li","Lin Zheng","Linge Du","Lingyu Yang","Lunbin Zeng","Minghui Yu","Mingliang Tao","Mingyuan Chi","Mozhi Zhang","Mujie Lin","Nan Hu","Nongyu Di","Peng Gao","Pengfei Li","Pengyu Zhao","Qibing Ren","Qidi Xu","Qile Li","Qin Wang","Rong Tian","Ruitao Leng","Shaoxiang Chen","Shaoyu Chen","Shengmin Shi","Shitong Weng","Shuchang Guan","Shuqi Yu","Sichen Li","Songquan Zhu","Tengfei Li","Tianchi Cai","Tianrun Liang","Weiyu Cheng","Weize Kong","Wenkai Li","Xiancai Chen","Xiangjun Song","Xiao Luo","Xiao Su","Xiaobo Li","Xiaodong Han","Xinzhu Hou","Xuan Lu","Xun Zou","Xuyang Shen","Yan Gong","Yan Ma","Yang Wang","Yiqi Shi","Yiran Zhong","Yonghong Duan","Yongxiang Fu","Yongyi Hu","Yu Gao","Yuanxiang Fan","Yufeng Yang","Yuhao Li","Yulin Hu","Yunan Huang","Yunji Li","Yunzhi Xu","Yuxin Mao","Yuxuan Shi","Yuze Wenren","Zehan Li","Zelin Li","Zhanxu Tian","Zhengmao Zhu","Zhenhua Fan","Zhenzhen Wu","Zhichao Xu","Zhihang Yu","Zhiheng Lyu","Zhuo Jiang","Zibo Gao","Zijia Wu","Zijian Song","Zijun Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13585v1.pdf","comment":"A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-source our MiniMax-M1 at\n  https://github.com/MiniMax-AI/MiniMax-M1"},{"id":"http://arxiv.org/abs/2506.13579v1","updated":"2025-06-16T15:02:12Z","published":"2025-06-16T15:02:12Z","title":"Flexible-length Text Infilling for Discrete Diffusion Models","summary":"  Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.\n","authors":["Andrew Zhang","Anushka Sivakumar","Chiawei Tang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2506.13579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13569v1","updated":"2025-06-16T14:54:56Z","published":"2025-06-16T14:54:56Z","title":"Characterizing Linguistic Shifts in Croatian News via Diachronic Word\n  Embeddings","summary":"  Measuring how semantics of words change over time improves our understanding\nof how cultures and perspectives change. Diachronic word embeddings help us\nquantify this shift, although previous studies leveraged substantial temporally\nannotated corpora. In this work, we use a corpus of 9.5 million Croatian news\narticles spanning the past 25 years and quantify semantic change using\nskip-gram word embeddings trained on five-year periods. Our analysis finds that\nword embeddings capture linguistic shifts of terms pertaining to major topics\nin this timespan (COVID-19, Croatia joining the European Union, technological\nadvancements). We also find evidence that embeddings from post-2020 encode\nincreased positivity in sentiment analysis tasks, contrasting studies reporting\na decline in mental health over the same period.\n","authors":["David Dukić","Ana Barić","Marko Čuljak","Josip Jukić","Martin Tutek"],"pdf_url":"https://arxiv.org/pdf/2506.13569v1.pdf","comment":"Accepted at Slavic NLP 2025"},{"id":"http://arxiv.org/abs/2506.13559v1","updated":"2025-06-16T14:45:08Z","published":"2025-06-16T14:45:08Z","title":"Understand the Implication: Learning to Think for Pragmatic\n  Understanding","summary":"  Pragmatics, the ability to infer meaning beyond literal interpretation, is\ncrucial for social cognition and communication. While LLMs have been\nbenchmarked for their pragmatic understanding, improving their performance\nremains underexplored. Existing methods rely on annotated labels but overlook\nthe reasoning process humans naturally use to interpret implicit meaning. To\nbridge this gap, we introduce a novel pragmatic dataset,\nImpliedMeaningPreference, that includes explicit reasoning (thoughts) for both\ncorrect and incorrect interpretations. Through preference-tuning and supervised\nfine-tuning, we demonstrate that thought-based learning significantly enhances\nLLMs' pragmatic understanding, improving accuracy by 11.12% across model\nfamilies. We further discuss a transfer-learning study where we evaluate the\nperformance of thought-based training for the other tasks of pragmatics\n(presupposition, deixis) that are not seen during the training time and observe\nan improvement of 16.10% compared to label-trained models.\n","authors":["Settaluri Lakshmi Sravanthi","Kishan Maharaj","Sravani Gunnu","Abhijit Mishra","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2506.13559v1.pdf","comment":"SS and KM contributed equally to this work"},{"id":"http://arxiv.org/abs/2408.08782v5","updated":"2025-06-16T14:32:22Z","published":"2024-08-16T14:54:41Z","title":"EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics","summary":"  Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.\n","authors":["Chenwei Wan","Matthieu Labeau","Chloé Clavel"],"pdf_url":"https://arxiv.org/pdf/2408.08782v5.pdf","comment":"Accepted to NAACL 2025 main, long paper"},{"id":"http://arxiv.org/abs/2506.11887v2","updated":"2025-06-16T14:30:20Z","published":"2025-06-13T15:36:22Z","title":"Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making","summary":"  Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.\n","authors":["Claudio Fanconi","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2506.11887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13541v1","updated":"2025-06-16T14:30:17Z","published":"2025-06-16T14:30:17Z","title":"Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization","summary":"  Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.\n","authors":["Guanghui Song","Dongping Liao","Yiren Zhao","Kejiang Ye","Cheng-zhong Xu","Xitong Gao"],"pdf_url":"https://arxiv.org/pdf/2506.13541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02670v3","updated":"2025-06-16T14:19:01Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13514v1","updated":"2025-06-16T14:09:43Z","published":"2025-06-16T14:09:43Z","title":"TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices","summary":"  Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.\n","authors":["Mingxue Xu","Yao Lei Xu","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2506.13514v1.pdf","comment":"ICML 2025 Workshop on Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models (TTODLer-FM)"},{"id":"http://arxiv.org/abs/2504.10512v2","updated":"2025-06-16T14:08:36Z","published":"2025-04-10T01:31:11Z","title":"JEPA4Rec: Learning Effective Language Representations for Sequential\n  Recommendation via Joint Embedding Predictive Architecture","summary":"  Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.\n","authors":["Minh-Anh Nguyen","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2504.10512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13513v1","updated":"2025-06-16T14:08:23Z","published":"2025-06-16T14:08:23Z","title":"K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly\n  Offensive Language in Korean","summary":"  Language detoxification involves removing toxicity from offensive language.\nWhile a neutral-toxic paired dataset provides a straightforward approach for\ntraining detoxification models, creating such datasets presents several\nchallenges: i) the need for human annotation to build paired data, and ii) the\nrapid evolution of offensive terms, rendering static datasets quickly outdated.\nTo tackle these challenges, we introduce an automated paired data generation\npipeline, called K/DA. This pipeline is designed to generate offensive language\nwith implicit offensiveness and trend-aligned slang, making the resulting\ndataset suitable for detoxification model training. We demonstrate that the\ndataset generated by K/DA exhibits high pair consistency and greater implicit\noffensiveness compared to existing Korean datasets, and also demonstrates\napplicability to other languages. Furthermore, it enables effective training of\na high-performing detoxification model with simple instruction fine-tuning.\n","authors":["Minkyeong Jeon","Hyemin Jeong","Yerang Kim","Jiyoung Kim","Jae Hyeon Cho","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2506.13513v1.pdf","comment":"9 pages, 3 figures, ACL 2025"},{"id":"http://arxiv.org/abs/2506.13502v1","updated":"2025-06-16T13:58:54Z","published":"2025-06-16T13:58:54Z","title":"BOW: Bottlenecked Next Word Exploration","summary":"  Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.\n","authors":["Ming Shen","Zhikun Xu","Xiao Ye","Jacob Dineen","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.13502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13487v1","updated":"2025-06-16T13:45:30Z","published":"2025-06-16T13:45:30Z","title":"TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs","summary":"  We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal\npairs, designed to evaluate the linguistic abilities of monolingual and\nmultilingual language models (LMs). Covering 16 linguistic phenomena with 1000\nminimal pairs each, TurBLiMP fills an important gap in linguistic evaluation\nresources for Turkish. In designing the benchmark, we give extra attention to\ntwo properties of Turkish that remain understudied in current syntactic\nevaluations of LMs, namely word order flexibility and subordination through\nmorphological processes. Our experiments on a wide range of LMs and a newly\ncollected set of human acceptability judgments reveal that even cutting-edge\nLarge LMs still struggle with grammatical phenomena that are not challenging\nfor humans, and may also exhibit different sensitivities to word order and\nmorphological complexity compared to humans.\n","authors":["Ezgi Başar","Francesca Padovani","Jaap Jumelet","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2506.13487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13479v1","updated":"2025-06-16T13:35:22Z","published":"2025-06-16T13:35:22Z","title":"Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover\n  Limits and Effectiveness","summary":"  Merging or routing low-rank adapters (LoRAs) has emerged as a popular\nsolution for enhancing large language models, particularly when data access is\nrestricted by regulatory or domain-specific constraints. This position paper\nargues that the research community should shift its focus from developing new\nmerging or routing algorithms to understanding the conditions under which\nreusing LoRAs is truly effective. Through theoretical analysis and synthetic\ntwo-hop reasoning and math word-problem tasks, we examine whether reusing LoRAs\nenables genuine compositional generalization or merely reflects shallow pattern\nmatching. Evaluating two data-agnostic methods--parameter averaging and dynamic\nadapter selection--we found that reusing LoRAs often fails to logically\nintegrate knowledge across disjoint fine-tuning datasets, especially when such\nknowledge is underrepresented during pretraining. Our empirical results,\nsupported by theoretical insights into LoRA's limited expressiveness, highlight\nthe preconditions and constraints of reusing them for unseen tasks and cast\ndoubt on its feasibility as a truly data-free approach. We advocate for pausing\nthe pursuit of novel methods for recycling LoRAs and emphasize the need for\nrigorous mechanisms to guide future academic research in adapter-based model\nmerging and practical system designs for practitioners.\n","authors":["Mei-Yen Chen","Thi Thu Uyen Hoang","Michael Hahn","M. Saquib Sarfraz"],"pdf_url":"https://arxiv.org/pdf/2506.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13474v1","updated":"2025-06-16T13:32:01Z","published":"2025-06-16T13:32:01Z","title":"Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning","summary":"  Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.\n","authors":["David Bani-Harouni","Chantal Pellegrini","Ege Özsoy","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2506.13474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09975v2","updated":"2025-06-16T13:31:25Z","published":"2025-06-11T17:51:28Z","title":"When Detection Fails: The Power of Fine-Tuned Models to Generate\n  Human-Like Social Media Text","summary":"  Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.\n","authors":["Hillary Dawkins","Kathleen C. Fraser","Svetlana Kiritchenko"],"pdf_url":"https://arxiv.org/pdf/2506.09975v2.pdf","comment":"to appear in ACL Findings"},{"id":"http://arxiv.org/abs/2506.13472v1","updated":"2025-06-16T13:30:33Z","published":"2025-06-16T13:30:33Z","title":"ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models","summary":"  Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.\n","authors":["Junho Yoon","Geom Lee","Donghyeon Jeon","Inho Kang","Seung-Hoon Na"],"pdf_url":"https://arxiv.org/pdf/2506.13472v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.13470v1","updated":"2025-06-16T13:28:37Z","published":"2025-06-16T13:28:37Z","title":"Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive\n  Inductive Reasoning","summary":"  Zero-shot stance detection (ZSSD) aims to identify the stance of text toward\npreviously unseen targets, a setting where conventional supervised models often\nfail due to reliance on labeled data and shallow lexical cues. Inspired by\nhuman cognitive reasoning, we propose the Cognitive Inductive Reasoning\nFramework (CIRF), which abstracts transferable reasoning schemas from unlabeled\ntext and encodes them as concept-level logic. To integrate these schemas with\ninput arguments, we introduce a Schema-Enhanced Graph Kernel Model (SEGKM) that\ndynamically aligns local and global reasoning structures. Experiments on\nSemEval-2016, VAST, and COVID-19-Stance benchmarks show that CIRF establishes\nnew state-of-the-art results, outperforming strong ZSSD baselines by 1.0, 4.5,\nand 3.3 percentage points in macro-F1, respectively, and achieving comparable\naccuracy with 70\\% fewer labeled examples. We will release the full code upon\npublication.\n","authors":["Jun Ma","Fuqiang Niu","Dong Li","Jinzhou Cao","Genan Dai","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13468v1","updated":"2025-06-16T13:27:44Z","published":"2025-06-16T13:27:44Z","title":"An Interdisciplinary Approach to Human-Centered Machine Translation","summary":"  Machine Translation (MT) tools are widely used today, often in contexts where\nprofessional translators are not present. Despite progress in MT technology, a\ngap persists between system development and real-world usage, particularly for\nnon-expert users who may struggle to assess translation reliability. This paper\nadvocates for a human-centered approach to MT, emphasizing the alignment of\nsystem design with diverse communicative goals and contexts of use. We survey\nthe literature in Translation Studies and Human-Computer Interaction to\nrecontextualize MT evaluation and design to address the diverse real-world\nscenarios in which MT is used today.\n","authors":["Marine Carpuat","Omri Asscher","Kalika Bali","Luisa Bentivogli","Frédéric Blain","Lynne Bowker","Monojit Choudhury","Hal Daumé III","Kevin Duh","Ge Gao","Alvin Grissom II","Marzena Karpinska","Elaine C. Khoong","William D. Lewis","André F. T. Martins","Mary Nurminen","Douglas W. Oard","Maja Popovic","Michel Simard","François Yvon"],"pdf_url":"https://arxiv.org/pdf/2506.13468v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2506.13467v1","updated":"2025-06-16T13:27:10Z","published":"2025-06-16T13:27:10Z","title":"Enhancing Omics Cohort Discovery for Research on Neurodegeneration\n  through Ontology-Augmented Embedding Models","summary":"  The growing volume of omics and clinical data generated for neurodegenerative\ndiseases (NDs) requires new approaches for their curation so they can be\nready-to-use in bioinformatics. NeuroEmbed is an approach for the engineering\nof semantically accurate embedding spaces to represent cohorts and samples. The\nNeuroEmbed method comprises four stages: (1) extraction of ND cohorts from\npublic repositories; (2) semi-automated normalization and augmentation of\nmetadata of cohorts and samples using biomedical ontologies and clustering on\nthe embedding space; (3) automated generation of a natural language\nquestion-answering (QA) dataset for cohorts and samples based on randomized\ncombinations of standardized metadata dimensions and (4) fine-tuning of a\ndomain-specific embedder to optimize queries. We illustrate the approach using\nthe GEO repository and the PubMedBERT pretrained embedder. Applying NeuroEmbed,\nwe semantically indexed 2,801 repositories and 150,924 samples. Amongst many\nbiology-relevant categories, we normalized more than 1,700 heterogeneous tissue\nlabels from GEO into 326 unique ontology-aligned concepts and enriched\nannotations with new ontology-aligned terms, leading to a fold increase in size\nfor the metadata terms between 2.7 and 20 fold. After fine-tuning PubMedBERT\nwith the QA training data augmented with the enlarged metadata, the model\nincreased its mean Retrieval Precision from 0.277 to 0.866 and its mean\nPercentile Rank from 0.355 to 0.896. The NeuroEmbed methodology for the\ncreation of electronic catalogues of omics cohorts and samples will foster\nautomated bioinformatic pipelines construction. The NeuroEmbed catalogue of\ncohorts and samples is available at https://github.com/JoseAdrian3/NeuroEmbed.\n","authors":["José A. Pardo","Alicia Gómez-Pascual","José T. Palma","Juan A. Botía"],"pdf_url":"https://arxiv.org/pdf/2506.13467v1.pdf","comment":"16 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.13464v1","updated":"2025-06-16T13:24:50Z","published":"2025-06-16T13:24:50Z","title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework\n  and Empirical Study","summary":"  Large language models (LLMs) have shown impressive capabilities across tasks\nsuch as mathematics, coding, and reasoning, yet their learning ability, which\nis crucial for adapting to dynamic environments and acquiring new knowledge,\nremains underexplored. In this work, we address this gap by introducing a\nframework inspired by cognitive psychology and education. Specifically, we\ndecompose general learning ability into three distinct, complementary\ndimensions: Learning from Instructor (acquiring knowledge via explicit\nguidance), Learning from Concept (internalizing abstract structures and\ngeneralizing to new contexts), and Learning from Experience (adapting through\naccumulated exploration and feedback). We conduct a comprehensive empirical\nstudy across the three learning dimensions and identify several insightful\nfindings, such as (i) interaction improves learning; (ii) conceptual\nunderstanding is scale-emergent and benefits larger models; and (iii) LLMs are\neffective few-shot learners but not many-shot learners. Based on our framework\nand empirical findings, we introduce a benchmark that provides a unified and\nrealistic evaluation of LLMs' general learning abilities across three learning\ncognition dimensions. It enables diagnostic insights and supports evaluation\nand development of more adaptive and human-like models.\n","authors":["Zhengyu Hu","Jianxun Lian","Zheyuan Xiao","Seraphina Zhang","Tianfu Wang","Nicholas Jing Yuan","Xing Xie","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.13464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13458v1","updated":"2025-06-16T13:15:02Z","published":"2025-06-16T13:15:02Z","title":"Leveraging Vision-Language Pre-training for Human Activity Recognition\n  in Still Images","summary":"  Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments.\n","authors":["Cristina Mahanta","Gagan Bhatia"],"pdf_url":"https://arxiv.org/pdf/2506.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13450v1","updated":"2025-06-16T13:09:24Z","published":"2025-06-16T13:09:24Z","title":"A Neural Model for Word Repetition","summary":"  It takes several years for the developing brain of a baby to fully master\nword repetition-the task of hearing a word and repeating it aloud. Repeating a\nnew word, such as from a new language, can be a challenging task also for\nadults. Additionally, brain damage, such as from a stroke, may lead to\nsystematic speech errors with specific characteristics dependent on the\nlocation of the brain damage. Cognitive sciences suggest a model with various\ncomponents for the different processing stages involved in word repetition.\nWhile some studies have begun to localize the corresponding regions in the\nbrain, the neural mechanisms and how exactly the brain performs word repetition\nremain largely unknown. We propose to bridge the gap between the cognitive\nmodel of word repetition and neural mechanisms in the human brain by modeling\nthe task using deep neural networks. Neural models are fully observable,\nallowing us to study the detailed mechanisms in their various substructures and\nmake comparisons with human behavior and, ultimately, the brain. Here, we make\nfirst steps in this direction by: (1) training a large set of models to\nsimulate the word repetition task; (2) creating a battery of tests to probe the\nmodels for known effects from behavioral studies in humans, and (3) simulating\nbrain damage through ablation studies, where we systematically remove neurons\nfrom the model, and repeat the behavioral study to examine the resulting speech\nerrors in the \"patient\" model. Our results show that neural models can mimic\nseveral effects known from human research, but might diverge in other aspects,\nhighlighting both the potential and the challenges for future research aimed at\ndeveloping human-like neural models.\n","authors":["Daniel Dager","Robin Sobczyk","Emmanuel Chemla","Yair Lakretz"],"pdf_url":"https://arxiv.org/pdf/2506.13450v1.pdf","comment":"To appear at Cognitive Computational Neuroscience 2025 (CCN)"},{"id":"http://arxiv.org/abs/2502.17533v2","updated":"2025-06-16T13:07:26Z","published":"2025-02-24T14:42:48Z","title":"From Euler to AI: Unifying Formulas for Mathematical Constants","summary":"  The constant $\\pi$ has fascinated scholars throughout the centuries,\ninspiring numerous formulas for its evaluation, such as infinite sums and\ncontinued fractions. Despite their individual significance, many of the\nunderlying connections among formulas remain unknown, missing unifying theories\nthat could unveil deeper understanding. The absence of a unifying theory\nreflects a broader challenge across math and science: knowledge is typically\naccumulated through isolated discoveries, while deeper connections often remain\nhidden. In this work, we present an automated framework for the unification of\nmathematical formulas. Our system combines large language models (LLMs) for\nsystematic formula harvesting, an LLM-code feedback loop for validation, and a\nnovel symbolic algorithm for clustering and eventual unification. We\ndemonstrate this methodology on the hallmark case of $\\pi$, an ideal testing\nground for symbolic unification. Applying this approach to 455,050 arXiv\npapers, we validate 407 distinct formulas for $\\pi$ and prove relations between\n381 (94%) of them, of which 188 (46%) can be derived from a single mathematical\nobject$\\unicode{x2014}$linking canonical formulas by Euler, Gauss, Brouncker,\nand newer ones from algorithmic discoveries by the Ramanujan Machine. Our\nmethod generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's\nconstant, demonstrating the potential of AI-assisted mathematics to uncover\nhidden structures and unify knowledge across domains.\n","authors":["Tomer Raz","Michael Shalyt","Elyasheev Leibtag","Rotem Kalisch","Shachar Weinbaum","Yaron Hadad","Ido Kaminer"],"pdf_url":"https://arxiv.org/pdf/2502.17533v2.pdf","comment":"60 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.13405v1","updated":"2025-06-16T12:19:08Z","published":"2025-06-16T12:19:08Z","title":"RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for\n  Evaluating LLM-Based Table Analysis","summary":"  With the rapid advancement of Large Language Models (LLMs), there is an\nincreasing need for challenging benchmarks to evaluate their capabilities in\nhandling complex tabular data. However, existing benchmarks are either based on\noutdated data setups or focus solely on simple, flat table structures. In this\npaper, we introduce RealHiTBench, a comprehensive benchmark designed to\nevaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a\nvariety of input formats for complex tabular data, including LaTeX, HTML, and\nPNG. RealHiTBench also includes a diverse collection of tables with intricate\nstructures, spanning a wide range of task types. Our experimental results,\nusing 25 state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a\nchallenging benchmark. Moreover, we also develop TreeThinker, a tree-based\npipeline that organizes hierarchical headers into a tree structure for enhanced\ntabular reasoning, validating the importance of improving LLMs' perception of\ntable hierarchies. We hope that our work will inspire further research on\ntabular data reasoning and the development of more robust models. The code and\ndata are available at https://github.com/cspzyy/RealHiTBench.\n","authors":["Pengzuo Wu","Yuhang Yang","Guangcheng Zhu","Chao Ye","Hong Gu","Xu Lu","Ruixuan Xiao","Bowen Bao","Yijing He","Liangyu Zha","Wentao Ye","Junbo Zhao","Haobo Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13405v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2506.13396v1","updated":"2025-06-16T12:03:23Z","published":"2025-06-16T12:03:23Z","title":"Bi-directional Context-Enhanced Speech Large Language Models for\n  Multilingual Conversational ASR","summary":"  This paper introduces the integration of language-specific bi-directional\ncontext into a speech large language model (SLLM) to improve multilingual\ncontinuous conversational automatic speech recognition (ASR). We propose a\ncharacter-level contextual masking strategy during training, which randomly\nremoves portions of the context to enhance robustness and better emulate the\nflawed transcriptions that may occur during inference. For decoding, a\ntwo-stage pipeline is utilized: initial isolated segment decoding followed by\ncontext-aware re-decoding using neighboring hypotheses. Evaluated on the\n1500-hour Multilingual Conversational Speech and Language Model (MLC-SLM)\ncorpus covering eleven languages, our method achieves an 18% relative\nimprovement compared to a strong baseline, outperforming even the model trained\non 6000 hours of data for the MLC-SLM competition. These results underscore the\nsignificant benefit of incorporating contextual information in multilingual\ncontinuous conversational ASR.\n","authors":["Yizhou Peng","Hexin Liu","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2506.13396v1.pdf","comment":"Submitted to Interspeech 2025 MLC-SLM workshop as a Research Paper"},{"id":"http://arxiv.org/abs/2411.12484v2","updated":"2025-06-16T11:46:29Z","published":"2024-11-19T13:08:03Z","title":"Regular-pattern-sensitive CRFs for Distant Label Interactions","summary":"  While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets.\n","authors":["Sean Papay","Roman Klinger","Sebastian Pado"],"pdf_url":"https://arxiv.org/pdf/2411.12484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13380v1","updated":"2025-06-16T11:44:28Z","published":"2025-06-16T11:44:28Z","title":"Decompositional Reasoning for Graph Retrieval with Large Language Models","summary":"  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n","authors":["Valentin Six","Evan Dufraisse","Gaël de Chalendar"],"pdf_url":"https://arxiv.org/pdf/2506.13380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11169v2","updated":"2025-06-16T11:37:38Z","published":"2025-02-16T15:39:57Z","title":"CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical\n  Reasoning in Large Language Model","summary":"  This paper introduces the Constrained Monte Carlo Tree Search (CMCTS)\nframework to enhance the mathematical reasoning capabilities of Large Language\nModels (LLM). By incorporating a constrained action space, Process Reward Model\n(PRM), and partial order rules, CMCTS effectively addresses the limitations of\nexisting MCTS methods in terms of state space diversity and action selection\nrationality. Specifically, during the expansion phase, CMCTS restricts action\nsampling to a predefined constrained action set to increase candidate state\ndiversity. In the simulation phase, it introduces partial order rules and PRM\nto optimize action selection and prevent unreasonable state transitions.\nExperimental results show that CMCTS performs outstandingly across multiple\nmathematical reasoning benchmarks. Under a zero-shot setting, a 7B-parameter\nmodel achieves an average accuracy of 83.4\\%, surpassing the 72B baseline model\nby 4.8\\%. Ablation studies demonstrate that each component of the framework is\ncrucial for performance improvement, and their combined use fully leverages\ntheir respective strengths. Overall, the CMCTS framework provides an effective\napproach to enhancing LLM mathematical reasoning capabilities, supported by\ntheoretical analysis, and offers novel insights for future reasoning tasks.\n","authors":["Qingwen Lin","Boyan Xu","Guimin Hu","Zijian Li","Zhifeng Hao","Keli Zhang","Ruichu Cai"],"pdf_url":"https://arxiv.org/pdf/2502.11169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13366v1","updated":"2025-06-16T11:15:21Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  This paper proposes a consistency reflection and correction method for\ngoal-oriented dialogue systems.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13363v1","updated":"2025-06-16T11:10:25Z","published":"2025-06-16T11:10:25Z","title":"Efficient Medical VIE via Reinforcement Learning","summary":"  Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.\n","authors":["Lijun Liu","Ruiyang Li","Zhaocheng Liu","Chenglin Zhu","Chong Li","Jiehan Cheng","Qiang Ju","Jian Xie"],"pdf_url":"https://arxiv.org/pdf/2506.13363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09387v3","updated":"2025-06-16T11:10:18Z","published":"2025-02-13T15:04:53Z","title":"Truth Knows No Language: Evaluating Truthfulness Beyond English","summary":"  We introduce a professionally translated extension of the TruthfulQA\nbenchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and\nSpanish. Truthfulness evaluations of large language models (LLMs) have\nprimarily been conducted in English. However, the ability of LLMs to maintain\ntruthfulness across languages remains under-explored. Our study evaluates 12\nstate-of-the-art open LLMs, comparing base and instruction-tuned models using\nhuman evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our\nfindings reveal that, while LLMs perform best in English and worst in Basque\n(the lowest-resourced language), overall truthfulness discrepancies across\nlanguages are smaller than anticipated. Furthermore, we show that\nLLM-as-a-Judge correlates more closely with human judgments than\nmultiple-choice metrics, and that informativeness plays a critical role in\ntruthfulness assessment. Our results also indicate that machine translation\nprovides a viable approach for extending truthfulness benchmarks to additional\nlanguages, offering a scalable alternative to professional translation.\nFinally, we observe that universal knowledge questions are better handled\nacross languages than context- and time-dependent ones, highlighting the need\nfor truthfulness evaluations that account for cultural and temporal\nvariability. Dataset and code are publicly available under open licenses.\n","authors":["Blanca Calvo Figueras","Eneko Sagarzazu","Julen Etxaniz","Jeremy Barnes","Pablo Gamallo","Iria De Dios Flores","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2502.09387v3.pdf","comment":"14 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.03249v4","updated":"2025-06-16T11:00:21Z","published":"2024-10-04T09:14:11Z","title":"How Much Can We Forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v4.pdf","comment":"ICML 2025 camera ready"},{"id":"http://arxiv.org/abs/2506.13356v1","updated":"2025-06-16T10:54:31Z","published":"2025-06-16T10:54:31Z","title":"StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with\n  Multi Turns","summary":"  Long-term memory (LTM) is essential for large language models (LLMs) to\nachieve autonomous intelligence in complex, evolving environments. Despite\nincreasing efforts in memory-augmented and retrieval-based architectures, there\nremains a lack of standardized benchmarks to systematically evaluate LLMs'\nlong-term memory abilities. Existing benchmarks still face challenges in\nevaluating knowledge retention and dynamic sequential reasoning, and in their\nown flexibility, all of which limit their effectiveness in assessing models'\nLTM capabilities. To address these gaps, we propose a novel benchmark framework\nbased on interactive fiction games, featuring dynamically branching storylines\nwith complex reasoning structures. These structures simulate real-world\nscenarios by requiring LLMs to navigate hierarchical decision trees, where each\nchoice triggers cascading dependencies across multi-turn interactions. Our\nbenchmark emphasizes two distinct settings to test reasoning complexity: one\nwith immediate feedback upon incorrect decisions, and the other requiring\nmodels to independently trace back and revise earlier choices after failure. As\npart of this benchmark, we also construct a new dataset designed to test LLMs'\nLTM within narrative-driven environments. We further validate the effectiveness\nof our approach through detailed experiments. Experimental results demonstrate\nthe benchmark's ability to robustly and reliably assess LTM in LLMs.\n","authors":["Luanbo Wan","Weizhi Ma"],"pdf_url":"https://arxiv.org/pdf/2506.13356v1.pdf","comment":"13pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.13351v1","updated":"2025-06-16T10:43:38Z","published":"2025-06-16T10:43:38Z","title":"Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks","summary":"  Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.\n","authors":["Yifei Xu","Tusher Chakraborty","Srinagesh Sharma","Leonardo Nunes","Emre Kıcıman","Songwu Lu","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2506.13351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13342v1","updated":"2025-06-16T10:32:10Z","published":"2025-06-16T10:32:10Z","title":"Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers","summary":"  Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers\n","authors":["Wooseok Seo","Seungju Han","Jaehun Jung","Benjamin Newman","Seungwon Lim","Seungbeen Lee","Ximing Lu","Yejin Choi","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2506.13342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13339v1","updated":"2025-06-16T10:28:27Z","published":"2025-06-16T10:28:27Z","title":"NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM\n  Challenge 2025","summary":"  This report details the NTU Speechlab system developed for the Interspeech\n2025 Multilingual Conversational Speech and Language Model (MLC-SLM) Challenge\n(Task I), where we achieved 5th place. We present comprehensive analyses of our\nmultilingual automatic speech recognition system, highlighting key advancements\nin model architecture, data selection, and training strategies. In particular,\nlanguage-specific prompts and model averaging techniques were instrumental in\nboosting system performance across diverse languages. Compared to the initial\nbaseline system, our final model reduced the average Mix Error Rate from 20.2%\nto 10.6%, representing an absolute improvement of 9.6% (a relative improvement\nof 48%) on the evaluation set. Our results demonstrate the effectiveness of our\napproach and offer practical insights for future Speech Large Language Models.\n","authors":["Yizhou Peng","Bin Wang","Yi-Wen Chao","Ziyang Ma","Haoyang Zhang","Hexin Liu","Xie Chen","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2506.13339v1.pdf","comment":"Submitted to Interspeech 2025 MLC-SLM challenge (5th place). System\n  report"},{"id":"http://arxiv.org/abs/2406.19384v3","updated":"2025-06-16T10:21:00Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.\n","authors":["Vedang Lad","Jin Hwa Lee","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v3.pdf","comment":"For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author"},{"id":"http://arxiv.org/abs/2506.13329v1","updated":"2025-06-16T10:18:50Z","published":"2025-06-16T10:18:50Z","title":"EAQuant: Enhancing Post-Training Quantization for MoE Models via\n  Expert-Aware Optimization","summary":"  Mixture-of-Experts (MoE) models have emerged as a cornerstone of large-scale\ndeep learning by efficiently distributing computation and enhancing\nperformance. However, their unique architecture-characterized by sparse expert\nactivation and dynamic routing mechanisms-introduces inherent complexities that\nchallenge conventional quantization techniques. Existing post-training\nquantization (PTQ) methods struggle to address activation outliers, router\nconsistency and sparse expert calibration, leading to significant performance\ndegradation. To bridge this gap, we propose EAQuant, a novel PTQ framework\ntailored for MoE architectures. Our method systematically tackles these\nchallenges through three key innovations: (1) expert-aware smoothing\naggregation to suppress activation outliers and stabilize quantization, (2)\nrouter logits distribution alignment to preserve expert selection consistency\npost-quantization, and (3) expert-level calibration data balance to optimize\nsparsely activated experts. Extensive experiments across W4A4 and extreme W3A4\nquantization configurations demonstrate that EAQuant significantly outperforms\nexisting methods, achieving average score improvements of 1.15 - 2.28% across\nthree diverse MoE architectures, with particularly pronounced gains in\nreasoning tasks and robust performance retention under aggressive quantization.\nBy integrating these innovations, EAQuant establishes a new state-of-the-art\nfor high-precision, efficient MoE model compression. Our code is available at\nhttps://github.com/darren-fzq/EAQuant.\n","authors":["Zhongqian Fu","Ning Ding","Kai Han","Xianzhi Yu","Xiaosong Li","Xinghao Chen","Yehui Tang","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13328v1","updated":"2025-06-16T10:17:21Z","published":"2025-06-16T10:17:21Z","title":"Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine\n  Approach","summary":"  Numerical consistency across tables in disclosure documents is critical for\nensuring accuracy, maintaining credibility, and avoiding reputational and\neconomic risks. Automated tabular numerical cross-checking presents two\nsignificant challenges: (C1) managing the combinatorial explosion of candidate\ninstances at the document level and (C2) comprehending multi-faceted numerical\nsemantics. Previous research typically depends on heuristic-based filtering or\nsimplified context extraction, often struggling to balance performance and\nefficiency. Recently, large language models (LLMs) have demonstrated remarkable\ncontextual understanding capabilities that helps address C2 at the instance\nlevel, yet they remain hampered by computational inefficiency (C1) and limited\ndomain expertise. This paper introduces CoFiTCheck, a novel LLM-based\ncoarse-to-fine framework that addresses these challenges through two sequential\nstages: embedding-based filtering and discriminative classification. The\nembedding-based filtering stage introduces an instructional parallel encoding\nmethod to efficiently represent all numerical mentions in a table with LLMs, as\nwell as a decoupled InfoNCE objective to mitigate the isolated mention problem.\nThe discriminative classification stage employs a specialized LLM for\nfine-grained analysis of the remaining candidate pairs. This stage is further\nenhanced by our crosstable numerical alignment pretraining paradigm, which\nleverages weak supervision from cross-table numerical equality relationships to\nenrich task-specific priors without requiring manual annotation. Comprehensive\nevaluation across three types of real-world disclosure documents demonstrates\nthat CoFiTCheck significantly outperforms previous methods while maintaining\npractical efficiency.\n","authors":["Chaoxu Pang","Yixuan Cao","Ganbin Zhou","Hongwei Li","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2506.13328v1.pdf","comment":"Submitted to IEEE TKDE"},{"id":"http://arxiv.org/abs/2506.13313v1","updated":"2025-06-16T09:54:56Z","published":"2025-06-16T09:54:56Z","title":"Large Language Models as 'Hidden Persuaders': Fake Product Reviews are\n  Indistinguishable to Humans and Machines","summary":"  Reading and evaluating product reviews is central to how most people decide\nwhat to buy and consume online. However, the recent emergence of Large Language\nModels and Generative Artificial Intelligence now means writing fraudulent or\nfake reviews is potentially easier than ever. Through three studies we\ndemonstrate that (1) humans are no longer able to distinguish between real and\nfake product reviews generated by machines, averaging only 50.8% accuracy\noverall - essentially the same that would be expected by chance alone; (2) that\nLLMs are likewise unable to distinguish between fake and real reviews and\nperform equivalently bad or even worse than humans; and (3) that humans and\nLLMs pursue different strategies for evaluating authenticity which lead to\nequivalently bad accuracy, but different precision, recall and F1 scores -\nindicating they perform worse at different aspects of judgment. The results\nreveal that review systems everywhere are now susceptible to mechanised fraud\nif they do not depend on trustworthy purchase verification to guarantee the\nauthenticity of reviewers. Furthermore, the results provide insight into the\nconsumer psychology of how humans judge authenticity, demonstrating there is an\ninherent 'scepticism bias' towards positive reviews and a special vulnerability\nto misjudge the authenticity of fake negative reviews. Additionally, results\nprovide a first insight into the 'machine psychology' of judging fake reviews,\nrevealing that the strategies LLMs take to evaluate authenticity radically\ndiffer from humans, in ways that are equally wrong in terms of accuracy, but\ndifferent in their misjudgments.\n","authors":["Weiyao Meng","John Harvey","James Goulding","Chris James Carter","Evgeniya Lukinova","Andrew Smith","Paul Frobisher","Mina Forrest","Georgiana Nica-Avram"],"pdf_url":"https://arxiv.org/pdf/2506.13313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13300v1","updated":"2025-06-16T09:42:05Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13285v1","updated":"2025-06-16T09:28:07Z","published":"2025-06-16T09:28:07Z","title":"Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs","summary":"  Large language models (LLMs) have shown strong performance across natural\nlanguage tasks, but remain vulnerable to backdoor attacks. Recent model\nediting-based approaches enable efficient backdoor injection by directly\nmodifying parameters to map specific triggers to attacker-desired responses.\nHowever, these methods often suffer from safety fallback, where the model\ninitially responds affirmatively but later reverts to refusals due to safety\nalignment. In this work, we propose DualEdit, a dual-objective model editing\nframework that jointly promotes affirmative outputs and suppresses refusal\nresponses. To address two key challenges -- balancing the trade-off between\naffirmative promotion and refusal suppression, and handling the diversity of\nrefusal expressions -- DualEdit introduces two complementary techniques. (1)\nDynamic loss weighting calibrates the objective scale based on the pre-edited\nmodel to stabilize optimization. (2) Refusal value anchoring compresses the\nsuppression target space by clustering representative refusal value vectors,\nreducing optimization conflict from overly diverse token sets. Experiments on\nsafety-aligned LLMs show that DualEdit improves attack success by 9.98\\% and\nreduces safety fallback rate by 10.88\\% over baselines.\n","authors":["Houcheng Jiang","Zetong Zhao","Junfeng Fang","Haokai Ma","Ruipeng Wang","Yang Deng","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2506.13285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13284v1","updated":"2025-06-16T09:27:48Z","published":"2025-06-16T09:27:48Z","title":"AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy","summary":"  In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B\n","authors":["Zihan Liu","Zhuolin Yang","Yang Chen","Chankyu Lee","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2506.13284v1.pdf","comment":"The AceReason-Nemotron collection:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485"},{"id":"http://arxiv.org/abs/2410.10209v4","updated":"2025-06-16T09:27:30Z","published":"2024-10-14T07:05:51Z","title":"EffiCoder: Enhancing Code Generation in Large Language Models through\n  Efficiency-Aware Fine-tuning","summary":"  As large language models (LLMs) play an increasingly important role in code\ngeneration, enhancing both correctness and efficiency has become crucial.\nCurrent methods primarily focus on correctness, often overlooking efficiency.\nTo address this gap, we introduce EffiCoder to improve both aspects by\nfine-tuning LLMs on a high-quality dataset comprising correct and efficient\ncode samples. Our methodology involves leveraging multiple LLMs to generate\ndiverse candidate code solutions for various tasks across different programming\nlanguages. We then evaluate these solutions by measuring their execution time\nand memory usage through local execution. The code solution with the lowest\nexecution time and memory consumption is selected as the final output for each\ntask. Experimental results demonstrate significant improvements when\nfine-tuning with Effi-Instruct. For instance, Qwen2.5-Coder-7B-Instruct's\npass@1 score increases from 44.8\\% to 57.7\\%, while the average execution time\nfor correct tasks decreases by 48.4\\%. EffiCoder offers a scalable and\neffective solution for advancing AI-driven code generation, benefiting software\ndevelopment and computational problem-solving. The source code of Effi-Code was\nreleased at https://github.com/huangd1999/EffiCoder.\n","authors":["Dong Huang","Guangtao Zeng","Jianbo Dai","Meng Luo","Han Weng","Yuhao Qing","Heming Cui","Zhijiang Guo","Jie M. Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10209v4.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.13277v1","updated":"2025-06-16T09:16:40Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huyang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13274v1","updated":"2025-06-16T09:14:01Z","published":"2025-06-16T09:14:01Z","title":"AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient\n  Foundation Model Pretraining","summary":"  Learning rate is widely regarded as crucial for effective foundation model\npretraining. Recent research explores and demonstrates the transferability of\nlearning rate configurations across varying model and dataset sizes, etc.\nNevertheless, these approaches are constrained to specific training scenarios\nand typically necessitate extensive hyperparameter tuning on proxy models. In\nthis work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning\nrate search algorithm that conducts online optimal learning rate search via\noptimizing loss descent velocities. We provide experiment results to show that\nthe optimization of training loss and loss descent velocity in foundation model\npretraining are both convex and share the same optimal learning rate. Relying\nsolely on training loss dynamics, AdaLRS involves few extra computations to\nguide the search process, and its convergence is guaranteed via theoretical\nanalysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts\nsuboptimal learning rates to the neighborhood of optimum with marked efficiency\nand effectiveness, with model performance improved accordingly. We also show\nthe robust generalizability of AdaLRS across varying training scenarios, such\nas different model sizes, training paradigms, and base learning rate scheduler\nchoices.\n","authors":["Hongyuan Dong","Dingkang Yang","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2506.13274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19510v2","updated":"2025-06-16T09:05:04Z","published":"2024-09-29T01:48:09Z","title":"Making LLMs Better Many-to-Many Speech-to-Text Translators with\n  Curriculum Learning","summary":"  Multimodal Large Language Models (MLLMs) have achieved significant success in\nSpeech-to-Text Translation (S2TT) tasks. While most existing research has\nfocused on English-centric translation directions, the exploration of\nmany-to-many translation is still limited by the scarcity of parallel data. To\naddress this, we propose a three-stage curriculum learning strategy that\nleverages the machine translation capabilities of large language models and\nadapts them to S2TT tasks, enabling effective learning in low-resource\nsettings. We trained MLLMs with varying parameter sizes (3B, 7B, and 32B) and\nevaluated the proposed strategy using the FLEURS and CoVoST-2 datasets.\nExperimental results show that the proposed strategy achieves state-of-the-art\naverage performance in $15\\times14$ language pairs, requiring fewer than 10\nhours of speech data per language to achieve competitive results. The source\ncode and models are released at https://github.com/yxduir/LLM-SRT.\n","authors":["Yexing Du","Youcheng Pan","Ziyang Ma","Bo Yang","Yifan Yang","Keqi Deng","Xie Chen","Yang Xiang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2409.19510v2.pdf","comment":"Accepted in ACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2506.13253v1","updated":"2025-06-16T08:49:42Z","published":"2025-06-16T08:49:42Z","title":"Distinct Computations Emerge From Compositional Curricula in In-Context\n  Learning","summary":"  In-context learning (ICL) research often considers learning a function\nin-context through a uniform sample of input-output pairs. Here, we investigate\nhow presenting a compositional subtask curriculum in context may alter the\ncomputations a transformer learns. We design a compositional algorithmic task\nbased on the modular exponential-a double exponential task composed of two\nsingle exponential subtasks and train transformer models to learn the task\nin-context. We compare (a) models trained using an in-context curriculum\nconsisting of single exponential subtasks and, (b) models trained directly on\nthe double exponential task without such a curriculum. We show that models\ntrained with a subtask curriculum can perform zero-shot inference on unseen\ncompositional tasks and are more robust given the same context length. We study\nhow the task and subtasks are represented across the two training regimes. We\nfind that the models employ diverse strategies modulated by the specific\ncurriculum design.\n","authors":["Jin Hwa Lee","Andrew K. Lampinen","Aaditya K. Singh","Andrew M. Saxe"],"pdf_url":"https://arxiv.org/pdf/2506.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07398v2","updated":"2025-06-16T08:45:10Z","published":"2025-06-09T03:43:46Z","title":"G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems","summary":"  Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.\n","authors":["Guibin Zhang","Muxin Fu","Guancheng Wan","Miao Yu","Kun Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2506.07398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13229v1","updated":"2025-06-16T08:28:19Z","published":"2025-06-16T08:28:19Z","title":"IGD: Token Decisiveness Modeling via Information Gain in LLMs for\n  Personalized Recommendation","summary":"  Large Language Models (LLMs) have shown strong potential for recommendation\nby framing item prediction as a token-by-token language generation task.\nHowever, existing methods treat all item tokens equally, simply pursuing\nlikelihood maximization during both optimization and decoding. This overlooks\ncrucial token-level differences in decisiveness-many tokens contribute little\nto item discrimination yet can dominate optimization or decoding. To quantify\ntoken decisiveness, we propose a novel perspective that models item generation\nas a decision process, measuring token decisiveness by the Information Gain\n(IG) each token provides in reducing uncertainty about the generated item. Our\nempirical analysis reveals that most tokens have low IG but often correspond to\nhigh logits, disproportionately influencing training loss and decoding, which\nmay impair model performance. Building on these insights, we introduce an\nInformation Gain-based Decisiveness-aware Token handling (IGD) strategy that\nintegrates token decisiveness into both tuning and decoding. Specifically, IGD\ndownweights low-IG tokens during tuning and rebalances decoding to emphasize\ntokens with high IG. In this way, IGD moves beyond pure likelihood\nmaximization, effectively prioritizing high-decisiveness tokens. Extensive\nexperiments on four benchmark datasets with two LLM backbones demonstrate that\nIGD consistently improves recommendation accuracy, achieving significant gains\non widely used ranking metrics compared to strong baselines.\n","authors":["Zijie Lin","Yang Zhang","Xiaoyan Zhao","Fengbin Zhu","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.13229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13216v1","updated":"2025-06-16T08:16:03Z","published":"2025-06-16T08:16:03Z","title":"Capability Salience Vector: Fine-grained Alignment of Loss and\n  Capabilities for Downstream Task Scaling Law","summary":"  Scaling law builds the relationship between training computation and\nvalidation loss, enabling researchers to effectively predict the loss trending\nof models across different levels of computation. However, a gap still remains\nbetween validation loss and the model's downstream capabilities, making it\nuntrivial to apply scaling law to direct performance prediction for downstream\ntasks. The loss typically represents a cumulative penalty for predicted tokens,\nwhich are implicitly considered to have equal importance. Nevertheless, our\nstudies have shown evidence that when considering different training data\ndistributions, we cannot directly model the relationship between downstream\ncapability and computation or token loss. To bridge the gap between validation\nloss and downstream task capabilities, in this work, we introduce Capability\nSalience Vector, which decomposes the overall loss and assigns different\nimportance weights to tokens to assess a specific meta-capability, aligning the\nvalidation loss with downstream task performance in terms of the model's\ncapabilities. Experiments on various popular benchmarks demonstrate that our\nproposed Capability Salience Vector could significantly improve the\npredictability of language model performance on downstream tasks.\n","authors":["Qiming Ge","Shuhao Xing","Songyang Gao","Yunhua Zhou","Yicheng Zou","Songyang Zhang","Zhi Chen","Hang Yan","Qi Zhang","Qipeng Guo","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13216v1.pdf","comment":"9 pages, 9 figures, ACL2025"},{"id":"http://arxiv.org/abs/2506.13206v1","updated":"2025-06-16T08:10:04Z","published":"2025-06-16T08:10:04Z","title":"Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models","summary":"  Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.\n","authors":["James Chua","Jan Betley","Mia Taylor","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2506.13206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13199v1","updated":"2025-06-16T08:05:41Z","published":"2025-06-16T08:05:41Z","title":"Do Music Preferences Reflect Cultural Values? A Cross-National Analysis\n  Using Music Embedding and World Values Survey","summary":"  This study explores the extent to which national music preferences reflect\nunderlying cultural values. We collected long-term popular music data from\nYouTube Music Charts across 62 countries, encompassing both Western and\nnon-Western regions, and extracted audio embeddings using the CLAP model. To\ncomplement these quantitative representations, we generated semantic captions\nfor each track using LP-MusicCaps and GPT-based summarization. Countries were\nclustered based on contrastive embeddings that highlight deviations from global\nmusical norms. The resulting clusters were projected into a two-dimensional\nspace via t-SNE for visualization and evaluated against cultural zones defined\nby the World Values Survey (WVS). Statistical analyses, including MANOVA and\nchi-squared tests, confirmed that music-based clusters exhibit significant\nalignment with established cultural groupings. Furthermore, residual analysis\nrevealed consistent patterns of overrepresentation, suggesting non-random\nassociations between specific clusters and cultural zones. These findings\nindicate that national-level music preferences encode meaningful cultural\nsignals and can serve as a proxy for understanding global cultural boundaries.\n","authors":["Yongjae Kim","Seongchan Park"],"pdf_url":"https://arxiv.org/pdf/2506.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13192v1","updated":"2025-06-16T07:59:51Z","published":"2025-06-16T07:59:51Z","title":"Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for\n  LLMs","summary":"  Large language models (LLMs) are often constrained by rigid reasoning\nprocesses, limiting their ability to generate creative and diverse responses.\nTo address this, a novel framework called LADDER is proposed, combining\nChain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and\nmulti-dimensional up/down-sampling strategies which breaks the limitations of\ntraditional LLMs. First, CoT reasoning guides the model through multi-step\nlogical reasoning, expanding the semantic space and breaking the rigidity of\nthought. Next, MoE distributes the reasoning tasks across multiple expert\nmodules, each focusing on specific sub-tasks. Finally, dimensionality reduction\nmaps the reasoning outputs back to a lower-dimensional semantic space, yielding\nmore precise and creative responses. Extensive experiments across multiple\ntasks demonstrate that LADDER significantly improves task completion,\ncreativity, and fluency, generating innovative and coherent responses that\noutperform traditional models. Ablation studies reveal the critical roles of\nCoT and MoE in enhancing reasoning abilities and creative output. This work\ncontributes to the development of more flexible and creative LLMs, capable of\naddressing complex and novel tasks.\n","authors":["Xintong Tang","Meiru Zhang","Shang Xiao","Junzhao Jin","Zihan Zhao","Liwei Li","Yang Zheng","Bangyi Wu"],"pdf_url":"https://arxiv.org/pdf/2506.13192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21138v2","updated":"2025-06-16T07:57:48Z","published":"2025-05-27T12:50:55Z","title":"Leveraging LLM and Self-Supervised Training Models for Speech\n  Recognition in Chinese Dialects: A Comparative Analysis","summary":"  Large-scale training corpora have significantly improved the performance of\nASR models. Unfortunately, due to the relative scarcity of data, Chinese\naccents and dialects remain a challenge for most ASR models. Recent\nadvancements in self-supervised learning have shown that self-supervised\npre-training, combined with large language models (LLM), can effectively\nenhance ASR performance in low-resource scenarios. We aim to investigate the\neffectiveness of this paradigm for Chinese dialects. Specifically, we pre-train\na Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech\ndata and do alignment training on a supervised dataset of 40,000 hours. Then,\nwe systematically examine the impact of various projectors and LLMs on\nMandarin, dialect, and accented speech recognition performance under this\nparadigm. Our method achieved SOTA results on multiple dialect datasets,\nincluding Kespeech. We will open-source our work to promote reproducible\nresearch\n","authors":["Tianyi Xu","Hongjie Chen","Wang Qing","Lv Hang","Jian Kang","Li Jie","Zhennan Lin","Yongxiang Li","Xie Lei"],"pdf_url":"https://arxiv.org/pdf/2505.21138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13188v1","updated":"2025-06-16T07:55:44Z","published":"2025-06-16T07:55:44Z","title":"SPOT: Bridging Natural Language and Geospatial Search for Investigative\n  Journalists","summary":"  OpenStreetMap (OSM) is a vital resource for investigative journalists doing\ngeolocation verification. However, existing tools to query OSM data such as\nOverpass Turbo require familiarity with complex query languages, creating\nbarriers for non-technical users. We present SPOT, an open source natural\nlanguage interface that makes OSM's rich, tag-based geographic data more\naccessible through intuitive scene descriptions. SPOT interprets user inputs as\nstructured representations of geospatial object configurations using fine-tuned\nLarge Language Models (LLMs), with results being displayed in an interactive\nmap interface. While more general geospatial search tasks are conceivable, SPOT\nis specifically designed for use in investigative journalism, addressing\nreal-world challenges such as hallucinations in model output, inconsistencies\nin OSM tagging, and the noisy nature of user input. It combines a novel\nsynthetic data pipeline with a semantic bundling system to enable robust,\naccurate query generation. To our knowledge, SPOT is the first system to\nachieve reliable natural language access to OSM data at this level of accuracy.\nBy lowering the technical barrier to geolocation verification, SPOT contributes\na practical tool to the broader efforts to support fact-checking and combat\ndisinformation.\n","authors":["Lynn Khellaf","Ipek Baris Schlicht","Tilman Mirass","Julia Bayer","Tilman Wagner","Ruben Bouwmeester"],"pdf_url":"https://arxiv.org/pdf/2506.13188v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2506.13187v1","updated":"2025-06-16T07:55:14Z","published":"2025-06-16T07:55:14Z","title":"Dynamic Context-oriented Decomposition for Task-aware Low-rank\n  Adaptation with Less Forgetting and Faster Convergence","summary":"  Conventional low-rank adaptation methods build adapters without considering\ndata context, leading to sub-optimal fine-tuning performance and severe\nforgetting of inherent world knowledge. In this paper, we propose\ncontext-oriented decomposition adaptation (CorDA), a novel method that\ninitializes adapters in a task-aware manner. Concretely, we develop\ncontext-oriented singular value decomposition, where we collect covariance\nmatrices of input activations for each linear layer using sampled data from the\ntarget task, and apply SVD to the product of weight matrix and its\ncorresponding covariance matrix. By doing so, the task-specific capability is\ncompacted into the principal components. Thanks to the task awareness, our\nmethod enables two optional adaptation modes, knowledge-preserved mode (KPM)\nand instruction-previewed mode (IPM), providing flexibility to choose between\nfreezing the principal components to preserve their associated knowledge or\nadapting them to better learn a new task. We further develop CorDA++ by\nderiving a metric that reflects the compactness of task-specific principal\ncomponents, and then introducing dynamic covariance selection and dynamic rank\nallocation strategies based on the same metric. The two strategies provide each\nlayer with the most representative covariance matrix and a proper rank\nallocation. Experimental results show that CorDA++ outperforms CorDA by a\nsignificant margin. CorDA++ in KPM not only achieves better fine-tuning\nperformance than LoRA, but also mitigates the forgetting of pre-trained\nknowledge in both large language models and vision language models. For IPM,\nour method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA,\nand improves adaptation performance in various scenarios, outperforming strong\nbaseline methods. Our method has been integrated into the PEFT library\ndeveloped by Hugging Face.\n","authors":["Yibo Yang","Sihao Liu","Chuan Rao","Bang An","Tiancheng Shen","Philip H. S. Torr","Ming-Hsuan Yang","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2506.13187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13181v1","updated":"2025-06-16T07:48:01Z","published":"2025-06-16T07:48:01Z","title":"Align-then-Unlearn: Embedding Alignment for LLM Unlearning","summary":"  As large language models (LLMs) are trained on massive datasets, they have\nraised significant privacy and ethical concerns due to their potential to\ninadvertently retain sensitive information. Unlearning seeks to selectively\nremove specific data from trained models, such as personal information or\ncopyrighted content. Current approaches targeting specific output sequences at\nthe token level often fail to achieve complete forgetting and remain\nsusceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel\nframework that performs unlearning in the semantic embedding space rather than\ndirectly on output tokens. Align-then-Unlearn first augments the LLM with an\nembedding prediction module trained to anticipate future context\nrepresentations. Unlearning is then achieved by fine-tuning the model to\nminimize the similarity between these predicted embeddings and a target\nembedding that represents the concept to be removed. Initial results show that\nAlign-then-Unlearn effectively removes targeted knowledge with minimal\ndegradation in overall model utility. These findings suggest that\nembedding-based unlearning offers a promising and robust approach to removing\nconceptual knowledge. Our code is available at\nhttps://github.com/ExplainableML/align-then-unlearn.\n","authors":["Philipp Spohn","Leander Girrbach","Jessica Bader","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2506.13181v1.pdf","comment":"Accepted at ICML 2025 Workshop on Machine Unlearning for Generative\n  AI"},{"id":"http://arxiv.org/abs/2408.06778v4","updated":"2025-06-16T07:47:43Z","published":"2024-08-13T10:04:29Z","title":"Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors","summary":"  We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a\nTransformer-based framework that unifies textual and structural information for\ninductive link prediction in text-attributed knowledge graphs. We demonstrate\nthat, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce\nthe reliance on resource-intensive textual encoders. This makes the model both\nfast at training and inference time, as well as frugal in terms of cost. We\nperform a comprehensive evaluation on three popular datasets and show that\nFnF-TG can achieve superior performance compared to previous state-of-the-art\nmethods. We also extend inductive learning to a fully inductive setting, where\nrelations don't rely on transductive (fixed) representations, as in previous\nwork, but are a function of their textual description. Additionally, we\nintroduce new variants of existing datasets, specifically designed to test the\nperformance of models on unseen relations at inference time, thus offering a\nnew test-bench for fully inductive link prediction.\n","authors":["Andrei C. Coman","Christos Theodoropoulos","Marie-Francine Moens","James Henderson"],"pdf_url":"https://arxiv.org/pdf/2408.06778v4.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.13180v1","updated":"2025-06-16T07:47:34Z","published":"2025-06-16T07:47:34Z","title":"Dynamic Acoustic Model Architecture Optimization in Training for ASR","summary":"  Architecture design is inherently complex. Existing approaches rely on either\nhandcrafted rules, which demand extensive empirical expertise, or automated\nmethods like neural architecture search, which are computationally intensive.\nIn this paper, we introduce DMAO, an architecture optimization framework that\nemploys a grow-and-drop strategy to automatically reallocate parameters during\ntraining. This reallocation shifts resources from less-utilized areas to those\nparts of the model where they are most beneficial. Notably, DMAO only\nintroduces negligible training overhead at a given model complexity. We\nevaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and\nSwitchboard datasets. The results show that, using the same amount of training\nresources, our proposed DMAO consistently improves WER by up to 6% relatively\nacross various architectures, model sizes, and datasets. Furthermore, we\nanalyze the pattern of parameter redistribution and uncover insightful\nfindings.\n","authors":["Jingjing Xu","Zijian Yang","Albert Zeyer","Eugen Beck","Ralf Schlueter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2506.13180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13178v1","updated":"2025-06-16T07:43:18Z","published":"2025-06-16T07:43:18Z","title":"Enhancing Large Language Models with Reliable Knowledge Graphs","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ntext generation and understanding, yet their reliance on implicit, unstructured\nknowledge often leads to factual inaccuracies and limited interpretability.\nKnowledge Graphs (KGs), with their structured, relational representations,\noffer a promising solution to ground LLMs in verified knowledge. However, their\npotential remains constrained by inherent noise, incompleteness, and the\ncomplexity of integrating their rigid structure with the flexible reasoning of\nLLMs. This thesis presents a systematic framework to address these limitations,\nadvancing the reliability of KGs and their synergistic integration with LLMs\nthrough five interconnected contributions. This thesis addresses these\nchallenges through a cohesive framework that enhances LLMs by refining and\nleveraging reliable KGs. First, we introduce contrastive error detection, a\nstructure-based method to identify incorrect facts in KGs. This approach is\nextended by an attribute-aware framework that unifies structural and semantic\nsignals for error correction. Next, we propose an inductive completion model\nthat further refines KGs by completing the missing relationships in evolving\nKGs. Building on these refined KGs, KnowGPT integrates structured graph\nreasoning into LLMs through dynamic prompting, improving factual grounding.\nThese contributions form a systematic pipeline (from error detection to LLM\nintegration), demonstrating that reliable KGs significantly enhance the\nrobustness, interpretability, and adaptability of LLMs.\n","authors":["Qinggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13178v1.pdf","comment":"Thesis"},{"id":"http://arxiv.org/abs/2506.09657v2","updated":"2025-06-16T07:42:18Z","published":"2025-06-11T12:26:08Z","title":"Team Anotheroption at SemEval-2025 Task 8: Bridging the Gap Between\n  Open-Source and Proprietary LLMs in Table QA","summary":"  This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.\n","authors":["Nikolas Evkarpidi","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2506.09657v2.pdf","comment":"Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures; full paper title was added"},{"id":"http://arxiv.org/abs/2506.01413v3","updated":"2025-06-16T07:40:34Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v3.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2506.13177v1","updated":"2025-06-16T07:38:04Z","published":"2025-06-16T07:38:04Z","title":"Development of the user-friendly decision aid Rule-based Evaluation and\n  Support Tool (REST) for optimizing the resources of an information extraction\n  task","summary":"  Rules could be an information extraction (IE) default option, compared to ML\nand LLMs in terms of sustainability, transferability, interpretability, and\ndevelopment burden. We suggest a sustainable and combined use of rules and ML\nas an IE method. Our approach starts with an exhaustive expert manual\nhighlighting in a single working session of a representative subset of the data\ncorpus. We developed and validated the feasibility and the performance metrics\nof the REST decision tool to help the annotator choose between rules as a by\ndefault option and ML for each entity of an IE task. REST makes the annotator\nvisualize the characteristics of each entity formalization in the free texts\nand the expected rule development feasibility and IE performance metrics. ML is\nconsidered as a backup IE option and manual annotation for training is\ntherefore minimized. The external validity of REST on a 12-entity use case\nshowed good reproducibility.\n","authors":["Guillaume Bazin","Xavier Tannier","Fanny Adda","Ariel Cohen","Akram Redjdal","Emmanuelle Kempf"],"pdf_url":"https://arxiv.org/pdf/2506.13177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11991v2","updated":"2025-06-16T07:35:52Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijian Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.15266v2","updated":"2025-06-16T07:35:23Z","published":"2025-02-21T07:48:54Z","title":"A Training-free LLM-based Approach to General Chinese Character Error\n  Correction","summary":"  Chinese spelling correction (CSC) is a crucial task that aims to correct\ncharacter errors in Chinese text. While conventional CSC focuses on character\nsubstitution errors caused by mistyping, two other common types of character\nerrors, missing and redundant characters, have received less attention. These\nerrors are often excluded from CSC datasets during the annotation process or\nignored during evaluation, even when they have been annotated. This issue\nlimits the practicality of the CSC task. To address this issue, we introduce\nthe task of General Chinese Character Error Correction (C2EC), which focuses on\nall three types of character errors. We construct a high-quality C2EC benchmark\nby combining and manually verifying data from CCTC and Lemon datasets. We\nextend the training-free prompt-free CSC method to C2EC by using Levenshtein\ndistance for handling length changes and leveraging an additional prompt-based\nlarge language model (LLM) to improve performance. Experiments show that our\nmethod enables a 14B-parameter LLM to be on par with models nearly 50 times\nlarger on both conventional CSC and C2EC tasks, without any fine-tuning.\n","authors":["Houquan Zhou","Bo Zhang","Zhenghua Li","Ming Yan","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.15266v2.pdf","comment":"Accepted at Main Conference of ACL 2025, 26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.13172v1","updated":"2025-06-16T07:34:31Z","published":"2025-06-16T07:34:31Z","title":"Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns","summary":"  We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks: identifying unsubstantiated claims in summaries\n(informational integrity) and flagging ambiguous pronoun references (linguistic\nclarity). We conducted a systematic, multi-run evaluation on two frontier\nmodels (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context\nconditions. Our results for the informational integrity task reveal a\nsignificant divergence in model performance: while both models successfully\nidentified an unsubstantiated head of a noun phrase (95% success), ChatGPT\nconsistently failed (0% success) to identify an unsubstantiated adjectival\nmodifier that Gemini correctly flagged (95% success), raising a question\nregarding potential influence of the target's syntactic role. For the\nlinguistic analysis task, both models performed well (80-90% success) with full\nmanuscript context. In a summary-only setting, however, ChatGPT achieved a\nperfect (100%) success rate, while Gemini's performance was substantially\ndegraded. Our findings suggest that structured prompting is a viable\nmethodology for complex textual analysis but show that prompt performance may\nbe highly dependent on the interplay between the model, task type, and context,\nhighlighting the need for rigorous, model-specific testing.\n","authors":["Evgeny Markhasin"],"pdf_url":"https://arxiv.org/pdf/2506.13172v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2506.13148v1","updated":"2025-06-16T07:00:48Z","published":"2025-06-16T07:00:48Z","title":"Adapting LLMs for Minimal-edit Grammatical Error Correction","summary":"  Decoder-only large language models have shown superior performance in the\nfluency-edit English Grammatical Error Correction, but their adaptation for\nminimal-edit English GEC is still underexplored. To improve their effectiveness\nin the minimal-edit approach, we explore the error rate adaptation topic and\npropose a novel training schedule method. Our experiments set a new\nstate-of-the-art result for a single-model system on the BEA-test set. We also\ndetokenize the most common English GEC datasets to match the natural way of\nwriting text. During the process, we find that there are errors in them. Our\nexperiments analyze whether training on detokenized datasets impacts the\nresults and measure the impact of the usage of the datasets with corrected\nerroneous examples. To facilitate reproducibility, we have released the source\ncode used to train our models.\n","authors":["Ryszard Staruch","Filip Graliński","Daniel Dzienisiewicz"],"pdf_url":"https://arxiv.org/pdf/2506.13148v1.pdf","comment":"Accepted at BEA-2025"},{"id":"http://arxiv.org/abs/2506.13143v1","updated":"2025-06-16T06:56:21Z","published":"2025-06-16T06:56:21Z","title":"CMU's IWSLT 2025 Simultaneous Speech Translation System","summary":"  This paper presents CMU's submission to the IWSLT 2025 Simultaneous Speech\nTranslation (SST) task for translating unsegmented English speech into Chinese\nand German text in a streaming manner. Our end-to-end speech-to-text system\nintegrates a chunkwise causal Wav2Vec 2.0 speech encoder, an adapter, and the\nQwen2.5-7B-Instruct as the decoder. We use a two-stage simultaneous training\nprocedure on robust speech segments curated from LibriSpeech, CommonVoice, and\nVoxPopuli datasets, utilizing standard cross-entropy loss. Our model supports\nadjustable latency through a configurable latency multiplier. Experimental\nresults demonstrate that our system achieves 44.3 BLEU for English-to-Chinese\nand 25.1 BLEU for English-to-German translations on the ACL60/60 development\nset, with computation-aware latencies of 2.7 seconds and 2.3 seconds, and\ntheoretical latencies of 2.2 and 1.7 seconds, respectively.\n","authors":["Siqi Ouyang","Xi Xu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2506.13143v1.pdf","comment":"IWSLT 2025 System Description"},{"id":"http://arxiv.org/abs/2502.05234v2","updated":"2025-06-16T06:53:48Z","published":"2025-02-07T19:35:25Z","title":"Optimizing Temperature for Language Models with Multi-Sample Inference","summary":"  Multi-sample aggregation strategies, such as majority voting and best-of-N\nsampling, are widely used in contemporary large language models (LLMs) to\nenhance predictive accuracy across various tasks. A key challenge in this\nprocess is temperature selection, which significantly impacts model\nperformance. Existing approaches either rely on a fixed default temperature or\nrequire labeled validation data for tuning, which are often scarce and\ndifficult to obtain. This paper addresses the challenge of automatically\nidentifying the (near)-optimal temperature for different LLMs using\nmulti-sample aggregation strategies, without relying on task-specific\nvalidation data. We provide a comprehensive analysis of temperature's role in\nperformance optimization, considering variations in model architectures,\ndatasets, task types, model sizes, and predictive accuracy. Furthermore, we\npropose a novel entropy-based metric for automated temperature optimization,\nwhich consistently outperforms fixed-temperature baselines. Additionally, we\nincorporate a stochastic process model to enhance interpretability, offering\ndeeper insights into the relationship between temperature and model\nperformance.\n","authors":["Weihua Du","Yiming Yang","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2502.05234v2.pdf","comment":"ICML2025, 21 pages. Code available at\n  https://github.com/StigLidu/TURN"},{"id":"http://arxiv.org/abs/2503.02969v2","updated":"2025-06-16T06:38:23Z","published":"2025-03-04T19:51:29Z","title":"InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model","summary":"  Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST\n","authors":["Siqi Ouyang","Xi Xu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2503.02969v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.13130v1","updated":"2025-06-16T06:27:59Z","published":"2025-06-16T06:27:59Z","title":"ZINA: Multimodal Fine-grained Hallucination Detection and Editing","summary":"  Multimodal Large Language Models (MLLMs) often generate hallucinations, where\nthe output deviates from the visual content. Given that these hallucinations\ncan take diverse forms, detecting hallucinations at a fine-grained level is\nessential for comprehensive evaluation and analysis. To this end, we propose a\nnovel task of multimodal fine-grained hallucination detection and editing for\nMLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated\nspans at a fine-grained level, classifies their error types into six\ncategories, and suggests appropriate refinements. To train and evaluate models\nfor this task, we constructed VisionHall, a dataset comprising 6.9k outputs\nfrom twelve MLLMs manually annotated by 211 annotators, and 20k synthetic\nsamples generated using a graph-based method that captures dependencies among\nerror types. We demonstrated that ZINA outperformed existing methods, including\nGPT-4o and LLama-3.2, in both detection and editing tasks.\n","authors":["Yuiga Wada","Kazuki Matsuda","Komei Sugiura","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2506.13130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17657v3","updated":"2025-06-16T06:25:31Z","published":"2024-10-23T08:19:18Z","title":"ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents","summary":"  Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2410.17657v3.pdf","comment":"ACL 2025 Main Paper"},{"id":"http://arxiv.org/abs/2506.09983v2","updated":"2025-06-16T06:09:38Z","published":"2025-06-11T17:56:10Z","title":"Step-by-step Instructions and a Simple Tabular Output Format Improve the\n  Dependency Parsing Accuracy of LLMs","summary":"  Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.\n","authors":["Hiroshi Matsuda","Chunpeng Ma","Masayuki Asahara"],"pdf_url":"https://arxiv.org/pdf/2506.09983v2.pdf","comment":"9 pages, 2 figures, accepted to SyntaxFest 2025"},{"id":"http://arxiv.org/abs/2503.16212v2","updated":"2025-06-16T05:58:00Z","published":"2025-03-20T15:00:41Z","title":"MathFusion: Enhancing Mathematical Problem-solving of LLM through\n  Instruction Fusion","summary":"  Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.\n","authors":["Qizhi Pei","Lijun Wu","Zhuoshi Pan","Yu Li","Honglin Lin","Chenlin Ming","Xin Gao","Conghui He","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2503.16212v2.pdf","comment":"Accepted by ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2506.07483v2","updated":"2025-06-16T05:48:44Z","published":"2025-06-09T07:00:04Z","title":"A Hybrid GA LLM Framework for Structured Task Optimization","summary":"  GA LLM is a hybrid framework that combines Genetic Algorithms with Large\nLanguage Models to handle structured generation tasks under strict constraints.\nEach output, such as a plan or report, is treated as a gene, and evolutionary\noperations like selection, crossover, and mutation are guided by the language\nmodel to iteratively improve solutions. The language model provides domain\nknowledge and creative variation, while the genetic algorithm ensures\nstructural integrity and global optimization. GA LLM has proven effective in\ntasks such as itinerary planning, academic outlining, and business reporting,\nconsistently producing well structured and requirement satisfying results. Its\nmodular design also makes it easy to adapt to new tasks. Compared to using a\nlanguage model alone, GA LLM achieves better constraint satisfaction and higher\nquality solutions by combining the strengths of both components.\n","authors":["William Shum","Rachel Chan","Jonas Lin","Benny Feng","Patrick Lau"],"pdf_url":"https://arxiv.org/pdf/2506.07483v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.12999v2","updated":"2025-06-16T05:47:36Z","published":"2024-10-16T19:56:22Z","title":"POROver: Improving Safety and Reducing Overrefusal in Large Language\n  Models with Overgeneration and Preference Optimization","summary":"  Achieving both high safety and high usefulness simultaneously in large\nlanguage models has become a critical challenge in recent years.Models often\nexhibit unsafe behavior or adopt an overly cautious approach leading to\nfrequent overrefusal of benign prompts, which reduces their usefulness. A major\nfactor underlying these behaviors is how the models are finetuned and aligned,\nparticularly the nature and extent of the data used.In this work, we examine\nhow overgenerating finetuning data with advanced teacher models (e.g.,\nGPT-4o)-covering both general-purpose and toxic prompts-affects safety and\nusefulness in instruction-following language models.Additionally, we present\nPOROver, an alignment strategy designed for models that are highly safe but\nprone to overrefusal. POROver employs preference optimization algorithms and\nleverages completions from an advanced teacher model to reduce overrefusals\nwhile maintaining safety.Our results show that overgenerating completions for\ngeneral-purpose prompts significantly boosts safety with only a minimal impact\non usefulness. Specifically, the F1 score calculated between safety and\nusefulness increases from 74.4% to 91.8% because of a substantial rise in\nsafety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1%\nto 57.6% while preserving safety. Finally, applying POROVer increases\nusefulness further-from 57.6% to 82.1%-while keeping safety at comparable\nlevels. Our data and code are available at\nhttps://github.com/batuhankmkaraman/POROver.\n","authors":["Batuhan K. Karaman","Ishmam Zabir","Alon Benhaim","Vishrav Chaudhary","Mert R. Sabuncu","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2410.12999v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13116v1","updated":"2025-06-16T05:47:29Z","published":"2025-06-16T05:47:29Z","title":"Crime Hotspot Prediction Using Deep Graph Convolutional Networks","summary":"  Crime hotspot prediction is critical for ensuring urban safety and effective\nlaw enforcement, yet it remains challenging due to the complex spatial\ndependencies inherent in criminal activity. The previous approaches tended to\nuse classical algorithms such as the KDE and SVM to model data distributions\nand decision boundaries. The methods often fail to capture these spatial\nrelationships, treating crime events as independent and ignoring geographical\ninteractions. To address this, we propose a novel framework based on Graph\nConvolutional Networks (GCNs), which explicitly model spatial dependencies by\nrepresenting crime data as a graph. In this graph, nodes represent discrete\ngeographic grid cells and edges capture proximity relationships. Using the\nChicago Crime Dataset, we engineer spatial features and train a multi-layer GCN\nmodel to classify crime types and predict high-risk zones. Our approach\nachieves 88% classification accuracy, significantly outperforming traditional\nmethods. Additionally, the model generates interpretable heat maps of crime\nhotspots, demonstrating the practical utility of graph-based learning for\npredictive policing and spatial criminology.\n","authors":["Tehreem Zubair","Syeda Kisaa Fatima","Noman Ahmed","Asifullah Khan"],"pdf_url":"https://arxiv.org/pdf/2506.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13109v1","updated":"2025-06-16T05:37:49Z","published":"2025-06-16T05:37:49Z","title":"Leveraging In-Context Learning for Language Model Agents","summary":"  In-context learning (ICL) with dynamically selected demonstrations combines\nthe flexibility of prompting large language models (LLMs) with the ability to\nleverage training data to improve performance. While ICL has been highly\nsuccessful for prediction and generation tasks, leveraging it for agentic tasks\nthat require sequential decision making is challenging -- one must think not\nonly about how to annotate long trajectories at scale and how to select\ndemonstrations, but also what constitutes demonstrations, and when and where to\nshow them. To address this, we first propose an algorithm that leverages an LLM\nwith retries along with demonstrations to automatically and efficiently\nannotate agentic tasks with solution trajectories. We then show that\nset-selection of trajectories of similar tasks as demonstrations significantly\nimproves performance, reliability, robustness, and efficiency of LLM agents.\nHowever, trajectory demonstrations have a large inference cost overhead. We\nshow that this can be mitigated by using small trajectory snippets at every\nstep instead of an additional trajectory. We find that demonstrations obtained\nfrom larger models (in the annotation phase) also improve smaller models, and\nthat ICL agents can even rival costlier trained agents. Thus, our results\nreveal that ICL, with careful use, can be very powerful for agentic tasks as\nwell.\n","authors":["Shivanshu Gupta","Sameer Singh","Ashish Sabharwal","Tushar Khot","Ben Bogin"],"pdf_url":"https://arxiv.org/pdf/2506.13109v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.03009v2","updated":"2025-06-16T05:27:08Z","published":"2025-02-05T09:11:13Z","title":"Scaling Laws for Upcycling Mixture-of-Experts Language Models","summary":"  Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints.\n","authors":["Seng Pei Liew","Takuya Kato","Sho Takase"],"pdf_url":"https://arxiv.org/pdf/2502.03009v2.pdf","comment":"ICML 2025. 16 figures, 8 tables. Code available at\n  https://github.com/sbintuitions/sparse-upcycling-scaling-laws"},{"id":"http://arxiv.org/abs/2506.13104v1","updated":"2025-06-16T05:23:42Z","published":"2025-06-16T05:23:42Z","title":"Equitable Electronic Health Record Prediction with FAME: Fairness-Aware\n  Multimodal Embedding","summary":"  Electronic Health Record (EHR) data encompass diverse modalities -- text,\nimages, and medical codes -- that are vital for clinical decision-making. To\nprocess these complex data, multimodal AI (MAI) has emerged as a powerful\napproach for fusing such information. However, most existing MAI models\noptimize for better prediction performance, potentially reinforcing biases\nacross patient subgroups. Although bias-reduction techniques for multimodal\nmodels have been proposed, the individual strengths of each modality and their\ninterplay in both reducing bias and optimizing performance remain\nunderexplored. In this work, we introduce FAME (Fairness-Aware Multimodal\nEmbeddings), a framework that explicitly weights each modality according to its\nfairness contribution. FAME optimizes both performance and fairness by\nincorporating a combined loss function. We leverage the Error Distribution\nDisparity Index (EDDI) to measure fairness across subgroups and propose a\nsign-agnostic aggregation method to balance fairness across subgroups, ensuring\nequitable model outcomes. We evaluate FAME with BEHRT and BioClinicalBERT,\ncombining structured and unstructured EHR data, and demonstrate its\neffectiveness in terms of performance and fairness compared with other\nbaselines across multiple EHR prediction tasks.\n","authors":["Nikkie Hooman","Zhongjie Wu","Eric C. Larson","Mehak Gupta"],"pdf_url":"https://arxiv.org/pdf/2506.13104v1.pdf","comment":"21 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.13102v1","updated":"2025-06-16T05:15:53Z","published":"2025-06-16T05:15:53Z","title":"Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware\n  Strategies for LLMs and VLMs","summary":"  Test-time scaling has recently emerged as a promising approach for enhancing\nthe reasoning capabilities of large language models or vision-language models\nduring inference. Although a variety of test-time scaling strategies have been\nproposed, and interest in their application to the medical domain is growing,\nmany critical aspects remain underexplored, including their effectiveness for\nvision-language models and the identification of optimal strategies for\ndifferent settings. In this paper, we conduct a comprehensive investigation of\ntest-time scaling in the medical domain. We evaluate its impact on both large\nlanguage models and vision-language models, considering factors such as model\nsize, inherent model characteristics, and task complexity. Finally, we assess\nthe robustness of these strategies under user-driven factors, such as\nmisleading information embedded in prompts. Our findings offer practical\nguidelines for the effective use of test-time scaling in medical applications\nand provide insights into how these strategies can be further refined to meet\nthe reliability and interpretability demands of the medical domain.\n","authors":["Gyutaek Oh","Seoyeon Kim","Sangjoon Park","Byung-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2506.13102v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.06560v3","updated":"2025-06-16T04:19:54Z","published":"2025-04-09T03:46:56Z","title":"NeedleInATable: Exploring Long-Context Capability of Large Language\n  Models towards Long-Structured Tables","summary":"  Processing structured tabular data, particularly large and lengthy tables,\nconstitutes a fundamental yet challenging task for large language models\n(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack\nprimarily focus on unstructured text, neglecting the challenge of diverse\nstructured tables. Meanwhile, previous tabular benchmarks mainly consider\ndownstream tasks that require high-level reasoning abilities, and overlook\nmodels' underlying fine-grained perception of individual table cells, which is\ncrucial for practical and robust LLM-based table applications. To address this\ngap, we introduce \\textsc{NeedleInATable} (NIAT), a new long-context tabular\nbenchmark that treats each table cell as a ``needle'' and requires models to\nextract the target cell based on cell locations or lookup questions. Our\ncomprehensive evaluation of various LLMs and multimodal LLMs reveals a\nsubstantial performance gap between popular downstream tabular tasks and the\nsimpler NIAT task, suggesting that they may rely on dataset-specific\ncorrelations or shortcuts to obtain better benchmark results but lack truly\nrobust long-context understanding towards structured tables. Furthermore, we\ndemonstrate that using synthesized NIAT training data can effectively improve\nperformance on both NIAT task and downstream tabular tasks, which validates the\nimportance of NIAT capability for LLMs' genuine table understanding ability.\nOur data, code and models will be released to facilitate future research.\n","authors":["Lanrui Wang","Mingyu Zheng","Hongyin Tang","Zheng Lin","Yanan Cao","Jingang Wang","Xunliang Cai","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06560v3.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2402.11827v2","updated":"2025-06-16T03:39:07Z","published":"2024-02-19T04:41:31Z","title":"Ask Optimal Questions: Aligning Large Language Models with Retriever's\n  Preference in Conversation","summary":"  Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM on this dataset to align\nit with the retrievers' feedback. Our resulting model demonstrates superiority\non two benchmarks, surpassing the previous state-of-the-art performance of\nrewrite-then-retrieve approaches.\n","authors":["Chanwoong Yoon","Gangwoo Kim","Byeongguk Jeon","Sungdong Kim","Yohan Jo","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2402.11827v2.pdf","comment":"NAACL 2025 (findings)"},{"id":"http://arxiv.org/abs/2502.02508v3","updated":"2025-06-16T03:29:47Z","published":"2025-02-04T17:26:58Z","title":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search","summary":"  Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced.\n","authors":["Maohao Shen","Guangtao Zeng","Zhenting Qi","Zhang-Wei Hong","Zhenfang Chen","Wei Lu","Gregory Wornell","Subhro Das","David Cox","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2502.02508v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13070v1","updated":"2025-06-16T03:26:10Z","published":"2025-06-16T03:26:10Z","title":"CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope --\n  Make Your LLM to Get Them Right","summary":"  In this paper, we describe our approach for the SemEval 2025 Task 2 on\nEntity-Aware Machine Translation (EA-MT). Our system aims to improve the\naccuracy of translating named entities by combining two key approaches:\nRetrieval Augmented Generation (RAG) and iterative self-refinement techniques\nusing Large Language Models (LLMs). A distinctive feature of our system is its\nself-evaluation mechanism, where the LLM assesses its own translations based on\ntwo key criteria: the accuracy of entity translations and overall translation\nquality. We demonstrate how these methods work together and effectively improve\nentity handling while maintaining high-quality translations.\n","authors":["Jaebok Lee","Yonghyun Ryu","Seongmin Park","Yoonjung Choi"],"pdf_url":"https://arxiv.org/pdf/2506.13070v1.pdf","comment":"The 19th International Workshop on Semantic Evaluation"},{"id":"http://arxiv.org/abs/2506.13066v1","updated":"2025-06-16T03:19:31Z","published":"2025-06-16T03:19:31Z","title":"FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data\n  and Reward Design","summary":"  Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning\ncapabilities. However, financial applications face challenges due to the lack\nof high-quality multimodal reasoning datasets and the inefficiency of existing\ntraining paradigms for reasoning enhancement. To address these issues, we\npropose an integrated framework, FinLMM-R1, combining an automated and scalable\npipeline for data construction with enhanced training strategies to improve the\nmultimodal reasoning of LMM. The Automated and Scalable Pipeline (ASP) resolves\ntextual-visual misalignment in financial reports through a separate paradigm of\nquestion-answer generation and image-question alignment, ensuring data\nintegrity and extraction efficiency. Through ASP, we collect 89,378 aligned\nimage-question pairs from 23,397 financial reports, covering tasks such as\narithmetic reasoning, statistics reasoning, financial explanation, and\nfinancial knowledge. Moreover, we introduce the Thinking with Adversarial\nReward in LMM (TAR-LMM), extending the prior two-stage training framework [1]\nwith additional reward mechanisms. In the first stage, we focus on text-only\ntasks with format and accuracy rewards to guide the model in generating\nwell-structured thinking contents. In the second stage, we construct\nmulti-image contrastive samples with additional reward components including\nimage selection, thinking content length, and adversarial reward to jointly\noptimize the LMM across visual perception, reasoning efficiency, and logical\ncoherence. Extensive experiments on 7 benchmarks show ASP-derived dataset and\ntraining framework significantly improve answer accuracy and reasoning depth\nover existing reasoning LMMs in both general and financial multimodal contexts.\n","authors":["Kai Lan","Jiayong Zhu","Jiangtong Li","Dawei Cheng","Guang Chen","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.13066v1.pdf","comment":"26 pages, 16 figures"},{"id":"http://arxiv.org/abs/2408.08089v2","updated":"2025-06-16T03:19:20Z","published":"2024-08-15T11:33:20Z","title":"AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents","summary":"  Current research in LLM-based simulation systems lacks comprehensive\nsolutions for modeling real-world court proceedings, while existing legal\nlanguage models struggle with dynamic courtroom interactions. We present\nAgentCourt, a comprehensive legal simulation framework that addresses these\nchallenges through adversarial evolution of LLM-based agents. Our AgentCourt\nintroduces a new adversarial evolutionary approach for agents called AdvEvol,\nwhich performs dynamic knowledge learning and evolution through structured\nadversarial interactions in a simulated courtroom program, breaking the\nlimitations of the traditional reliance on static knowledge bases or manual\nannotations. By simulating 1,000 civil cases, we construct an evolving\nknowledge base that enhances the agents' legal reasoning abilities. The evolved\nlawyer agents demonstrated outstanding performance on our newly introduced\nCourtBench benchmark, achieving a 12.1% improvement in performance compared to\nthe original lawyer agents. Evaluations by professional lawyers confirm the\neffectiveness of our approach across three critical dimensions: cognitive\nagility, professional knowledge, and logical rigor. Beyond outperforming\nspecialized legal models in interactive reasoning tasks, our findings emphasize\nthe importance of adversarial learning in legal AI and suggest promising\ndirections for extending simulation-based legal reasoning to broader judicial\nand regulatory contexts. The project's code is available at:\nhttps://github.com/relic-yuexi/AgentCourt\n","authors":["Guhong Chen","Liyang Fan","Zihan Gong","Nan Xie","Zixuan Li","Ziqiang Liu","Chengming Li","Qiang Qu","Hamid Alinejad-Rokny","Shiwen Ni","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13065v1","updated":"2025-06-16T03:18:28Z","published":"2025-06-16T03:18:28Z","title":"MotiveBench: How Far Are We From Human-Like Motivational Reasoning in\n  Large Language Models?","summary":"  Large language models (LLMs) have been widely adopted as the core of agent\nframeworks in various scenarios, such as social simulations and AI companions.\nHowever, the extent to which they can replicate human-like motivations remains\nan underexplored question. Existing benchmarks are constrained by simplistic\nscenarios and the absence of character identities, resulting in an information\nasymmetry with real-world situations. To address this gap, we propose\nMotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning\ntasks covering multiple levels of motivation. Using MotiveBench, we conduct\nextensive experiments on seven popular model families, comparing different\nscales and versions within each family. The results show that even the most\nadvanced LLMs still fall short in achieving human-like motivational reasoning.\nOur analysis reveals key findings, including the difficulty LLMs face in\nreasoning about \"love & belonging\" motivations and their tendency toward\nexcessive rationality and idealism. These insights highlight a promising\ndirection for future research on the humanization of LLMs. The dataset,\nbenchmark, and code are available at https://aka.ms/motivebench.\n","authors":["Xixian Yong","Jianxun Lian","Xiaoyuan Yi","Xiao Zhou","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2506.13065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13063v1","updated":"2025-06-16T03:12:51Z","published":"2025-06-16T03:12:51Z","title":"PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical\n  Dialogue","summary":"  Recent pathology foundation models can provide rich tile-level\nrepresentations but fall short of delivering general-purpose clinical utility\nwithout further extensive model development. These models lack whole-slide\nimage (WSI) understanding and are not trained with large-scale diagnostic data,\nlimiting their performance on diverse downstream tasks. We introduce PRISM2, a\nmulti-modal slide-level foundation model trained via clinical dialogue to\nenable scalable, generalizable pathology AI. PRISM2 is trained on nearly\n700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic\nreports in a two-stage process. In Stage 1, a vision-language model is trained\nusing contrastive and captioning objectives to align whole slide embeddings\nwith textual clinical diagnosis. In Stage 2, the language model is unfrozen to\nenable diagnostic conversation and extract more clinically meaningful\nrepresentations from hidden states. PRISM2 achieves strong performance on\ndiagnostic and biomarker prediction tasks, outperforming prior slide-level\nmodels including PRISM and TITAN. It also introduces a zero-shot yes/no\nclassification approach that surpasses CLIP-style methods without prompt tuning\nor class enumeration. By aligning visual features with clinical reasoning,\nPRISM2 improves generalization on both data-rich and low-sample tasks, offering\na scalable path forward for building general pathology AI agents capable of\nassisting diagnostic and prognostic decisions.\n","authors":["George Shaikovski","Eugene Vorontsov","Adam Casson","Julian Viret","Eric Zimmermann","Neil Tenenholtz","Yi Kan Wang","Jan H. Bernhard","Ran A. Godrich","Juan A. Retamero","Razik Yousfi","Nicolo Fusi","Thomas J. Fuchs","Kristen Severson","Siqi Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11999v2","updated":"2025-06-16T03:10:31Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v2.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"},{"id":"http://arxiv.org/abs/2506.13059v1","updated":"2025-06-16T03:00:40Z","published":"2025-06-16T03:00:40Z","title":"Multipole Attention for Efficient Long Context Reasoning","summary":"  Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.\n","authors":["Coleman Hooper","Sebastian Zhao","Luca Manolache","Sehoon Kim","Michael W. Mahoney","Yakun Sophia Shao","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2506.13059v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.09342v2","updated":"2025-06-16T02:57:37Z","published":"2025-06-11T02:48:16Z","title":"Latent Multi-Head Attention for Small Language Models","summary":"  We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.\n","authors":["Sushant Mehta","Raj Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2506.09342v2.pdf","comment":"6 pages, 1 figure. 5 tables"},{"id":"http://arxiv.org/abs/2506.13055v1","updated":"2025-06-16T02:52:44Z","published":"2025-06-16T02:52:44Z","title":"CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal\n  Large Language Model","summary":"  Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth\nof Large Language Models (LLMs) and are now applied in various fields. In\nfinance, the integration of diverse modalities such as text, charts, and tables\nis crucial for accurate and efficient decision-making. Therefore, an effective\nevaluation system that incorporates these data types is essential for advancing\nfinancial application. In this paper, we introduce CFBenchmark-MM, a Chinese\nmultimodal financial benchmark with over 9,000 image-question pairs featuring\ntables, histogram charts, line charts, pie charts, and structural diagrams.\nAdditionally, we develop a staged evaluation system to assess MLLMs in handling\nmultimodal information by providing different visual content step by step.\nDespite MLLMs having inherent financial knowledge, experimental results still\nshow limited efficiency and robustness in handling multimodal financial\ncontext. Further analysis on incorrect responses reveals the misinterpretation\nof visual content and the misunderstanding of financial concepts are the\nprimary issues. Our research validates the significant, yet underexploited,\npotential of MLLMs in financial analysis, highlighting the need for further\ndevelopment and domain-specific optimization to encourage the enhanced use in\nfinancial domain.\n","authors":["Jiangtong Li","Yiyun Zhu","Dawei Cheng","Zhijun Ding","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.13055v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.13051v1","updated":"2025-06-16T02:40:33Z","published":"2025-06-16T02:40:33Z","title":"Stress-Testing Multimodal Foundation Models for Crystallographic\n  Reasoning","summary":"  Evaluating foundation models for crystallographic reasoning requires\nbenchmarks that isolate generalization behavior while enforcing physical\nconstraints. This work introduces a multiscale multicrystal dataset with two\nphysically grounded evaluation protocols to stress-test multimodal generative\nmodels. The Spatial-Exclusion benchmark withholds all supercells of a given\nradius from a diverse dataset, enabling controlled assessments of spatial\ninterpolation and extrapolation. The Compositional-Exclusion benchmark omits\nall samples of a specific chemical composition, probing generalization across\nstoichiometries. Nine vision--language foundation models are prompted with\ncrystallographic images and textual context to generate structural annotations.\nResponses are evaluated via (i) relative errors in lattice parameters and\ndensity, (ii) a physics-consistency index penalizing volumetric violations, and\n(iii) a hallucination score capturing geometric outliers and invalid\nspace-group predictions. These benchmarks establish a reproducible, physically\ninformed framework for assessing generalization, consistency, and reliability\nin large-scale multimodal models. Dataset and code are available at\nhttps://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.\n","authors":["Can Polat","Hasan Kurban","Erchin Serpedin","Mustafa Kurban"],"pdf_url":"https://arxiv.org/pdf/2506.13051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07311v9","updated":"2025-06-16T02:37:13Z","published":"2024-03-12T04:47:29Z","title":"Knowledge Graph Large Language Model (KG-LLM) for Link Prediction","summary":"  The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios.\n","authors":["Dong Shu","Tianle Chen","Mingyu Jin","Chong Zhang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07311v9.pdf","comment":"Accepted by ACML 2024"},{"id":"http://arxiv.org/abs/2410.07524v2","updated":"2025-06-16T02:29:18Z","published":"2024-10-10T01:36:03Z","title":"Upcycling Large Language Models into Mixture of Experts","summary":"  Upcycling pre-trained dense language models into sparse mixture-of-experts\n(MoE) models is an efficient approach to increase the model capacity of already\ntrained models. However, optimal techniques for upcycling at scale remain\nunclear. In this work, we conduct an extensive study of upcycling methods and\nhyperparameters for billion-parameter scale language models. We propose a novel\n\"virtual group\" initialization scheme and weight scaling approach to enable\nupcycling into fine-grained MoE architectures. Through ablations, we find that\nupcycling outperforms continued dense model training. In addition, we show that\nsoftmax-then-topK expert routing improves over topK-then-softmax approach and\nhigher granularity MoEs can help improve accuracy. Finally, we upcycled\nNemotron-4 15B on 1T tokens and compared it to a continuously trained version\nof the same model on the same 1T tokens: the continuous trained model achieved\n65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer\ninsights and best practices to effectively leverage upcycling for building MoE\nlanguage models. Code is available.\n","authors":["Ethan He","Abhinav Khattar","Ryan Prenger","Vijay Korthikanti","Zijie Yan","Tong Liu","Shiqing Fan","Ashwath Aithal","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2410.07524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11105v2","updated":"2025-06-16T02:22:18Z","published":"2025-06-07T01:37:42Z","title":"Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation","summary":"  Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.\n","authors":["Uttej Kallakurik","Edward Humes","Rithvik Jonna","Xiaomin Lin","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2506.11105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13044v1","updated":"2025-06-16T02:21:15Z","published":"2025-06-16T02:21:15Z","title":"Just Go Parallel: Improving the Multilingual Capabilities of Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated impressive translation\ncapabilities even without being explicitly trained on parallel data. This\nremarkable property has led some to believe that parallel data is no longer\nnecessary for building multilingual language models. While some attribute this\nto the emergent abilities of LLMs due to scale, recent work suggests that it is\nactually caused by incidental bilingual signals present in the training data.\nVarious methods have been proposed to maximize the utility of parallel data to\nenhance the multilingual capabilities of multilingual encoder-based and\nencoder-decoder language models. However, some decoder-based LLMs opt to ignore\nparallel data instead. In this work, we conduct a systematic study on the\nimpact of adding parallel data on LLMs' multilingual capabilities, focusing\nspecifically on translation and multilingual common-sense reasoning. Through\ncontrolled experiments, we demonstrate that parallel data can significantly\nimprove LLMs' multilingual capabilities.\n","authors":["Muhammad Reza Qorib","Junyi Li","Hwee Tou Ng"],"pdf_url":"https://arxiv.org/pdf/2506.13044v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2409.04267v3","updated":"2025-06-16T02:13:39Z","published":"2024-09-06T13:24:22Z","title":"An overview of domain-specific foundation model: key technologies,\n  applications and challenges","summary":"  The impressive performance of ChatGPT and other foundation-model-based\nproducts in human language understanding has prompted both academia and\nindustry to explore how these models can be tailored for specific industries\nand application scenarios. This process, known as the customization of\ndomain-specific foundation models (FMs), addresses the limitations of\ngeneral-purpose models, which may not fully capture the unique patterns and\nrequirements of domain-specific data. Despite its importance, there is a\nnotable lack of comprehensive overview papers on building domain-specific FMs,\nwhile numerous resources exist for general-purpose models. To bridge this gap,\nthis article provides a timely and thorough overview of the methodology for\ncustomizing domain-specific FMs. It introduces basic concepts, outlines the\ngeneral architecture, and surveys key methods for constructing domain-specific\nmodels. Furthermore, the article discusses various domains that can benefit\nfrom these specialized models and highlights the challenges ahead. Through this\noverview, we aim to offer valuable guidance and reference for researchers and\npractitioners from diverse fields to develop their own customized FMs.\n","authors":["Haolong Chen","Hanzhi Chen","Zijian Zhao","Kaifeng Han","Guangxu Zhu","Yichen Zhao","Ying Du","Wei Xu","Qingjiang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.04267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10588v4","updated":"2025-06-16T02:12:47Z","published":"2024-11-15T21:19:04Z","title":"A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems","summary":"  We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.\n","authors":["Caspar Oesterheld","Emery Cooper","Miles Kodama","Linh Chi Nguyen","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2411.10588v4.pdf","comment":"48 pages, 15 figures; code and data at\n  https://github.com/casparoe/newcomblike_questions_dataset"},{"id":"http://arxiv.org/abs/2505.21549v4","updated":"2025-06-16T01:45:22Z","published":"2025-05-25T07:08:07Z","title":"Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal\n  Transformer Distillation","summary":"  We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md.\n","authors":["Daniel Csizmadia","Andrei Codreanu","Victor Sim","Vighnesh Prabhu","Michael Lu","Kevin Zhu","Sean O'Brien","Vasu Sharma"],"pdf_url":"https://arxiv.org/pdf/2505.21549v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11031v2","updated":"2025-06-16T01:28:03Z","published":"2025-05-20T22:44:04Z","title":"Task-aligned prompting improves zero-shot detection of AI-generated\n  images by Vision-Language Models","summary":"  As image generators produce increasingly realistic images, concerns about\npotential misuse continue to grow. Supervised detection relies on large,\ncurated datasets and struggles to generalize across diverse generators. In this\nwork, we investigate the use of pre-trained Vision-Language Models (VLMs) for\nzero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit\nsome task-specific reasoning and chain-of-thought prompting offers gains, we\nshow that task-aligned prompting elicits more focused reasoning and\nsignificantly improves performance without fine-tuning. Specifically, prefixing\nthe model's response with the phrase \"Let's examine the style and the synthesis\nartifacts\" -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by\n8%-29%. These gains are consistent for two widely used open-source models and\nacross three recent, diverse datasets spanning human faces, objects, and\nanimals with images generated by 16 different models -- demonstrating strong\ngeneralization. We further evaluate the approach across three additional model\nsizes and observe improvements in most dataset-model combinations -- suggesting\nrobustness to model scale. Surprisingly, self-consistency, a behavior\npreviously observed in language reasoning, where aggregating answers from\ndiverse reasoning paths improves performance, also holds in this setting. Even\nhere, zero-shot-s$^2$ scales better than chain-of-thought in most cases --\nindicating that it elicits more useful diversity. Our findings show that\ntask-aligned prompts elicit more focused reasoning and enhance latent\ncapabilities in VLMs, like the detection of AI-generated images -- offering a\nsimple, generalizable, and explainable alternative to supervised methods. Our\ncode is publicly available on github: https://github.com/Zoher15/Zero-shot-s2.\n","authors":["Zoher Kachwala","Danishjeet Singh","Danielle Yang","Filippo Menczer"],"pdf_url":"https://arxiv.org/pdf/2506.11031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13026v1","updated":"2025-06-16T01:26:08Z","published":"2025-06-16T01:26:08Z","title":"Knowledge Graph Fusion with Large Language Models for Accurate,\n  Explainable Manufacturing Process Planning","summary":"  Precision process planning in Computer Numerical Control (CNC) machining\ndemands rapid, context-aware decisions on tool selection, feed-speed pairs, and\nmulti-axis routing, placing immense cognitive and procedural burdens on\nengineers from design specification through final part inspection. Conventional\nrule-based computer-aided process planning and knowledge-engineering shells\nfreeze domain know-how into static tables, which become limited when dealing\nwith unseen topologies, novel material states, shifting\ncost-quality-sustainability weightings, or shop-floor constraints such as tool\nunavailability and energy caps. Large language models (LLMs) promise flexible,\ninstruction-driven reasoning for tasks but they routinely hallucinate numeric\nvalues and provide no provenance. We present Augmented Retrieval Knowledge\nNetwork Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that\nfuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented\ngeneration to deliver verifiable, numerically exact answers for CNC process\nplanning. ARKNESS (1) automatically distills heterogeneous machining documents,\nG-code annotations, and vendor datasheets into augmented triple,\nmulti-relational graphs without manual labeling, and (2) couples any on-prem\nLLM with a retriever that injects the minimal, evidence-linked subgraph needed\nto answer a query. Benchmarked on 155 industry-curated questions spanning tool\nsizing and feed-speed optimization, a lightweight 3B-parameter Llama-3\naugmented by ARKNESS matches GPT-4o accuracy while achieving a +25 percentage\npoint gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1x ROUGE-L on\nopen-ended responses.\n","authors":["Danny Hoang","David Gorsich","Matthew P. Castanier","Farhad Imani"],"pdf_url":"https://arxiv.org/pdf/2506.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13020v1","updated":"2025-06-16T01:14:52Z","published":"2025-06-16T01:14:52Z","title":"Edeflip: Supervised Word Translation between English and Yoruba","summary":"  In recent years, embedding alignment has become the state-of-the-art machine\ntranslation approach, as it can yield high-quality translation without training\non parallel corpora. However, existing research and application of embedding\nalignment mostly focus on high-resource languages with high-quality monolingual\nembeddings. It is unclear if and how low-resource languages may be similarly\nbenefited. In this study, we implement an established supervised embedding\nalignment method for word translation from English to Yoruba, the latter a\nlow-resource language. We found that higher embedding quality and normalizing\nembeddings increase word translation precision, with, additionally, an\ninteraction effect between the two. Our results demonstrate the limitations of\nthe state-of-the-art supervised embedding alignment when it comes to\nlow-resource languages, for which there are additional factors that need to be\ntaken into consideration, such as the importance of curating high-quality\nmonolingual embeddings. We hope our work will be a starting point for further\nmachine translation research that takes into account the challenges that\nlow-resource languages face.\n","authors":["Ikeoluwa Abioye","Jiani Ge"],"pdf_url":"https://arxiv.org/pdf/2506.13020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00332v2","updated":"2025-06-16T01:12:52Z","published":"2025-05-31T01:09:04Z","title":"Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus","summary":"  Code-mixing involves the seamless integration of linguistic elements from\nmultiple languages within a single discourse, reflecting natural multilingual\ncommunication patterns. Despite its prominence in informal interactions such as\nsocial media, chat messages and instant-messaging exchanges, there has been a\nlack of publicly available corpora that are author-labeled and suitable for\nmodeling human conversations and relationships. This study introduces the first\nlabeled and general-purpose corpus for understanding code-mixing in context\nwhile maintaining rigorous privacy and ethical standards. Our live project will\ncontinuously gather, verify, and integrate code-mixed messages into a\nstructured dataset released in JSON format, accompanied by detailed metadata\nand linguistic statistics. To date, it includes over 355,641 messages spanning\nvarious code-mixing patterns, with a primary focus on English, Mandarin, and\nother languages. We expect the Codemix Corpus to serve as a foundational\ndataset for research in computational linguistics, sociolinguistics, and NLP\napplications.\n","authors":["Svetlana Churina","Akshat Gupta","Insyirah Mujtahid","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2506.00332v2.pdf","comment":"19 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2503.23243v2","updated":"2025-06-16T00:52:04Z","published":"2025-03-29T22:53:15Z","title":"Evaluating how LLM annotations represent diverse views on contentious\n  topics","summary":"  Researchers have proposed the use of generative large language models (LLMs)\nto label data for research and applied settings. This literature emphasizes the\nimproved performance of these models relative to other natural language models,\nnoting that generative LLMs typically outperform other models and even humans\nacross several metrics. Previous literature has examined bias across many\napplications and contexts, but less work has focused specifically on bias in\ngenerative LLMs' responses to subjective annotation tasks. This bias could\nresult in labels applied by LLMs that disproportionately align with majority\ngroups over a more diverse set of viewpoints. In this paper, we evaluate how\nLLMs represent diverse viewpoints on these contentious tasks. Across four\nannotation tasks on four datasets, we show that LLMs do not show systematic\nsubstantial disagreement with annotators on the basis of demographics. Rather,\nwe find that multiple LLMs tend to be biased in the same directions on the same\ndemographic categories within the same datasets. Moreover, the disagreement\nbetween human annotators on the labeling task -- a measure of item difficulty\n-- is far more predictive of LLM agreement with human annotators. We conclude\nwith a discussion of the implications for researchers and practitioners using\nLLMs for automated data annotation tasks. Specifically, we emphasize that\nfairness evaluations must be contextual, model choice alone will not solve\npotential issues of bias, and item difficulty must be integrated into bias\nassessments.\n","authors":["Megan A. Brown","Shubham Atreja","Libby Hemphill","Patrick Y. Wu"],"pdf_url":"https://arxiv.org/pdf/2503.23243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13013v1","updated":"2025-06-16T00:48:09Z","published":"2025-06-16T00:48:09Z","title":"Missing the human touch? A computational stylometry analysis of GPT-4\n  translations of online Chinese literature","summary":"  Existing research indicates that machine translations (MTs) of literary texts\nare often unsatisfactory. MTs are typically evaluated using automated metrics\nand subjective human ratings, with limited focus on stylistic features.\nEvidence is also limited on whether state-of-the-art large language models\n(LLMs) will reshape literary translation. This study examines the stylistic\nfeatures of LLM translations, comparing GPT-4's performance to human\ntranslations in a Chinese online literature task. Computational stylometry\nanalysis shows that GPT-4 translations closely align with human translations in\nlexical, syntactic, and content features, suggesting that LLMs might replicate\nthe 'human touch' in literary translation style. These findings offer insights\ninto AI's impact on literary translation from a posthuman perspective, where\ndistinctions between machine and human translations become increasingly blurry.\n","authors":["Xiaofang Yao","Yong-Bin Kang","Anthony McCosker"],"pdf_url":"https://arxiv.org/pdf/2506.13013v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.14133v2","updated":"2025-06-16T00:24:41Z","published":"2025-02-19T22:27:59Z","title":"Self-Regularization with Sparse Autoencoders for Controllable LLM-based\n  Classification","summary":"  Modern text classification methods heavily rely on contextual embeddings from\nlarge language models (LLMs). Compared to human-engineered features, these\nembeddings provide automatic and effective representations for classification\nmodel training. However, they also introduce a challenge: we lose the ability\nto manually remove unintended features, such as sensitive or task-irrelevant\nfeatures, to guarantee regulatory compliance or improve the generalizability of\nclassification models. This limitation arises because LLM embeddings are opaque\nand difficult to interpret. In this paper, we propose a novel framework to\nidentify and regularize unintended features in the LLM latent space.\nSpecifically, we first pre-train a sparse autoencoder (SAE) to extract\ninterpretable features from LLM latent spaces. To ensure the SAE can capture\ntask-specific features, we further fine-tune it on task-specific datasets. In\ntraining the classification model, we propose a simple and effective\nregularizer, by minimizing the similarity between the classifier weights and\nthe identified unintended feature, to remove the impact of these unintended\nfeatures on classification. We evaluate the proposed framework on three\nreal-world tasks, including toxic chat detection, reward modeling, and disease\ndiagnosis. Results show that the proposed self-regularization framework can\nimprove the classifier's generalizability by regularizing those features that\nare not semantically correlated to the task. This work pioneers controllable\ntext classification on LLM latent spaces by leveraging interpreted features to\naddress generalizability, fairness, and privacy challenges. The code and data\nare publicly available at\nhttps://github.com/JacksonWuxs/Controllable_LLM_Classifier.\n","authors":["Xuansheng Wu","Wenhao Yu","Xiaoming Zhai","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2502.14133v2.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2502.04322v2","updated":"2025-06-16T00:13:14Z","published":"2025-02-06T18:59:02Z","title":"Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple\n  Interactions","summary":"  Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.\n","authors":["Yik Siu Chan","Narutatsu Ri","Yuxin Xiao","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2502.04322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14064v1","updated":"2025-06-16T23:48:04Z","published":"2025-06-16T23:48:04Z","title":"Automatic Extraction of Clausal Embedding Based on Large-Scale English\n  Text Data","summary":"  For linguists, embedded clauses have been of special interest because of\ntheir intricate distribution of syntactic and semantic features. Yet, current\nresearch relies on schematically created language examples to investigate these\nconstructions, missing out on statistical information and naturally-occurring\nexamples that can be gained from large language corpora. Thus, we present a\nmethodological approach for detecting and annotating naturally-occurring\nexamples of English embedded clauses in large-scale text data using\nconstituency parsing and a set of parsing heuristics. Our tool has been\nevaluated on our dataset Golden Embedded Clause Set (GECS), which includes\nhand-annotated examples of naturally-occurring English embedded clause\nsentences. Finally, we present a large-scale dataset of naturally-occurring\nEnglish embedded clauses which we have extracted from the open-source corpus\nDolma using our extraction tool.\n","authors":["Iona Carslaw","Sivan Milton","Nicolas Navarre","Ciyang Qing","Wataru Uegaki"],"pdf_url":"https://arxiv.org/pdf/2506.14064v1.pdf","comment":"Accepted in the Society for Computation in Linguistics"},{"id":"http://arxiv.org/abs/2506.14046v1","updated":"2025-06-16T22:40:16Z","published":"2025-06-16T22:40:16Z","title":"Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic\n  Difficulty of Conversational Texts for LLM Applications","summary":"  There is an unmet need to evaluate the language difficulty of short,\nconversational passages of text, particularly for training and filtering Large\nLanguage Models (LLMs). We introduce Ace-CEFR, a dataset of English\nconversational text passages expert-annotated with their corresponding level of\ntext difficulty. We experiment with several models on Ace-CEFR, including\nTransformer-based models and LLMs. We show that models trained on Ace-CEFR can\nmeasure text difficulty more accurately than human experts and have latency\nappropriate to production environments. Finally, we release the Ace-CEFR\ndataset to the public for research and development.\n","authors":["David Kogan","Max Schumacher","Sam Nguyen","Masanori Suzuki","Melissa Smith","Chloe Sophia Bellows","Jared Bernstein"],"pdf_url":"https://arxiv.org/pdf/2506.14046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14040v1","updated":"2025-06-16T22:26:17Z","published":"2025-06-16T22:26:17Z","title":"An Interdisciplinary Review of Commonsense Reasoning and Intent\n  Detection","summary":"  This review explores recent advances in commonsense reasoning and intent\ndetection, two key challenges in natural language understanding. We analyze 28\npapers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and\napplication. Commonsense reasoning is reviewed across zero-shot learning,\ncultural adaptation, structured evaluation, and interactive contexts. Intent\ndetection is examined through open-set models, generative formulations,\nclustering, and human-centered systems. By bridging insights from NLP and HCI,\nwe highlight emerging trends toward more adaptive, multilingual, and\ncontext-aware models, and identify key gaps in grounding, generalization, and\nbenchmark design.\n","authors":["Md Nazmus Sakib"],"pdf_url":"https://arxiv.org/pdf/2506.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16464v3","updated":"2025-06-16T22:23:37Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by Browsing Agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-Based Agents outperform web Browsing Agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n24.0% absolute improvement over web browsing alone, achieving a success rate of\n38.9%, the SOTA performance among task-agnostic agents. These results strongly\nsuggest that when APIs are available, they present an attractive alternative to\nrelying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v3.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.14028v1","updated":"2025-06-16T22:01:49Z","published":"2025-06-16T22:01:49Z","title":"MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation","summary":"  Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.\n","authors":["Xueqing Peng","Lingfei Qian","Yan Wang","Ruoyu Xiang","Yueru He","Yang Ren","Mingyang Jiang","Jeff Zhao","Huan He","Yi Han","Yun Feng","Yuechen Jiang","Yupeng Cao","Haohang Li","Yangyang Yu","Xiaoyu Wang","Penglei Gao","Shengyuan Lin","Keyi Wang","Shanshan Yang","Yilun Zhao","Zhiwei Liu","Peng Lu","Jerry Huang","Suyuchen Wang","Triantafillos Papadopoulos","Polydoros Giannouris","Efstathia Soufleri","Nuo Chen","Guojun Xiong","Zhiyang Deng","Yijia Zhao","Mingquan Lin","Meikang Qiu","Kaleb E Smith","Arman Cohan","Xiao-Yang Liu","Jimin Huang","Alejandro Lopez-Lira","Xi Chen","Junichi Tsujii","Jian-Yun Nie","Sophia Ananiadou","Qianqian Xie"],"pdf_url":"https://arxiv.org/pdf/2506.14028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14012v1","updated":"2025-06-16T21:19:27Z","published":"2025-06-16T21:19:27Z","title":"Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text","summary":"  Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English text$\\unicode{x2013}$even under linguistic\nconstraints$\\unicode{x2013}$embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.\n","authors":["Amr Mohamed","Yang Zhang","Michalis Vazirgiannis","Guokan Shang"],"pdf_url":"https://arxiv.org/pdf/2506.14012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13497v3","updated":"2025-06-16T20:58:21Z","published":"2025-02-19T07:29:58Z","title":"Towards Geo-Culturally Grounded LLM Generations","summary":"  Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness.\n","authors":["Piyawat Lertvittayakumjorn","David Kinney","Vinodkumar Prabhakaran","Donald Martin Jr.","Sunipa Dev"],"pdf_url":"https://arxiv.org/pdf/2502.13497v3.pdf","comment":"ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2506.07801v2","updated":"2025-06-16T20:28:26Z","published":"2025-06-09T14:27:47Z","title":"MultiMatch: Multihead Consistency Regularization Matching for\n  Semi-Supervised Text Classification","summary":"  We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.\n","authors":["Iustin Sirbu","Robert-Adrian Popovici","Cornelia Caragea","Stefan Trausan-Matu","Traian Rebedea"],"pdf_url":"https://arxiv.org/pdf/2506.07801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07313v4","updated":"2025-06-16T20:19:19Z","published":"2024-07-10T02:20:19Z","title":"ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models","summary":"  The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics - Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) - suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.\n","authors":["Benjamin G. Ascoli","Yasoda Sai Ram Kandikonda","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2407.07313v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13965v1","updated":"2025-06-16T20:15:57Z","published":"2025-06-16T20:15:57Z","title":"Are manual annotations necessary for statutory interpretations\n  retrieval?","summary":"  One of the elements of legal research is looking for cases where judges have\nextended the meaning of a legal concept by providing interpretations of what a\nconcept means or does not mean. This allow legal professionals to use such\ninterpretations as precedents as well as laymen to better understand the legal\nconcept. The state-of-the-art approach for retrieving the most relevant\ninterpretations for these concepts currently depends on the ranking of\nsentences and the training of language models over annotated examples. That\nmanual annotation process can be quite expensive and need to be repeated for\neach such concept, which prompted recent research in trying to automate this\nprocess. In this paper, we highlight the results of various experiments\nconducted to determine the volume, scope and even the need for manual\nannotation. First of all, we check what is the optimal number of annotations\nper a legal concept. Second, we check if we can draw the sentences for\nannotation randomly or there is a gain in the performance of the model, when\nonly the best candidates are annotated. As the last question we check what is\nthe outcome of automating the annotation process with the help of an LLM.\n","authors":["Aleksander Smywiński-Pohl","Tomer Libal","Adam Kaczmarczyk","Magdalena Król"],"pdf_url":"https://arxiv.org/pdf/2506.13965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13956v1","updated":"2025-06-16T19:58:54Z","published":"2025-06-16T19:58:54Z","title":"ASMR: Augmenting Life Scenario using Large Generative Models for Robotic\n  Action Reflection","summary":"  When designing robots to assist in everyday human activities, it is crucial\nto enhance user requests with visual cues from their surroundings for improved\nintent understanding. This process is defined as a multimodal classification\ntask. However, gathering a large-scale dataset encompassing both visual and\nlinguistic elements for model training is challenging and time-consuming. To\naddress this issue, our paper introduces a novel framework focusing on data\naugmentation in robotic assistance scenarios, encompassing both dialogues and\nrelated environmental imagery. This approach involves leveraging a\nsophisticated large language model to simulate potential conversations and\nenvironmental contexts, followed by the use of a stable diffusion model to\ncreate images depicting these environments. The additionally generated data\nserves to refine the latest multimodal models, enabling them to more accurately\ndetermine appropriate actions in response to user interactions with the limited\ntarget data. Our experimental results, based on a dataset collected from\nreal-world scenarios, demonstrate that our methodology significantly enhances\nthe robot's action selection capabilities, achieving the state-of-the-art\nperformance.\n","authors":["Shang-Chi Tsai","Seiya Kawano","Angel Garcia Contreras","Koichiro Yoshino","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13956v1.pdf","comment":"IWSDS 2024 Best Paper Award"},{"id":"http://arxiv.org/abs/2505.07897v2","updated":"2025-06-16T19:55:19Z","published":"2025-05-12T05:38:03Z","title":"LongCodeBench: Evaluating Coding LLMs at 1M Context Windows","summary":"  Context lengths for models have grown rapidly, from thousands to millions of\ntokens in just a few years. The extreme context sizes of modern long-context\nmodels have made it difficult to construct realistic long-context benchmarks --\nnot only due to the cost of collecting million-context tasks but also in\nidentifying realistic scenarios that require significant contexts. We identify\ncode comprehension and repair as a natural testbed and challenge task for\nlong-context models and introduce LongCodeBench (LCB), a benchmark to test LLM\ncoding abilities in long-context scenarios. Our benchmark tests both the\ncomprehension and repair capabilities of LCLMs in realistic and important\nsettings by drawing from real-world GitHub issues and constructing QA\n(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the\ncomplexity of our benchmark, enabling us to evaluate models across different\nscales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.\nWe find that long-context remains a weakness for all models, with performance\ndrops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for\nQwen2.5.\n","authors":["Stefano Rando","Luca Romani","Alessio Sampieri","Luca Franco","John Yang","Yuta Kyuragi","Fabio Galasso","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2505.07897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20612v2","updated":"2025-06-16T19:08:27Z","published":"2025-05-27T01:24:29Z","title":"Roboflow100-VL: A Multi-Domain Object Detection Benchmark for\n  Vision-Language Models","summary":"  Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025\nFoundational FSOD competition and share insights from the community. Notably,\nthe winning team significantly outperforms our baseline by 16.8 mAP! Our code\nand dataset are available at https://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/\n","authors":["Peter Robicheaux","Matvei Popov","Anish Madan","Isaac Robinson","Joseph Nelson","Deva Ramanan","Neehar Peri"],"pdf_url":"https://arxiv.org/pdf/2505.20612v2.pdf","comment":"The first two authors contributed equally. Project Page:\n  https://rf100-vl.org/"},{"id":"http://arxiv.org/abs/2506.13923v1","updated":"2025-06-16T19:03:06Z","published":"2025-06-16T19:03:06Z","title":"Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models","summary":"  We study the process through which reasoning models trained with\nreinforcement learning on verifiable rewards (RLVR) can learn to solve new\nproblems. We find that RLVR drives performance through two main means: (1) by\ncompressing pass@$k$ into pass@1 and (2) via \"capability gain\" in which models\nlearn to solve new problems that they previously could not solve even at high\n$k$. We find that while capability gain exists across model scales, learning to\nsolve new problems is primarily driven through self-distillation. We\ndemonstrate these findings across model scales ranging from 0.5B to 72B on\n>500,000 reasoning problems with prompts and verifiable final answers across\nmath, science, and code domains. We further show that we can significantly\nimprove pass@$k$ rates by leveraging natural language guidance for the model to\nconsider within context while still requiring the model to derive a solution\nchain from scratch. Based of these insights, we derive $\\text{Guide}$ - a new\nclass of online training algorithms. $\\text{Guide}$ adaptively incorporates\nhints into the model's context on problems for which all rollouts were\ninitially incorrect and adjusts the importance sampling ratio for the\n\"off-policy\" trajectories in order to optimize the policy for contexts in which\nthe hints are no longer present. We describe variants of $\\text{Guide}$ for\nGRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter\nmodels improves generalization over its vanilla counterpart with up to 4$\\%$\nmacro-average improvement across math benchmarks. We include careful ablations\nto analyze $\\text{Guide}$'s components and theoretically analyze Guide's\nlearning efficiency.\n","authors":["Vaskar Nath","Elaine Lau","Anisha Gunjal","Manasi Sharma","Nikhil Baharte","Sean Hendryx"],"pdf_url":"https://arxiv.org/pdf/2506.13923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10274v2","updated":"2025-06-16T18:38:29Z","published":"2025-06-12T01:35:43Z","title":"Discrete Audio Tokens: More Than a Survey!","summary":"  Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks. They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.\n","authors":["Pooneh Mousavi","Gallil Maimon","Adel Moumen","Darius Petermann","Jiatong Shi","Haibin Wu","Haici Yang","Anastasia Kuznetsova","Artem Ploujnikov","Ricard Marxer","Bhuvana Ramabhadran","Benjamin Elizalde","Loren Lugosch","Jinyu Li","Cem Subakan","Phil Woodland","Minje Kim","Hung-yi Lee","Shinji Watanabe","Yossi Adi","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2506.10274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04079v2","updated":"2025-06-16T18:23:31Z","published":"2025-06-04T15:43:31Z","title":"EuroLLM-9B: Technical Report","summary":"  This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.\n","authors":["Pedro Henrique Martins","João Alves","Patrick Fernandes","Nuno M. Guerreiro","Ricardo Rei","Amin Farajian","Mateusz Klimaszewski","Duarte M. Alves","José Pombal","Nicolas Boizard","Manuel Faysse","Pierre Colombo","François Yvon","Barry Haddow","José G. C. de Souza","Alexandra Birch","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2506.04079v2.pdf","comment":"56 pages"},{"id":"http://arxiv.org/abs/2506.13901v1","updated":"2025-06-16T18:22:28Z","published":"2025-06-16T18:22:28Z","title":"Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations","summary":"  Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.\n","authors":["Abhilekh Borah","Chhavi Sharma","Danush Khanna","Utkarsh Bhatt","Gurpreet Singh","Hasnat Md Abdullah","Raghav Kaushik Ravi","Vinija Jain","Jyoti Patel","Shubham Singh","Vasu Sharma","Arpita Vats","Rahul Raja","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2506.13901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13894v1","updated":"2025-06-16T18:16:04Z","published":"2025-06-16T18:16:04Z","title":"EmoNews: A Spoken Dialogue System for Expressive News Conversations","summary":"  We develop a task-oriented spoken dialogue system (SDS) that regulates\nemotional speech based on contextual cues to enable more empathetic news\nconversations. Despite advancements in emotional text-to-speech (TTS)\ntechniques, task-oriented emotional SDSs remain underexplored due to the\ncompartmentalized nature of SDS and emotional TTS research, as well as the lack\nof standardized evaluation metrics for social goals. We address these\nchallenges by developing an emotional SDS for news conversations that utilizes\na large language model (LLM)-based sentiment analyzer to identify appropriate\nemotions and PromptTTS to synthesize context-appropriate emotional speech. We\nalso propose subjective evaluation scale for emotional SDSs and judge the\nemotion regulation performance of the proposed and baseline systems.\nExperiments showed that our emotional SDS outperformed a baseline system in\nterms of the emotion regulation and engagement. These results suggest the\ncritical role of speech emotion for more engaging conversations. All our source\ncode is open-sourced at\nhttps://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1\n","authors":["Ryuki Matsuura","Shikhar Bharadwaj","Jiarui Liu","Dhatchi Kunde Govindarajan"],"pdf_url":"https://arxiv.org/pdf/2506.13894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19110v3","updated":"2025-06-16T18:13:10Z","published":"2025-02-26T13:01:49Z","title":"Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity","summary":"  Language model outputs are not always reliable, thus prompting research into\nhow to adapt model responses based on uncertainty. Common approaches include:\n\\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unified view,\nConformal Linguistic Calibration (CLC), which reinterprets linguistic\ncalibration as \\emph{answer set prediction}. First we present a framework\nconnecting abstention and linguistic calibration through the lens of linguistic\npragmatics. We then describe an implementation of CLC that allows for\ncontrolling the level of imprecision in model responses. Results demonstrate\nour method produces calibrated outputs with conformal guarantees on factual\naccuracy. Further, our approach enables fine-tuning models to perform\nuncertainty-aware adaptive claim rewriting, offering a controllable balance\nbetween factuality and specificity.\n","authors":["Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.19110v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13888v1","updated":"2025-06-16T18:10:51Z","published":"2025-06-16T18:10:51Z","title":"VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and\n  Iterative Training","summary":"  Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large\nlanguage models but remains underexplored for Vision-Language (VL) models. The\nVision-Language Reward Model (VL-RM) is key to aligning VL models by providing\nstructured feedback, yet training effective VL-RMs faces two major challenges.\nFirst, the bootstrapping dilemma arises as high-quality training data depends\non already strong VL models, creating a cycle where self-generated supervision\nreinforces existing biases. Second, modality bias and negative example\namplification occur when VL models hallucinate incorrect visual attributes,\nleading to flawed preference data that further misguides training. To address\nthese issues, we propose an iterative training framework leveraging vision\nexperts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection\nSampling. Our approach refines preference datasets, enhances structured\ncritiques, and iteratively improves reasoning. Experiments across VL-RM\nbenchmarks demonstrate superior performance in hallucination detection and\nmultimodal reasoning, advancing VL model alignment with reinforcement learning.\n","authors":["Jipeng Zhang","Kehao Miao","Renjie Pi","Zhaowei Wang","Runtao Liu","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13886v1","updated":"2025-06-16T18:09:38Z","published":"2025-06-16T18:09:38Z","title":"Investigating the interaction of linguistic and mathematical reasoning\n  in language models using multilingual number puzzles","summary":"  Across languages, numeral systems vary widely in how they construct and\ncombine numbers. While humans consistently learn to navigate this diversity,\nlarge language models (LLMs) struggle with linguistic-mathematical puzzles\ninvolving cross-linguistic numeral systems, which humans can learn to solve\nsuccessfully. We investigate why this task is difficult for LLMs through a\nseries of experiments that untangle the linguistic and mathematical aspects of\nnumbers in language. Our experiments establish that models cannot consistently\nsolve such problems unless the mathematical operations in the problems are\nexplicitly marked using known symbols ($+$, $\\times$, etc, as in \"twenty +\nthree\"). In further ablation studies, we probe how individual parameters of\nnumeral construction and combination affect performance. While humans use their\nlinguistic understanding of numbers to make inferences about the implicit\ncompositional structure of numerals, LLMs seem to lack this notion of implicit\nnumeral structure. We conclude that the ability to flexibly infer compositional\nrules from implicit patterns in human-scale data remains an open challenge for\ncurrent reasoning models.\n","authors":["Antara Raaghavi Bhattacharya","Isabel Papadimitriou","Kathryn Davidson","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2506.13886v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2506.13766v1","updated":"2025-06-16T17:59:56Z","published":"2025-06-16T17:59:56Z","title":"PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated\n  Human Images","summary":"  Reconstructing an animatable 3D human from casually captured images of an\narticulated subject without camera or human pose information is a practical yet\nchallenging task due to view misalignment, occlusions, and the absence of\nstructural priors. While optimization-based methods can produce high-fidelity\nresults from monocular or multi-view videos, they require accurate pose\nestimation and slow iterative optimization, limiting scalability in\nunconstrained scenarios. Recent feed-forward approaches enable efficient\nsingle-image reconstruction but struggle to effectively leverage multiple input\nimages to reduce ambiguity and improve reconstruction accuracy. To address\nthese challenges, we propose PF-LHM, a large human reconstruction model that\ngenerates high-quality 3D avatars in seconds from one or multiple casually\ncaptured pose-free images. Our approach introduces an efficient Encoder-Decoder\nPoint-Image Transformer architecture, which fuses hierarchical geometric point\nfeatures and multi-view image features through multimodal attention. The fused\nfeatures are decoded to recover detailed geometry and appearance, represented\nusing 3D Gaussian splats. Extensive experiments on both real and synthetic\ndatasets demonstrate that our method unifies single- and multi-image 3D human\nreconstruction, achieving high-fidelity and animatable 3D human avatars without\nrequiring camera and human pose annotations. Code and models will be released\nto the public.\n","authors":["Lingteng Qiu","Peihao Li","Qi Zuo","Xiaodong Gu","Yuan Dong","Weihao Yuan","Siyu Zhu","Xiaoguang Han","Guanying Chen","Zilong Dong"],"pdf_url":"https://arxiv.org/pdf/2506.13766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13763v1","updated":"2025-06-16T17:59:54Z","published":"2025-06-16T17:59:54Z","title":"Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss\n  Value","summary":"  Diffusion models have achieved remarkable success in generative modeling.\nDespite more stable training, the loss of diffusion models is not indicative of\nabsolute data-fitting quality, since its optimal value is typically not zero\nbut unknown, leading to confusion between large optimal loss and insufficient\nmodel capacity. In this work, we advocate the need to estimate the optimal loss\nvalue for diagnosing and improving diffusion models. We first derive the\noptimal loss in closed form under a unified formulation of diffusion models,\nand develop effective estimators for it, including a stochastic variant\nscalable to large datasets with proper control of variance and bias. With this\ntool, we unlock the inherent metric for diagnosing the training quality of\nmainstream diffusion model variants, and develop a more performant training\nschedule based on the optimal loss. Moreover, using models with 120M to 1.5B\nparameters, we find that the power law is better demonstrated after subtracting\nthe optimal loss from the actual training loss, suggesting a more principled\nsetting for investigating the scaling law for diffusion models.\n","authors":["Yixian Xu","Shengjie Luo","Liwei Wang","Di He","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13763v1.pdf","comment":"29 pages, 8 figures, 3 tables. Preprint. Work in Progress"},{"id":"http://arxiv.org/abs/2506.13762v1","updated":"2025-06-16T17:59:48Z","published":"2025-06-16T17:59:48Z","title":"Touch begins where vision ends: Generalizable policies for contact-rich\n  manipulation","summary":"  Data-driven approaches struggle with precise manipulation; imitation learning\nrequires many hard-to-obtain demonstrations, while reinforcement learning\nyields brittle, non-generalizable policies. We introduce VisuoTactile Local\n(ViTaL) policy learning, a framework that solves fine-grained manipulation\ntasks by decomposing them into two phases: a reaching phase, where a\nvision-language model (VLM) enables scene-level reasoning to localize the\nobject of interest, and a local interaction phase, where a reusable,\nscene-agnostic ViTaL policy performs contact-rich manipulation using egocentric\nvision and tactile sensing. This approach is motivated by the observation that\nwhile scene context varies, the low-level interaction remains consistent across\ntask instances. By training local policies once in a canonical setting, they\ncan generalize via a localize-then-execute strategy. ViTaL achieves around 90%\nsuccess on contact-rich tasks in unseen environments and is robust to\ndistractors. ViTaL's effectiveness stems from three key insights: (1)\nfoundation models for segmentation enable training robust visual encoders via\nbehavior cloning; (2) these encoders improve the generalizability of policies\nlearned using residual RL; and (3) tactile sensing significantly boosts\nperformance in contact-rich tasks. Ablation studies validate each of these\ninsights, and we demonstrate that ViTaL integrates well with high-level VLMs,\nenabling robust, reusable low-level skills. Results and videos are available at\nhttps://vitalprecise.github.io.\n","authors":["Zifan Zhao","Siddhant Haldar","Jinda Cui","Lerrel Pinto","Raunaq Bhirangi"],"pdf_url":"https://arxiv.org/pdf/2506.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13757v1","updated":"2025-06-16T17:58:50Z","published":"2025-06-16T17:58:50Z","title":"AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous\n  Driving with Adaptive Reasoning and Reinforcement Fine-Tuning","summary":"  Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios.\n","authors":["Zewei Zhou","Tianhui Cai","Seth Z. Zhao","Yun Zhang","Zhiyu Huang","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2506.13757v1.pdf","comment":"Website link:https://autovla.github.io/"},{"id":"http://arxiv.org/abs/2506.13756v1","updated":"2025-06-16T17:58:29Z","published":"2025-06-16T17:58:29Z","title":"UltraZoom: Generating Gigapixel Images from Regular Photos","summary":"  We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input.\n","authors":["Jingwei Ma","Vivek Jayaram","Brian Curless","Ira Kemelmacher-Shlizerman","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2506.13756v1.pdf","comment":"Project page: https://ultra-zoom.github.io/"},{"id":"http://arxiv.org/abs/2506.13754v1","updated":"2025-06-16T17:58:00Z","published":"2025-06-16T17:58:00Z","title":"VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models","summary":"  We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.\n","authors":["Edward Li","Zichen Wang","Jiahe Huang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2506.13754v1.pdf","comment":"Submitted to NeurIPS 2025. Project page: https://videopde.github.io/"},{"id":"http://arxiv.org/abs/2506.13750v1","updated":"2025-06-16T17:56:22Z","published":"2025-06-16T17:56:22Z","title":"Test3R: Learning to Reconstruct 3D at Test Time","summary":"  Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n($I_1,I_2,I_3$), Test3R generates reconstructions from pairs ($I_1,I_2$) and\n($I_1,I_3$). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image $I_1$. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R.\n","authors":["Yuheng Yuan","Qiuhong Shen","Shizun Wang","Xingyi Yang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13723v1","updated":"2025-06-16T17:27:47Z","published":"2025-06-16T17:27:47Z","title":"OTFusion: Bridging Vision-only and Vision-Language Models via Optimal\n  Transport for Transductive Zero-Shot Learning","summary":"  Transductive zero-shot learning (ZSL) aims to classify unseen categories by\nleveraging both semantic class descriptions and the distribution of unlabeled\ntest data. While Vision-Language Models (VLMs) such as CLIP excel at aligning\nvisual inputs with textual semantics, they often rely too heavily on\nclass-level priors and fail to capture fine-grained visual cues. In contrast,\nVision-only Foundation Models (VFMs) like DINOv2 provide rich perceptual\nfeatures but lack semantic alignment. To exploit the complementary strengths of\nthese models, we propose OTFusion, a simple yet effective training-free\nframework that bridges VLMs and VFMs via Optimal Transport. Specifically,\nOTFusion aims to learn a shared probabilistic representation that aligns visual\nand semantic information by minimizing the transport cost between their\nrespective distributions. This unified distribution enables coherent class\npredictions that are both semantically meaningful and visually grounded.\nExtensive experiments on 11 benchmark datasets demonstrate that OTFusion\nconsistently outperforms the original CLIP model, achieving an average accuracy\nimprovement of nearly $10\\%$, all without any fine-tuning or additional\nannotations. The code will be publicly released after the paper is accepted.\n","authors":["Qiyu Xu","Wenyang Chen","Zhanxuan Hu","Huafeng Li","Yonghang Tai"],"pdf_url":"https://arxiv.org/pdf/2506.13723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13722v1","updated":"2025-06-16T17:27:43Z","published":"2025-06-16T17:27:43Z","title":"How Real is CARLAs Dynamic Vision Sensor? A Study on the Sim-to-Real Gap\n  in Traffic Object Detection","summary":"  Event cameras are gaining traction in traffic monitoring applications due to\ntheir low latency, high temporal resolution, and energy efficiency, which makes\nthem well-suited for real-time object detection at traffic intersections.\nHowever, the development of robust event-based detection models is hindered by\nthe limited availability of annotated real-world datasets. To address this,\nseveral simulation tools have been developed to generate synthetic event data.\nAmong these, the CARLA driving simulator includes a built-in dynamic vision\nsensor (DVS) module that emulates event camera output. Despite its potential,\nthe sim-to-real gap for event-based object detection remains insufficiently\nstudied. In this work, we present a systematic evaluation of this gap by\ntraining a recurrent vision transformer model exclusively on synthetic data\ngenerated using CARLAs DVS and testing it on varying combinations of synthetic\nand real-world event streams. Our experiments show that models trained solely\non synthetic data perform well on synthetic-heavy test sets but suffer\nsignificant performance degradation as the proportion of real-world data\nincreases. In contrast, models trained on real-world data demonstrate stronger\ngeneralization across domains. This study offers the first quantifiable\nanalysis of the sim-to-real gap in event-based object detection using CARLAs\nDVS. Our findings highlight limitations in current DVS simulation fidelity and\nunderscore the need for improved domain adaptation techniques in neuromorphic\nvision for traffic monitoring.\n","authors":["Kaiyuan Tan","Pavan Kumar B N","Bharatesh Chakravarthi"],"pdf_url":"https://arxiv.org/pdf/2506.13722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01607v5","updated":"2025-06-16T17:07:17Z","published":"2024-05-02T04:53:42Z","title":"Deep Learning for Wildfire Risk Prediction: Integrating Remote Sensing\n  and Environmental Data","summary":"  Wildfires pose a significant threat to ecosystems, wildlife, and human\ncommunities, leading to habitat destruction, pollutant emissions, and\nbiodiversity loss. Accurate wildfire risk prediction is crucial for mitigating\nthese impacts and safeguarding both environmental and human health. This paper\nprovides a comprehensive review of wildfire risk prediction methodologies, with\na particular focus on deep learning approaches combined with remote sensing. We\nbegin by defining wildfire risk and summarizing the geographical distribution\nof related studies. In terms of data, we analyze key predictive features,\nincluding fuel characteristics, meteorological and climatic conditions,\nsocioeconomic factors, topography, and hydrology, while also reviewing publicly\navailable wildfire prediction datasets derived from remote sensing.\nAdditionally, we emphasize the importance of feature collinearity assessment\nand model interpretability to improve the understanding of prediction outcomes.\nRegarding methodology, we classify deep learning models into three primary\ncategories: time-series forecasting, image segmentation, and spatiotemporal\nprediction, and further discuss methods for converting model outputs into risk\nclassifications or probability-adjusted predictions. Finally, we identify the\nkey challenges and limitations of current wildfire-risk prediction models and\noutline several research opportunities. These include integrating diverse\nremote sensing data, developing multimodal models, designing more\ncomputationally efficient architectures, and incorporating cross-disciplinary\nmethods--such as coupling with numerical weather-prediction models--to enhance\nthe accuracy and robustness of wildfire-risk assessments.\n","authors":["Zhengsen Xu","Jonathan Li","Sibo Cheng","Xue Rui","Yu Zhao","Hongjie He","Haiyan Guan","Aryan Sharma","Matthew Erxleben","Ryan Chang","Linlin Xu"],"pdf_url":"https://arxiv.org/pdf/2405.01607v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13697v1","updated":"2025-06-16T17:02:47Z","published":"2025-06-16T17:02:47Z","title":"Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering\n  from Estimated Geometry","summary":"  We introduce Vid-CamEdit, a novel framework for video camera trajectory\nediting, enabling the re-synthesis of monocular videos along user-defined\ncamera paths. This task is challenging due to its ill-posed nature and the\nlimited multi-view video data for training. Traditional reconstruction methods\nstruggle with extreme trajectory changes, and existing generative models for\ndynamic novel view synthesis cannot handle in-the-wild videos. Our approach\nconsists of two steps: estimating temporally consistent geometry, and\ngenerative rendering guided by this geometry. By integrating geometric priors,\nthe generative model focuses on synthesizing realistic details where the\nestimated geometry is uncertain. We eliminate the need for extensive 4D\ntraining data through a factorized fine-tuning framework that separately trains\nspatial and temporal components using multi-view image and video data. Our\nmethod outperforms baselines in producing plausible videos from novel camera\ntrajectories, especially in extreme extrapolation scenarios on real-world\nfootage.\n","authors":["Junyoung Seo","Jisang Han","Jaewoo Jung","Siyoon Jin","Joungbin Lee","Takuya Narihira","Kazumi Fukuda","Takashi Shibuya","Donghoon Ahn","Shoukang Hu","Seungryong Kim","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2506.13697v1.pdf","comment":"Our project page can be found at\n  https://cvlab-kaist.github.io/Vid-CamEdit/"},{"id":"http://arxiv.org/abs/2506.13691v1","updated":"2025-06-16T16:52:52Z","published":"2025-06-16T16:52:52Z","title":"UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions","summary":"  The quality of the video dataset (image quality, resolution, and fine-grained\ncaption) greatly influences the performance of the video generation model. The\ngrowing demand for video applications sets higher requirements for high-quality\nvideo generation models. For example, the generation of movie-level Ultra-High\nDefinition (UHD) videos and the creation of 4K short video content. However,\nthe existing public datasets cannot support related research and applications.\nIn this paper, we first propose a high-quality open-sourced UHD-4K (22.4\\% of\nwhich are 8K) text-to-video dataset named UltraVideo, which contains a wide\nrange of topics (more than 100 kinds), and each video has 9 structured captions\nwith one summarized caption (average of 824 words). Specifically, we carefully\ndesign a highly automated curation process with four stages to obtain the final\nhigh-quality dataset: \\textit{i)} collection of diverse and high-quality video\nclips. \\textit{ii)} statistical data filtering. \\textit{iii)} model-based data\npurification. \\textit{iv)} generation of comprehensive, structured captions. In\naddition, we expand Wan to UltraWan-1K/-4K, which can natively generate\nhigh-quality 1K/4K videos with more consistent text controllability,\ndemonstrating the effectiveness of our data curation.We believe that this work\ncan make a significant contribution to future research on UHD video generation.\nUltraVideo dataset and UltraWan models are available at\nhttps://xzc-zju.github.io/projects/UltraVideo.\n","authors":["Zhucun Xue","Jiangning Zhang","Teng Hu","Haoyang He","Yinan Chen","Yuxuan Cai","Yabiao Wang","Chengjie Wang","Yong Liu","Xiangtai Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.13691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06349v2","updated":"2025-06-16T16:40:48Z","published":"2025-06-02T05:16:16Z","title":"Heart Rate Classification in ECG Signals Using Machine Learning and Deep\n  Learning","summary":"  This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.\n","authors":["Thien Nhan Vo"],"pdf_url":"https://arxiv.org/pdf/2506.06349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13679v1","updated":"2025-06-16T16:34:20Z","published":"2025-06-16T16:34:20Z","title":"ROSA: Harnessing Robot States for Vision-Language and Action Alignment","summary":"  Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes.\n","authors":["Yuqing Wen","Kefan Gu","Haoxuan Liu","Yucheng Zhao","Tiancai Wang","Haoqiang Fan","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13667v1","updated":"2025-06-16T16:25:13Z","published":"2025-06-16T16:25:13Z","title":"MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework\n  via Latent Diffusion Model","summary":"  Multimodal medical imaging integrates diverse data types, such as structural\nand functional neuroimaging, to provide complementary insights that enhance\ndeep learning predictions and improve outcomes. This study focuses on a\nneuroimaging prediction framework based on both structural and functional\nneuroimaging data. We propose a next-generation prediction model,\n\\textbf{MultiViT2}, which combines a pretrained representative learning base\nmodel with a vision transformer backbone for prediction output. Additionally,\nwe developed a data augmentation module based on the latent diffusion model\nthat enriches input data by generating augmented neuroimaging samples, thereby\nenhancing predictive performance through reduced overfitting and improved\ngeneralizability. We show that MultiViT2 significantly outperforms the\nfirst-generation model in schizophrenia classification accuracy and\ndemonstrates strong scalability and portability.\n","authors":["Bi Yuda","Jia Sihan","Gao Yutong","Abrol Anees","Fu Zening","Calhoun Vince"],"pdf_url":"https://arxiv.org/pdf/2506.13667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13657v1","updated":"2025-06-16T16:18:21Z","published":"2025-06-16T16:18:21Z","title":"Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual\n  Object Detection in Educational Videos","summary":"  We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain.\n","authors":["Dipayan Biswas","Shishir Shah","Jaspal Subhlok"],"pdf_url":"https://arxiv.org/pdf/2506.13657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13654v1","updated":"2025-06-16T16:17:08Z","published":"2025-06-16T16:17:08Z","title":"Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning","summary":"  We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.\n","authors":["Shulin Tian","Ruiqi Wang","Hongming Guo","Penghao Wu","Yuhao Dong","Xiuying Wang","Jingkang Yang","Hao Zhang","Hongyuan Zhu","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13654v1.pdf","comment":"Project page: https://egolife-ai.github.io/Ego-R1/"},{"id":"http://arxiv.org/abs/2506.13642v1","updated":"2025-06-16T16:06:45Z","published":"2025-06-16T16:06:45Z","title":"Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model","summary":"  The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.\n","authors":["Shaolei Zhang","Shoutao Guo","Qingkai Fang","Yan Zhou","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2506.13642v1.pdf","comment":"Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b"},{"id":"http://arxiv.org/abs/2506.13638v1","updated":"2025-06-16T16:04:16Z","published":"2025-06-16T16:04:16Z","title":"DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models","summary":"  Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics.\n","authors":["Zhiyi Shi","Binjie Wang","Chongjie Si","Yichen Wu","Junsik Kim","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2506.13638v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2506.13629v1","updated":"2025-06-16T15:56:50Z","published":"2025-06-16T15:56:50Z","title":"FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding","summary":"  Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.\n","authors":["Chenlu Zhan","Gaoang Wang","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11996v2","updated":"2025-06-16T15:52:58Z","published":"2025-06-13T17:51:14Z","title":"Improving Surgical Risk Prediction Through Integrating Automated Body\n  Composition Analysis: a Retrospective Trial on Colectomy Surgery","summary":"  Objective: To evaluate whether preoperative body composition metrics\nautomatically extracted from CT scans can predict postoperative outcomes after\ncolectomy, either alone or combined with clinical variables or existing risk\npredictors. Main outcomes and measures: The primary outcome was the predictive\nperformance for 1-year all-cause mortality following colectomy. A Cox\nproportional hazards model with 1-year follow-up was used, and performance was\nevaluated using the concordance index (C-index) and Integrated Brier Score\n(IBS). Secondary outcomes included postoperative complications, unplanned\nreadmission, blood transfusion, and severe infection, assessed using AUC and\nBrier Score from logistic regression. Odds ratios (OR) described associations\nbetween individual CT-derived body composition metrics and outcomes. Over 300\nfeatures were extracted from preoperative CTs across multiple vertebral levels,\nincluding skeletal muscle area, density, fat areas, and inter-tissue metrics.\nNSQIP scores were available for all surgeries after 2012.\n","authors":["Hanxue Gu","Yaqian Chen","Jisoo Lee","Diego Schaps","Regina Woody","Roy Colglazier","Maciej A. Mazurowski","Christopher Mantyh"],"pdf_url":"https://arxiv.org/pdf/2506.11996v2.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.00599v2","updated":"2025-06-16T15:48:51Z","published":"2025-05-31T15:15:27Z","title":"XYZ-IBD: A High-precision Bin-picking Dataset for Object 6D Pose\n  Estimation Capturing Real-world Industrial Complexity","summary":"  We introduce XYZ-IBD, a bin-picking dataset for 6D pose estimation that\ncaptures real-world industrial complexity, including challenging object\ngeometries, reflective materials, severe occlusions, and dense clutter. The\ndataset reflects authentic robotic manipulation scenarios with\nmillimeter-accurate annotations. Unlike existing datasets that primarily focus\non household objects, which approach saturation,XYZ-IBD represents the unsolved\nrealistic industrial conditions. The dataset features 15 texture-less,\nmetallic, and mostly symmetrical objects of varying shapes and sizes. These\nobjects are heavily occluded and randomly arranged in bins with high density,\nreplicating the challenges of real-world bin-picking. XYZ-IBD was collected\nusing two high-precision industrial cameras and one commercially available\ncamera, providing RGB, grayscale, and depth images. It contains 75 multi-view\nreal-world scenes, along with a large-scale synthetic dataset rendered under\nsimulated bin-picking conditions. We employ a meticulous annotation pipeline\nthat includes anti-reflection spray, multi-view depth fusion, and\nsemi-automatic annotation, achieving millimeter-level pose labeling accuracy\nrequired for industrial manipulation. Quantification in simulated environments\nconfirms the reliability of the ground-truth annotations. We benchmark\nstate-of-the-art methods on 2D detection, 6D pose estimation, and depth\nestimation tasks on our dataset, revealing significant performance degradation\nin our setups compared to current academic household benchmarks. By capturing\nthe complexity of real-world bin-picking scenarios, XYZ-IBD introduces more\nrealistic and challenging problems for future research. The dataset and\nbenchmark are publicly available at https://xyz-ibd.github.io/XYZ-IBD/.\n","authors":["Junwen Huang","Jizhong Liang","Jiaqi Hu","Martin Sundermeyer","Peter KT Yu","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2506.00599v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18215v2","updated":"2025-06-16T15:48:32Z","published":"2025-04-25T09:49:23Z","title":"Unify3D: An Augmented Holistic End-to-end Monocular 3D Human\n  Reconstruction via Anatomy Shaping and Twins Negotiating","summary":"  Monocular 3D clothed human reconstruction aims to create a complete 3D avatar\nfrom a single image. To tackle the human geometry lacking in one RGB image,\ncurrent methods typically resort to a preceding model for an explicit geometric\nrepresentation. For the reconstruction itself, focus is on modeling both it and\nthe input image. This routine is constrained by the preceding model, and\noverlooks the integrity of the reconstruction task. To address this, this paper\nintroduces a novel paradigm that treats human reconstruction as a holistic\nprocess, utilizing an end-to-end network for direct prediction from 2D image to\n3D avatar, eliminating any explicit intermediate geometry display. Based on\nthis, we further propose a novel reconstruction framework consisting of two\ncore components: the Anatomy Shaping Extraction module, which captures implicit\nshape features taking into account the specialty of human anatomy, and the\nTwins Negotiating Reconstruction U-Net, which enhances reconstruction through\nfeature interaction between two U-Nets of different modalities. Moreover, we\npropose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to\nbolster model performance in more complex case input. Extensive experiments on\ntwo test sets and many in-the-wild cases show the superiority of our method\nover SOTA methods. Our demos can be found in :\nhttps://e2e3dgsrecon.github.io/e2e3dgsrecon/.\n","authors":["Nanjie Yao","Gangjian Zhang","Wenhao Shen","Jian Shu","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.18215v2.pdf","comment":"The experiment result shown in Ablation Study is insufficient to\n  support the effectiveness of the proposed methodology"},{"id":"http://arxiv.org/abs/2409.17823v2","updated":"2025-06-16T15:47:51Z","published":"2024-09-26T13:21:02Z","title":"Enhancing Logits Distillation with Plug\\&Play Kendall's $τ$ Ranking\n  Loss","summary":"  Knowledge distillation typically minimizes the Kullback-Leibler (KL)\ndivergence between teacher and student logits. However, optimizing the KL\ndivergence can be challenging for the student and often leads to sub-optimal\nsolutions. We further show that gradients induced by KL divergence scale with\nthe magnitude of the teacher logits, thereby diminishing updates on\nlow-probability channels. This imbalance weakens the transfer of inter-class\ninformation and in turn limits the performance improvements achievable by the\nstudent. To mitigate this issue, we propose a plug-and-play auxiliary ranking\nloss based on Kendall's $\\tau$ coefficient that can be seamlessly integrated\ninto any logit-based distillation framework. It supplies inter-class relational\ninformation while rebalancing gradients toward low-probability channels. We\ndemonstrate that the proposed ranking loss is largely invariant to channel\nscaling and optimizes an objective aligned with that of KL divergence, making\nit a natural complement rather than a replacement. Extensive experiments on\nCIFAR-100, ImageNet, and COCO datasets, as well as various CNN and ViT\nteacher-student architecture combinations, demonstrate that our plug-and-play\nranking loss consistently boosts the performance of multiple distillation\nbaselines. Code is available at https://github.com/OvernighTea/RankingLoss-KD\n","authors":["Yuchen Guan","Runxi Cheng","Kang Liu","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.17823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13614v1","updated":"2025-06-16T15:43:28Z","published":"2025-06-16T15:43:28Z","title":"Exploiting the Exact Denoising Posterior Score in Training-Free Guidance\n  of Diffusion Models","summary":"  The success of diffusion models has driven interest in performing conditional\nsampling via training-free guidance of the denoising process to solve image\nrestoration and other inverse problems. A popular class of methods, based on\nDiffusion Posterior Sampling (DPS), attempts to approximate the intractable\nposterior score function directly. In this work, we present a novel expression\nfor the exact posterior score for purely denoising tasks that is tractable in\nterms of the unconditional score function. We leverage this result to analyze\nthe time-dependent error in the DPS score for denoising tasks and compute step\nsizes on the fly to minimize the error at each time step. We demonstrate that\nthese step sizes are transferable to related inverse problems such as\ncolorization, random inpainting, and super resolution. Despite its simplicity,\nthis approach is competitive with state-of-the-art techniques and enables\nsampling with fewer time steps than DPS.\n","authors":["Gregory Bellchambers"],"pdf_url":"https://arxiv.org/pdf/2506.13614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20129v2","updated":"2025-06-16T15:42:37Z","published":"2025-05-26T15:28:17Z","title":"Agentic 3D Scene Generation with Spatially Contextualized VLMs","summary":"  Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications. Project page:\nhttps://spatctxvlm.github.io/project_page/.\n","authors":["Xinhang Liu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2505.20129v2.pdf","comment":"Project page: https://spatctxvlm.github.io/project_page/"},{"id":"http://arxiv.org/abs/2403.09471v6","updated":"2025-06-16T15:28:28Z","published":"2024-03-14T15:10:54Z","title":"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models","summary":"  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models. Our project is publicly available at\nhttps://kkakkkka.github.io/MambaTalk\n","authors":["Zunnan Xu","Yukang Lin","Haonan Han","Sicheng Yang","Ronghui Li","Yachao Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2403.09471v6.pdf","comment":"Accepted to NeurlPS 2024"},{"id":"http://arxiv.org/abs/2406.01425v5","updated":"2025-06-16T15:26:53Z","published":"2024-06-03T15:25:45Z","title":"Adaptive Sensitivity Analysis for Robust Augmentation against Natural\n  Corruptions in Image Segmentation","summary":"  Achieving robustness in image segmentation models is challenging due to the\nfine-grained nature of pixel-level classification. These models, which are\ncrucial for many real-time perception applications, particularly struggle when\nfaced with natural corruptions in the wild for autonomous systems. While\nsensitivity analysis can help us understand how input variables influence model\noutputs, its application to natural and uncontrollable corruptions in training\ndata is computationally expensive. In this work, we present an adaptive,\nsensitivity-guided augmentation method to enhance robustness against natural\ncorruptions. Our sensitivity analysis on average runs 10x faster and requires\nabout 200x less storage than previous sensitivity analysis, enabling practical,\non-the-fly estimation during training for a model-free augmentation policy.\nWith minimal fine-tuning, our sensitivity-guided augmentation method achieves\nimproved robustness on both real-world and synthetic datasets compared to\nstate-of-the-art data augmentation techniques in image segmentation. Code\nimplementation for this work can be found at:\nhttps://github.com/laurayuzheng/SensAug.\n","authors":["Laura Zheng","Wenjie Wei","Tony Wu","Jacob Clements","Shreelekha Revankar","Andre Harrison","Yu Shen","Ming C. Lin"],"pdf_url":"https://arxiv.org/pdf/2406.01425v5.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2506.13594v1","updated":"2025-06-16T15:21:30Z","published":"2025-06-16T15:21:30Z","title":"Dive3D: Diverse Distillation-based Text-to-3D Generation via Score\n  Implicit Matching","summary":"  Distilling pre-trained 2D diffusion models into 3D assets has driven\nremarkable advances in text-to-3D synthesis. However, existing methods\ntypically rely on Score Distillation Sampling (SDS) loss, which involves\nasymmetric KL divergence--a formulation that inherently favors mode-seeking\nbehavior and limits generation diversity. In this paper, we introduce Dive3D, a\nnovel text-to-3D generation framework that replaces KL-based objectives with\nScore Implicit Matching (SIM) loss, a score-based objective that effectively\nmitigates mode collapse. Furthermore, Dive3D integrates both diffusion\ndistillation and reward-guided optimization under a unified divergence\nperspective. Such reformulation, together with SIM loss, yields significantly\nmore diverse 3D outputs while improving text alignment, human preference, and\noverall visual fidelity. We validate Dive3D across various 2D-to-3D prompts and\nfind that it consistently outperforms prior methods in qualitative assessments,\nincluding diversity, photorealism, and aesthetic appeal. We further evaluate\nits performance on the GPTEval3D benchmark, comparing against nine\nstate-of-the-art baselines. Dive3D also achieves strong results on quantitative\nmetrics, including text-asset alignment, 3D plausibility, text-geometry\nconsistency, texture quality, and geometric detail.\n","authors":["Weimin Bai","Yubo Li","Wenzheng Chen","Weijian Luo","He Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13589v1","updated":"2025-06-16T15:18:15Z","published":"2025-06-16T15:18:15Z","title":"Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for\n  Efficient Long Video Understanding","summary":"  Multimodal Large Language Models (MLLMs) struggle with long videos due to\nfixed context windows and weak long-term dependency modeling. Existing\nRetrieval-Augmented Generation (RAG) methods for videos use static retrieval\nstrategies, leading to inefficiencies for simple queries and information loss\nfor complex tasks. To address this, we propose AdaVideoRAG, a novel framework\nthat dynamically adapts retrieval granularity based on query complexity using a\nlightweight intent classifier. Our framework employs an Omni-Knowledge Indexing\nmodule to build hierarchical databases from text (captions, ASR, OCR), visual\nfeatures, and semantic graphs, enabling optimal resource allocation across\ntasks. We also introduce the HiVU benchmark for comprehensive evaluation.\nExperiments demonstrate improved efficiency and accuracy for long-video\nunderstanding, with seamless integration into existing MLLMs. AdaVideoRAG\nestablishes a new paradigm for adaptive retrieval in video analysis. Codes will\nbe open-sourced at https://github.com/xzc-zju/AdaVideoRAG.\n","authors":["Zhucun Xue","Jiangning Zhang","Xurong Xie","Yuxuan Cai","Yong Liu","Xiangtai Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.13589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05517v2","updated":"2025-06-16T15:10:45Z","published":"2025-02-08T10:35:51Z","title":"Evaluation of Vision Transformers for Multimodal Image Classification: A\n  Case Study on Brain, Lung, and Kidney Tumors","summary":"  Neural networks have become the standard technique for medical diagnostics,\nespecially in cancer detection and classification. This work evaluates the\nperformance of Vision Transformers architectures, including Swin Transformer\nand MaxViT, in several datasets of magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans. We used three training sets of images with\nbrain, lung, and kidney tumors. Each dataset includes different classification\nlabels, from brain gliomas and meningiomas to benign and malignant lung\nconditions and kidney anomalies such as cysts and cancers. This work aims to\nanalyze the behavior of the neural networks in each dataset and the benefits of\ncombining different image modalities and tumor classes. We designed several\nexperiments by fine-tuning the models on combined and individual datasets. The\nresults revealed that the Swin Transformer provided high accuracy, achieving up\nto 99\\% on average for individual datasets and 99.4\\% accuracy for the combined\ndataset. This research highlights the adaptability of Transformer-based models\nto various image modalities and features. However, challenges persist,\nincluding limited annotated data and interpretability issues. Future work will\nexpand this study by incorporating other image modalities and enhancing\ndiagnostic capabilities. Integrating these models across diverse datasets could\nmark a significant advance in precision medicine, paving the way for more\nefficient and comprehensive healthcare solutions.\n","authors":["Óscar A. Martín","Javier Sánchez"],"pdf_url":"https://arxiv.org/pdf/2502.05517v2.pdf","comment":"19 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.06158v3","updated":"2025-06-16T15:09:49Z","published":"2023-12-11T06:50:27Z","title":"Adaptive Feature Selection for No-Reference Image Quality Assessment by\n  Mitigating Semantic Noise Sensitivity","summary":"  The current state-of-the-art No-Reference Image Quality Assessment (NR-IQA)\nmethods typically rely on feature extraction from upstream semantic backbone\nnetworks, assuming that all extracted features are relevant. However, we make a\nkey observation that not all features are beneficial, and some may even be\nharmful, necessitating careful selection. Empirically, we find that many image\npairs with small feature spatial distances can have vastly different quality\nscores, indicating that the extracted features may contain a significant amount\nof quality-irrelevant noise. To address this issue, we propose a Quality-Aware\nFeature Matching IQA Metric (QFM-IQM) that employs an adversarial perspective\nto remove harmful semantic noise features from the upstream task. Specifically,\nQFM-IQM enhances the semantic noise distinguish capabilities by matching image\npairs with similar quality scores but varying semantic features as adversarial\nsemantic noise and adaptively adjusting the upstream task's features by\nreducing sensitivity to adversarial noise perturbation. Furthermore, we utilize\na distillation framework to expand the dataset and improve the model's\ngeneralization ability. Our approach achieves superior performance to the\nstate-of-the-art NR-IQA methods on eight standard IQA datasets.\n","authors":["Xudong Li","Timin Gao","Runze Hu","Yan Zhang","Shengchuan Zhang","Xiawu Zheng","Jingyuan Zheng","Yunhang Shen","Ke Li","Yutao Liu","Pingyang Dai","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2312.06158v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13579v1","updated":"2025-06-16T15:02:12Z","published":"2025-06-16T15:02:12Z","title":"Flexible-length Text Infilling for Discrete Diffusion Models","summary":"  Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.\n","authors":["Andrew Zhang","Anushka Sivakumar","Chiawei Tang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2506.13579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13573v1","updated":"2025-06-16T14:57:05Z","published":"2025-06-16T14:57:05Z","title":"Integrated Pipeline for Monocular 3D Reconstruction and Finite Element\n  Simulation in Industrial Applications","summary":"  To address the challenges of 3D modeling and structural simulation in\nindustrial environment, such as the difficulty of equipment deployment, and the\ndifficulty of balancing accuracy and real-time performance, this paper proposes\nan integrated workflow, which integrates high-fidelity 3D reconstruction based\non monocular video, finite element simulation analysis, and mixed reality\nvisual display, aiming to build an interactive digital twin system for\nindustrial inspection, equipment maintenance and other scenes. Firstly, the\nNeuralangelo algorithm based on deep learning is used to reconstruct the 3D\nmesh model with rich details from the surround-shot video. Then, the QuadRemesh\ntool of Rhino is used to optimize the initial triangular mesh and generate a\nstructured mesh suitable for finite element analysis. The optimized mesh is\nfurther discretized by HyperMesh, and the material parameter setting and stress\nsimulation are carried out in Abaqus to obtain high-precision stress and\ndeformation results. Finally, combined with Unity and Vuforia engine, the\nreal-time superposition and interactive operation of simulation results in the\naugmented reality environment are realized, which improves users 'intuitive\nunderstanding of structural response. Experiments show that the method has good\nsimulation efficiency and visualization effect while maintaining high geometric\naccuracy. It provides a practical solution for digital modeling, mechanical\nanalysis and interactive display in complex industrial scenes, and lays a\nfoundation for the deep integration of digital twin and mixed reality\ntechnology in industrial applications.\n","authors":["Bowen Zheng"],"pdf_url":"https://arxiv.org/pdf/2506.13573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13564v1","updated":"2025-06-16T14:49:49Z","published":"2025-06-16T14:49:49Z","title":"MambaMia: A State-Space-Model-Based Compression for Efficient Video\n  Understanding in Large Multimodal Models","summary":"  We propose an efficient framework to compress multiple video-frame features\nbefore feeding them into large multimodal models, thereby mitigating the severe\ntoken explosion arising from long or dense videos. Our design leverages a\nbidirectional state-space-based block equipped with a gated skip connection and\na learnable weighted-average pooling mechanism applied to periodically inserted\nlearned queries. This structure enables hierarchical downsampling across both\nspatial and temporal dimensions, preserving performance in a cost-effective\nmanner. Across challenging long and dense video understanding tasks, our\napproach demonstrates competitive results against state-of-the-art models,\nwhile significantly reducing overall token budget. Notably, replacing our\nproposed state-space block with a conventional Transformer results in\nsubstantial performance degradation, highlighting the advantages of state-space\nmodeling for effectively compressing multi-frame video data. Our framework\nemphasizes resource-conscious efficiency, making it practical for real-world\ndeployments. We validate its scalability and generality across multiple\nbenchmarks, achieving the dual objectives of efficient resource usage and\ncomprehensive video understanding.\n","authors":["Geewook Kim","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2506.13564v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.15659v2","updated":"2025-06-16T14:43:25Z","published":"2025-01-26T19:43:41Z","title":"AirIO: Learning Inertial Odometry with Enhanced IMU Feature\n  Observability","summary":"  Inertial odometry (IO) using only Inertial Measurement Units (IMUs) offers a\nlightweight and cost-effective solution for Unmanned Aerial Vehicle (UAV)\napplications, yet existing learning-based IO models often fail to generalize to\nUAVs due to the highly dynamic and non-linear-flight patterns that differ from\npedestrian motion. In this work, we identify that the conventional practice of\ntransforming raw IMU data to global coordinates undermines the observability of\ncritical kinematic information in UAVs. By preserving the body-frame\nrepresentation, our method achieves substantial performance improvements, with\na 66.7% average increase in accuracy across three datasets. Furthermore,\nexplicitly encoding attitude information into the motion network results in an\nadditional 23.8% improvement over prior results. Combined with a data-driven\nIMU correction model (AirIMU) and an uncertainty-aware Extended Kalman Filter\n(EKF), our approach ensures robust state estimation under aggressive UAV\nmaneuvers without relying on external sensors or control inputs. Notably, our\nmethod also demonstrates strong generalizability to unseen data not included in\nthe training set, underscoring its potential for real-world UAV applications.\n","authors":["Yuheng Qiu","Can Xu","Yutian Chen","Shibo Zhao","Junyi Geng","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2501.15659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13558v1","updated":"2025-06-16T14:43:18Z","published":"2025-06-16T14:43:18Z","title":"X-Scene: Large-Scale Driving Scene Generation with High Fidelity and\n  Flexible Controllability","summary":"  Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.\n","authors":["Yu Yang","Alan Liang","Jianbiao Mei","Yukai Ma","Yong Liu","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2506.13558v1.pdf","comment":"28 pages, 9 figures, Project page at https://x-scene.github.io/"},{"id":"http://arxiv.org/abs/2506.13553v1","updated":"2025-06-16T14:40:28Z","published":"2025-06-16T14:40:28Z","title":"RelTopo: Enhancing Relational Modeling for Driving Scene Topology\n  Reasoning","summary":"  Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released.\n","authors":["Yueru Luo","Changqing Zhou","Yiming Yang","Erlong Li","Chao Zheng","Shuqi Mei","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2506.13553v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.13552v1","updated":"2025-06-16T14:39:03Z","published":"2025-06-16T14:39:03Z","title":"A Comprehensive Survey on Video Scene Parsing:Advances, Challenges, and\n  Prospects","summary":"  Video Scene Parsing (VSP) has emerged as a cornerstone in computer vision,\nfacilitating the simultaneous segmentation, recognition, and tracking of\ndiverse visual entities in dynamic scenes. In this survey, we present a\nholistic review of recent advances in VSP, covering a wide array of vision\ntasks, including Video Semantic Segmentation (VSS), Video Instance Segmentation\n(VIS), Video Panoptic Segmentation (VPS), as well as Video Tracking and\nSegmentation (VTS), and Open-Vocabulary Video Segmentation (OVVS). We\nsystematically analyze the evolution from traditional hand-crafted features to\nmodern deep learning paradigms -- spanning from fully convolutional networks to\nthe latest transformer-based architectures -- and assess their effectiveness in\ncapturing both local and global temporal contexts. Furthermore, our review\ncritically discusses the technical challenges, ranging from maintaining\ntemporal consistency to handling complex scene dynamics, and offers a\ncomprehensive comparative study of datasets and evaluation metrics that have\nshaped current benchmarking standards. By distilling the key contributions and\nshortcomings of state-of-the-art methodologies, this survey highlights emerging\ntrends and prospective research directions that promise to further elevate the\nrobustness and adaptability of VSP in real-world applications.\n","authors":["Guohuan Xie","Syed Ariff Syed Hesham","Wenya Guo","Bing Li","Ming-Ming Cheng","Guolei Sun","Yun Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13545v1","updated":"2025-06-16T14:32:16Z","published":"2025-06-16T14:32:16Z","title":"Limited-Angle CBCT Reconstruction via Geometry-Integrated Cycle-domain\n  Denoising Diffusion Probabilistic Models","summary":"  Cone-beam CT (CBCT) is widely used in clinical radiotherapy for image-guided\ntreatment, improving setup accuracy, adaptive planning, and motion management.\nHowever, slow gantry rotation limits performance by introducing motion\nartifacts, blurring, and increased dose. This work aims to develop a clinically\nfeasible method for reconstructing high-quality CBCT volumes from consecutive\nlimited-angle acquisitions, addressing imaging challenges in time- or\ndose-constrained settings. We propose a limited-angle (LA) geometry-integrated\ncycle-domain (LA-GICD) framework for CBCT reconstruction, comprising two\ndenoising diffusion probabilistic models (DDPMs) connected via analytic\ncone-beam forward and back projectors. A Projection-DDPM completes missing\nprojections, followed by back-projection, and an Image-DDPM refines the volume.\nThis dual-domain design leverages complementary priors from projection and\nimage spaces to achieve high-quality reconstructions from limited-angle (<= 90\ndegrees) scans. Performance was evaluated against full-angle reconstruction.\nFour board-certified medical physicists conducted assessments. A total of 78\nplanning CTs in common CBCT geometries were used for training and evaluation.\nThe method achieved a mean absolute error of 35.5 HU, SSIM of 0.84, and PSNR of\n29.8 dB, with visibly reduced artifacts and improved soft-tissue clarity.\nLA-GICD's geometry-aware dual-domain learning, embedded in analytic\nforward/backward operators, enabled artifact-free, high-contrast\nreconstructions from a single 90-degree scan, reducing acquisition time and\ndose four-fold. LA-GICD improves limited-angle CBCT reconstruction with strong\ndata fidelity and anatomical realism. It offers a practical solution for\nshort-arc acquisitions, enhancing CBCT use in radiotherapy by providing\nclinically applicable images with reduced scan time and dose for more accurate,\npersonalized treatments.\n","authors":["Yuan Gao","Shaoyan Pan","Mingzhe Hu","Huiqiao Xie","Jill Remick","Chih-Wei Chang","Justin Roper","Zhen Tian","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2506.13545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13542v1","updated":"2025-06-16T14:30:37Z","published":"2025-06-16T14:30:37Z","title":"Atomizer: Generalizing to new modalities by breaking satellite images\n  down to a set of scalars","summary":"  The growing number of Earth observation satellites has led to increasingly\ndiverse remote sensing data, with varying spatial, spectral, and temporal\nconfigurations. Most existing models rely on fixed input formats and\nmodality-specific encoders, which require retraining when new configurations\nare introduced, limiting their ability to generalize across modalities. We\nintroduce Atomizer, a flexible architecture that represents remote sensing\nimages as sets of scalars, each corresponding to a spectral band value of a\npixel. Each scalar is enriched with contextual metadata (acquisition time,\nspatial resolution, wavelength, and bandwidth), producing an atomic\nrepresentation that allows a single encoder to process arbitrary modalities\nwithout interpolation or resampling. Atomizer uses structured tokenization with\nFourier features and non-uniform radial basis functions to encode content and\ncontext, and maps tokens into a latent space via cross-attention. Under\nmodality-disjoint evaluations, Atomizer outperforms standard models and\ndemonstrates robust performance across varying resolutions and spatial sizes.\n","authors":["Hugo Riffaud de Turckheim","Sylvain Lobry","Roberto Interdonato","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2506.13542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12337v2","updated":"2025-06-16T14:16:15Z","published":"2025-05-18T10:02:57Z","title":"Structureless VIO","summary":"  Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of a\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solutions that do not require a map have not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy.\n","authors":["Junlin Song","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2505.12337v2.pdf","comment":"Accepted by the SLAM Workshop at RSS 2025"},{"id":"http://arxiv.org/abs/2506.13516v1","updated":"2025-06-16T14:11:13Z","published":"2025-06-16T14:11:13Z","title":"Micro-macro Gaussian Splatting with Enhanced Scalability for\n  Unconstrained Scene Reconstruction","summary":"  Reconstructing 3D scenes from unconstrained image collections poses\nsignificant challenges due to variations in appearance. In this paper, we\npropose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel\nmethod that enhances 3D reconstruction across diverse scales by decomposing\nscene representations into global, refined, and intrinsic components. SMW-GS\nincorporates the following innovations: Micro-macro Projection, which enables\nGaussian points to sample multi-scale details with improved diversity; and\nWavelet-based Sampling, which refines feature representations using\nfrequency-domain information to better capture complex scene appearances. To\nachieve scalability, we further propose a large-scale scene promotion strategy,\nwhich optimally assigns camera views to scene partitions by maximizing their\ncontributions to Gaussian points, achieving consistent and high-quality\nreconstructions even in expansive environments. Extensive experiments\ndemonstrate that SMW-GS significantly outperforms existing methods in both\nreconstruction quality and scalability, particularly excelling in large-scale\nurban environments with challenging illumination variations. Project is\navailable at https://github.com/Kidleyh/SMW-GS.\n","authors":["Yihui Li","Chengxin Lv","Hongyu Yang","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2506.13516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13509v1","updated":"2025-06-16T14:04:48Z","published":"2025-06-16T14:04:48Z","title":"A Semantically-Aware Relevance Measure for Content-Based Medical Image\n  Retrieval Evaluation","summary":"  Performance evaluation for Content-Based Image Retrieval (CBIR) remains a\ncrucial but unsolved problem today especially in the medical domain. Various\nevaluation metrics have been discussed in the literature to solve this problem.\nMost of the existing metrics (e.g., precision, recall) are adapted from\nclassification tasks which require manual labels as ground truth. However, such\nlabels are often expensive and unavailable in specific thematic domains.\nFurthermore, medical images are usually associated with (radiological) case\nreports or annotated with descriptive captions in literature figures, such text\ncontains information that can help to assess CBIR.Several researchers have\nargued that the medical concepts hidden in the text can serve as the basis for\nCBIR evaluation purpose. However, these works often consider these medical\nconcepts as independent and isolated labels while in fact the subtle\nrelationships between various concepts are neglected. In this work, we\nintroduce the use of knowledge graphs to measure the distance between various\nmedical concepts and propose a novel relevance measure for the evaluation of\nCBIR by defining an approximate matching-based relevance score between two sets\nof medical concepts which allows us to indirectly measure the similarity\nbetween medical images.We quantitatively demonstrate the effectiveness and\nfeasibility of our relevance measure using a public dataset.\n","authors":["Xiaoyang Wei","Camille Kurtz","Florence Cloppet"],"pdf_url":"https://arxiv.org/pdf/2506.13509v1.pdf","comment":"This paper has been accepted by the International Conference on Image\n  Analysis and Processing 2025"},{"id":"http://arxiv.org/abs/2506.13508v1","updated":"2025-06-16T14:02:46Z","published":"2025-06-16T14:02:46Z","title":"Multiview Geometric Regularization of Gaussian Splatting for Accurate\n  Radiance Fields","summary":"  Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields,\nhave aimed to address the geometric inaccuracies of 3D Gaussian Splatting while\nretaining its superior rendering quality. However, these approaches still\nstruggle to reconstruct smooth and reliable geometry, particularly in scenes\nwith significant color variation across viewpoints, due to their per-point\nappearance modeling and single-view optimization constraints. In this paper, we\npropose an effective multiview geometric regularization strategy that\nintegrates multiview stereo (MVS) depth, RGB, and normal constraints into\nGaussian Splatting initialization and optimization. Our key insight is the\ncomplementary relationship between MVS-derived depth points and Gaussian\nSplatting-optimized positions: MVS robustly estimates geometry in regions of\nhigh color variation through local patch-based matching and epipolar\nconstraints, whereas Gaussian Splatting provides more reliable and less noisy\ndepth estimates near object boundaries and regions with lower color variation.\nTo leverage this insight, we introduce a median depth-based multiview relative\ndepth loss with uncertainty estimation, effectively integrating MVS depth\ninformation into Gaussian Splatting optimization. We also propose an MVS-guided\nGaussian Splatting initialization to avoid Gaussians falling into suboptimal\npositions. Extensive experiments validate that our approach successfully\ncombines these strengths, enhancing both geometric accuracy and rendering\nquality across diverse indoor and outdoor scenes.\n","authors":["Jungeon Kim","Geonsoo Park","Seungyong Lee"],"pdf_url":"https://arxiv.org/pdf/2506.13508v1.pdf","comment":"Accepted to Computer Graphics Forum (EGSR 2025)"},{"id":"http://arxiv.org/abs/2506.13506v1","updated":"2025-06-16T14:00:49Z","published":"2025-06-16T14:00:49Z","title":"Stimulus Motion Perception Studies Imply Specific Neural Computations in\n  Human Visual Stabilization","summary":"  Even during fixation the human eye is constantly in low amplitude motion,\njittering over small angles in random directions at up to 100Hz. This motion\nresults in all features of the image on the retina constantly traversing a\nnumber of cones, yet objects which are stable in the world are perceived to be\nstable, and any object which is moving in the world is perceived to be moving.\nA series of experiments carried out over a dozen years revealed the\npsychophysics of visual stabilization to be more nuanced than might be assumed,\nsay, from the mechanics of stabilization of camera images, or what might be\nassumed to be the simplest solution from an evolutionary perspective. The\npsychophysics revealed by the experiments strongly implies a specific set of\noperations on retinal signals resulting in the observed stabilization behavior.\nThe presentation is in two levels. First is a functional description of the\naction of the mechanism that is very likely responsible for the experimentally\nobserved behavior. Second is a more speculative proposal of circuit-level\nneural elements that might implement the functional behavior.\n","authors":["David W Arathorn","Josephine C. D'Angelo","Austin Roorda"],"pdf_url":"https://arxiv.org/pdf/2506.13506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13501v1","updated":"2025-06-16T13:58:49Z","published":"2025-06-16T13:58:49Z","title":"FOAM: A General Frequency-Optimized Anti-Overlapping Framework for\n  Overlapping Object Perception","summary":"  Overlapping object perception aims to decouple the randomly overlapping\nforeground-background features, extracting foreground features while\nsuppressing background features, which holds significant application value in\nfields such as security screening and medical auxiliary diagnosis. Despite some\nresearch efforts to tackle the challenge of overlapping object perception, most\nsolutions are confined to the spatial domain. Through frequency domain\nanalysis, we observe that the degradation of contours and textures due to the\noverlapping phenomenon can be intuitively reflected in the magnitude spectrum.\nBased on this observation, we propose a general Frequency-Optimized\nAnti-Overlapping Framework (FOAM) to assist the model in extracting more\ntexture and contour information, thereby enhancing the ability for\nanti-overlapping object perception. Specifically, we design the Frequency\nSpatial Transformer Block (FSTB), which can simultaneously extract features\nfrom both the frequency and spatial domains, helping the network capture more\ntexture features from the foreground. In addition, we introduce the\nHierarchical De-Corrupting (HDC) mechanism, which aligns adjacent features in\nthe separately constructed base branch and corruption branch using a specially\ndesigned consistent loss during the training phase. This mechanism suppresses\nthe response to irrelevant background features of FSTBs, thereby improving the\nperception of foreground contour. We conduct extensive experiments to validate\nthe effectiveness and generalization of the proposed FOAM, which further\nimproves the accuracy of state-of-the-art models on four datasets, specifically\nfor the three overlapping object perception tasks: Prohibited Item Detection,\nProhibited Item Segmentation, and Pneumonia Detection. The code will be open\nsource once the paper is accepted.\n","authors":["Mingyuan Li","Tong Jia","Han Gu","Hui Lu","Hao Wang","Bowen Ma","Shuyang Lin","Shiyi Guo","Shizhuo Deng","Dongyue Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00513v2","updated":"2025-06-16T13:53:34Z","published":"2025-03-01T14:38:42Z","title":"Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal\n  Instruction Tuning","summary":"  Despite encouraging progress in 3D scene understanding, it remains\nchallenging to develop an effective Large Multi-modal Model (LMM) that is\ncapable of understanding and reasoning in complex 3D environments. Most\nprevious methods typically encode 3D point and 2D image features separately,\nneglecting interactions between 2D semantics and 3D object properties, as well\nas the spatial relationships within the 3D environment. This limitation not\nonly hinders comprehensive representations of 3D scene, but also compromises\ntraining and inference efficiency. To address these challenges, we propose a\nunified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with\nmultiple 3D scene understanding tasks simultaneously. To obtain the\nfine-grained instance-level visual tokens, we first introduce a novel\nMulti-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D\nsemantics into their corresponding 3D geometric features. For scene-level\nrelation-aware tokens, we further present a 3D Instance Spatial Relation\n(3D-ISR) module to capture the intricate pairwise spatial relationships among\nobjects. Additionally, we perform end-to-end multi-task instruction tuning\nsimultaneously without the subsequent task-specific fine-tuning. Extensive\nexperiments demonstrate that our approach outperforms the state-of-the-art\nmethods across 3D scene understanding, reasoning and grounding tasks. Source\ncode is available at https://github.com/hanxunyu/Inst3D-LMM\n","authors":["Hanxun Yu","Wentong Li","Song Wang","Junbo Chen","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.00513v2.pdf","comment":"CVPR2025, Code Link: https://github.com/hanxunyu/Inst3D-LMM"},{"id":"http://arxiv.org/abs/2506.13496v1","updated":"2025-06-16T13:53:02Z","published":"2025-06-16T13:53:02Z","title":"Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval","summary":"  Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware.\n","authors":["Kshitij Kavimandan","Angelos Nalmpantis","Emma Beauxis-Aussalet","Robert-Jan Sips"],"pdf_url":"https://arxiv.org/pdf/2506.13496v1.pdf","comment":"5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025"},{"id":"http://arxiv.org/abs/2506.13492v1","updated":"2025-06-16T13:50:55Z","published":"2025-06-16T13:50:55Z","title":"GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field","summary":"  Plane Geometry Diagram Synthesis has been a crucial task in computer\ngraphics, with applications ranging from educational tools to AI-driven\nmathematical reasoning. Traditionally, we rely on computer tools (e.g.,\nMatplotlib and GeoGebra) to manually generate precise diagrams, but it usually\nrequires huge, complicated calculations cost. Recently, researchers start to\nwork on learning-based methods (e.g., Stable Diffusion and GPT4) to\nautomatically generate diagrams, saving operational cost but usually suffering\nfrom limited realism and insufficient accuracy. In this paper, we propose a\nnovel framework GeoSDF to automatically generate diagrams efficiently and\naccurately with Signed Distance Field (SDF). Specifically, we first represent\ngeometric elements in the SDF, then construct a series of constraint functions\nto represent geometric relationships, next we optimize such constraint\nfunctions to get an optimized field of both elements and constraints, finally\nby rendering the optimized field, we can obtain the synthesized diagram. In our\nGeoSDF, we define a symbolic language to easily represent geometric elements\nand those constraints, and our synthesized geometry diagrams can be\nself-verified in the SDF, ensuring both mathematical accuracy and visual\nplausibility. In experiments, our GeoSDF synthesized both normal high-school\nlevel and IMO-level geometry diagrams. Through both qualitative and\nquantitative analysis, we can see that synthesized diagrams are realistic and\naccurate, and our synthesizing process is simple and efficient. Furthermore, we\nobtain a very high accuracy of solving geometry problems (over 95\\% while the\ncurrent SOTA accuracy is around 75%) by leveraging our self-verification\nproperty. All of these demonstrate the advantage of GeoSDF, paving the way for\nmore sophisticated, accurate, and flexible generation of geometric diagrams for\na wide array of applications.\n","authors":["Chengrui Zhang","Maizhen Ning","Zihao Zhou","Jie Sun","Kaizhu Huang","Qiufeng Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08234v2","updated":"2025-06-16T13:47:24Z","published":"2024-08-15T15:58:11Z","title":"Comparative Evaluation of 3D Reconstruction Methods for Object Pose\n  Estimation","summary":"  Object pose estimation is essential to many industrial applications involving\nrobotic manipulation, navigation, and augmented reality. Current generalizable\nobject pose estimators, i.e., approaches that do not need to be trained per\nobject, rely on accurate 3D models. Predominantly, CAD models are used, which\ncan be hard to obtain in practice. At the same time, it is often possible to\nacquire images of an object. Naturally, this leads to the question whether 3D\nmodels reconstructed from images are sufficient to facilitate accurate object\npose estimation. We aim to answer this question by proposing a novel benchmark\nfor measuring the impact of 3D reconstruction quality on pose estimation\naccuracy. Our benchmark provides calibrated images for object reconstruction\nregistered with the test images of the YCB-V dataset for pose evaluation under\nthe BOP benchmark format. Detailed experiments with multiple state-of-the-art\n3D reconstruction and object pose estimation approaches show that the geometry\nproduced by modern reconstruction methods is often sufficient for accurate pose\nestimation. Our experiments lead to interesting observations: (1) Standard\nmetrics for measuring 3D reconstruction quality are not necessarily indicative\nof pose estimation accuracy, which shows the need for dedicated benchmarks such\nas ours. (2) Classical, non-learning-based approaches can perform on par with\nmodern learning-based reconstruction techniques and can even offer a better\nreconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap\nbetween performance with reconstructed and with CAD models. To foster research\non closing this gap, our benchmark is publicly available at\nhttps://github.com/VarunBurde/reconstruction_pose_benchmark}.\n","authors":["Varun Burde","Assia Benbihi","Pavel Burget","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2408.08234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13484v1","updated":"2025-06-16T13:42:51Z","published":"2025-06-16T13:42:51Z","title":"Deep Diffusion Models and Unsupervised Hyperspectral Unmixing for\n  Realistic Abundance Map Synthesis","summary":"  This paper presents a novel methodology for generating realistic abundance\nmaps from hyperspectral imagery using an unsupervised, deep-learning-driven\napproach. Our framework integrates blind linear hyperspectral unmixing with\nstate-of-the-art diffusion models to enhance the realism and diversity of\nsynthetic abundance maps. First, we apply blind unmixing to extract endmembers\nand abundance maps directly from raw hyperspectral data. These abundance maps\nthen serve as inputs to a diffusion model, which acts as a generative engine to\nsynthesize highly realistic spatial distributions. Diffusion models have\nrecently revolutionized image synthesis by offering superior performance,\nflexibility, and stability, making them well-suited for high-dimensional\nspectral data. By leveraging this combination of physically interpretable\nunmixing and deep generative modeling, our approach enables the simulation of\nhyperspectral sensor outputs under diverse imaging conditions--critical for\ndata augmentation, algorithm benchmarking, and model evaluation in\nhyperspectral analysis. Notably, our method is entirely unsupervised, ensuring\nadaptability to different datasets without the need for labeled training data.\nWe validate our approach using real hyperspectral imagery from the PRISMA space\nmission for Earth observation, demonstrating its effectiveness in producing\nrealistic synthetic abundance maps that capture the spatial and spectral\ncharacteristics of natural scenes.\n","authors":["Martina Pastorino","Michael Alibani","Nicola Acito","Gabriele Moser"],"pdf_url":"https://arxiv.org/pdf/2506.13484v1.pdf","comment":"CVPRw2025"},{"id":"http://arxiv.org/abs/2506.13477v1","updated":"2025-06-16T13:34:36Z","published":"2025-06-16T13:34:36Z","title":"From Flat to Feeling: A Feasibility and Impact Study on Dynamic Facial\n  Emotions in AI-Generated Avatars","summary":"  Dynamic facial emotion is essential for believable AI-generated avatars;\nhowever, most systems remain visually inert, limiting their utility in\nhigh-stakes simulations such as virtual training for investigative interviews\nwith abused children. We introduce and evaluate a real-time architecture fusing\nUnreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to\ntranslate vocal prosody into high-fidelity facial expressions on photorealistic\nchild avatars. We implemented a distributed two-PC setup that decouples\nlanguage processing and speech synthesis from GPU-intensive rendering, designed\nto support low-latency interaction in desktop and VR environments. A\nbetween-subjects study ($N=70$) using audio+visual and visual-only conditions\nassessed perceptual impacts as participants rated emotional clarity, facial\nrealism, and empathy for two avatars expressing joy, sadness, and anger.\n  Results demonstrate that avatars could express emotions recognizably, with\nsadness and joy achieving high identification rates. However, anger recognition\nsignificantly dropped without audio, highlighting the importance of congruent\nvocal cues for high-arousal emotions. Interestingly, removing audio boosted\nperceived facial realism, suggesting that audiovisual desynchrony remains a key\ndesign challenge. These findings confirm the technical feasibility of\ngenerating emotionally expressive avatars and provide guidance for improving\nnon-verbal communication in sensitive training simulations.\n","authors":["Pegah Salehi","Sajad Amouei Sheshkal","Vajira Thambawita","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2506.13477v1.pdf","comment":"15 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.13476v1","updated":"2025-06-16T13:34:35Z","published":"2025-06-16T13:34:35Z","title":"ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for\n  tiny Printed Circuit Board Defect detection","summary":"  Printed Circuit Boards (PCBs) are critical components in modern electronics,\nwhich require stringent quality control to ensure proper functionality.\nHowever, the detection of defects in small-scale PCBs images poses significant\nchallenges as a result of the low resolution of the captured images, leading to\npotential confusion between defects and noise. To overcome these challenges,\nthis paper proposes a novel framework, named ESRPCB (edgeguided\nsuper-resolution for PCBs defect detection), which combines edgeguided\nsuper-resolution with ensemble learning to enhance PCBs defect detection. The\nframework leverages the edge information to guide the EDSR (Enhanced Deep\nSuper-Resolution) model with a novel ResCat (Residual Concatenation) structure,\nenabling it to reconstruct high-resolution images from small PCBs inputs. By\nincorporating edge features, the super-resolution process preserves critical\nstructural details, ensuring that tiny defects remain distinguishable in the\nenhanced image. Following this, a multi-modal defect detection model employs\nensemble learning to analyze the super-resolved\n","authors":["Xiem HoangVan","Dang Bui Dinh","Thanh Nguyen Canh","Van-Truong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2506.13476v1.pdf","comment":"Published in Engineering Applications of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2503.08434v4","updated":"2025-06-16T13:29:51Z","published":"2025-03-11T13:49:12Z","title":"Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models","summary":"  Recent advances in large-scale text-to-image models have revolutionized\ncreative fields by generating visually captivating outputs from textual\nprompts; however, while traditional photography offers precise control over\ncamera settings to shape visual aesthetics - such as depth-of-field via\naperture - current diffusion models typically rely on prompt engineering to\nmimic such effects. This approach often results in crude approximations and\ninadvertently alters the scene content. In this work, we propose Bokeh\nDiffusion, a scene-consistent bokeh control framework that explicitly\nconditions a diffusion model on a physical defocus blur parameter. To overcome\nthe scarcity of paired real-world images captured under different camera\nsettings, we introduce a hybrid training pipeline that aligns in-the-wild\nimages with synthetic blur augmentations, providing diverse scenes and subjects\nas well as supervision to learn the separation of image content from lens blur.\nCentral to our framework is our grounded self-attention mechanism, trained on\nimage pairs with different bokeh levels of the same scene, which enables blur\nstrength to be adjusted in both directions while preserving the underlying\nscene. Extensive experiments demonstrate that our approach enables flexible,\nlens-like blur control, supports downstream applications such as real image\nediting via inversion, and generalizes effectively across both Stable Diffusion\nand FLUX architectures.\n","authors":["Armando Fortes","Tianyi Wei","Shangchen Zhou","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2503.08434v4.pdf","comment":"Project page: https://atfortes.github.io/projects/bokeh-diffusion/"},{"id":"http://arxiv.org/abs/2501.00942v2","updated":"2025-06-16T13:28:17Z","published":"2025-01-01T19:52:19Z","title":"Efficient Unsupervised Shortcut Learning Detection and Mitigation in\n  Transformers","summary":"  Shortcut learning, i.e., a model's reliance on undesired features not\ndirectly relevant to the task, is a major challenge that severely limits the\napplications of machine learning algorithms, particularly when deploying them\nto assist in making sensitive decisions, such as in medical diagnostics. In\nthis work, we leverage recent advancements in machine learning to create an\nunsupervised framework that is capable of both detecting and mitigating\nshortcut learning in transformers. We validate our method on multiple datasets.\nResults demonstrate that our framework significantly improves both worst-group\naccuracy (samples misclassified due to shortcuts) and average accuracy, while\nminimizing human annotation effort. Moreover, we demonstrate that the detected\nshortcuts are meaningful and informative to human experts, and that our\nframework is computationally efficient, allowing it to be run on consumer\nhardware.\n","authors":["Lukas Kuhn","Sari Sadiya","Jorg Schlotterer","Florian Buettner","Christin Seifert","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2501.00942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10019v2","updated":"2025-06-16T13:26:24Z","published":"2023-08-19T14:01:04Z","title":"Dissecting RGB-D Learning for Improved Multi-modal Fusion","summary":"  In the RGB-D vision community, extensive research has been focused on\ndesigning multi-modal learning strategies and fusion structures. However, the\ncomplementary and fusion mechanisms in RGB-D models remain a black box. In this\npaper, we present an analytical framework and a novel score to dissect the\nRGB-D vision community. Our approach involves measuring proposed semantic\nvariance and feature similarity across modalities and levels, conducting visual\nand quantitative analyzes on multi-modal learning through comprehensive\nexperiments. Specifically, we investigate the consistency and specialty of\nfeatures across modalities, evolution rules within each modality, and the\ncollaboration logic used when optimizing a RGB-D model. Our studies\nreveal/verify several important findings, such as the discrepancy in\ncross-modal features and the hybrid multi-modal cooperation rule, which\nhighlights consistency and specialty simultaneously for complementary\ninference. We also showcase the versatility of the proposed RGB-D dissection\nmethod and introduce a straightforward fusion strategy based on our findings,\nwhich delivers significant enhancements across various tasks and even other\nmulti-modal data.\n","authors":["Hao Chen","Haoran Zhou","Yunshu Zhang","Zheng Lin","Yongjian Deng"],"pdf_url":"https://arxiv.org/pdf/2308.10019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13465v1","updated":"2025-06-16T13:25:12Z","published":"2025-06-16T13:25:12Z","title":"SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style\n  Transfer","summary":"  Photorealistic style transfer (PST) enables real-world color grading by\nadapting reference image colors while preserving content structure. Existing\nmethods mainly follow either approaches: generation-based methods that\nprioritize stylistic fidelity at the cost of content integrity and efficiency,\nor global color transformation methods such as LUT, which preserve structure\nbut lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D\nLook-Up Table (SA-LUT), combining LUT efficiency with neural network\nadaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that\nextracts multi-scale features from the style image to predict a 4D LUT, and (2)\na Context Generator using content-style cross-attention to produce a context\nmap. This context map enables spatially-adaptive adjustments, allowing our 4D\nLUT to apply precise color transformations while preserving structural\nintegrity. To establish a rigorous evaluation framework for photorealistic\nstyle transfer, we introduce PST50, the first benchmark specifically designed\nfor PST assessment. Experiments demonstrate that SA-LUT substantially\noutperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS\nscore compared to 3D LUT approaches, while maintaining real-time performance at\n16 FPS for video stylization. Our code and benchmark are available at\nhttps://github.com/Ry3nG/SA-LUT\n","authors":["Zerui Gong","Zhonghua Wu","Qingyi Tao","Qinyue Li","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2506.13465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13458v1","updated":"2025-06-16T13:15:02Z","published":"2025-06-16T13:15:02Z","title":"Leveraging Vision-Language Pre-training for Human Activity Recognition\n  in Still Images","summary":"  Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments.\n","authors":["Cristina Mahanta","Gagan Bhatia"],"pdf_url":"https://arxiv.org/pdf/2506.13458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13457v1","updated":"2025-06-16T13:15:01Z","published":"2025-06-16T13:15:01Z","title":"Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from\n  Foundations to State-of-the-Art","summary":"  Multi-object tracking (MOT) is a core task in computer vision that involves\ndetecting objects in video frames and associating them across time. The rise of\ndeep learning has significantly advanced MOT, particularly within the\ntracking-by-detection paradigm, which remains the dominant approach.\nAdvancements in modern deep learning-based methods accelerated in 2022 with the\nintroduction of ByteTrack for tracking-by-detection and MOTR for end-to-end\ntracking. Our survey provides an in-depth analysis of deep learning-based MOT\nmethods, systematically categorizing tracking-by-detection approaches into five\ngroups: joint detection and embedding, heuristic-based, motion-based, affinity\nlearning, and offline methods. In addition, we examine end-to-end tracking\nmethods and compare them with existing alternative approaches. We evaluate the\nperformance of recent trackers across multiple benchmarks and specifically\nassess their generality by comparing results across different domains. Our\nfindings indicate that heuristic-based methods achieve state-of-the-art results\non densely populated datasets with linear object motion, while deep\nlearning-based association methods, in both tracking-by-detection and\nend-to-end approaches, excel in scenarios with complex motion patterns.\n","authors":["Momir Adžemović"],"pdf_url":"https://arxiv.org/pdf/2506.13457v1.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2506.13445v1","updated":"2025-06-16T13:00:05Z","published":"2025-06-16T13:00:05Z","title":"Overcoming Occlusions in the Wild: A Multi-Task Age Head Approach to Age\n  Estimation","summary":"  Facial age estimation has achieved considerable success under controlled\nconditions. However, in unconstrained real-world scenarios, which are often\nreferred to as 'in the wild', age estimation remains challenging, especially\nwhen faces are partially occluded, which may obscure their visibility. To\naddress this limitation, we propose a new approach integrating generative\nadversarial networks (GANs) and transformer architectures to enable robust age\nestimation from occluded faces. We employ an SN-Patch GAN to effectively remove\nocclusions, while an Attentive Residual Convolution Module (ARCM), paired with\na Swin Transformer, enhances feature representation. Additionally, we introduce\na Multi-Task Age Head (MTAH) that combines regression and distribution\nlearning, further improving age estimation under occlusion. Experimental\nresults on the FG-NET, UTKFace, and MORPH datasets demonstrate that our\nproposed approach surpasses existing state-of-the-art techniques for occluded\nfacial age estimation by achieving an MAE of $3.00$, $4.54$, and $2.53$ years,\nrespectively.\n","authors":["Waqar Tanveer","Laura Fernández-Robles","Eduardo Fidalgo","Víctor González-Castro","Enrique Alegre"],"pdf_url":"https://arxiv.org/pdf/2506.13445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13444v1","updated":"2025-06-16T12:57:58Z","published":"2025-06-16T12:57:58Z","title":"Self-Supervised Enhancement for Depth from a Lightweight ToF Sensor with\n  Monocular Images","summary":"  Depth map enhancement using paired high-resolution RGB images offers a\ncost-effective solution for improving low-resolution depth data from\nlightweight ToF sensors. Nevertheless, naively adopting a depth estimation\npipeline to fuse the two modalities requires groundtruth depth maps for\nsupervision. To address this, we propose a self-supervised learning framework,\nSelfToF, which generates detailed and scale-aware depth maps. Starting from an\nimage-based self-supervised depth estimation pipeline, we add low-resolution\ndepth as inputs, design a new depth consistency loss, propose a scale-recovery\nmodule, and finally obtain a large performance boost. Furthermore, since the\nToF signal sparsity varies in real-world applications, we upgrade SelfToF to\nSelfToF* with submanifold convolution and guided feature fusion. Consequently,\nSelfToF* maintain robust performance across varying sparsity levels in ToF\ndata. Overall, our proposed method is both efficient and effective, as verified\nby extensive experiments on the NYU and ScanNet datasets. The code will be made\npublic.\n","authors":["Laiyan Ding","Hualie Jiang","Jiwei Chen","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2506.13444v1.pdf","comment":"accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2506.13443v1","updated":"2025-06-16T12:57:29Z","published":"2025-06-16T12:57:29Z","title":"PRO: Projection Domain Synthesis for CT Imaging","summary":"  Synthesizing high quality CT images remains a signifi-cant challenge due to\nthe limited availability of annotat-ed data and the complex nature of CT\nimaging. In this work, we present PRO, a novel framework that, to the best of\nour knowledge, is the first to perform CT image synthesis in the projection\ndomain using latent diffusion models. Unlike previous approaches that operate\nin the image domain, PRO learns rich structural representa-tions from raw\nprojection data and leverages anatomi-cal text prompts for controllable\nsynthesis. This projec-tion domain strategy enables more faithful modeling of\nunderlying imaging physics and anatomical structures. Moreover, PRO functions\nas a foundation model, capa-ble of generalizing across diverse downstream tasks\nby adjusting its generative behavior via prompt inputs. Experimental results\ndemonstrated that incorporating our synthesized data significantly improves\nperfor-mance across multiple downstream tasks, including low-dose and\nsparse-view reconstruction, even with limited training data. These findings\nunderscore the versatility and scalability of PRO in data generation for\nvarious CT applications. These results highlight the potential of projection\ndomain synthesis as a powerful tool for data augmentation and robust CT\nimaging. Our source code is publicly available at:\nhttps://github.com/yqx7150/PRO.\n","authors":["Kang Chen","Bin Huang","Xuebin Yang","Junyan Zhang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13440v1","updated":"2025-06-16T12:54:27Z","published":"2025-06-16T12:54:27Z","title":"Sparse Convolutional Recurrent Learning for Efficient Event-based\n  Neuromorphic Object Detection","summary":"  Leveraging the high temporal resolution and dynamic range, object detection\nwith event cameras can enhance the performance and safety of automotive and\nrobotics applications in real-world scenarios. However, processing sparse event\ndata requires compute-intensive convolutional recurrent units, complicating\ntheir integration into resource-constrained edge applications. Here, we propose\nthe Sparse Event-based Efficient Detector (SEED) for efficient event-based\nobject detection on neuromorphic processors. We introduce sparse convolutional\nrecurrent learning, which achieves over 92% activation sparsity in recurrent\nprocessing, vastly reducing the cost for spatiotemporal reasoning on sparse\nevent data. We validated our method on Prophesee's 1 Mpx and Gen1 event-based\nobject detection datasets. Notably, SEED sets a new benchmark in computational\nefficiency for event-based object detection which requires long-term temporal\nlearning. Compared to state-of-the-art methods, SEED significantly reduces\nsynaptic operations while delivering higher or same-level mAP. Our hardware\nsimulations showcase the critical role of SEED's hardware-aware design in\nachieving energy-efficient and low-latency neuromorphic processing.\n","authors":["Shenqi Wang","Yingfu Xu","Amirreza Yousefzadeh","Sherif Eissa","Henk Corporaal","Federico Corradi","Guangzhi Tang"],"pdf_url":"https://arxiv.org/pdf/2506.13440v1.pdf","comment":"Accepted by IJCNN 2025"},{"id":"http://arxiv.org/abs/2506.13430v1","updated":"2025-06-16T12:47:37Z","published":"2025-06-16T12:47:37Z","title":"Uncertainty-Aware Remaining Lifespan Prediction from Images","summary":"  Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.\n","authors":["Tristan Kenneweg","Philip Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2506.13430v1.pdf","comment":"Submitted to IMPACT 2025"},{"id":"http://arxiv.org/abs/2506.13425v1","updated":"2025-06-16T12:43:02Z","published":"2025-06-16T12:43:02Z","title":"JENGA: Object selection and pose estimation for robotic grasping from a\n  stack","summary":"  Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario.\n","authors":["Sai Srinivas Jeevanandam","Sandeep Inuganti","Shreedhar Govil","Didier Stricker","Jason Rambach"],"pdf_url":"https://arxiv.org/pdf/2506.13425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13419v1","updated":"2025-06-16T12:34:48Z","published":"2025-06-16T12:34:48Z","title":"Audio-Visual Driven Compression for Low-Bitrate Talking Head Videos","summary":"  Talking head video compression has advanced with neural rendering and\nkeypoint-based methods, but challenges remain, especially at low bit rates,\nincluding handling large head movements, suboptimal lip synchronization, and\ndistorted facial reconstructions. To address these problems, we propose a novel\naudio-visual driven video codec that integrates compact 3D motion features and\naudio signals. This approach robustly models significant head rotations and\naligns lip movements with speech, improving both compression efficiency and\nreconstruction quality. Experiments on the CelebV-HQ dataset show that our\nmethod reduces bitrate by 22% compared to VVC and by 8.5% over state-of-the-art\nlearning-based codec. Furthermore, it provides superior lip-sync accuracy and\nvisual fidelity at comparable bitrates, highlighting its effectiveness in\nbandwidth-constrained scenarios.\n","authors":["Riku Takahashi","Ryugo Morita","Jinjia Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.13419v1.pdf","comment":"Accepted to ICMR2025"},{"id":"http://arxiv.org/abs/2506.13415v1","updated":"2025-06-16T12:31:48Z","published":"2025-06-16T12:31:48Z","title":"Simple is what you need for efficient and accurate medical image\n  segmentation","summary":"  While modern segmentation models often prioritize performance over\npracticality, we advocate a design philosophy prioritizing simplicity and\nefficiency, and attempted high performance segmentation model design. This\npaper presents SimpleUNet, a scalable ultra-lightweight medical image\nsegmentation model with three key innovations: (1) A partial feature selection\nmechanism in skip connections for redundancy reduction while enhancing\nsegmentation performance; (2) A fixed-width architecture that prevents\nexponential parameter growth across network stages; (3) An adaptive feature\nfusion module achieving enhanced representation with minimal computational\noverhead. With a record-breaking 16 KB parameter configuration, SimpleUNet\noutperforms LBUNet and other lightweight benchmarks across multiple public\ndatasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and\naccuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast\nlesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin\nlesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp\nsegmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent\ndominance over state-of-the-art models. This work demonstrates that extreme\nmodel compression need not compromise performance, providing new insights for\nefficient and accurate medical image segmentation. Codes can be found at\nhttps://github.com/Frankyu5666666/SimpleUNet.\n","authors":["Xiang Yu","Yayan Chen","Guannan He","Qing Zeng","Yue Qin","Meiling Liang","Dandan Luo","Yimei Liao","Zeyu Ren","Cheng Kang","Delong Yang","Bocheng Liang","Bin Pu","Ying Yuan","Shengli Li"],"pdf_url":"https://arxiv.org/pdf/2506.13415v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.12179v3","updated":"2025-06-16T12:30:26Z","published":"2023-07-22T22:19:11Z","title":"Recognizing Unseen States of Unknown Objects by Leveraging Knowledge\n  Graphs","summary":"  We investigate the problem of Object State Classification (OSC) as a\nzero-shot learning problem. Specifically, we propose the first Object-agnostic\nState Classification (OaSC) method that infers the state of a certain object\nwithout relying on the knowledge or the estimation of the object class. In that\ndirection, we capitalize on Knowledge Graphs (KGs) for structuring and\norganizing knowledge, which, in combination with visual information, enable the\ninference of the states of objects in object/state pairs that have not been\nencountered in the method's training set. A series of experiments investigate\nthe performance of the proposed method in various settings, against several\nhypotheses and in comparison with state of the art approaches for object\nattribute classification. The experimental results demonstrate that the\nknowledge of an object class is not decisive for the prediction of its state.\nMoreover, the proposed OaSC method outperforms existing methods in all datasets\nand benchmarks by a great margin.\n","authors":["Filipos Gouidis","Konstantinos Papoutsakis","Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"pdf_url":"https://arxiv.org/pdf/2307.12179v3.pdf","comment":"This is the authors' version of the paper published at IEEE/CVF\n  Winter Conference on Applications of Computer Vision (WACV) 2025. The\n  definitive version is available at:\n  https://openaccess.thecvf.com/content/WACV2025/html/Gouidis_Recognizing_Unseen_States_of_Unknown_Objects_by_Leveraging_Knowledge_Graphs_WACV_2025_paper.html"},{"id":"http://arxiv.org/abs/2506.00868v2","updated":"2025-06-16T12:09:09Z","published":"2025-06-01T07:17:16Z","title":"Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\n  Person-Centric Visual and Conceptual Manipulations","summary":"  The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.\n","authors":["Parul Gupta","Shreya Ghosh","Tom Gedeon","Thanh-Toan Do","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2506.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09665v2","updated":"2025-06-16T12:02:05Z","published":"2025-06-11T12:36:49Z","title":"VideoMat: Extracting PBR Materials from Video Diffusion Models","summary":"  We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.\n","authors":["Jacob Munkberg","Zian Wang","Ruofan Liang","Tianchang Shen","Jon Hasselgren"],"pdf_url":"https://arxiv.org/pdf/2506.09665v2.pdf","comment":"Project website: https://nvlabs.github.io/videomat/"},{"id":"http://arxiv.org/abs/2506.13391v1","updated":"2025-06-16T11:56:50Z","published":"2025-06-16T11:56:50Z","title":"Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined\n  Likelihood Guided Diffusion Models","summary":"  Diffusion models have achieved remarkable success in imaging inverse problems\nowing to their powerful generative capabilities. However, existing approaches\ntypically rely on models trained for specific degradation types, limiting their\ngeneralizability to various degradation scenarios. To address this limitation,\nwe propose a zero-shot framework capable of handling various imaging inverse\nproblems without model retraining. We introduce a likelihood-guided noise\nrefinement mechanism that derives a closed-form approximation of the likelihood\nscore, simplifying score estimation and avoiding expensive gradient\ncomputations. This estimated score is subsequently utilized to refine the\nmodel-predicted noise, thereby better aligning the restoration process with the\ngenerative framework of diffusion models. In addition, we integrate the\nDenoising Diffusion Implicit Models (DDIM) sampling strategy to further improve\ninference efficiency. The proposed mechanism can be applied to both\noptimization-based and sampling-based schemes, providing an effective and\nflexible zero-shot solution for imaging inverse problems. Extensive experiments\ndemonstrate that our method achieves superior performance across multiple\ninverse problems, particularly in compressive sensing, delivering high-quality\nreconstructions even at an extremely low sampling rate (5%).\n","authors":["Zhen Wang","Hongyi Liu","Zhihui Wei"],"pdf_url":"https://arxiv.org/pdf/2506.13391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13387v1","updated":"2025-06-16T11:50:00Z","published":"2025-06-16T11:50:00Z","title":"TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast","summary":"  This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)\n","authors":["Beilei Cui","Yiming Huang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2506.13387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03662v3","updated":"2025-06-16T11:47:36Z","published":"2025-06-04T07:52:46Z","title":"Zero-Shot Temporal Interaction Localization for Egocentric Videos","summary":"  Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.\n","authors":["Erhang Zhang","Junyi Ma","Yin-Dong Zheng","Yixuan Zhou","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2506.03662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10750v3","updated":"2025-06-16T11:32:37Z","published":"2025-04-14T22:49:08Z","title":"Real-time Seafloor Segmentation and Mapping","summary":"  Posidonia oceanica meadows are a species of seagrass highly dependent on\nrocks for their survival and conservation. In recent years, there has been a\nconcerning global decline in this species, emphasizing the critical need for\nefficient monitoring and assessment tools. While deep learning-based semantic\nsegmentation and visual automated monitoring systems have shown promise in a\nvariety of applications, their performance in underwater environments remains\nchallenging due to complex water conditions and limited datasets. This paper\nintroduces a framework that combines machine learning and computer vision\ntechniques to enable an autonomous underwater vehicle (AUV) to inspect the\nboundaries of Posidonia oceanica meadows autonomously. The framework\nincorporates an image segmentation module using an existing Mask R-CNN model\nand a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a\nnew class dedicated to rocks is introduced to enhance the existing model,\naiming to contribute to a comprehensive monitoring approach and provide a\ndeeper understanding of the intricate interactions between the meadow and its\nsurrounding environment. The image segmentation model is validated using real\nunderwater images, while the overall inspection framework is evaluated in a\nrealistic simulation environment, replicating actual monitoring scenarios with\nreal underwater images. The results demonstrate that the proposed framework\nenables the AUV to autonomously accomplish the main tasks of underwater\ninspection and segmentation of rocks. Consequently, this work holds significant\npotential for the conservation and protection of marine environments, providing\nvaluable insights into the status of Posidonia oceanica meadows and supporting\ntargeted preservation efforts\n","authors":["Michele Grimaldi","Nouf Alkaabi","Francesco Ruscio","Sebastian Realpe Rua","Rafael Garcia","Nuno Gracias"],"pdf_url":"https://arxiv.org/pdf/2504.10750v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05061v3","updated":"2025-06-16T11:01:40Z","published":"2024-07-06T12:18:43Z","title":"Test-time Contrastive Concepts for Open-world Semantic Segmentation with\n  Vision-Language Models","summary":"  Recent CLIP-like Vision-Language Models (VLMs), pre-trained on large amounts\nof image-text pairs to align both modalities with a simple contrastive\nobjective, have paved the way to open-vocabulary semantic segmentation. Given\nan arbitrary set of textual queries, image pixels are assigned the closest\nquery in feature space. However, this works well when a user exhaustively lists\nall possible visual concepts in an image that contrast against each other for\nthe assignment. This corresponds to the current evaluation setup in the\nliterature, which relies on having access to a list of in-domain relevant\nconcepts, typically classes of a benchmark dataset. Here, we consider the more\nchallenging (and realistic) scenario of segmenting a single concept, given a\ntextual prompt and nothing else. To achieve good results, besides contrasting\nwith the generic 'background' text, we propose two different approaches to\nautomatically generate, at test time, query-specific textual contrastive\nconcepts. We do so by leveraging the distribution of text in the VLM's training\nset or crafted LLM prompts. We also propose a metric designed to evaluate this\nscenario and show the relevance of our approach on commonly used datasets.\n","authors":["Monika Wysoczańska","Antonin Vobecky","Amaia Cardiel","Tomasz Trzciński","Renaud Marlet","Andrei Bursuc","Oriane Siméoni"],"pdf_url":"https://arxiv.org/pdf/2407.05061v3.pdf","comment":"TMLR camera-ready"},{"id":"http://arxiv.org/abs/2506.13355v1","updated":"2025-06-16T10:54:28Z","published":"2025-06-16T10:54:28Z","title":"DicFace: Dirichlet-Constrained Variational Codebook Learning for\n  Temporally Coherent Video Face Restoration","summary":"  Video face restoration faces a critical challenge in maintaining temporal\nconsistency while recovering fine facial details from degraded inputs. This\npaper presents a novel approach that extends Vector-Quantized Variational\nAutoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a\nvideo restoration framework through variational latent space modeling. Our key\ninnovation lies in reformulating discrete codebook representations as\nDirichlet-distributed continuous variables, enabling probabilistic transitions\nbetween facial features across frames. A spatio-temporal Transformer\narchitecture jointly models inter-frame dependencies and predicts latent\ndistributions, while a Laplacian-constrained reconstruction loss combined with\nperceptual (LPIPS) regularization enhances both pixel accuracy and visual\nquality. Comprehensive evaluations on blind face restoration, video inpainting,\nand facial colorization tasks demonstrate state-of-the-art performance. This\nwork establishes an effective paradigm for adapting intensive image priors,\npretrained on high-quality images, to video restoration while addressing the\ncritical challenge of flicker artifacts. The source code has been open-sourced\nand is available at https://github.com/fudan-generative-vision/DicFace.\n","authors":["Yan Chen","Hanlin Shang","Ce Liu","Yuxuan Chen","Hui Li","Weihao Yuan","Hao Zhu","Zilong Dong","Siyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13348v1","updated":"2025-06-16T10:41:40Z","published":"2025-06-16T10:41:40Z","title":"TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian\n  Splatting","summary":"  Gaussian Splatting have demonstrated remarkable novel view synthesis\nperformance at high rendering frame rates. Optimization-based inverse rendering\nwithin complex capture scenarios remains however a challenging problem. A\nparticular case is modelling complex surface light interactions for highly\nreflective scenes, which results in intricate high frequency specular radiance\ncomponents. We hypothesize that such challenging settings can benefit from\nincreased representation power. We hence propose a method that tackles this\nissue through a geometrically and physically grounded Gaussian Splatting borne\nradiance field, where normals and material properties are spatially variable in\nthe primitive's local space. Using per-primitive texture maps for this purpose,\nwe also propose to harness the GPU hardware to accelerate rendering at test\ntime via unified material texture atlas.\n","authors":["Mae Younes","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2506.13348v1.pdf","comment":"Code will be available at https://github.com/maeyounes/TextureSplat"},{"id":"http://arxiv.org/abs/2506.09095v3","updated":"2025-06-16T10:28:22Z","published":"2025-06-10T12:14:05Z","title":"Foundation Models in Medical Imaging -- A Review and Outlook","summary":"  Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.\n","authors":["Vivien van Veldhuizen","Vanessa Botha","Chunyao Lu","Melis Erdal Cesur","Kevin Groot Lipman","Edwin D. de Jong","Hugo Horlings","Clárisa I. Sanchez","Cees G. M. Snoek","Lodewyk Wessels","Ritse Mann","Eric Marcus","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2506.09095v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13335v1","updated":"2025-06-16T10:25:23Z","published":"2025-06-16T10:25:23Z","title":"Advancing Image-Based Grapevine Variety Classification with a New\n  Benchmark and Evaluation of Masked Autoencoders","summary":"  Grapevine varieties are essential for the economies of many wine-producing\ncountries, influencing the production of wine, juice, and the consumption of\nfruits and leaves. Traditional identification methods, such as ampelography and\nmolecular analysis, have limitations: ampelography depends on expert knowledge\nand is inherently subjective, while molecular methods are costly and\ntime-intensive. To address these limitations, recent studies have applied deep\nlearning (DL) models to classify grapevine varieties using image data. However,\ndue to the small dataset sizes, these methods often depend on transfer learning\nfrom datasets from other domains, e.g., ImageNet1K (IN1K), which can lead to\nperformance degradation due to domain shift and supervision collapse. In this\ncontext, self-supervised learning (SSL) methods can be a good tool to avoid\nthis performance degradation, since they can learn directly from data, without\nexternal labels. This study presents an evaluation of Masked Autoencoders\n(MAEs) for identifying grapevine varieties based on field-acquired images. The\nmain contributions of this study include two benchmarks comprising 43 grapevine\nvarieties collected across different seasons, an analysis of MAE's application\nin the agricultural context, and a performance comparison of trained models\nacross seasons. Our results show that a ViT-B/16 model pre-trained with MAE and\nthe unlabeled dataset achieved an F1 score of 0.7956, outperforming all other\nmodels. Additionally, we observed that pre-trained models benefit from long\npre-training, perform well under low-data training regime, and that simple data\naugmentation methods are more effective than complex ones. The study also found\nthat the mask ratio in MAE impacts performance only marginally.\n","authors":["Gabriel A. Carneiro","Thierry J. Aubry","António Cunha","Petia Radeva","Joaquim Sousa"],"pdf_url":"https://arxiv.org/pdf/2506.13335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13327v1","updated":"2025-06-16T10:16:00Z","published":"2025-06-16T10:16:00Z","title":"Joint Analysis of Optical and SAR Vegetation Indices for Vineyard\n  Monitoring: Assessing Biomass Dynamics and Phenological Stages over Po\n  Valley, Italy","summary":"  Multi-polarized Synthetic Aperture Radar (SAR) technology has gained\nincreasing attention in agriculture, offering unique capabilities for\nmonitoring vegetation dynamics thanks to its all-weather, day-and-night\noperation and high revisit frequency. This study presents, for the first time,\na comprehensive analysis combining dual-polarimetric radar vegetation index\n(DpRVI) with optical indices to characterize vineyard crops. Vineyards exhibit\ndistinct non-isotropic scattering behavior due to their pronounced row\norientation, making them particularly challenging and interesting targets for\nremote sensing. The research further investigates the relationship between\nDpRVI and optical vegetation indices, demonstrating the complementary nature of\ntheir information. We demonstrate that DpRVI and optical indices provide\ncomplementary information, with low correlation suggesting that they capture\ndistinct vineyard features. Key findings reveal a parabolic trend in DpRVI over\nthe growing season, potentially linked to biomass dynamics estimated via the\nWinkler Index. Unlike optical indices reflecting vegetation greenness, DpRVI\nappears more directly related to biomass growth, aligning with specific\nphenological phases. Preliminary results also highlight the potential of DpRVI\nfor distinguishing vineyards from other crops. This research aligns with the\nobjectives of the PNRR-NODES project, which promotes nature-based solutions\n(NbS) for sustainable vineyard management. The application of DpRVI for\nmonitoring vineyards is part of integrating remote sensing techniques into the\nbroader field of strategies for climate-related change adaptation and risk\nreduction, emphasizing the role of innovative SAR-based monitoring in\nsustainable agriculture.\n","authors":["Andrea Bergamaschi","Abhinav Verma","Avik Bhattacharya","Fabio Dell'Acqua"],"pdf_url":"https://arxiv.org/pdf/2506.13327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13326v1","updated":"2025-06-16T10:15:38Z","published":"2025-06-16T10:15:38Z","title":"VIS-Shepherd: Constructing Critic for LLM-based Data Visualization\n  Generation","summary":"  Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd.\n","authors":["Bo Pan","Yixiao Fu","Ke Wang","Junyu Lu","Lunke Pan","Ziyang Qian","Yuhan Chen","Guoliang Wang","Yitao Zhou","Li Zheng","Yinghao Tang","Zhen Wen","Yuchen Wu","Junhua Lu","Biao Zhu","Minfeng Zhu","Bo Zhang","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13322v1","updated":"2025-06-16T10:10:56Z","published":"2025-06-16T10:10:56Z","title":"Active Multimodal Distillation for Few-shot Action Recognition","summary":"  Owing to its rapid progress and broad application prospects, few-shot action\nrecognition has attracted considerable interest. However, current methods are\npredominantly based on limited single-modal data, which does not fully exploit\nthe potential of multimodal information. This paper presents a novel framework\nthat actively identifies reliable modalities for each sample using\ntask-specific contextual cues, thus significantly improving recognition\nperformance. Our framework integrates an Active Sample Inference (ASI) module,\nwhich utilizes active inference to predict reliable modalities based on\nposterior distributions and subsequently organizes them accordingly. Unlike\nreinforcement learning, active inference replaces rewards with evidence-based\npreferences, making more stable predictions. Additionally, we introduce an\nactive mutual distillation module that enhances the representation learning of\nless reliable modalities by transferring knowledge from more reliable ones.\nAdaptive multimodal inference is employed during the meta-test to assign higher\nweights to reliable modalities. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches.\n","authors":["Weijia Feng","Yichen Zhu","Ruojia Zhang","Chenyang Wang","Fei Ma","Xiaobao Wang","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2506.13322v1.pdf","comment":"IJCAI 2025, the 34th International Joint Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2506.13320v1","updated":"2025-06-16T10:09:32Z","published":"2025-06-16T10:09:32Z","title":"Action Dubber: Timing Audible Actions via Inflectional Flow","summary":"  We introduce the task of Audible Action Temporal Localization, which aims to\nidentify the spatio-temporal coordinates of audible movements. Unlike\nconventional tasks such as action recognition and temporal action localization,\nwhich broadly analyze video content, our task focuses on the distinct kinematic\ndynamics of audible actions. It is based on the premise that key actions are\ndriven by inflectional movements; for example, collisions that produce sound\noften involve abrupt changes in motion. To capture this, we propose\n$TA^{2}Net$, a novel architecture that estimates inflectional flow using the\nsecond derivative of motion to determine collision timings without relying on\naudio input. $TA^{2}Net$ also integrates a self-supervised spatial localization\nstrategy during training, combining contrastive learning with spatial analysis.\nThis dual design improves temporal localization accuracy and simultaneously\nidentifies sound sources within video frames. To support this task, we\nintroduce a new benchmark dataset, $Audible623$, derived from Kinetics and\nUCF101 by removing non-essential vocalization subsets. Extensive experiments\nconfirm the effectiveness of our approach on $Audible623$ and show strong\ngeneralizability to other domains, such as repetitive counting and sound source\nlocalization. Code and dataset are available at\nhttps://github.com/WenlongWan/Audible623.\n","authors":["Wenlong Wan","Weiying Zheng","Tianyi Xiang","Guiqing Li","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2506.13320v1.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2405.18302v2","updated":"2025-06-16T10:04:46Z","published":"2024-05-28T15:57:58Z","title":"Deep Network Pruning: A Comparative Study on CNNs in Face Recognition","summary":"  The widespread use of mobile devices for all kinds of transactions makes\nnecessary reliable and real-time identity authentication, leading to the\nadoption of face recognition (FR) via the cameras embedded in such devices.\nProgress of deep Convolutional Neural Networks (CNNs) has provided substantial\nadvances in FR. Nonetheless, the size of state-of-the-art architectures is\nunsuitable for mobile deployment, since they often encompass hundreds of\nmegabytes and millions of parameters. We address this by studying methods for\ndeep network compression applied to FR. In particular, we apply network pruning\nbased on Taylor scores, where less important filters are removed iteratively.\nThe method is tested on three networks based on the small SqueezeNet (1.24M\nparameters) and the popular MobileNetv2 (3.5M) and ResNet50 (23.5M)\narchitectures. These have been selected to showcase the method on CNNs with\ndifferent complexities and sizes. We observe that a substantial percentage of\nfilters can be removed with minimal performance loss. Also, filters with the\nhighest amount of output channels tend to be removed first, suggesting that\nhigh-dimensional spaces within popular CNNs are over-dimensioned.\n","authors":["Fernando Alonso-Fernandez","Kevin Hernandez-Diaz","Jose Maria Buades Rubio","Prayag Tiwari","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2405.18302v2.pdf","comment":"Accepted at Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2506.13307v1","updated":"2025-06-16T09:48:01Z","published":"2025-06-16T09:48:01Z","title":"Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent\n  Diffusion Models in the Generation of Unseen SAR Image Concepts","summary":"  This work investigates the adaptation of large pre-trained latent diffusion\nmodels to a radically new imaging domain: Synthetic Aperture Radar (SAR). While\nthese generative models, originally trained on natural images, demonstrate\nimpressive capabilities in text-to-image synthesis, they are not natively\nadapted to represent SAR data, which involves different physics, statistical\ndistributions, and visual characteristics. Using a sizeable SAR dataset (on the\norder of 100,000 to 1 million images), we address the fundamental question of\nfine-tuning such models for this unseen modality. We explore and compare\nmultiple fine-tuning strategies, including full model fine-tuning and\nparameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing\nseparately on the UNet diffusion backbone and the text encoder components. To\nevaluate generative quality, we combine several metrics: statistical distance\nfrom real SAR distributions, textural similarity via GLCM descriptors, and\nsemantic alignment assessed with a CLIP model fine-tuned on SAR data. Our\nresults show that a hybrid tuning strategy yields the best performance: full\nfine-tuning of the UNet is better at capturing low-level SAR-specific patterns,\nwhile LoRA-based partial tuning of the text encoder, combined with embedding\nlearning of the <SAR> token, suffices to preserve prompt alignment. This work\nprovides a methodical strategy for adapting foundation models to unconventional\nimaging modalities beyond natural image domains.\n","authors":["Solène Debuysère","Nicolas Trouvé","Nathan Letheule","Olivier Lévêque","Elise Colin"],"pdf_url":"https://arxiv.org/pdf/2506.13307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05066v3","updated":"2025-06-16T09:48:00Z","published":"2025-02-07T16:39:39Z","title":"Beautiful Images, Toxic Words: Understanding and Addressing Offensive\n  Text in Generated Images","summary":"  State-of-the-art Diffusion Models (DMs) produce highly realistic images.\nWhile prior work has successfully mitigated Not Safe For Work (NSFW) content in\nthe visual domain, we identify a novel threat: the generation of NSFW text\nembedded within images. This includes offensive language, such as insults,\nracial slurs, and sexually explicit terms, posing significant risks to users.\nWe show that all state-of-the-art DMs (e.g., SD3, SDXL, Flux, DeepFloyd IF) are\nvulnerable to this issue. Through extensive experiments, we demonstrate that\nexisting mitigation techniques, effective for visual content, fail to prevent\nharmful text generation while substantially degrading benign text generation.\nAs an initial step toward addressing this threat, we introduce a novel\nfine-tuning strategy that targets only the text-generation layers in DMs.\nTherefore, we construct a safety fine-tuning dataset by pairing each NSFW\nprompt with two images: one with the NSFW term, and another where that term is\nreplaced with a carefully crafted benign alternative while leaving the image\nunchanged otherwise. By training on this dataset, the model learns to avoid\ngenerating harmful text while preserving benign content and overall image\nquality. Finally, to advance research in the area, we release ToxicBench, an\nopen-source benchmark for evaluating NSFW text generation in images. It\nincludes our curated fine-tuning dataset, a set of harmful prompts, new\nevaluation metrics, and a pipeline that assesses both NSFW-ness and text and\nimage quality. Our benchmark aims to guide future efforts in mitigating NSFW\ntext generation in text-to-image models, thereby contributing to their safe\ndeployment. The benchmark is available online for download.\n","authors":["Aditya Kumar","Tom Blanchard","Adam Dziedzic","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2502.05066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13306v1","updated":"2025-06-16T09:46:46Z","published":"2025-06-16T09:46:46Z","title":"Brain Imaging Foundation Models, Are We There Yet? A Systematic Review\n  of Foundation Models for Brain Imaging and Biomedical Research","summary":"  Foundation models (FMs), large neural networks pretrained on extensive and\ndiverse datasets, have revolutionized artificial intelligence and shown\nsignificant promise in medical imaging by enabling robust performance with\nlimited labeled data. Although numerous surveys have reviewed the application\nof FM in healthcare care, brain imaging remains underrepresented, despite its\ncritical role in the diagnosis and treatment of neurological diseases using\nmodalities such as MRI, CT, and PET. Existing reviews either marginalize brain\nimaging or lack depth on the unique challenges and requirements of FM in this\ndomain, such as multimodal data integration, support for diverse clinical\ntasks, and handling of heterogeneous, fragmented datasets.\n  To address this gap, we present the first comprehensive and curated review of\nFMs for brain imaging. We systematically analyze 161 brain imaging datasets and\n86 FM architectures, providing information on key design choices, training\nparadigms, and optimizations driving recent advances. Our review highlights the\nleading models for various brain imaging tasks, summarizes their innovations,\nand critically examines current limitations and blind spots in the literature.\nWe conclude by outlining future research directions to advance FM applications\nin brain imaging, with the aim of fostering progress in both clinical and\nresearch settings.\n","authors":["Salah Ghamizi","Georgia Kanli","Yu Deng","Magali Perquin","Olivier Keunen"],"pdf_url":"https://arxiv.org/pdf/2506.13306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13301v1","updated":"2025-06-16T09:42:38Z","published":"2025-06-16T09:42:38Z","title":"AttentionDrag: Exploiting Latent Correlation Knowledge in Pre-trained\n  Diffusion Models for Image Editing","summary":"  Traditional point-based image editing methods rely on iterative latent\noptimization or geometric transformations, which are either inefficient in\ntheir processing or fail to capture the semantic relationships within the\nimage. These methods often overlook the powerful yet underutilized image\nediting capabilities inherent in pre-trained diffusion models. In this work, we\npropose a novel one-step point-based image editing method, named AttentionDrag,\nwhich leverages the inherent latent knowledge and feature correlations within\npre-trained diffusion models for image editing tasks. This framework enables\nsemantic consistency and high-quality manipulation without the need for\nextensive re-optimization or retraining. Specifically, we reutilize the latent\ncorrelations knowledge learned by the self-attention mechanism in the U-Net\nmodule during the DDIM inversion process to automatically identify and adjust\nrelevant image regions, ensuring semantic validity and consistency.\nAdditionally, AttentionDrag adaptively generates masks to guide the editing\nprocess, enabling precise and context-aware modifications with friendly\ninteraction. Our results demonstrate a performance that surpasses most\nstate-of-the-art methods with significantly faster speeds, showing a more\nefficient and semantically coherent solution for point-based image editing\ntasks.\n","authors":["Biao Yang","Muqi Huang","Yuhui Zhang","Yun Xiong","Kun Zhou","Xi Chen","Shiyang Zhou","Huishuai Bao","Chuan Li","Feng Shi","Hualei Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13298v1","updated":"2025-06-16T09:40:32Z","published":"2025-06-16T09:40:32Z","title":"Fair Generation without Unfair Distortions: Debiasing Text-to-Image\n  Generation with Entanglement-Free Attention","summary":"  Recent advancements in diffusion-based text-to-image (T2I) models have\nenabled the generation of high-quality and photorealistic images from text\ndescriptions. However, they often exhibit societal biases related to gender,\nrace, and socioeconomic status, thereby reinforcing harmful stereotypes and\nshaping public perception in unintended ways. While existing bias mitigation\nmethods demonstrate effectiveness, they often encounter attribute entanglement,\nwhere adjustments to attributes relevant to the bias (i.e., target attributes)\nunintentionally alter attributes unassociated with the bias (i.e., non-target\nattributes), causing undesirable distribution shifts. To address this\nchallenge, we introduce Entanglement-Free Attention (EFA), a method that\naccurately incorporates target attributes (e.g., White, Black, Asian, and\nIndian) while preserving non-target attributes (e.g., background details)\nduring bias mitigation. At inference time, EFA randomly samples a target\nattribute with equal probability and adjusts the cross-attention in selected\nlayers to incorporate the sampled attribute, achieving a fair distribution of\ntarget attributes. Extensive experiments demonstrate that EFA outperforms\nexisting methods in mitigating bias while preserving non-target attributes,\nthereby maintaining the output distribution and generation capability of the\noriginal model.\n","authors":["Jeonghoon Park","Juyoung Lee","Chaeyeon Chung","Jaeseong Lee","Jaegul Choo","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2506.13298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15894v2","updated":"2025-06-16T09:39:37Z","published":"2025-02-21T19:28:05Z","title":"RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion\n  Transformers","summary":"  Recent advancements in video generation have enabled models to synthesize\nhigh-quality, minute-long videos. However, generating even longer videos with\ntemporal coherence remains a major challenge and existing length extrapolation\nmethods lead to temporal repetition or motion deceleration. In this work, we\nsystematically analyze the role of frequency components in positional\nembeddings and identify an intrinsic frequency that primarily governs\nextrapolation behavior. Based on this insight, we propose RIFLEx, a minimal yet\neffective approach that reduces the intrinsic frequency to suppress repetition\nwhile preserving motion consistency, without requiring any additional\nmodifications. RIFLEx offers a true free lunch--achieving high-quality 2x\nextrapolation on state-of-the-art video diffusion transformers in a completely\ntraining-free manner. Moreover, it enhances quality and enables 3x\nextrapolation by minimal fine-tuning without long videos. Project page and\ncodes: https://riflex-video.github.io/.\n","authors":["Min Zhao","Guande He","Yixiao Chen","Hongzhou Zhu","Chongxuan Li","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.15894v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.13292v1","updated":"2025-06-16T09:33:37Z","published":"2025-06-16T09:33:37Z","title":"Automatic Multi-View X-Ray/CT Registration Using Bone Substructure\n  Contours","summary":"  Purpose: Accurate intraoperative X-ray/CT registration is essential for\nsurgical navigation in orthopedic procedures. However, existing methods\nstruggle with consistently achieving sub-millimeter accuracy, robustness under\nbroad initial pose estimates or need manual key-point annotations. This work\naims to address these challenges by proposing a novel multi-view X-ray/CT\nregistration method for intraoperative bone registration. Methods: The proposed\nregistration method consists of a multi-view, contour-based iterative closest\npoint (ICP) optimization. Unlike previous methods, which attempt to match bone\ncontours across the entire silhouette in both imaging modalities, we focus on\nmatching specific subcategories of contours corresponding to bone\nsubstructures. This leads to reduced ambiguity in the ICP matches, resulting in\na more robust and accurate registration solution. This approach requires only\ntwo X-ray images and operates fully automatically. Additionally, we contribute\na dataset of 5 cadaveric specimens, including real X-ray images, X-ray image\nposes and the corresponding CT scans. Results: The proposed registration method\nis evaluated on real X-ray images using mean reprojection error (mRPD). The\nmethod consistently achieves sub-millimeter accuracy with a mRPD 0.67mm\ncompared to 5.35mm by a commercial solution requiring manual intervention.\nFurthermore, the method offers improved practical applicability, being fully\nautomatic. Conclusion: Our method offers a practical, accurate, and efficient\nsolution for multi-view X-ray/CT registration in orthopedic surgeries, which\ncan be easily combined with tracking systems. By improving registration\naccuracy and minimizing manual intervention, it enhances intraoperative\nnavigation, contributing to more accurate and effective surgical outcomes in\ncomputer-assisted surgery (CAS).\n","authors":["Roman Flepp","Leon Nissen","Bastian Sigrist","Arend Nieuwland","Nicola Cavalcanti","Philipp Fürnstahl","Thomas Dreher","Lilian Calvet"],"pdf_url":"https://arxiv.org/pdf/2506.13292v1.pdf","comment":"This paper was accepted to IPCAI 2025"},{"id":"http://arxiv.org/abs/2506.13282v1","updated":"2025-06-16T09:21:01Z","published":"2025-06-16T09:21:01Z","title":"Anomaly Object Segmentation with Vision-Language Models for Steel Scrap\n  Recycling","summary":"  Recycling steel scrap can reduce carbon dioxide (CO2) emissions from the\nsteel industry. However, a significant challenge in steel scrap recycling is\nthe inclusion of impurities other than steel. To address this issue, we propose\nvision-language-model-based anomaly detection where a model is finetuned in a\nsupervised manner, enabling it to handle niche objects effectively. This model\nenables automated detection of anomalies at a fine-grained level within steel\nscrap. Specifically, we finetune the image encoder, equipped with multi-scale\nmechanism and text prompts aligned with both normal and anomaly images. The\nfinetuning process trains these modules using a multiclass classification as\nthe supervision.\n","authors":["Daichi Tanaka","Takumi Karasawa","Shu Takenouchi","Rei Kawakami"],"pdf_url":"https://arxiv.org/pdf/2506.13282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16458v2","updated":"2025-06-16T09:20:38Z","published":"2025-01-27T19:37:18Z","title":"BiFold: Bimanual Cloth Folding with Language Guidance","summary":"  Cloth folding is a complex task due to the inevitable self-occlusions of\nclothes, their complicated dynamics, and the disparate materials, geometries,\nand textures that garments can have. In this work, we learn folding actions\nconditioned on text commands. Translating high-level, abstract instructions\ninto precise robotic actions requires sophisticated language understanding and\nmanipulation capabilities. To do that, we leverage a pre-trained\nvision-language model and repurpose it to predict manipulation actions. Our\nmodel, BiFold, can take context into account and achieves state-of-the-art\nperformance on an existing language-conditioned folding benchmark. To address\nthe lack of annotated bimanual folding data, we introduce a novel dataset with\nautomatically parsed actions and language-aligned instructions, enabling better\nlearning of text-conditioned manipulation. BiFold attains the best performance\non our dataset and demonstrates strong generalization to new instructions,\ngarments, and environments.\n","authors":["Oriol Barbany","Adrià Colomé","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2501.16458v2.pdf","comment":"Accepted at ICRA 2025. Project page at\n  https://barbany.github.io/bifold/"},{"id":"http://arxiv.org/abs/2403.13238v3","updated":"2025-06-16T09:18:52Z","published":"2024-03-20T01:59:43Z","title":"Learning Coherent Matrixized Representation in Latent Space for\n  Volumetric 4D Generation","summary":"  Directly learning to model 4D content, including shape, color, and motion, is\nchallenging. Existing methods rely on pose priors for motion control, resulting\nin limited motion diversity and continuity in details. To address this, we\npropose a framework that generates volumetric 4D sequences, where 3D shapes are\nanimated under given conditions (text-image guidance) with dynamic evolution in\nshape and color across spatial and temporal dimensions, allowing for free\nnavigation and rendering from any direction. We first use a coherent 3D shape\nand color modeling to encode the shape and color of each detailed 3D geometry\nframe into a latent space. Then we propose a matrixized 4D sequence\nrepresentation allowing efficient diffusion model operation. Finally, we\nintroduce spatio-temporal diffusion for 4D volumetric generation under given\nimages and text prompts. Extensive experiments on the ShapeNet, 3DBiCar,\nDeformingThings4D and Objaverse datasets for several tasks demonstrate that our\nmethod effectively learns to generate high quality 3D shapes with consistent\ncolor and coherent mesh animations, improving over the current methods. Our\ncode will be publicly available.\n","authors":["Qitong Yang","Mingtao Feng","Zijie Wu","Shijie Sun","Weisheng Dong","Yaonan Wang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.13238v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09323v2","updated":"2025-06-16T09:18:30Z","published":"2024-12-12T14:48:46Z","title":"T-SVG: Text-Driven Stereoscopic Video Generation","summary":"  The advent of stereoscopic videos has opened new horizons in multimedia,\nparticularly in extended reality (XR) and virtual reality (VR) applications,\nwhere immersive content captivates audiences across various platforms. Despite\nits growing popularity, producing stereoscopic videos remains challenging due\nto the technical complexities involved in generating stereo parallax. This\nrefers to the positional differences of objects viewed from two distinct\nperspectives and is crucial for creating depth perception. This complex process\nposes significant challenges for creators aiming to deliver convincing and\nengaging presentations. To address these challenges, this paper introduces the\nText-driven Stereoscopic Video Generation (T-SVG) system. This innovative,\nmodel-agnostic, zero-shot approach streamlines video generation by using text\nprompts to create reference videos. These videos are transformed into 3D point\ncloud sequences, which are rendered from two perspectives with subtle parallax\ndifferences, achieving a natural stereoscopic effect. T-SVG represents a\nsignificant advancement in stereoscopic content creation by integrating\nstate-of-the-art, training-free techniques in text-to-video generation, depth\nestimation, and video inpainting. Its flexible architecture ensures high\nefficiency and user-friendliness, allowing seamless updates with newer models\nwithout retraining. By simplifying the production pipeline, T-SVG makes\nstereoscopic video generation accessible to a broader audience, demonstrating\nits potential to revolutionize the field.\n","authors":["Qiao Jin","Xiaodong Chen","Wu Liu","Tao Mei","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09323v2.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.13277v1","updated":"2025-06-16T09:16:40Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huyang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13265v1","updated":"2025-06-16T09:03:51Z","published":"2025-06-16T09:03:51Z","title":"Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware\n  Learning","summary":"  Autonomous vehicles that navigate in open-world environments may encounter\npreviously unseen object classes. However, most existing LiDAR panoptic\nsegmentation models rely on closed-set assumptions, failing to detect unknown\nobject instances. In this work, we propose ULOPS, an uncertainty-guided\nopen-set panoptic segmentation framework that leverages Dirichlet-based\nevidential learning to model predictive uncertainty. Our architecture\nincorporates separate decoders for semantic segmentation with uncertainty\nestimation, embedding with prototype association, and instance center\nprediction. During inference, we leverage uncertainty estimates to identify and\nsegment unknown instances. To strengthen the model's ability to differentiate\nbetween known and unknown objects, we introduce three uncertainty-driven loss\nfunctions. Uniform Evidence Loss to encourage high uncertainty in unknown\nregions. Adaptive Uncertainty Separation Loss ensures a consistent difference\nin uncertainty estimates between known and unknown objects at a global scale.\nContrastive Uncertainty Loss refines this separation at the fine-grained level.\nTo evaluate open-set performance, we extend benchmark settings on KITTI-360 and\nintroduce a new open-set evaluation for nuScenes. Extensive experiments\ndemonstrate that ULOPS consistently outperforms existing open-set LiDAR\npanoptic segmentation methods.\n","authors":["Rohit Mohan","Julia Hindel","Florian Drews","Claudius Gläser","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2506.13265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13260v1","updated":"2025-06-16T09:01:09Z","published":"2025-06-16T09:01:09Z","title":"COME: Adding Scene-Centric Forecasting Control to Occupancy World Model","summary":"  World models are critical for autonomous driving to simulate environmental\ndynamics and generate synthetic data. Existing methods struggle to disentangle\nego-vehicle motion (perspective shifts) from scene evolvement (agent\ninteractions), leading to suboptimal predictions. Instead, we propose to\nseparate environmental changes from ego-motion by leveraging the scene-centric\ncoordinate systems. In this paper, we introduce COME: a framework that\nintegrates scene-centric forecasting Control into the Occupancy world ModEl.\nSpecifically, COME first generates ego-irrelevant, spatially consistent future\nfeatures through a scene-centric prediction branch, which are then converted\ninto scene condition using a tailored ControlNet. These condition features are\nsubsequently injected into the occupancy world model, enabling more accurate\nand controllable future occupancy predictions. Experimental results on the\nnuScenes-Occ3D dataset show that COME achieves consistent and significant\nimprovements over state-of-the-art (SOTA) methods across diverse\nconfigurations, including different input sources (ground-truth, camera-based,\nfusion-based occupancy) and prediction horizons (3s and 8s). For example, under\nthe same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7%\nbetter mIoU metric than UniScene. These results highlight the efficacy of\ndisentangled representation learning in enhancing spatio-temporal prediction\nfidelity for world models. Code and videos will be available at\nhttps://github.com/synsin0/COME.\n","authors":["Yining Shi","Kun Jiang","Qiang Meng","Ke Wang","Jiabao Wang","Wenchao Sun","Tuopu Wen","Mengmeng Yang","Diange Yang"],"pdf_url":"https://arxiv.org/pdf/2506.13260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08915v2","updated":"2025-06-16T08:52:37Z","published":"2025-06-10T15:41:22Z","title":"Inherently Faithful Attention Maps for Vision Transformers","summary":"  We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds. Code: https://github.com/ananthu-aniraj/ifam\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2506.08915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10600v2","updated":"2025-06-16T08:50:31Z","published":"2025-06-12T11:43:50Z","title":"EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence","summary":"  Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.\n","authors":["Xinjie Wang","Liu Liu","Yu Cao","Ruiqi Wu","Wenkang Qin","Dehui Wang","Wei Sui","Zhizhong Su"],"pdf_url":"https://arxiv.org/pdf/2506.10600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18458v2","updated":"2025-06-16T08:33:51Z","published":"2024-04-29T06:32:28Z","title":"A robust and scalable framework for hallucination detection in virtual\n  tissue staining and digital pathology","summary":"  Histopathological staining of human tissue is essential for disease\ndiagnosis. Recent advances in virtual tissue staining technologies using\nartificial intelligence (AI) alleviate some of the costly and tedious steps\ninvolved in traditional histochemical staining processes, permitting\nmultiplexed staining and tissue preservation. However, potential hallucinations\nand artifacts in these virtually stained tissue images pose concerns,\nespecially for the clinical uses of these approaches. Quality assessment of\nhistology images by experts can be subjective. Here, we present an autonomous\nquality and hallucination assessment method, AQuA, for virtual tissue staining\nand digital pathology. AQuA autonomously achieves 99.8% accuracy when detecting\nacceptable and unacceptable virtually stained tissue images without access to\nhistochemically stained ground truth, and presents an agreement of 98.5% with\nthe manual assessments made by board-certified pathologists, including\nidentifying realistic-looking images that could mislead diagnosticians. We\ndemonstrate the wide adaptability of AQuA across various virtually and\nhistochemically stained human tissue images. This framework enhances the\nreliability of virtual tissue staining and provides autonomous quality\nassurance for image generation and transformation tasks in digital pathology\nand computational imaging.\n","authors":["Luzhe Huang","Yuzhu Li","Nir Pillar","Tal Keidar Haran","William Dean Wallace","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2404.18458v2.pdf","comment":"45 Pages, 22 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2506.13233v1","updated":"2025-06-16T08:32:57Z","published":"2025-06-16T08:32:57Z","title":"High-Quality Facial Albedo Generation for 3D Face Reconstruction from a\n  Single Image using a Coarse-to-Fine Approach","summary":"  Facial texture generation is crucial for high-fidelity 3D face reconstruction\nfrom a single image. However, existing methods struggle to generate UV albedo\nmaps with high-frequency details. To address this challenge, we propose a novel\nend-to-end coarse-to-fine approach for UV albedo map generation. Our method\nfirst utilizes a UV Albedo Parametric Model (UVAPM), driven by low-dimensional\ncoefficients, to generate coarse albedo maps with skin tones and low-frequency\ntexture details. To capture high-frequency details, we train a detail generator\nusing a decoupled albedo map dataset, producing high-resolution albedo maps.\nExtensive experiments demonstrate that our method can generate high-fidelity\ntextures from a single image, outperforming existing methods in terms of\ntexture quality and realism. The code and pre-trained model are publicly\navailable at https://github.com/MVIC-DAI/UVAPM, facilitating reproducibility\nand further research.\n","authors":["Jiashu Dai","Along Wang","Binfan Ni","Tao Cao"],"pdf_url":"https://arxiv.org/pdf/2506.13233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18500v2","updated":"2025-06-16T08:32:29Z","published":"2025-01-30T17:10:53Z","title":"HSRMamba: Contextual Spatial-Spectral State Space Model for Single Image\n  Hyperspectral Super-Resolution","summary":"  Mamba has demonstrated exceptional performance in visual tasks due to its\npowerful global modeling capabilities and linear computational complexity,\noffering considerable potential in hyperspectral image super-resolution\n(HSISR). However, in HSISR, Mamba faces challenges as transforming images into\n1D sequences neglects the spatial-spectral structural relationships between\nlocally adjacent pixels, and its performance is highly sensitive to input\norder, which affects the restoration of both spatial and spectral details. In\nthis paper, we propose HSRMamba, a contextual spatial-spectral modeling state\nspace model for HSISR, to address these issues both locally and globally.\nSpecifically, a local spatial-spectral partitioning mechanism is designed to\nestablish patch-wise causal relationships among adjacent pixels in 3D features,\nmitigating the local forgetting issue. Furthermore, a global spectral\nreordering strategy based on spectral similarity is employed to enhance the\ncausal representation of similar pixels across both spatial and spectral\ndimensions. Finally, experimental results demonstrate our HSRMamba outperforms\nthe state-of-the-art methods in quantitative quality and visual results. Code\nis available at: https://github.com/Tomchenshi/HSRMamba.\n","authors":["Shi Chen","Lefei Zhang","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13224v1","updated":"2025-06-16T08:22:11Z","published":"2025-06-16T08:22:11Z","title":"SASep: Saliency-Aware Structured Separation of Geometry and Feature for\n  Open Set Learning on Point Clouds","summary":"  Recent advancements in deep learning have greatly enhanced 3D object\nrecognition, but most models are limited to closed-set scenarios, unable to\nhandle unknown samples in real-world applications. Open-set recognition (OSR)\naddresses this limitation by enabling models to both classify known classes and\nidentify novel classes. However, current OSR methods rely on global features to\ndifferentiate known and unknown classes, treating the entire object uniformly\nand overlooking the varying semantic importance of its different parts. To\naddress this gap, we propose Salience-Aware Structured Separation (SASep),\nwhich includes (i) a tunable semantic decomposition (TSD) module to\nsemantically decompose objects into important and unimportant parts, (ii) a\ngeometric synthesis strategy (GSS) to generate pseudo-unknown objects by\ncombining these unimportant parts, and (iii) a synth-aided margin separation\n(SMS) module to enhance feature-level separation by expanding the feature\ndistributions between classes. Together, these components improve both\ngeometric and feature representations, enhancing the model's ability to\neffectively distinguish known and unknown classes. Experimental results show\nthat SASep achieves superior performance in 3D OSR, outperforming existing\nstate-of-the-art methods.\n","authors":["Jinfeng Xu","Xianzhi Li","Yuan Tang","Xu Han","Qiao Yu","Yixue Hao","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13224v1.pdf","comment":"10 pages, conference"},{"id":"http://arxiv.org/abs/2506.13215v1","updated":"2025-06-16T08:15:22Z","published":"2025-06-16T08:15:22Z","title":"DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior\n  for Multi-View Stereo","summary":"  Recently, patch deformation-based methods have demonstrated significant\neffectiveness in multi-view stereo due to their incorporation of deformable and\nexpandable perception for reconstructing textureless areas. However, these\nmethods generally focus on identifying reliable pixel correlations to mitigate\nmatching ambiguity of patch deformation, while neglecting the deformation\ninstability caused by edge-skipping and visibility occlusions, which may cause\npotential estimation deviations. To address these issues, we propose DVP-MVS++,\nan innovative approach that synergizes both depth-normal-edge aligned and\nharmonized cross-view priors for robust and visibility-aware patch deformation.\nSpecifically, to avoid edge-skipping, we first apply DepthPro, Metric3Dv2 and\nRoberts operator to generate coarse depth maps, normal maps and edge maps,\nrespectively. These maps are then aligned via an erosion-dilation strategy to\nproduce fine-grained homogeneous boundaries for facilitating robust patch\ndeformation. Moreover, we reformulate view selection weights as visibility\nmaps, and then implement both an enhanced cross-view depth reprojection and an\narea-maximization strategy to help reliably restore visible areas and\neffectively balance deformed patch, thus acquiring harmonized cross-view priors\nfor visibility-aware patch deformation. Additionally, we obtain geometry\nconsistency by adopting both aggregated normals via view selection and\nprojection depth differences via epipolar lines, and then employ SHIQ for\nhighlight correction to enable geometry consistency with highlight-aware\nperception, thus improving reconstruction quality during propagation and\nrefinement stage. Evaluation results on ETH3D, Tanks & Temples and Strecha\ndatasets exhibit the state-of-the-art performance and robust generalization\ncapability of our proposed method.\n","authors":["Zhenlong Yuan","Dapeng Zhang","Zehao Li","Chengxuan Qian","Jianing Chen","Yinda Chen","Kehua Chen","Tianlu Mao","Zhaoxin Li","Hao Jiang","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07497v3","updated":"2025-06-16T08:12:30Z","published":"2025-06-09T07:20:49Z","title":"Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and\n  Cross-Modal Consistency","summary":"  We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.\n","authors":["Xiangyu Guo","Zhanqian Wu","Kaixin Xiong","Ziyang Xu","Lijun Zhou","Gangwei Xu","Shaoqing Xu","Haiyang Sun","Bing Wang","Guang Chen","Hangjun Ye","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.07497v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13201v1","updated":"2025-06-16T08:06:18Z","published":"2025-06-16T08:06:18Z","title":"A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping","summary":"  Flooding remains a major global challenge, worsened by climate change and\nurbanization, demanding advanced solutions for effective disaster management.\nWhile traditional 2D flood mapping techniques provide limited insights, 3D\nflood mapping, powered by deep learning (DL), offers enhanced capabilities by\nintegrating flood extent and depth. This paper presents a comprehensive survey\nof deep learning-based 3D flood mapping, emphasizing its advancements over 2D\nmaps by integrating flood extent and depth for effective disaster management\nand urban planning. The survey categorizes deep learning techniques into task\ndecomposition and end-to-end approaches, applicable to both static and dynamic\nflood features. We compare key DL architectures, highlighting their respective\nroles in enhancing prediction accuracy and computational efficiency.\nAdditionally, this work explores diverse data sources such as digital elevation\nmodels, satellite imagery, rainfall, and simulated data, outlining their roles\nin 3D flood mapping. The applications reviewed range from real-time flood\nprediction to long-term urban planning and risk assessment. However,\nsignificant challenges persist, including data scarcity, model\ninterpretability, and integration with traditional hydrodynamic models. This\nsurvey concludes by suggesting future directions to address these limitations,\nfocusing on enhanced datasets, improved models, and policy implications for\nflood management. This survey aims to guide researchers and practitioners in\nleveraging DL techniques for more robust and reliable 3D flood mapping,\nfostering improved flood management strategies.\n","authors":["Wenfeng Jia","Bin Liang","Yuxi Liu","Muhammad Arif Khan","Lihong Zheng"],"pdf_url":"https://arxiv.org/pdf/2506.13201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13195v1","updated":"2025-06-16T08:01:14Z","published":"2025-06-16T08:01:14Z","title":"ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework\n  for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs","summary":"  Dental diagnosis relies on two primary imaging modalities: panoramic\nradiographs (PX) providing 2D oral cavity representations, and Cone-Beam\nComputed Tomography (CBCT) offering detailed 3D anatomical information. While\nPX images are cost-effective and accessible, their lack of depth information\nlimits diagnostic accuracy. CBCT addresses this but presents drawbacks\nincluding higher costs, increased radiation exposure, and limited\naccessibility. Existing reconstruction models further complicate the process by\nrequiring CBCT flattening or prior dental arch information, often unavailable\nclinically. We introduce ViT-NeBLa, a vision transformer-based Neural\nBeer-Lambert model enabling accurate 3D reconstruction directly from single PX.\nOur key innovations include: (1) enhancing the NeBLa framework with Vision\nTransformers for improved reconstruction capabilities without requiring CBCT\nflattening or prior dental arch information, (2) implementing a novel\nhorseshoe-shaped point sampling strategy with non-intersecting rays that\neliminates intermediate density aggregation required by existing models due to\nintersecting rays, reducing sampling point computations by $52 \\%$, (3)\nreplacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior\nglobal and local feature extraction, and (4) implementing learnable hash\npositional encoding for better higher-dimensional representation of 3D sample\npoints compared to existing Fourier-based dense positional encoding.\nExperiments demonstrate that ViT-NeBLa significantly outperforms prior\nstate-of-the-art methods both quantitatively and qualitatively, offering a\ncost-effective, radiation-efficient alternative for enhanced dental\ndiagnostics.\n","authors":["Bikram Keshari Parida","Anusree P. Sunilkumar","Abhijit Sen","Wonsang You"],"pdf_url":"https://arxiv.org/pdf/2506.13195v1.pdf","comment":"10 figures, 19 pages"},{"id":"http://arxiv.org/abs/2506.13187v1","updated":"2025-06-16T07:55:14Z","published":"2025-06-16T07:55:14Z","title":"Dynamic Context-oriented Decomposition for Task-aware Low-rank\n  Adaptation with Less Forgetting and Faster Convergence","summary":"  Conventional low-rank adaptation methods build adapters without considering\ndata context, leading to sub-optimal fine-tuning performance and severe\nforgetting of inherent world knowledge. In this paper, we propose\ncontext-oriented decomposition adaptation (CorDA), a novel method that\ninitializes adapters in a task-aware manner. Concretely, we develop\ncontext-oriented singular value decomposition, where we collect covariance\nmatrices of input activations for each linear layer using sampled data from the\ntarget task, and apply SVD to the product of weight matrix and its\ncorresponding covariance matrix. By doing so, the task-specific capability is\ncompacted into the principal components. Thanks to the task awareness, our\nmethod enables two optional adaptation modes, knowledge-preserved mode (KPM)\nand instruction-previewed mode (IPM), providing flexibility to choose between\nfreezing the principal components to preserve their associated knowledge or\nadapting them to better learn a new task. We further develop CorDA++ by\nderiving a metric that reflects the compactness of task-specific principal\ncomponents, and then introducing dynamic covariance selection and dynamic rank\nallocation strategies based on the same metric. The two strategies provide each\nlayer with the most representative covariance matrix and a proper rank\nallocation. Experimental results show that CorDA++ outperforms CorDA by a\nsignificant margin. CorDA++ in KPM not only achieves better fine-tuning\nperformance than LoRA, but also mitigates the forgetting of pre-trained\nknowledge in both large language models and vision language models. For IPM,\nour method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA,\nand improves adaptation performance in various scenarios, outperforming strong\nbaseline methods. Our method has been integrated into the PEFT library\ndeveloped by Hugging Face.\n","authors":["Yibo Yang","Sihao Liu","Chuan Rao","Bang An","Tiancheng Shen","Philip H. S. Torr","Ming-Hsuan Yang","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2506.13187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13183v1","updated":"2025-06-16T07:49:43Z","published":"2025-06-16T07:49:43Z","title":"MT-PCR: A Hybrid Mamba-Transformer with Spatial Serialization for\n  Hierarchical Point Cloud Registration","summary":"  Point cloud registration (PCR) is a fundamental task in 3D computer vision\nand robotics. Most existing learning-based PCR methods rely on Transformers,\nwhich suffer from quadratic computational complexity. This limitation restricts\nthe resolution of point clouds that can be processed, inevitably leading to\ninformation loss. In contrast, Mamba-a recently proposed model based on state\nspace models (SSMs)-achieves linear computational complexity while maintaining\nstrong long-range contextual modeling capabilities. However, directly applying\nMamba to PCR tasks yields suboptimal performance due to the unordered and\nirregular nature of point cloud data. To address this challenge, we propose\nMT-PCR, the first point cloud registration framework that integrates both Mamba\nand Transformer modules. Specifically, we serialize point cloud features using\nZ-order space-filling curves to enforce spatial locality, enabling Mamba to\nbetter model the geometric structure of the input. Additionally, we remove the\norder indicator module commonly used in Mamba-based sequence modeling, leads to\nimproved performance in our setting. The serialized features are then processed\nby an optimized Mamba encoder, followed by a Transformer refinement stage.\nExtensive experiments on multiple benchmarks demonstrate that MT-PCR\noutperforms Transformer-based and concurrent state-of-the-art methods in both\naccuracy and efficiency, significantly reducing while GPU memory usage and\nFLOPs.\n","authors":["Bingxi Liu","An Liu","Hao Chen","Jinqiang Cui","Yiqun Wang","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13183v1.pdf","comment":"11 Pages"},{"id":"http://arxiv.org/abs/2506.03177v2","updated":"2025-06-16T07:42:49Z","published":"2025-05-29T11:11:41Z","title":"Deep Learning-Based Breast Cancer Detection in Mammography: A\n  Multi-Center Validation Study in Thai Population","summary":"  This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice.\n","authors":["Isarun Chamveha","Supphanut Chaiyungyuen","Sasinun Worakriangkrai","Nattawadee Prasawang","Warasinee Chaisangmongkon","Pornpim Korpraphong","Voraparee Suvannarerg","Shanigarn Thiravit","Chalermdej Kannawat","Kewalin Rungsinaporn","Suwara Issaragrisil","Payia Chadbunchachai","Pattiya Gatechumpol","Chawiporn Muktabhant","Patarachai Sereerat"],"pdf_url":"https://arxiv.org/pdf/2506.03177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01413v3","updated":"2025-06-16T07:40:34Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v3.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2506.11991v2","updated":"2025-06-16T07:35:52Z","published":"2025-06-13T17:47:43Z","title":"VGR: Visual Grounded Reasoning","summary":"  In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.\n","authors":["Jiacong Wang","Zijian Kang","Haochen Wang","Haiyong Jiang","Jiawen Li","Bohong Wu","Ya Wang","Jiao Ran","Xiao Liang","Chao Feng","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.11991v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.15371v2","updated":"2025-06-16T07:22:26Z","published":"2024-10-20T12:10:24Z","title":"FrameBridge: Improving Image-to-Video Generation with Bridge Models","summary":"  Diffusion models have achieved remarkable progress on image-to-video (I2V)\ngeneration, while their noise-to-data generation process is inherently\nmismatched with this task, which may lead to suboptimal synthesis quality. In\nthis work, we present FrameBridge. By modeling the frame-to-frames generation\nprocess with a bridge model based data-to-data generative process, we are able\nto fully exploit the information contained in the given image and improve the\nconsistency between the generation process and I2V task. Moreover, we propose\ntwo novel techniques toward the two popular settings of training I2V models,\nrespectively. Firstly, we propose SNR-Aligned Fine-tuning (SAF), making the\nfirst attempt to fine-tune a diffusion model to a bridge model and, therefore,\nallowing us to utilize the pre-trained diffusion-based text-to-video (T2V)\nmodels. Secondly, we propose neural prior, further improving the synthesis\nquality of FrameBridge when training from scratch. Experiments conducted on\nWebVid-2M and UCF-101 demonstrate the superior quality of FrameBridge in\ncomparison with the diffusion counterpart (zero-shot FVD 95 vs. 192 on MSR-VTT\nand non-zero-shot FVD 122 vs. 171 on UCF-101), and the advantages of our\nproposed SAF and neural prior for bridge-based I2V models. The project page:\nhttps://framebridge-icml.github.io/.\n","authors":["Yuji Wang","Zehua Chen","Xiaoyu Chen","Yixiang Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13166v1","updated":"2025-06-16T07:21:11Z","published":"2025-06-16T07:21:11Z","title":"GreedyPrune: Retenting Critical Visual Token Set for Large Vision\n  Language Models","summary":"  Although Large Vision Language Models (LVLMs) have demonstrated remarkable\nperformance in image understanding tasks, their computational efficiency\nremains a significant challenge, particularly on resource-constrained devices\ndue to the high cost of processing large numbers of visual tokens. Recently,\ntraining-free visual token pruning methods have gained popularity as a low-cost\nsolution to this issue. However, existing approaches suffer from two key\nlimitations: semantic saliency-based strategies primarily focus on high\ncross-attention visual tokens, often neglecting visual diversity, whereas\nvisual diversity-based methods risk inadvertently discarding semantically\nimportant tokens, especially under high compression ratios. In this paper, we\nintroduce GreedyPrune, a training-free plug-and-play visual token pruning\nalgorithm designed to jointly optimize semantic saliency and visual diversity.\nWe formalize the token pruning process as a combinatorial optimization problem\nand demonstrate that greedy algorithms effectively balance computational\nefficiency with model accuracy. Extensive experiments validate the\neffectiveness of our approach, showing that GreedyPrune achieves\nstate-of-the-art accuracy across various multimodal tasks and models while\nsignificantly reducing end-to-end inference latency.\n","authors":["Ruiguang Pei","Weiqing Sun","Zhihui Fu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13160v1","updated":"2025-06-16T07:17:23Z","published":"2025-06-16T07:17:23Z","title":"CertDW: Towards Certified Dataset Ownership Verification via Conformal\n  Prediction","summary":"  Deep neural networks (DNNs) rely heavily on high-quality open-source datasets\n(e.g., ImageNet) for their success, making dataset ownership verification (DOV)\ncrucial for protecting public dataset copyrights. In this paper, we find\nexisting DOV methods (implicitly) assume that the verification process is\nfaithful, where the suspicious model will directly verify ownership by using\nthe verification samples as input and returning their results. However, this\nassumption may not necessarily hold in practice and their performance may\ndegrade sharply when subjected to intentional or unintentional perturbations.\nTo address this limitation, we propose the first certified dataset watermark\n(i.e., CertDW) and CertDW-based certified dataset ownership verification method\nthat ensures reliable verification even under malicious attacks, under certain\nconditions (e.g., constrained pixel-level perturbation). Specifically, inspired\nby conformal prediction, we introduce two statistical measures, including\nprincipal probability (PP) and watermark robustness (WR), to assess model\nprediction stability on benign and watermarked samples under noise\nperturbations. We prove there exists a provable lower bound between PP and WR,\nenabling ownership verification when a suspicious model's WR value\nsignificantly exceeds the PP values of multiple benign models trained on\nwatermark-free datasets. If the number of PP values smaller than WR exceeds a\nthreshold, the suspicious model is regarded as having been trained on the\nprotected dataset. Extensive experiments on benchmark datasets verify the\neffectiveness of our CertDW method and its resistance to potential adaptive\nattacks. Our codes are at\n\\href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}.\n","authors":["Ting Qiao","Yiming Li","Jianbin Li","Yingjia Wang","Leyi Qi","Junfeng Guo","Ruili Feng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.13160v1.pdf","comment":"The first two authors contributed equally to this work. 16 pages"},{"id":"http://arxiv.org/abs/2502.07225v2","updated":"2025-06-16T07:12:59Z","published":"2025-02-11T03:35:35Z","title":"CAT: Contrastive Adversarial Training for Evaluating the Robustness of\n  Protective Perturbations in Latent Diffusion Models","summary":"  Latent diffusion models have recently demonstrated superior capabilities in\nmany downstream image synthesis tasks. However, customization of latent\ndiffusion models using unauthorized data can severely compromise the privacy\nand intellectual property rights of data owners. Adversarial examples as\nprotective perturbations have been developed to defend against unauthorized\ndata usage by introducing imperceptible noise to customization samples,\npreventing diffusion models from effectively learning them. In this paper, we\nfirst reveal that the primary reason adversarial examples are effective as\nprotective perturbations in latent diffusion models is the distortion of their\nlatent representations, as demonstrated through qualitative and quantitative\nexperiments. We then propose the Contrastive Adversarial Training (CAT)\nutilizing lightweight adapters as an adaptive attack against these protection\nmethods, highlighting their lack of robustness. Extensive experiments\ndemonstrate that our CAT method significantly reduces the effectiveness of\nprotective perturbations in customization, urging the community to reconsider\nand improve the robustness of existing protective perturbations. The code is\navailable at https://github.com/senp98/CAT.\n","authors":["Sen Peng","Mingyue Wang","Jianfei He","Jijia Yang","Xiaohua Jia"],"pdf_url":"https://arxiv.org/pdf/2502.07225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13156v1","updated":"2025-06-16T07:09:51Z","published":"2025-06-16T07:09:51Z","title":"StgcDiff: Spatial-Temporal Graph Condition Diffusion for Sign Language\n  Transition Generation","summary":"  Sign language transition generation seeks to convert discrete sign language\nsegments into continuous sign videos by synthesizing smooth transitions.\nHowever,most existing methods merely concatenate isolated signs, resulting in\npoor visual coherence and semantic accuracy in the generated videos. Unlike\ntextual languages,sign language is inherently rich in spatial-temporal cues,\nmaking it more complex to model. To address this,we propose StgcDiff, a\ngraph-based conditional diffusion framework that generates smooth transitions\nbetween discrete signs by capturing the unique spatial-temporal dependencies of\nsign language. Specifically, we first train an encoder-decoder architecture to\nlearn a structure-aware representation of spatial-temporal skeleton sequences.\nNext, we optimize a diffusion denoiser conditioned on the representations\nlearned by the pre-trained encoder, which is tasked with predicting transition\nframes from noise. Additionally, we design the Sign-GCN module as the key\ncomponent in our framework, which effectively models the spatial-temporal\nfeatures. Extensive experiments conducted on the PHOENIX14T, USTC-CSL100,and\nUSTC-SLR500 datasets demonstrate the superior performance of our method.\n","authors":["Jiashu He","Jiayi He","Shengeng Tang","Huixia Ben","Lechao Cheng","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2506.13156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13138v1","updated":"2025-06-16T06:53:05Z","published":"2025-06-16T06:53:05Z","title":"STAGE: A Stream-Centric Generative World Model for Long-Horizon\n  Driving-Scene Simulation","summary":"  The generation of temporally consistent, high-fidelity driving videos over\nextended horizons presents a fundamental challenge in autonomous driving world\nmodeling. Existing approaches often suffer from error accumulation and feature\nmisalignment due to inadequate decoupling of spatio-temporal dynamics and\nlimited cross-frame feature propagation mechanisms. To address these\nlimitations, we present STAGE (Streaming Temporal Attention Generative Engine),\na novel auto-regressive framework that pioneers hierarchical feature\ncoordination and multi-phase optimization for sustainable video synthesis. To\nachieve high-quality long-horizon driving video generation, we introduce\nHierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training\nstrategy. HTFT enhances temporal consistency between video frames throughout\nthe video generation process by modeling the temporal and denoising process\nseparately and transferring denoising features between frames. The multi-stage\ntraining strategy is to divide the training into three stages, through model\ndecoupling and auto-regressive inference process simulation, thereby\naccelerating model convergence and reducing error accumulation. Experiments on\nthe Nuscenes dataset show that STAGE has significantly surpassed existing\nmethods in the long-horizon driving video generation task. In addition, we also\nexplored STAGE's ability to generate unlimited-length driving videos. We\ngenerated 600 frames of high-quality driving videos on the Nuscenes dataset,\nwhich far exceeds the maximum length achievable by existing methods.\n","authors":["Jiamin Wang","Yichen Yao","Xiang Feng","Hang Wu","Yaming Wang","Qingqiu Huang","Yuexin Ma","Xinge Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09605v3","updated":"2025-06-16T06:47:45Z","published":"2024-03-14T17:47:01Z","title":"Counterfactual contrastive learning: robust representations via causal\n  image synthesis","summary":"  Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.\n","authors":["Melanie Roschewitz","Fabio De Sousa Ribeiro","Tian Xia","Galvin Khara","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2403.09605v3.pdf","comment":"Extended version available at\n  https://doi.org/10.1016/j.media.2025.103668. This version was published in\n  the proceedings of the MICCAI 2024 Data Engineering in Medical Imaging\n  workshop. Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive"},{"id":"http://arxiv.org/abs/2506.13133v1","updated":"2025-06-16T06:40:12Z","published":"2025-06-16T06:40:12Z","title":"EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints\n  for Visual Place Recognition","summary":"  Visual Place Recognition (VPR) is a scene-oriented image retrieval problem in\ncomputer vision in which re-ranking based on local features is commonly\nemployed to improve performance. In robotics, VPR is also referred to as Loop\nClosure Detection, which emphasizes spatial-temporal verification within a\nsequence. However, designing local features specifically for VPR is\nimpractical, and relying on motion sequences imposes limitations. Inspired by\nthese observations, we propose a novel, simple re-ranking method that refines\nglobal features through a Mixture-of-Features (MoF) approach under embodied\nconstraints. First, we analyze the practical feasibility of embodied\nconstraints in VPR and categorize them according to existing datasets, which\ninclude GPS tags, sequential timestamps, local feature matching, and\nself-similarity matrices. We then propose a learning-based MoF\nweight-computation approach, utilizing a multi-metric loss function.\nExperiments demonstrate that our method improves the state-of-the-art (SOTA)\nperformance on public datasets with minimal additional computational overhead.\nFor instance, with only 25 KB of additional parameters and a processing time of\n10 microseconds per frame, our method achieves a 0.9\\% improvement over a\nDINOv2-based baseline performance on the Pitts-30k test set.\n","authors":["Bingxi Liu","Hao Chen","Shiyi Guo","Yihong Wu","Jinqiang Cui","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13133v1.pdf","comment":"17 Pages"},{"id":"http://arxiv.org/abs/2506.13130v1","updated":"2025-06-16T06:27:59Z","published":"2025-06-16T06:27:59Z","title":"ZINA: Multimodal Fine-grained Hallucination Detection and Editing","summary":"  Multimodal Large Language Models (MLLMs) often generate hallucinations, where\nthe output deviates from the visual content. Given that these hallucinations\ncan take diverse forms, detecting hallucinations at a fine-grained level is\nessential for comprehensive evaluation and analysis. To this end, we propose a\nnovel task of multimodal fine-grained hallucination detection and editing for\nMLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated\nspans at a fine-grained level, classifies their error types into six\ncategories, and suggests appropriate refinements. To train and evaluate models\nfor this task, we constructed VisionHall, a dataset comprising 6.9k outputs\nfrom twelve MLLMs manually annotated by 211 annotators, and 20k synthetic\nsamples generated using a graph-based method that captures dependencies among\nerror types. We demonstrated that ZINA outperformed existing methods, including\nGPT-4o and LLama-3.2, in both detection and editing tasks.\n","authors":["Yuiga Wada","Kazuki Matsuda","Komei Sugiura","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2506.13130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10353v3","updated":"2025-06-16T06:23:11Z","published":"2025-06-12T05:21:43Z","title":"Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for\n  Human Motion Generation","summary":"  Recent advances in large language models, especially in natural language\nunderstanding and reasoning, have opened new possibilities for text-to-motion\ngeneration. Although existing approaches have made notable progress in semantic\nalignment and motion synthesis, they often rely on end-to-end mapping\nstrategies that fail to capture deep linguistic structures and logical\nreasoning. Consequently, generated motions tend to lack controllability,\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\na unified motion-language modeling framework that integrates a Chain-of-Thought\nmechanism. By explicitly decomposing complex textual instructions into\nlogically structured action paths, Motion-R1 provides high-level semantic\nguidance for motion generation, significantly enhancing the model's ability to\ninterpret and execute multi-step, long-horizon, and compositionally rich\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\nreinforcement learning algorithm designed for large models, which leverages\nmotion quality feedback to optimize reasoning chains and motion synthesis\njointly. Extensive experiments across multiple benchmark datasets demonstrate\nthat Motion-R1 achieves competitive or superior performance compared to\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\nunderstanding and long-term temporal coherence. The code, model and data will\nbe publicly available.\n","authors":["Runqi Ouyang","Haoyun Li","Zhenyuan Zhang","Xiaofeng Wang","Zheng Zhu","Guan Huang","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10353v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07742v5","updated":"2025-06-16T06:17:33Z","published":"2024-11-12T12:07:27Z","title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial\n  Pruning","summary":"  This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive point cloud sweeps, resulting in a remarkable\nimprovement in perception accuracy. However, the computation cost also\nincreases, hindering previous approaches from utilizing a large number of point\ncloud sweeps. To tackle this challenge, we find that a considerable portion of\npoints in the accumulated point cloud is redundant, and discarding these points\nhas minimal impact on perception accuracy. We introduce a simple yet effective\nGumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a\nlearned end-to-end sampling. The GSP layer is decoupled from other network\ncomponents and thus can be seamlessly integrated into existing point cloud\nnetwork architectures. Without incurring additional computational overhead, we\nincrease the number of point cloud sweeps from 10, a common practice, to as\nmany as 40. Consequently, there is a significant enhancement in perception\nperformance. For instance, in nuScenes 3D object detection and BEV map\nsegmentation tasks, our pruning strategy improves several 3D perception\nbaseline methods.\n","authors":["Tianyu Sun","Jianhao Li","Xueqian Zhang","Zhongdao Wang","Bailan Feng","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.07742v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21955v2","updated":"2025-06-16T06:09:39Z","published":"2024-10-29T11:18:04Z","title":"ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian\n  Splatting","summary":"  We propose ActiveSplat, an autonomous high-fidelity reconstruction system\nleveraging Gaussian splatting. Taking advantage of efficient and realistic\nrendering, the system establishes a unified framework for online mapping,\nviewpoint selection, and path planning. The key to ActiveSplat is a hybrid map\nrepresentation that integrates both dense information about the environment and\na sparse abstraction of the workspace. Therefore, the system leverages sparse\ntopology for efficient viewpoint sampling and path planning, while exploiting\nview-dependent dense prediction for viewpoint selection, facilitating efficient\ndecision-making with promising accuracy and completeness. A hierarchical\nplanning strategy based on the topological map is adopted to mitigate\nrepetitive trajectories and improve local granularity given limited time\nbudgets, ensuring high-fidelity reconstruction with photorealistic view\nsynthesis. Extensive experiments and ablation studies validate the efficacy of\nthe proposed method in terms of reconstruction accuracy, data coverage, and\nexploration efficiency. The released code will be available on our project\npage: https://li-yuetao.github.io/ActiveSplat/.\n","authors":["Yuetao Li","Zijia Kuang","Ting Li","Qun Hao","Zike Yan","Guyue Zhou","Shaohui Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21955v2.pdf","comment":"Accepted to IEEE RA-L. Code:\n  https://github.com/Li-Yuetao/ActiveSplat, Project:\n  https://li-yuetao.github.io/ActiveSplat/"},{"id":"http://arxiv.org/abs/2410.08567v2","updated":"2025-06-16T05:52:28Z","published":"2024-10-11T06:45:15Z","title":"Diffusion-Based Depth Inpainting for Transparent and Reflective Objects","summary":"  Transparent and reflective objects, which are common in our everyday lives,\npresent a significant challenge to 3D imaging techniques due to their unique\nvisual and optical properties. Faced with these types of objects, RGB-D cameras\nfail to capture the real depth value with their accurate spatial information.\nTo address this issue, we propose DITR, a diffusion-based Depth Inpainting\nframework specifically designed for Transparent and Reflective objects. This\nnetwork consists of two stages, including a Region Proposal stage and a Depth\nInpainting stage. DITR dynamically analyzes the optical and geometric depth\nloss and inpaints them automatically. Furthermore, comprehensive experimental\nresults demonstrate that DITR is highly effective in depth inpainting tasks of\ntransparent and reflective objects with robust adaptability.\n","authors":["Tianyu Sun","Dingchang Hu","Yixiang Dai","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.08567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13110v1","updated":"2025-06-16T05:40:16Z","published":"2025-06-16T05:40:16Z","title":"GS-2DGS: Geometrically Supervised 2DGS for Reflective Object\n  Reconstruction","summary":"  3D modeling of highly reflective objects remains challenging due to strong\nview-dependent appearances. While previous SDF-based methods can recover\nhigh-quality meshes, they are often time-consuming and tend to produce\nover-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the\nadvantage of high speed and detailed real-time rendering, but extracting\nsurfaces from the Gaussians can be noisy due to the lack of geometric\nconstraints. To bridge the gap between these approaches, we propose a novel\nreconstruction method called GS-2DGS for reflective objects based on 2D\nGaussian Splatting (2DGS). Our approach combines the rapid rendering\ncapabilities of Gaussian Splatting with additional geometric information from\nfoundation models. Experimental results on synthetic and real datasets\ndemonstrate that our method significantly outperforms Gaussian-based techniques\nin terms of reconstruction and relighting and achieves performance comparable\nto SDF-based methods while being an order of magnitude faster. Code is\navailable at https://github.com/hirotong/GS2DGS\n","authors":["Jinguang Tong","Xuesong li","Fahira Afzal Maken","Sundaram Muthu","Lars Petersson","Chuong Nguyen","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2506.13110v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2506.11932v2","updated":"2025-06-16T05:38:38Z","published":"2025-06-13T16:33:54Z","title":"Evaluating Sensitivity Parameters in Smartphone-Based Gaze Estimation: A\n  Comparative Study of Appearance-Based and Infrared Eye Trackers","summary":"  This study evaluates a smartphone-based, deep-learning eye-tracking algorithm\nby comparing its performance against a commercial infrared-based eye tracker,\nthe Tobii Pro Nano. The aim is to investigate the feasibility of\nappearance-based gaze estimation under realistic mobile usage conditions. Key\nsensitivity factors, including age, gender, vision correction, lighting\nconditions, device type, and head position, were systematically analysed. The\nappearance-based algorithm integrates a lightweight convolutional neural\nnetwork (MobileNet-V3) with a recurrent structure (Long Short-Term Memory) to\npredict gaze coordinates from grayscale facial images. Gaze data were collected\nfrom 51 participants using dynamic visual stimuli, and accuracy was measured\nusing Euclidean distance. The deep learning model produced a mean error of\n17.76 mm, compared to 16.53 mm for the Tobii Pro Nano. While overall accuracy\ndifferences were small, the deep learning-based method was more sensitive to\nfactors such as lighting, vision correction, and age, with higher failure rates\nobserved under low-light conditions among participants using glasses and in\nolder age groups. Device-specific and positional factors also influenced\ntracking performance. These results highlight the potential of appearance-based\napproaches for mobile eye tracking and offer a reference framework for\nevaluating gaze estimation systems across varied usage conditions.\n","authors":["Nishan Gunawardena","Gough Yumu Lui","Bahman Javadi","Jeewani Anupama Ginige"],"pdf_url":"https://arxiv.org/pdf/2506.11932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07351v4","updated":"2025-06-16T05:32:08Z","published":"2025-02-11T08:22:21Z","title":"Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for\n  Vision-driven Intelligent Systems","summary":"  Salient object detection (SOD) plays a critical role in Intelligent Imaging,\nfacilitating the detection and segmentation of key visual elements in an image.\nHowever, adverse imaging conditions such as haze during the day, low light, and\nhaze at night severely degrade image quality and hinder reliable object\ndetection in real-world scenarios. To address these challenges, we propose a\nmulti-knowledge-oriented nighttime haze imaging enhancer (MKoIE), which\nintegrates three tasks: daytime dehazing, low-light enhancement, and nighttime\ndehazing. The MKoIE incorporates two key innovative components: First, the\nnetwork employs a task-oriented node learning mechanism to handle three\nspecific degradation types: day-time haze, low light, and night-time haze\nconditions, with an embedded self-attention module enhancing its performance in\nnighttime imaging. In addition, multi-receptive field enhancement module that\nefficiently extracts multi-scale features through three parallel depthwise\nseparable convolution branches with different dilation rates, capturing\ncomprehensive spatial information with minimal computational overhead to meet\nthe requirements of real-time imaging deployment. To ensure optimal image\nreconstruction quality and visual characteristics, we suggest a hybrid loss\nfunction. Extensive experiments on different types of weather/imaging\nconditions illustrate that MKoIE surpasses existing methods, enhancing the\nreliability, accuracy, and operational efficiency of intelligent imaging.\n","authors":["Ai Chen","Yuxu Lu","Dong Yang","Junlin Zhou","Yan Fu","Duanbing Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07351v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17097v2","updated":"2025-06-16T05:21:22Z","published":"2025-03-21T12:30:33Z","title":"R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging\n  Diffusion Model","summary":"  We introduce R2LDM, an innovative approach for generating dense and accurate\n4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of\nutilizing range images or bird's eye view (BEV) images, we represent both LiDAR\nand 4D radar point clouds using voxel features, which more effectively capture\n3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model\n(LVDM), which performs the diffusion process in the latent space. Additionally,\na novel Latent Point Cloud Reconstruction (LPCR) module is utilized to\nreconstruct point clouds from high-dimensional latent voxel features. As a\nresult, R2LDM effectively generates LiDAR-like point clouds from paired raw\nradar data. We evaluate our approach on two different datasets, and the\nexperimental results demonstrate that our model achieves 6- to 10-fold\ndensification of radar point clouds, outperforming state-of-the-art baselines\nin 4D radar point cloud super-resolution. Furthermore, the enhanced radar point\nclouds generated by our method significantly improve downstream tasks,\nachieving up to 31.7% improvement in point cloud registration recall rate and\n24.9% improvement in object detection accuracy.\n","authors":["Boyuan Zheng","Shouyi Lu","Renbo Huang","Minqing Huang","Fan Lu","Wei Tian","Guirong Zhuo","Lu Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.17097v2.pdf","comment":"8 pages, 9 figures, accepted to IROS 2025"},{"id":"http://arxiv.org/abs/2506.13100v1","updated":"2025-06-16T05:11:57Z","published":"2025-06-16T05:11:57Z","title":"A Novel ViDAR Device With Visual Inertial Encoder Odometry and\n  Reinforcement Learning-Based Active SLAM Method","summary":"  In the field of multi-sensor fusion for simultaneous localization and mapping\n(SLAM), monocular cameras and IMUs are widely used to build simple and\neffective visual-inertial systems. However, limited research has explored the\nintegration of motor-encoder devices to enhance SLAM performance. By\nincorporating such devices, it is possible to significantly improve active\ncapability and field of view (FOV) with minimal additional cost and structural\ncomplexity. This paper proposes a novel visual-inertial-encoder tightly coupled\nodometry (VIEO) based on a ViDAR (Video Detection and Ranging) device. A ViDAR\ncalibration method is introduced to ensure accurate initialization for VIEO. In\naddition, a platform motion decoupled active SLAM method based on deep\nreinforcement learning (DRL) is proposed. Experimental data demonstrate that\nthe proposed ViDAR and the VIEO algorithm significantly increase cross-frame\nco-visibility relationships compared to its corresponding visual-inertial\nodometry (VIO) algorithm, improving state estimation accuracy. Additionally,\nthe DRL-based active SLAM algorithm, with the ability to decouple from platform\nmotion, can increase the diversity weight of the feature points and further\nenhance the VIEO algorithm's performance. The proposed methodology sheds fresh\ninsights into both the updated platform design and decoupled approach of active\nSLAM systems in complex environments.\n","authors":["Zhanhua Xin","Zhihao Wang","Shenghao Zhang","Wanchao Chi","Yan Meng","Shihan Kong","Yan Xiong","Chong Zhang","Yuzhen Liu","Junzhi Yu"],"pdf_url":"https://arxiv.org/pdf/2506.13100v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.13097v1","updated":"2025-06-16T05:04:12Z","published":"2025-06-16T05:04:12Z","title":"Pro-AD: Learning Comprehensive Prototypes with Prototype-based\n  Constraint for Multi-class Unsupervised Anomaly Detection","summary":"  Prototype-based reconstruction methods for unsupervised anomaly detection\nutilize a limited set of learnable prototypes which only aggregates\ninsufficient normal information, resulting in undesirable reconstruction.\nHowever, increasing the number of prototypes may lead to anomalies being well\nreconstructed through the attention mechanism, which we refer to as the \"Soft\nIdentity Mapping\" problem. In this paper, we propose Pro-AD to address these\nissues and fully utilize the prototypes to boost the performance of anomaly\ndetection. Specifically, we first introduce an expanded set of learnable\nprototypes to provide sufficient capacity for semantic information. Then we\nemploy a Dynamic Bidirectional Decoder which integrates the process of the\nnormal information aggregation and the target feature reconstruction via\nprototypes, with the aim of allowing the prototypes to aggregate more\ncomprehensive normal semantic information from different levels of the image\nfeatures and the target feature reconstruction to not only utilize its\ncontextual information but also dynamically leverage the learned comprehensive\nprototypes. Additionally, to prevent the anomalies from being well\nreconstructed using sufficient semantic information through the attention\nmechanism, Pro-AD introduces a Prototype-based Constraint that applied within\nthe target feature reconstruction process of the decoder, which further\nimproves the performance of our approach. Extensive experiments on multiple\nchallenging benchmarks demonstrate that our Pro-AD achieve state-of-the-art\nperformance, highlighting its superior robustness and practical effectiveness\nfor Multi-class Unsupervised Anomaly Detection task.\n","authors":["Ziqing Zhou","Binbin Gao","Yuri Pan","Lidong Wang","Wenbing Zhu","Yong Liu","Jun Liu","MIngmin Chi","Dong Wu","Bo Peng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13095v1","updated":"2025-06-16T04:56:58Z","published":"2025-06-16T04:56:58Z","title":"Learning Event Completeness for Weakly Supervised Video Anomaly\n  Detection","summary":"  Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing\ntemporal intervals containing anomalous events within untrimmed videos,\nutilizing only video-level annotations. However, a significant challenge arises\ndue to the absence of dense frame-level annotations, often leading to\nincomplete localization in existing WS-VAD methods. To address this issue, we\npresent a novel LEC-VAD, Learning Event Completeness for Weakly Supervised\nVideo Anomaly Detection, which features a dual structure designed to encode\nboth category-aware and category-agnostic semantics between vision and\nlanguage. Within LEC-VAD, we devise semantic regularities that leverage an\nanomaly-aware Gaussian mixture to learn precise event boundaries, thereby\nyielding more complete event instances. Besides, we develop a novel memory\nbank-based prototype learning mechanism to enrich concise text descriptions\nassociated with anomaly-event categories. This innovation bolsters the text's\nexpressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates\nremarkable advancements over the current state-of-the-art methods on two\nbenchmark datasets XD-Violence and UCF-Crime.\n","authors":["Yu Wang","Shiwei Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13095v1.pdf","comment":"Accepted by ICML"},{"id":"http://arxiv.org/abs/2407.11348v2","updated":"2025-06-16T04:53:44Z","published":"2024-07-16T03:32:10Z","title":"Flatfish Lesion Detection Based on Part Segmentation Approach and Lesion\n  Image Generation","summary":"  The flatfish is a major farmed species consumed globally in large quantities.\nHowever, due to the densely populated farming environment, flatfish are\nsusceptible to lesions and diseases, making early lesion detection crucial.\nTraditionally, lesions were detected through visual inspection, but observing\nlarge numbers of fish is challenging. Automated approaches based on deep\nlearning technologies have been widely used to address this problem, but\naccurate detection remains difficult due to the diversity of the fish and the\nlack of a fish lesion and disease dataset. This study augments fish lesion\nimages using generative adversarial networks and image harmonization methods.\nNext, lesion detectors are trained separately for three body parts (head, fins,\nand body) to address individual lesions properly. Additionally, a flatfish\nlesion and disease image dataset, called FlatIMG, is created and verified using\nthe proposed methods on the dataset. A flash salmon lesion dataset is also\ntested to validate the generalizability of the proposed methods. The results\nachieved 12% higher performance than the baseline framework. This study is the\nfirst attempt to create a high-quality flatfish lesion image dataset with\ndetailed annotations and propose an effective lesion detection framework.\nAutomatic lesion and disease monitoring can be achieved in farming environments\nusing the proposed methods and dataset.\n","authors":["Seo-Bin Hwang","Han-Young Kim","Chae-Yeon Heo","Hie-Yong Jeong","Sung-Ju Jung","Yeong-Jun Cho"],"pdf_url":"https://arxiv.org/pdf/2407.11348v2.pdf","comment":"16 page, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.24402v2","updated":"2025-06-16T04:47:44Z","published":"2025-05-30T09:33:01Z","title":"Leveraging Intermediate Features of Vision Transformer for Face\n  Anti-Spoofing","summary":"  Face recognition systems are designed to be robust against changes in head\npose, illumination, and blurring during image capture. If a malicious person\npresents a face photo of the registered user, they may bypass the\nauthentication process illegally. Such spoofing attacks need to be detected\nbefore face recognition. In this paper, we propose a spoofing attack detection\nmethod based on Vision Transformer (ViT) to detect minute differences between\nlive and spoofed face images. The proposed method utilizes the intermediate\nfeatures of ViT, which have a good balance between local and global features\nthat are important for spoofing attack detection, for calculating loss in\ntraining and score in inference. The proposed method also introduces two data\naugmentation methods: face anti-spoofing data augmentation and patch-wise data\naugmentation, to improve the accuracy of spoofing attack detection. We\ndemonstrate the effectiveness of the proposed method through experiments using\nthe OULU-NPU and SiW datasets. The project page is available at:\nhttps://gsisaoki.github.io/FAS-ViT-CVPRW/ .\n","authors":["Mika Feng","Koichi Ito","Takafumi Aoki","Tetsushi Ohki","Masakatsu Nishigaki"],"pdf_url":"https://arxiv.org/pdf/2505.24402v2.pdf","comment":"2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)"},{"id":"http://arxiv.org/abs/2506.13089v1","updated":"2025-06-16T04:27:32Z","published":"2025-06-16T04:27:32Z","title":"SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS,\n  and Learning-Based Loop Closure","summary":"  Visual simultaneous localization and mapping (SLAM) must remain accurate\nunder extreme viewpoint, scale and illumination variations. The widely adopted\nORB-SLAM3 falters in these regimes because it relies on hand-crafted ORB\nkeypoints. We introduce SuperPoint-SLAM3, a drop-in upgrade that (i) replaces\nORB with the self-supervised SuperPoint detector--descriptor, (ii) enforces\nspatially uniform keypoints via adaptive non-maximal suppression (ANMS), and\n(iii) integrates a lightweight NetVLAD place-recognition head for\nlearning-based loop closure.\n  On the KITTI Odometry benchmark SuperPoint-SLAM3 reduces mean translational\nerror from 4.15% to 0.34% and mean rotational error from 0.0027 deg/m to 0.0010\ndeg/m. On the EuRoC MAV dataset it roughly halves both errors across every\nsequence (e.g., V2\\_03: 1.58% -> 0.79%). These gains confirm that fusing modern\ndeep features with a learned loop-closure module markedly improves ORB-SLAM3\naccuracy while preserving its real-time operation.\n  Implementation, pretrained weights and reproducibility scripts are available\nat https://github.com/shahram95/SuperPointSLAM3.\n","authors":["Shahram Najam Syed","Ishir Roongta","Kavin Ravie","Gangadhar Nageswar"],"pdf_url":"https://arxiv.org/pdf/2506.13089v1.pdf","comment":"10 pages, 6 figures, code at\n  https://github.com/shahram95/SuperPointSLAM3"},{"id":"http://arxiv.org/abs/2506.11150v2","updated":"2025-06-16T03:56:04Z","published":"2025-06-11T10:22:19Z","title":"ADAgent: LLM Agent for Alzheimer's Disease Analysis with Collaborative\n  Coordinator","summary":"  Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative\ndisease. Early and precise diagnosis of AD is crucial for timely intervention\nand treatment planning to alleviate the progressive neurodegeneration. However,\nmost existing methods rely on single-modality data, which contrasts with the\nmultifaceted approach used by medical experts. While some deep learning\napproaches process multi-modal data, they are limited to specific tasks with a\nsmall set of input modalities and cannot handle arbitrary combinations. This\nhighlights the need for a system that can address diverse AD-related tasks,\nprocess multi-modal or missing input, and integrate multiple advanced methods\nfor improved performance. In this paper, we propose ADAgent, the first\nspecialized AI agent for AD analysis, built on a large language model (LLM) to\naddress user queries and support decision-making. ADAgent integrates a\nreasoning engine, specialized medical tools, and a collaborative outcome\ncoordinator to facilitate multi-modal diagnosis and prognosis tasks in AD.\nExtensive experiments demonstrate that ADAgent outperforms SOTA methods,\nachieving significant improvements in accuracy, including a 2.7% increase in\nmulti-modal diagnosis, a 0.7% improvement in multi-modal prognosis, and\nenhancements in MRI and PET diagnosis tasks.\n","authors":["Wenlong Hou","Guangqian Yang","Ye Du","Yeung Lau","Lihao Liu","Junjun He","Ling Long","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2506.11150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08926v2","updated":"2025-06-16T03:47:15Z","published":"2024-01-17T02:25:42Z","title":"Stochasticity-aware No-Reference Point Cloud Quality Assessment","summary":"  The evolution of point cloud processing algorithms necessitates an accurate\nassessment for their quality. Previous works consistently regard point cloud\nquality assessment (PCQA) as a MOS regression problem and devise a\ndeterministic mapping, ignoring the stochasticity in generating MOS from\nsubjective tests. This work presents the first probabilistic architecture for\nno-reference PCQA, motivated by the labeling process of existing datasets. The\nproposed method can model the quality judging stochasticity of subjects through\na tailored conditional variational autoencoder (CVAE) and produces multiple\nintermediate quality ratings. These intermediate ratings simulate the judgments\nfrom different subjects and are then integrated into an accurate quality\nprediction, mimicking the generation process of a ground truth MOS.\nSpecifically, our method incorporates a Prior Module, a Posterior Module, and a\nQuality Rating Generator, where the former two modules are introduced to model\nthe judging stochasticity in subjective tests, while the latter is developed to\ngenerate diverse quality ratings. Extensive experiments indicate that our\napproach outperforms previous cutting-edge methods by a large margin and\nexhibits gratifying cross-dataset robustness. Codes are available at\nhttps://git.openi.org.cn/OpenPointCloud/nrpcqa.\n","authors":["Songlin Fan","Wei Gao","Zhineng Chen","Ge Li","Guoqing Liu","Qicheng Wang"],"pdf_url":"https://arxiv.org/pdf/2401.08926v2.pdf","comment":"Accepted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2503.07950v2","updated":"2025-06-16T03:46:17Z","published":"2025-03-11T01:19:45Z","title":"Decoupled Cross-Modal Alignment Network for Text-RGBT Person Retrieval\n  and A High-Quality Benchmark","summary":"  The performance of traditional text-image person retrieval task is easily\naffected by lighting variations due to imaging limitations of visible spectrum\nsensors. In recent years, cross-modal information fusion has emerged as an\neffective strategy to enhance retrieval robustness. By integrating\ncomplementary information from different spectral modalities, it becomes\npossible to achieve more stable person recognition and matching under complex\nreal-world conditions. Motivated by this, we introduce a novel task: Text-RGBT\nPerson Retrieval, which incorporates cross-spectrum information fusion by\ncombining the complementary cues from visible and thermal modalities for robust\nperson retrieval in challenging environments. The key challenge of Text-RGBT\nperson retrieval lies in aligning text with multi-modal visual features.\nHowever, the inherent heterogeneity between visible and thermal modalities may\ninterfere with the alignment between vision and language. To handle this\nproblem, we propose a Decoupled Cross-modal Alignment network (DCAlign), which\nsufficiently mines the relationships between modality-specific and\nmodality-collaborative visual with the text, for Text-RGBT person retrieval. To\npromote the research and development of this field, we create a high-quality\nText-RGBT person retrieval dataset, RGBT-PEDES. RGBT-PEDES contains 1,822\nidentities from different age groups and genders with 4,723 pairs of calibrated\nRGB and T images, and covers high-diverse scenes from both daytime and\nnighttime with a various of challenges such as occlusion, weak alignment and\nadverse lighting conditions. Additionally, we carefully annotate 7,987\nfine-grained textual descriptions for all RGBT person image pairs. Extensive\nexperiments on RGBT-PEDES demonstrate that our method outperforms existing\ntext-image person retrieval methods.\n","authors":["Yifei Deng","Chenglong Li","Zhenyu Chen","Zihen Xu","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2503.07950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13073v1","updated":"2025-06-16T03:40:49Z","published":"2025-06-16T03:40:49Z","title":"SuperPlace: The Renaissance of Classical Feature Aggregation for Visual\n  Place Recognition in the Era of Foundation Models","summary":"  Recent visual place recognition (VPR) approaches have leveraged foundation\nmodels (FM) and introduced novel aggregation techniques. However, these methods\nhave failed to fully exploit key concepts of FM, such as the effective\nutilization of extensive training sets, and they have overlooked the potential\nof classical aggregation methods, such as GeM and NetVLAD. Building on these\ninsights, we revive classical feature aggregation methods and develop more\nfundamental VPR models, collectively termed SuperPlace. First, we introduce a\nsupervised label alignment method that enables training across various VPR\ndatasets within a unified framework. Second, we propose G$^2$M, a compact\nfeature aggregation method utilizing two GeMs, where one GeM learns the\nprincipal components of feature maps along the channel dimension and calibrates\nthe output of the other. Third, we propose the secondary fine-tuning (FT$^2$)\nstrategy for NetVLAD-Linear (NVL). NetVLAD first learns feature vectors in a\nhigh-dimensional space and then compresses them into a lower-dimensional space\nvia a single linear layer. Extensive experiments highlight our contributions\nand demonstrate the superiority of SuperPlace. Specifically, G$^2$M achieves\npromising results with only one-tenth of the feature dimensions compared to\nrecent methods. Moreover, NVL-FT$^2$ ranks first on the MSLS leaderboard.\n","authors":["Bingxi Liu","Pengju Zhang","Li He","Hao Chen","Shiyi Guo","Yihong Wu","Jinqiang Cui","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13073v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2506.07603v2","updated":"2025-06-16T03:31:25Z","published":"2025-06-09T10:02:58Z","title":"SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis","summary":"  Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.\n","authors":["Jianhui Wei","Zikai Xiao","Danyu Sun","Luqi Gong","Zongxin Yang","Zuozhu Liu","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2506.07603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13067v1","updated":"2025-06-16T03:20:00Z","published":"2025-06-16T03:20:00Z","title":"Video Individual Counting With Implicit One-to-Many Matching","summary":"  Video Individual Counting (VIC) is a recently introduced task that aims to\nestimate pedestrian flux from a video. It extends conventional Video Crowd\nCounting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that\nonly learns to count repeated pedestrian patterns across frames, the key\nproblem of VIC is how to identify co-existent pedestrians between frames, which\nturns out to be a correspondence problem. Existing VIC approaches, however,\nmainly follow a one-to-one (O2O) matching strategy where the same pedestrian\nmust be exactly matched between frames, leading to sensitivity to appearance\nvariations or missing detections. In this work, we show that the O2O matching\ncould be relaxed to a one-to-many (O2M) matching problem, which better fits the\nproblem nature of VIC and can leverage the social grouping behavior of walking\npedestrians. We therefore introduce OMAN, a simple but effective VIC model with\nimplicit One-to-Many mAtchiNg, featuring an implicit context generator and a\none-to-many pairwise matcher. Experiments on the SenseCrowd and CroHD\nbenchmarks show that OMAN achieves the state-of-the-art performance. Code is\navailable at \\href{https://github.com/tiny-smart/OMAN}{OMAN}.\n","authors":["Xuhui Zhu","Jing Xu","Bingjie Wang","Huikang Dai","Hao Lu"],"pdf_url":"https://arxiv.org/pdf/2506.13067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11140v2","updated":"2025-06-16T03:18:56Z","published":"2025-06-11T02:21:19Z","title":"Autonomous Computer Vision Development with Agentic AI","summary":"  Agentic Artificial Intelligence (AI) systems leveraging Large Language Models\n(LLMs) exhibit significant potential for complex reasoning, planning, and tool\nutilization. We demonstrate that a specialized computer vision system can be\nbuilt autonomously from a natural language prompt using Agentic AI methods.\nThis involved extending SimpleMind (SM), an open-source Cognitive AI\nenvironment with configurable tools for medical image analysis, with an\nLLM-based agent, implemented using OpenManus, to automate the planning (tool\nconfiguration) for a particular computer vision task. We provide a\nproof-of-concept demonstration that an agentic system can interpret a computer\nvision task prompt, plan a corresponding SimpleMind workflow by decomposing the\ntask and configuring appropriate tools. From the user input prompt, \"provide sm\n(SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest\nx-ray)\"), the agent LLM was able to generate the plan (tool configuration file\nin YAML format), and execute SM-Learn (training) and SM-Think (inference)\nscripts autonomously. The computer vision agent automatically configured,\ntrained, and tested itself on 50 chest x-ray images, achieving mean dice scores\nof 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows\nthe potential for autonomous planning and tool configuration that has\ntraditionally been performed by a data scientist in the development of computer\nvision applications.\n","authors":["Jin Kim","Muhammad Wahi-Anwa","Sangyun Park","Shawn Shin","John M. Hoffman","Matthew S. Brown"],"pdf_url":"https://arxiv.org/pdf/2506.11140v2.pdf","comment":"The paper is 13 pages long and contains 4 figures"},{"id":"http://arxiv.org/abs/2506.13063v1","updated":"2025-06-16T03:12:51Z","published":"2025-06-16T03:12:51Z","title":"PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical\n  Dialogue","summary":"  Recent pathology foundation models can provide rich tile-level\nrepresentations but fall short of delivering general-purpose clinical utility\nwithout further extensive model development. These models lack whole-slide\nimage (WSI) understanding and are not trained with large-scale diagnostic data,\nlimiting their performance on diverse downstream tasks. We introduce PRISM2, a\nmulti-modal slide-level foundation model trained via clinical dialogue to\nenable scalable, generalizable pathology AI. PRISM2 is trained on nearly\n700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic\nreports in a two-stage process. In Stage 1, a vision-language model is trained\nusing contrastive and captioning objectives to align whole slide embeddings\nwith textual clinical diagnosis. In Stage 2, the language model is unfrozen to\nenable diagnostic conversation and extract more clinically meaningful\nrepresentations from hidden states. PRISM2 achieves strong performance on\ndiagnostic and biomarker prediction tasks, outperforming prior slide-level\nmodels including PRISM and TITAN. It also introduces a zero-shot yes/no\nclassification approach that surpasses CLIP-style methods without prompt tuning\nor class enumeration. By aligning visual features with clinical reasoning,\nPRISM2 improves generalization on both data-rich and low-sample tasks, offering\na scalable path forward for building general pathology AI agents capable of\nassisting diagnostic and prognostic decisions.\n","authors":["George Shaikovski","Eugene Vorontsov","Adam Casson","Julian Viret","Eric Zimmermann","Neil Tenenholtz","Yi Kan Wang","Jan H. Bernhard","Ran A. Godrich","Juan A. Retamero","Razik Yousfi","Nicolo Fusi","Thomas J. Fuchs","Kristen Severson","Siqi Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05779v5","updated":"2025-06-16T03:04:24Z","published":"2024-06-09T13:25:02Z","title":"Learning to utilize image second-order derivative information for crisp\n  edge detection","summary":"  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) the lack of\nprior knowledge regarding image edges, and (2) the issue of imbalanced pixel\ndistribution. We propose a second-order derivative-based multi-scale contextual\nenhancement module (SDMCM) to help the model locate true edge pixels accurately\nby introducing the edge prior knowledge. We also construct a hybrid focal loss\nfunction (HFL) to alleviate the imbalanced distribution issue. In addition, we\nemploy the conditionally parameterized convolution (CondConv) to develop a\nnovel boundary refinement module (BRM), which can further refine the final\noutput edge maps. In the end, we propose a U-shape network named LUS-Net which\nis based on the SDMCM and BRM for crisp edge detection. We perform extensive\nexperiments on three standard benchmarks, and the experiment results illustrate\nthat our method can predict crisp and clean edge maps and achieves\nstate-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2\ndataset (ODS=0.768), and BIPED dataset (ODS=0.903).\n","authors":["Changsong Liu","Yimeng Fan","Mingyang Li","Wei Zhang","Yanyan Liu","Yuming Li","Wenlin Li","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.05779v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13058v1","updated":"2025-06-16T02:59:57Z","published":"2025-06-16T02:59:57Z","title":"DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models","summary":"  Diffusion probabilistic models (DPMs) have achieved impressive success in\nvisual generation. While, they suffer from slow inference speed due to\niterative sampling. Employing fewer sampling steps is an intuitive solution,\nbut this will also introduces discretization error. Existing fast samplers make\ninspiring efforts to reduce discretization error through the adoption of\nhigh-order solvers, potentially reaching a plateau in terms of optimization.\nThis raises the question: can the sampling process be accelerated further? In\nthis paper, we re-examine the nature of sampling errors, discerning that they\ncomprise two distinct elements: the widely recognized discretization error and\nthe less explored approximation error. Our research elucidates the dynamics\nbetween these errors and the step by implementing a dual-error disentanglement\nstrategy. Building on these foundations, we introduce an unified and\ntraining-free acceleration framework, DualFast, designed to enhance the speed\nof DPM sampling by concurrently accounting for both error types, thereby\nminimizing the total sampling error. DualFast is seamlessly compatible with\nexisting samplers and significantly boost their sampling quality and speed,\nparticularly in extremely few sampling steps. We substantiate the effectiveness\nof our framework through comprehensive experiments, spanning both unconditional\nand conditional sampling domains, across both pixel-space and latent-space\nDPMs.\n","authors":["Hu Yu","Hao Luo","Fan Wang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.13058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13056v1","updated":"2025-06-16T02:56:13Z","published":"2025-06-16T02:56:13Z","title":"Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model\n  Learning","summary":"  Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall.\n","authors":["Haibo Qiu","Xiaohan Lan","Fanfan Liu","Xiaohu Sun","Delian Ruan","Peng Shi","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2506.13056v1.pdf","comment":"Project Page: https://github.com/MM-Thinking/Metis-RISE"},{"id":"http://arxiv.org/abs/2506.13051v1","updated":"2025-06-16T02:40:33Z","published":"2025-06-16T02:40:33Z","title":"Stress-Testing Multimodal Foundation Models for Crystallographic\n  Reasoning","summary":"  Evaluating foundation models for crystallographic reasoning requires\nbenchmarks that isolate generalization behavior while enforcing physical\nconstraints. This work introduces a multiscale multicrystal dataset with two\nphysically grounded evaluation protocols to stress-test multimodal generative\nmodels. The Spatial-Exclusion benchmark withholds all supercells of a given\nradius from a diverse dataset, enabling controlled assessments of spatial\ninterpolation and extrapolation. The Compositional-Exclusion benchmark omits\nall samples of a specific chemical composition, probing generalization across\nstoichiometries. Nine vision--language foundation models are prompted with\ncrystallographic images and textual context to generate structural annotations.\nResponses are evaluated via (i) relative errors in lattice parameters and\ndensity, (ii) a physics-consistency index penalizing volumetric violations, and\n(iii) a hallucination score capturing geometric outliers and invalid\nspace-group predictions. These benchmarks establish a reproducible, physically\ninformed framework for assessing generalization, consistency, and reliability\nin large-scale multimodal models. Dataset and code are available at\nhttps://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.\n","authors":["Can Polat","Hasan Kurban","Erchin Serpedin","Mustafa Kurban"],"pdf_url":"https://arxiv.org/pdf/2506.13051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13050v1","updated":"2025-06-16T02:39:45Z","published":"2025-06-16T02:39:45Z","title":"NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling","summary":"  Neural implicit shape representation has drawn significant attention in\nrecent years due to its smoothness, differentiability, and topological\nflexibility. However, directly modeling the shape of a neural implicit surface,\nespecially as the zero-level set of a neural signed distance function (SDF),\nwith sparse geometric control is still a challenging task. Sparse input shape\ncontrol typically includes 3D curve networks or, more generally, 3D curve\nsketches, which are unstructured and cannot be connected to form a curve\nnetwork, and therefore more difficult to deal with. While 3D curve networks or\ncurve sketches provide intuitive shape control, their sparsity and varied\ntopology pose challenges in generating high-quality surfaces to meet such curve\nconstraints. In this paper, we propose NeuVAS, a variational approach to shape\nmodeling using neural implicit surfaces constrained under sparse input shape\ncontrol, including unstructured 3D curve sketches as well as connected 3D curve\nnetworks. Specifically, we introduce a smoothness term based on a functional of\nsurface curvatures to minimize shape variation of the zero-level set surface of\na neural SDF. We also develop a new technique to faithfully model G0 sharp\nfeature curves as specified in the input curve sketches. Comprehensive\ncomparisons with the state-of-the-art methods demonstrate the significant\nadvantages of our method.\n","authors":["Pengfei Wang","Qiujie Dong","Fangtian Liang","Hao Pan","Lei Yang","Congyi Zhang","Guying Lin","Caiming Zhang","Yuanfeng Zhou","Changhe Tu","Shiqing Xin","Alla Sheffer","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13196v2","updated":"2025-06-16T22:37:17Z","published":"2025-02-18T17:46:57Z","title":"GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting\n  View Synthesis","summary":"  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance\nFields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to\nrepresent complex geometry and appearance, GS achieves faster rendering times\nand reduced memory consumption compared to the neural network approach used in\nNeRF. However, quality assessment of GS-generated static content is not yet\nexplored in-depth. This paper describes a subjective quality assessment study\nthat aims to evaluate synthesized videos obtained with several static GS\nstate-of-the-art methods. The methods were applied to diverse visual scenes,\ncovering both 360-degree and forward-facing (FF) camera trajectories. Moreover,\nthe performance of 18 objective quality metrics was analyzed using the scores\nresulting from the subjective study, providing insights into their strengths,\nlimitations, and alignment with human perception. All videos and scores are\nmade available providing a comprehensive database that can be used as benchmark\non GS view synthesis and objective quality metrics.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2502.13196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14035v1","updated":"2025-06-16T22:15:58Z","published":"2025-06-16T22:15:58Z","title":"SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page\n  Retrieval and Iterative Refinement","summary":"  Document Visual Question Answering (DocVQA) is a practical yet challenging\ntask, which is to ask questions based on documents while referring to multiple\npages and different modalities of information, e.g, images and tables. To\nhandle multi-modality, recent methods follow a similar Retrieval Augmented\nGeneration (RAG) pipeline, but utilize Visual Language Models (VLMs) based\nembedding model to embed and retrieve relevant pages as images, and generate\nanswers with VLMs that can accept an image as input. In this paper, we\nintroduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework\nfor DocVQA. It boosts evidence page gathering by first retrieving candidates\nthrough embedding similarity and then filtering and re-ranking these candidates\nbased on page summaries. A single VLM-based reasoner agent repeatedly invokes\nthis dual-cue retriever, iteratively pulling fresh pages into a working memory\nuntil the question is confidently answered. SimpleDoc outperforms previous\nbaselines by 3.2% on average on 4 DocVQA datasets with much fewer pages\nretrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.\n","authors":["Chelsi Jain","Yiran Wu","Yifan Zeng","Jiale Liu","S hengyu Dai","Zhenwen Shao","Qingyun Wu","Huazheng Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21264v2","updated":"2025-06-16T22:06:19Z","published":"2024-10-28T17:57:07Z","title":"LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior","summary":"  We present LARP, a novel video tokenizer designed to overcome limitations in\ncurrent video tokenization methods for autoregressive (AR) generative models.\nUnlike traditional patchwise tokenizers that directly encode local visual\npatches into discrete tokens, LARP introduces a holistic tokenization scheme\nthat gathers information from the visual content using a set of learned\nholistic queries. This design allows LARP to capture more global and semantic\nrepresentations, rather than being limited to local patch-level information.\nFurthermore, it offers flexibility by supporting an arbitrary number of\ndiscrete tokens, enabling adaptive and efficient tokenization based on the\nspecific requirements of the task. To align the discrete token space with\ndownstream AR generation tasks, LARP integrates a lightweight AR transformer as\na training-time prior model that predicts the next token on its discrete latent\nspace. By incorporating the prior model during training, LARP learns a latent\nspace that is not only optimized for video reconstruction but is also\nstructured in a way that is more conducive to autoregressive generation.\nMoreover, this process defines a sequential order for the discrete tokens,\nprogressively pushing them toward an optimal configuration during training,\nensuring smoother and more accurate AR generation at inference time.\nComprehensive experiments demonstrate LARP's strong performance, achieving\nstate-of-the-art FVD on the UCF101 class-conditional video generation\nbenchmark. LARP enhances the compatibility of AR models with videos and opens\nup the potential to build unified high-fidelity multimodal large language\nmodels (MLLMs).\n","authors":["Hanyu Wang","Saksham Suri","Yixuan Ren","Hao Chen","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2410.21264v2.pdf","comment":"ICLR 2025. Project page: https://hywang66.github.io/larp/"},{"id":"http://arxiv.org/abs/2506.14015v1","updated":"2025-06-16T21:26:45Z","published":"2025-06-16T21:26:45Z","title":"Disentangling 3D from Large Vision-Language Models for Controlled\n  Portrait Generation","summary":"  We consider the problem of disentangling 3D from large vision-language\nmodels, which we show on generative 3D portraits. This allows free-form text\ncontrol of appearance attributes like age, hair style, and glasses, and 3D\ngeometry control of face expression and camera pose. In this setting, we assume\nwe use a pre-trained large vision-language model (LVLM; CLIP) to generate from\na smaller 2D dataset with no additional paired labels and with a pre-defined 3D\nmorphable model (FLAME). First, we disentangle using canonicalization to a 2D\nreference frame from a deformable neural 3D triplane representation. But\nanother form of entanglement arises from the significant noise in the LVLM's\nembedding space that describes irrelevant features. This damages output quality\nand diversity, but we overcome this with a Jacobian regularization that can be\ncomputed efficiently with a stochastic approximator. Compared to existing\nmethods, our approach produces portraits with added text and 3D control, where\nportraits remain consistent when either control is changed. Broadly, this\napproach lets creators control 3D generators on their own 2D face data without\nneeding resources to label large data or train large models.\n","authors":["Nick Yiwen Huang","Akin Caliskan","Berkay Kicanaoglu","James Tompkin","Hyeongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2506.14015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07392v3","updated":"2025-06-16T21:22:00Z","published":"2024-05-12T23:00:53Z","title":"NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU","summary":"  Many existing visual SLAM methods can achieve high localization accuracy in\ndynamic environments by leveraging deep learning to mask moving objects.\nHowever, these methods incur significant computational overhead as the camera\ntracking needs to wait for the deep neural network to generate mask at each\nframe, and they typically require GPUs for real-time operation, which restricts\ntheir practicality in real-world robotic applications. Therefore, this paper\nproposes a real-time dynamic SLAM system that runs exclusively on a CPU. Our\napproach incorporates a mask propagation mechanism that decouples camera\ntracking and deep learning-based masking for each frame. We also introduce a\nhybrid tracking strategy that integrates ORB features with optical flow\nmethods, enhancing both robustness and efficiency by selectively allocating\ncomputational resources to input frames. Compared to previous methods, our\nsystem maintains high localization accuracy in dynamic environments while\nachieving a tracking frame rate of 60 FPS on a laptop CPU. These results\ndemonstrate the feasibility of utilizing deep learning for dynamic SLAM without\nGPU support. Since most existing dynamic SLAM systems are not open-source, we\nmake our code publicly available at: https://github.com/yuhaozhang7/NGD-SLAM\n","authors":["Yuhao Zhang","Mihai Bujanca","Mikel Luján"],"pdf_url":"https://arxiv.org/pdf/2405.07392v3.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14008v1","updated":"2025-06-16T21:11:56Z","published":"2025-06-16T21:11:56Z","title":"FindMeIfYouCan: Bringing Open Set metrics to $\\textit{near} $, $\n  \\textit{far} $ and $\\textit{farther}$ Out-of-Distribution Object Detection","summary":"  State-of-the-art Object Detection (OD) methods predominantly operate under a\nclosed-world assumption, where test-time categories match those encountered\nduring training. However, detecting and localizing unknown objects is crucial\nfor safety-critical applications in domains such as autonomous driving and\nmedical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a\nvital research direction for OD, focusing on identifying incorrect predictions\ntypically associated with unknown objects. This paper shows that the current\nevaluation protocol for OOD-OD violates the assumption of non-overlapping\nobjects with respect to the In-Distribution (ID) datasets, and obscures crucial\nsituations such as ignoring unknown objects, potentially leading to\noverconfidence in deployment scenarios where truly novel objects might be\nencountered. To address these limitations, we manually curate, and enrich the\nexisting benchmark by exploiting semantic similarity to create new evaluation\nsplits categorized as $\\textit{near}$, $\\textit{far}$, and $\\textit{farther}$\nfrom ID distributions. Additionally, we incorporate established metrics from\nthe Open Set community, providing deeper insights into how effectively methods\ndetect unknowns, when they ignore them, and when they mistakenly classify OOD\nobjects as ID. Our comprehensive evaluation demonstrates that semantically and\nvisually close OOD objects are easier to localize than far ones, but are also\nmore easily confounded with ID objects. $\\textit{Far}$ and $\\textit{farther}$\nobjects are harder to localize but less prone to be taken for an ID object.\n","authors":["Daniel Montoya","Aymen Bouguerra","Alexandra Gomez-Villa","Fabio Arnez"],"pdf_url":"https://arxiv.org/pdf/2506.14008v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2506.13993v1","updated":"2025-06-16T20:50:05Z","published":"2025-06-16T20:50:05Z","title":"Mapping Farmed Landscapes from Remote Sensing","summary":"  Effective management of agricultural landscapes is critical for meeting\nglobal biodiversity targets, but efforts are hampered by the absence of\ndetailed, large-scale ecological maps. To address this, we introduce\nFarmscapes, the first large-scale (covering most of England), high-resolution\n(25cm) map of rural landscape features, including ecologically vital elements\nlike hedgerows, woodlands, and stone walls. This map was generated using a deep\nlearning segmentation model trained on a novel, dataset of 942 manually\nannotated tiles derived from aerial imagery. Our model accurately identifies\nkey habitats, achieving high f1-scores for woodland (96\\%) and farmed land\n(95\\%), and demonstrates strong capability in segmenting linear features, with\nan F1-score of 72\\% for hedgerows. By releasing the England-wide map on Google\nEarth Engine, we provide a powerful, open-access tool for ecologists and\npolicymakers. This work enables data-driven planning for habitat restoration,\nsupports the monitoring of initiatives like the EU Biodiversity Strategy, and\nlays the foundation for advanced analysis of landscape connectivity.\n","authors":["Michelangelo Conserva","Alex Wilson","Charlotte Stanton","Vishal Batchu","Varun Gulshan"],"pdf_url":"https://arxiv.org/pdf/2506.13993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00262v4","updated":"2025-06-16T20:13:11Z","published":"2023-06-01T00:46:40Z","title":"Maximizing Information in Domain-Invariant Representation Improves\n  Transfer Learning","summary":"  The most effective domain adaptation (DA) technique involves the\ndecomposition of data representation into a domain-independent representation\n(DIRep) and a domain-dependent representation (DDRep). A classifier is trained\nby using the DIRep on the labeled source images. Since the DIRep is domain\ninvariant, the classifier can be \"transferred\" to make predictions for the\ntarget domain with no (or few) labels. However, information useful for\nclassification in the target domain can \"hide\" in the DDRep. Current DA\nalgorithms, such as Domain-Separation Networks (DSN), do not adequately address\nthis issue. DSN's weak constraint to enforce the orthogonality of DIRep and\nDDRep allows this hiding effect and can result in poor performance. To address\nthis shortcoming, we develop a new algorithm wherein a stronger constraint is\nimposed to minimize the information content in DDRep to create a DIRep that\nretains relevant information about the target labels and, in turn, results in a\nbetter invariant representation. By using synthetic datasets, we show\nexplicitly that depending on the initialization, DSN, with its weaker\nconstraint, can lead to sub-optimal solutions with poorer DA performance. In\ncontrast, our algorithm is robust against such perturbations. We demonstrate\nthe equal-or-better performance of our approach against DSN and other recent DA\nmethods by using several standard benchmark image datasets. We further\nhighlight the compatibility of our algorithm with pre-trained models for\nclassifying real-world images and showcase its adaptability and versatility\nthrough its application in network intrusion detection.\n","authors":["Adrian Shuai Li","Elisa Bertino","Xuan-Hong Dang","Ankush Singla","Yuhai Tu","Mark N Wegman"],"pdf_url":"https://arxiv.org/pdf/2306.00262v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20612v2","updated":"2025-06-16T19:08:27Z","published":"2025-05-27T01:24:29Z","title":"Roboflow100-VL: A Multi-Domain Object Detection Benchmark for\n  Vision-Language Models","summary":"  Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025\nFoundational FSOD competition and share insights from the community. Notably,\nthe winning team significantly outperforms our baseline by 16.8 mAP! Our code\nand dataset are available at https://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/\n","authors":["Peter Robicheaux","Matvei Popov","Anish Madan","Isaac Robinson","Joseph Nelson","Deva Ramanan","Neehar Peri"],"pdf_url":"https://arxiv.org/pdf/2505.20612v2.pdf","comment":"The first two authors contributed equally. Project Page:\n  https://rf100-vl.org/"},{"id":"http://arxiv.org/abs/2307.15220v5","updated":"2025-06-16T19:07:50Z","published":"2023-07-27T22:38:12Z","title":"Learning Multi-modal Representations by Watching Hundreds of Surgical\n  Video Lectures","summary":"  Recent advancements in surgical computer vision applications have been driven\nby vision-only models, which do not explicitly integrate the rich semantics of\nlanguage into their design. These methods rely on manually annotated surgical\nvideos to predict a fixed set of object categories, limiting their\ngeneralizability to unseen surgical procedures and downstream tasks. In this\nwork, we put forward the idea that the surgical video lectures available\nthrough open surgical e-learning platforms can provide effective vision and\nlanguage supervisory signals for multi-modal representation learning without\nrelying on manual annotations. We address the surgery-specific linguistic\nchallenges present in surgical video lectures by employing multiple\ncomplementary automatic speech recognition systems to generate text\ntranscriptions. We then present a novel method, SurgVLP - Surgical Vision\nLanguage Pre-training, for multi-modal representation learning. Extensive\nexperiments across diverse surgical procedures and tasks demonstrate that the\nmulti-modal representations learned by SurgVLP exhibit strong transferability\nand adaptability in surgical video analysis. Furthermore, our zero-shot\nevaluations highlight SurgVLP's potential as a general-purpose foundation model\nfor surgical workflow analysis, reducing the reliance on extensive manual\nannotations for downstream tasks, and facilitating adaptation methods such as\nfew-shot learning to build a scalable and data-efficient solution for various\ndownstream surgical applications. The [training\ncode](https://github.com/CAMMA-public/PeskaVLP) and\n[weights](https://github.com/CAMMA-public/SurgVLP) are public.\n","authors":["Kun Yuan","Vinkle Srivastav","Tong Yu","Joel L. Lavanchy","Jacques Marescaux","Pietro Mascagni","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2307.15220v5.pdf","comment":"Accepted by Medical Image Analysis (MedIA), 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.13743v1","updated":"2025-06-16T17:53:18Z","published":"2025-06-16T17:53:18Z","title":"LTRR: Learning To Rank Retrievers for LLMs","summary":"  Retrieval-Augmented Generation (RAG) systems typically rely on a single fixed\nretriever, despite growing evidence that no single retriever performs optimally\nacross all query types. In this paper, we explore a query routing approach that\ndynamically selects from a pool of retrievers based on the query, using both\ntrain-free heuristics and learned routing models. We frame routing as a\nlearning-to-rank (LTR) problem and introduce LTRR, a framework that learns to\nrank retrievers by their expected utility gain to downstream LLM performance.\nOur experiments, conducted on synthetic QA data with controlled query type\nvariations, show that routing-based RAG systems can outperform the best\nsingle-retriever-based systems. Performance gains are especially pronounced in\nmodels trained with the Answer Correctness (AC) metric and with pairwise\nlearning approaches, especially with XGBoost. We also observe improvements in\ngeneralization to out-of-distribution queries. As part of the SIGIR 2025\nLiveRAG challenge, our submitted system demonstrated the practical viability of\nour approach, achieving competitive performance in both answer correctness and\nfaithfulness. These findings highlight the importance of both training\nmethodology and metric selection in query routing for RAG systems.\n","authors":["To Eun Kim","Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2506.13743v1.pdf","comment":"SIGIR 2025 LiveRAG Spotlight"},{"id":"http://arxiv.org/abs/2506.13695v1","updated":"2025-06-16T16:58:55Z","published":"2025-06-16T16:58:55Z","title":"OneRec Technical Report","summary":"  Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.\n","authors":["Guorui Zhou","Jiaxin Deng","Jinghao Zhang","Kuo Cai","Lejian Ren","Qiang Luo","Qianqian Wang","Qigen Hu","Rui Huang","Shiyao Wang","Weifeng Ding","Wuchao Li","Xinchen Luo","Xingmei Wang","Zexuan Cheng","Zixing Zhang","Bin Zhang","Boxuan Wang","Chaoyi Ma","Chengru Song","Chenhui Wang","Di Wang","Dongxue Meng","Fan Yang","Fangyu Zhang","Feng Jiang","Fuxing Zhang","Gang Wang","Guowang Zhang","Han Li","Hengrui Hu","Hezheng Lin","Hongtao Cheng","Hongyang Cao","Huanjie Wang","Jiaming Huang","Jiapeng Chen","Jiaqiang Liu","Jinghui Jia","Kun Gai","Lantao Hu","Liang Zeng","Liao Yu","Qiang Wang","Qidong Zhou","Shengzhe Wang","Shihui He","Shuang Yang","Shujie Yang","Sui Huang","Tao Wu","Tiantian He","Tingting Gao","Wei Yuan","Xiao Liang","Xiaoxiao Xu","Xugang Liu","Yan Wang","Yi Wang","Yiwu Liu","Yue Song","Yufei Zhang","Yunfan Wu","Yunfeng Zhao","Zhanyu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13695v1.pdf","comment":"Authors are listed alphabetically by their first name"},{"id":"http://arxiv.org/abs/2504.05317v2","updated":"2025-06-16T16:22:39Z","published":"2025-02-21T09:43:18Z","title":"On Synthesizing Data for Context Attribution in Question Answering","summary":"  Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.\n","authors":["Gorjan Radevski","Kiril Gashteovski","Shahbaz Syed","Christopher Malon","Sebastien Nicolas","Chia-Chien Hung","Timo Sztyler","Verena Heußer","Wiem Ben Rim","Masafumi Enomoto","Kunihiro Takeoka","Masafumi Oyamada","Goran Glavaš","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2504.05317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13607v1","updated":"2025-06-16T15:34:29Z","published":"2025-06-16T15:34:29Z","title":"Tree-Based Text Retrieval via Hierarchical Clustering in RAGFrameworks:\n  Application on Taiwanese Regulations","summary":"  Traditional Retrieval-Augmented Generation (RAG) systems employ brute-force\ninner product search to retrieve the top-k most similar documents, then\ncombined with the user query and passed to a language model. This allows the\nmodel to access external knowledge and reduce hallucinations. However,\nselecting an appropriate k value remains a significant challenge in practical\napplications: a small k may fail to retrieve sufficient information, while a\nlarge k can introduce excessive and irrelevant content. To address this, we\npropose a hierarchical clustering-based retrieval method that eliminates the\nneed to predefine k. Our approach maintains the accuracy and relevance of\nsystem responses while adaptively selecting semantically relevant content. In\nthe experiment stage, we applied our method to a Taiwanese legal dataset with\nexpert-graded queries. The results show that our approach achieves superior\nperformance in expert evaluations and maintains high precision while\neliminating the need to predefine k, demonstrating improved accuracy and\ninterpretability in legal text retrieval tasks. Our framework is simple to\nimplement and easily integrates with existing RAG pipelines, making it a\npractical solution for real-world applications under limited resources.\n","authors":["Chia-Heng Yu","Yen-Lung Tsai"],"pdf_url":"https://arxiv.org/pdf/2506.13607v1.pdf","comment":"19 pages, 5 figures, Code available at\n  https://github.com/arthur422tp/hierachical"},{"id":"http://arxiv.org/abs/2506.11502v2","updated":"2025-06-16T14:36:50Z","published":"2025-06-13T06:58:25Z","title":"A Reference Model and Patterns for Production Event Data Enrichment","summary":"  With the advent of digital transformation, organisations are increasingly\ngenerating large volumes of data through the execution of various processes\nacross disparate systems. By integrating data from these heterogeneous sources,\nit becomes possible to derive new insights essential for tasks such as\nmonitoring and analysing process performance. Typically, this information is\nextracted during a data pre-processing or engineering phase. However, this step\nis often performed in an ad-hoc manner and is time-consuming and\nlabour-intensive. To streamline this process, we introduce a reference model\nand a collection of patterns designed to enrich production event data. The\nreference model provides a standard way for storing and extracting production\nevent data. The patterns describe common information extraction tasks and how\nsuch tasks can be automated effectively. The reference model is developed by\ncombining the ISA-95 industry standard with the Event Knowledge Graph\nformalism. The patterns are developed based on empirical observations from\nevent data sets originating in manufacturing processes and are formalised using\nthe reference model. We evaluate the relevance and applicability of these\npatterns by demonstrating their application to use cases.\n","authors":["Mark van der Pas","Remco Dijkman","Alp Akçay","Ivo Adan","John Walker"],"pdf_url":"https://arxiv.org/pdf/2506.11502v2.pdf","comment":"Extended version of the paper submitted to EDOC 2025"},{"id":"http://arxiv.org/abs/2502.19596v3","updated":"2025-06-16T14:27:30Z","published":"2025-02-26T22:20:08Z","title":"Reference-Aligned Retrieval-Augmented Question Answering over\n  Heterogeneous Proprietary Documents","summary":"  Proprietary corporate documents contain rich domain-specific knowledge, but\ntheir overwhelming volume and disorganized structure make it difficult even for\nemployees to access the right information when needed. For example, in the\nautomotive industry, vehicle crash-collision tests, each costing hundreds of\nthousands of dollars, produce highly detailed documentation. However,\nretrieving relevant content during decision-making remains time-consuming due\nto the scale and complexity of the material. While Retrieval-Augmented\nGeneration (RAG)-based Question Answering (QA) systems offer a promising\nsolution, building an internal RAG-QA system poses several challenges: (1)\nhandling heterogeneous multi-modal data sources, (2) preserving data\nconfidentiality, and (3) enabling traceability between each piece of\ninformation in the generated answer and its original source document. To\naddress these, we propose a RAG-QA framework for internal enterprise use,\nconsisting of: (1) a data pipeline that converts raw multi-modal documents into\na structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving\narchitecture, and (3) a lightweight reference matcher that links answer\nsegments to supporting content. Applied to the automotive domain, our system\nimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),\nand helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale\nratings from both human and LLM judge.\n","authors":["Nayoung Choi","Grace Byun","Andrew Chung","Ellie S. Paek","Shinsun Lee","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2502.19596v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02670v3","updated":"2025-06-16T14:19:01Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10512v2","updated":"2025-06-16T14:08:36Z","published":"2025-04-10T01:31:11Z","title":"JEPA4Rec: Learning Effective Language Representations for Sequential\n  Recommendation via Joint Embedding Predictive Architecture","summary":"  Language representation learning has emerged as a promising approach for\nsequential recommendation, thanks to its ability to learn generalizable\nrepresentations. However, despite its advantages, this approach still struggles\nwith data sparsity and a limited understanding of common-sense user\npreferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a\nframework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding\n$\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item\ntextual descriptions. JEPA4Rec captures semantically rich and transferable\nrepresentations, improving recommendation performance and reducing reliance on\nlarge-scale pre-training data. Specifically, JEPA4Rec represents items as text\nsentences by flattening descriptive information such as $\\textit{title,\ncategory}$, and other attributes. To encode these sentences, we employ a\nbidirectional Transformer encoder with modified embedding layers tailored for\ncapturing item information in recommendation datasets. We apply masking to text\nsentences and use them to predict the representations of the unmasked\nsentences, helping the model learn generalizable item embeddings. To further\nimprove recommendation performance and language understanding, we employ a\ntwo-stage training strategy incorporating self-supervised learning losses.\nExperiments on six real-world datasets demonstrate that JEPA4Rec consistently\noutperforms state-of-the-art methods, particularly in cross-domain,\ncross-platform, and low-resource scenarios.\n","authors":["Minh-Anh Nguyen","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2504.10512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13496v1","updated":"2025-06-16T13:53:02Z","published":"2025-06-16T13:53:02Z","title":"Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval","summary":"  Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware.\n","authors":["Kshitij Kavimandan","Angelos Nalmpantis","Emma Beauxis-Aussalet","Robert-Jan Sips"],"pdf_url":"https://arxiv.org/pdf/2506.13496v1.pdf","comment":"5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025"},{"id":"http://arxiv.org/abs/2506.13409v1","updated":"2025-06-16T12:23:17Z","published":"2025-06-16T12:23:17Z","title":"Beyond One-Size-Fits-All: A Study of Neural and Behavioural Variability\n  Across Different Recommendation Categories","summary":"  Traditionally, Recommender Systems (RS) have primarily measured performance\nbased on the accuracy and relevance of their recommendations. However, this\nalgorithmic-centric approach overlooks how different types of recommendations\nimpact user engagement and shape the overall quality of experience. In this\npaper, we shift the focus to the user and address for the first time the\nchallenge of decoding the neural and behavioural variability across distinct\nrecommendation categories, considering more than just relevance. Specifically,\nwe conducted a controlled study using a comprehensive e-commerce dataset\ncontaining various recommendation types, and collected Electroencephalography\nand behavioural data. We analysed both neural and behavioural responses to\nrecommendations that were categorised as Exact, Substitute, Complement, or\nIrrelevant products within search query results. Our findings offer novel\ninsights into user preferences and decision-making processes, revealing\nmeaningful relationships between behavioural and neural patterns for each\ncategory, but also indicate inter-subject variability.\n","authors":["Georgios Koutroumpas","Sebastian Idesis","Mireia Masias Bruns","Carlos Segura","Joemon M. Jose","Sergi Abadal","Ioannis Arapakis"],"pdf_url":"https://arxiv.org/pdf/2506.13409v1.pdf","comment":"11 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.13380v1","updated":"2025-06-16T11:44:28Z","published":"2025-06-16T11:44:28Z","title":"Decompositional Reasoning for Graph Retrieval with Large Language Models","summary":"  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n","authors":["Valentin Six","Evan Dufraisse","Gaël de Chalendar"],"pdf_url":"https://arxiv.org/pdf/2506.13380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13333v1","updated":"2025-06-16T10:23:46Z","published":"2025-06-16T10:23:46Z","title":"Digital Transformation of Urban Planning in Australia: Influencing\n  Factors and Key Challenges","summary":"  Over the past two decades, several governments in developing and developed\ncountries have started their journey toward digital transformation. However,\nthe pace and maturity of digital technologies and strategies are different\nbetween public services. Current literature indicates that research on the\ndigital transformation of urban planning is still developing. Therefore, the\naim of this study is to understand the influencing factors and key challenges\nfor the digital transformation of urban planning in Australia. The study adopts\nthe inter-organisational theory and Planning Support Science (PSScience) under\nthe Technological, Organisational, and External Environmental (TOE) framework.\nIt involves a multiple case study, administered semi-structured interviews with\nthirteen IT and urban planning experts across Victoria and New South Wales\ngovernments and private industries. The study findings indicate that the main\nchallenges for digital transformation of the Australian urban planning system\nare related to organisational and external environmental factors. Furthermore,\na digital maturity model is absent in the Australian urban planning industry.\nThis study offers important implications to research and practice related to\ndigital transformation in urban planning.\n","authors":["Soheil Sabri","Sherah Kurnia"],"pdf_url":"https://arxiv.org/pdf/2506.13333v1.pdf","comment":"30 pages, 2 figures, Master's Thesis"},{"id":"http://arxiv.org/abs/2506.13315v1","updated":"2025-06-16T09:56:10Z","published":"2025-06-16T09:56:10Z","title":"Gated Rotary-Enhanced Linear Attention for Long-term Sequential\n  Recommendation","summary":"  In Sequential Recommendation Systems (SRSs), Transformer models show\nremarkable performance but face computation cost challenges when modeling\nlong-term user behavior sequences due to the quadratic complexity of the\ndot-product attention mechanism. By approximating the dot-product attention,\nlinear attention provides an efficient option with linear complexity. However,\nexisting linear attention methods face two limitations: 1) they often use\nlearnable position encodings, which incur extra computational costs in\nlong-term sequence scenarios, and 2) they may not consider the user's\nfine-grained local preferences and confuse these with the actual change of\nlong-term interests. To remedy these drawbacks, we propose a long-term\nsequential Recommendation model with Gated Rotary Enhanced Linear Attention\n(RecGRELA). Specifically, we first propose a Rotary-Enhanced Linear Attention\n(RELA) module to model long-range dependency within the user's historical\ninformation using rotary position encodings. We then introduce a local short\noperation to incorporate local preferences and demonstrate the theoretical\ninsight. We further introduce a SiLU-based Gated mechanism for RELA (GRELA) to\nhelp the model determine whether a user's behavior indicates local interest or\na genuine shift in long-term preferences. Experimental results on four public\ndatasets demonstrate that our RecGRELA achieves state-of-the-art performance\ncompared to existing SRSs while maintaining low memory overhead.\n","authors":["Juntao Hu","Wei Zhou","Huayi Shen","Xiao Du","Jie Liao","Junhao Wen","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2506.13315v1.pdf","comment":"24 pages,9 figures"},{"id":"http://arxiv.org/abs/2506.13256v1","updated":"2025-06-16T08:52:58Z","published":"2025-06-16T08:52:58Z","title":"Accessibility Barriers in Multi-Terabyte Public Datasets: The Gap\n  Between Promise and Practice","summary":"  The promise of \"free and open\" multi-terabyte datasets often collides with\nharsh realities. While these datasets may be technically accessible, practical\nbarriers -- from processing complexity to hidden costs -- create a system that\nprimarily serves well-funded institutions. This study examines accessibility\nchallenges across web crawls, satellite imagery, scientific data, and\ncollaborative projects, revealing a consistent two-tier system where\ntheoretical openness masks practical exclusivity. Our analysis demonstrates\nthat datasets marketed as \"publicly accessible\" typically require minimum\ninvestments of \\$1,000+ for meaningful analysis, with complex processing\npipelines demanding \\$10,000-100,000+ in infrastructure costs. The\ninfrastructure requirements -- distributed computing knowledge, domain\nexpertise, and substantial budgets -- effectively gatekeep these datasets\ndespite their \"open\" status, limiting practical accessibility to those with\ninstitutional support or substantial resources.\n","authors":["Marc Bara"],"pdf_url":"https://arxiv.org/pdf/2506.13256v1.pdf","comment":"5 pages, 28 references. Analysis of practical barriers to accessing\n  multi-terabyte public datasets"},{"id":"http://arxiv.org/abs/2506.13252v1","updated":"2025-06-16T08:49:21Z","published":"2025-06-16T08:49:21Z","title":"Vector Ontologies as an LLM world view extraction method","summary":"  Large Language Models (LLMs) possess intricate internal representations of\nthe world, yet these latent structures are notoriously difficult to interpret\nor repurpose beyond the original prediction task. Building on our earlier work\n(Rothenfusser, 2025), which introduced the concept of vector ontologies as a\nframework for translating high-dimensional neural representations into\ninterpretable geometric structures, this paper provides the first empirical\nvalidation of that approach. A vector ontology defines a domain-specific vector\nspace spanned by ontologically meaningful dimensions, allowing geometric\nanalysis of concepts and relationships within a domain. We construct an\n8-dimensional vector ontology of musical genres based on Spotify audio features\nand test whether an LLM's internal world model of music can be consistently and\naccurately projected into this space. Using GPT-4o-mini, we extract genre\nrepresentations through multiple natural language prompts and analyze the\nconsistency of these projections across linguistic variations and their\nalignment with ground-truth data. Our results show (1) high spatial consistency\nof genre projections across 47 query formulations, (2) strong alignment between\nLLM-inferred genre locations and real-world audio feature distributions, and\n(3) evidence of a direct relationship between prompt phrasing and spatial\nshifts in the LLM's inferred vector ontology. These findings demonstrate that\nLLMs internalize structured, repurposable knowledge and that vector ontologies\noffer a promising method for extracting and analyzing this knowledge in a\ntransparent and verifiable way.\n","authors":["Kaspar Rothenfusser","Bekk Blando"],"pdf_url":"https://arxiv.org/pdf/2506.13252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13188v1","updated":"2025-06-16T07:55:44Z","published":"2025-06-16T07:55:44Z","title":"SPOT: Bridging Natural Language and Geospatial Search for Investigative\n  Journalists","summary":"  OpenStreetMap (OSM) is a vital resource for investigative journalists doing\ngeolocation verification. However, existing tools to query OSM data such as\nOverpass Turbo require familiarity with complex query languages, creating\nbarriers for non-technical users. We present SPOT, an open source natural\nlanguage interface that makes OSM's rich, tag-based geographic data more\naccessible through intuitive scene descriptions. SPOT interprets user inputs as\nstructured representations of geospatial object configurations using fine-tuned\nLarge Language Models (LLMs), with results being displayed in an interactive\nmap interface. While more general geospatial search tasks are conceivable, SPOT\nis specifically designed for use in investigative journalism, addressing\nreal-world challenges such as hallucinations in model output, inconsistencies\nin OSM tagging, and the noisy nature of user input. It combines a novel\nsynthetic data pipeline with a semantic bundling system to enable robust,\naccurate query generation. To our knowledge, SPOT is the first system to\nachieve reliable natural language access to OSM data at this level of accuracy.\nBy lowering the technical barrier to geolocation verification, SPOT contributes\na practical tool to the broader efforts to support fact-checking and combat\ndisinformation.\n","authors":["Lynn Khellaf","Ipek Baris Schlicht","Tilman Mirass","Julia Bayer","Tilman Wagner","Ruben Bouwmeester"],"pdf_url":"https://arxiv.org/pdf/2506.13188v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2402.11827v2","updated":"2025-06-16T03:39:07Z","published":"2024-02-19T04:41:31Z","title":"Ask Optimal Questions: Aligning Large Language Models with Retriever's\n  Preference in Conversation","summary":"  Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM on this dataset to align\nit with the retrievers' feedback. Our resulting model demonstrates superiority\non two benchmarks, surpassing the previous state-of-the-art performance of\nrewrite-then-retrieve approaches.\n","authors":["Chanwoong Yoon","Gangwoo Kim","Byeongguk Jeon","Sungdong Kim","Yohan Jo","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2402.11827v2.pdf","comment":"NAACL 2025 (findings)"},{"id":"http://arxiv.org/abs/2506.11603v2","updated":"2025-06-16T03:35:12Z","published":"2025-06-13T09:17:36Z","title":"TongSearch-QR: Reinforced Query Reasoning for Retrieval","summary":"  Traditional information retrieval (IR) methods excel at textual and semantic\nmatching but struggle in reasoning-intensive retrieval tasks that require\nmulti-hop inference or complex semantic understanding between queries and\ndocuments. One promising solution is to explicitly rewrite or augment queries\nusing large language models (LLMs) to elicit reasoning-relevant content prior\nto retrieval. However, the widespread use of large-scale language models like\nGPT-4 or LLaMA3-70B remains impractical due to their high inference cost and\nlimited deployability in real-world systems. In this work, we introduce\nTongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of\nsmall-scale language models for query reasoning and rewriting in\nreasoning-intensive retrieval. With a novel semi-rule-based reward function, we\nemploy reinforcement learning approaches enabling smaller language models, e,g,\nQwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning\nperformance rivaling large-scale language models without their prohibitive\ninference costs. Experiment results on BRIGHT benchmark show that with BM25 as\nretrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly\noutperform existing baselines, including prompt-based query reasoners and some\nlatest dense retrievers trained for reasoning-intensive retrieval tasks,\noffering superior adaptability for real-world deployment.\n","authors":["Xubo Qin","Jun Bai","Jiaqi Li","Zixia Jia","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2506.11603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11999v2","updated":"2025-06-16T03:10:31Z","published":"2025-06-13T17:54:12Z","title":"Generative Representational Learning of Foundation Models for\n  Recommendation","summary":"  Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.\n","authors":["Zheli Zhou","Chenxu Zhu","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2506.11999v2.pdf","comment":"Project page is available at https://junkfood436.github.io/RecFound/"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2506.13763v1","updated":"2025-06-16T17:59:54Z","published":"2025-06-16T17:59:54Z","title":"Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss\n  Value","summary":"  Diffusion models have achieved remarkable success in generative modeling.\nDespite more stable training, the loss of diffusion models is not indicative of\nabsolute data-fitting quality, since its optimal value is typically not zero\nbut unknown, leading to confusion between large optimal loss and insufficient\nmodel capacity. In this work, we advocate the need to estimate the optimal loss\nvalue for diagnosing and improving diffusion models. We first derive the\noptimal loss in closed form under a unified formulation of diffusion models,\nand develop effective estimators for it, including a stochastic variant\nscalable to large datasets with proper control of variance and bias. With this\ntool, we unlock the inherent metric for diagnosing the training quality of\nmainstream diffusion model variants, and develop a more performant training\nschedule based on the optimal loss. Moreover, using models with 120M to 1.5B\nparameters, we find that the power law is better demonstrated after subtracting\nthe optimal loss from the actual training loss, suggesting a more principled\nsetting for investigating the scaling law for diffusion models.\n","authors":["Yixian Xu","Shengjie Luo","Liwei Wang","Di He","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13763v1.pdf","comment":"29 pages, 8 figures, 3 tables. Preprint. Work in Progress"},{"id":"http://arxiv.org/abs/2506.13759v1","updated":"2025-06-16T17:59:08Z","published":"2025-06-16T17:59:08Z","title":"Discrete Diffusion in Large Language and Multimodal Models: A Survey","summary":"  In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey\n","authors":["Runpeng Yu","Qi Li","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13758v1","updated":"2025-06-16T17:59:02Z","published":"2025-06-16T17:59:02Z","title":"AI reconstruction of European weather from the Euro-Atlantic regimes","summary":"  We present a non-linear AI-model designed to reconstruct monthly mean\nanomalies of the European temperature and precipitation based on the\nEuro-Atlantic Weather regimes (WR) indices. WR represent recurrent,\nquasi-stationary, and persistent states of the atmospheric circulation that\nexert considerable influence over the European weather, therefore offering an\nopportunity for sub-seasonal to seasonal forecasting. While much research has\nfocused on studying the correlation and impacts of the WR on European weather,\nthe estimation of ground-level climate variables, such as temperature and\nprecipitation, from Euro-Atlantic WR remains largely unexplored and is\ncurrently limited to linear methods. The presented AI model can capture and\nintroduce complex non-linearities in the relation between the WR indices,\ndescribing the state of the Euro-Atlantic atmospheric circulation and the\ncorresponding surface temperature and precipitation anomalies in Europe. We\ndiscuss the AI-model performance in reconstructing the monthly mean two-meter\ntemperature and total precipitation anomalies in the European winter and\nsummer, also varying the number of WR used to describe the monthly atmospheric\ncirculation. We assess the impact of errors on the WR indices in the\nreconstruction and show that a mean absolute relative error below 80% yields\nimproved seasonal reconstruction compared to the ECMWF operational seasonal\nforecast system, SEAS5. As a demonstration of practical applicability, we\nevaluate the model using WR indices predicted by SEAS5, finding slightly better\nor comparable skill relative to the SEAS5 forecast itself. Our findings\ndemonstrate that WR-based anomaly reconstruction, powered by AI tools, offers a\npromising pathway for sub-seasonal and seasonal forecasting.\n","authors":["A. Camilletti","G. Franch","E. Tomasi","M. Cristoforetti"],"pdf_url":"https://arxiv.org/pdf/2506.13758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13755v1","updated":"2025-06-16T17:58:09Z","published":"2025-06-16T17:58:09Z","title":"MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering","summary":"  This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%.\n","authors":["Arya Fayyazi","Mehdi Kamal","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2506.13755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13754v1","updated":"2025-06-16T17:58:00Z","published":"2025-06-16T17:58:00Z","title":"VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models","summary":"  We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.\n","authors":["Edward Li","Zichen Wang","Jiahe Huang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2506.13754v1.pdf","comment":"Submitted to NeurIPS 2025. Project page: https://videopde.github.io/"},{"id":"http://arxiv.org/abs/2506.13746v1","updated":"2025-06-16T17:54:28Z","published":"2025-06-16T17:54:28Z","title":"Evaluating Large Language Models for Phishing Detection,\n  Self-Consistency, Faithfulness, and Explainability","summary":"  Phishing attacks remain one of the most prevalent and persistent\ncybersecurity threat with attackers continuously evolving and intensifying\ntactics to evade the general detection system. Despite significant advances in\nartificial intelligence and machine learning, faithfully reproducing the\ninterpretable reasoning with classification and explainability that underpin\nphishing judgments remains challenging. Due to recent advancement in Natural\nLanguage Processing, Large Language Models (LLMs) show a promising direction\nand potential for improving domain specific phishing classification tasks.\nHowever, enhancing the reliability and robustness of classification models\nrequires not only accurate predictions from LLMs but also consistent and\ntrustworthy explanations aligning with those predictions. Therefore, a key\nquestion remains: can LLMs not only classify phishing emails accurately but\nalso generate explanations that are reliably aligned with their predictions and\ninternally self-consistent? To answer these questions, we have fine-tuned\ntransformer based models, including BERT, Llama models, and Wizard, to improve\ndomain relevance and make them more tailored to phishing specific distinctions,\nusing Binary Sequence Classification, Contrastive Learning (CL) and Direct\nPreference Optimization (DPO). To that end, we examined their performance in\nphishing classification and explainability by applying the ConsistenCy measure\nbased on SHAPley values (CC SHAP), which measures prediction explanation token\nalignment to test the model's internal faithfulness and consistency and uncover\nthe rationale behind its predictions and reasoning. Overall, our findings show\nthat Llama models exhibit stronger prediction explanation token alignment with\nhigher CC SHAP scores despite lacking reliable decision making accuracy,\nwhereas Wizard achieves better prediction accuracy but lower CC SHAP scores.\n","authors":["Shova Kuikel","Aritran Piplai","Palvi Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2506.13746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13741v1","updated":"2025-06-16T17:51:33Z","published":"2025-06-16T17:51:33Z","title":"PB$^2$: Preference Space Exploration via Population-Based Methods in\n  Preference-Based Reinforcement Learning","summary":"  Preference-based reinforcement learning (PbRL) has emerged as a promising\napproach for learning behaviors from human feedback without predefined reward\nfunctions. However, current PbRL methods face a critical challenge in\neffectively exploring the preference space, often converging prematurely to\nsuboptimal policies that satisfy only a narrow subset of human preferences. In\nthis work, we identify and address this preference exploration problem through\npopulation-based methods. We demonstrate that maintaining a diverse population\nof agents enables more comprehensive exploration of the preference landscape\ncompared to single-agent approaches. Crucially, this diversity improves reward\nmodel learning by generating preference queries with clearly distinguishable\nbehaviors, a key factor in real-world scenarios where humans must easily\ndifferentiate between options to provide meaningful feedback. Our experiments\nreveal that current methods may fail by getting stuck in local optima,\nrequiring excessive feedback, or degrading significantly when human evaluators\nmake errors on similar trajectories, a realistic scenario often overlooked by\nmethods relying on perfect oracle teachers. Our population-based approach\ndemonstrates robust performance when teachers mislabel similar trajectory\nsegments and shows significantly enhanced preference exploration\ncapabilities,particularly in environments with complex reward landscapes.\n","authors":["Brahim Driss","Alex Davey","Riad Akrour"],"pdf_url":"https://arxiv.org/pdf/2506.13741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13734v1","updated":"2025-06-16T17:42:35Z","published":"2025-06-16T17:42:35Z","title":"Instruction Following by Boosting Attention of Large Language Models","summary":"  Controlling the generation of large language models (LLMs) remains a central\nchallenge to ensure their safe and reliable deployment. While prompt\nengineering and finetuning are common approaches, recent work has explored\nlatent steering, a lightweight technique that alters LLM internal activations\nto guide generation. However, subsequent studies revealed latent steering's\neffectiveness to be limited, often underperforming simple instruction\nprompting. To address this limitation, we first establish a benchmark across\ndiverse behaviors for standardized evaluation of steering techniques. Building\non insights from this benchmark, we introduce Instruction Attention Boosting\n(InstABoost), a latent steering method that boosts the strength of instruction\nprompting by altering the model's attention during generation. InstABoost\ncombines the strengths of existing approaches and is theoretically supported by\nprior work that suggests that in-context rule following in transformer-based\nmodels can be controlled by manipulating attention on instructions.\nEmpirically, InstABoost demonstrates superior control success compared to both\ntraditional prompting and latent steering.\n","authors":["Vitoria Guardieiro","Adam Stein","Avishree Khare","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2506.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13727v1","updated":"2025-06-16T17:38:36Z","published":"2025-06-16T17:38:36Z","title":"Attribution-guided Pruning for Compression, Circuit Discovery, and\n  Targeted Correction in LLMs","summary":"  Large Language Models (LLMs) are central to many contemporary AI\napplications, yet their extensive parameter counts pose significant challenges\nfor deployment in memory- and compute-constrained environments. Recent works in\neXplainable AI (XAI), particularly on attribution methods, suggest that\ninterpretability can also enable model compression by identifying and removing\ncomponents irrelevant to inference. In this paper, we leverage Layer-wise\nRelevance Propagation (LRP) to perform attribution-guided pruning of LLMs.\nWhile LRP has shown promise in structured pruning for vision models, we extend\nit to unstructured pruning in LLMs and demonstrate that it can substantially\nreduce model size with minimal performance loss. Our method is especially\neffective in extracting task-relevant subgraphs -- so-called ``circuits'' --\nwhich can represent core functions (e.g., indirect object identification).\nBuilding on this, we introduce a technique for model correction, by selectively\nremoving circuits responsible for spurious behaviors (e.g., toxic outputs). All\nin all, we gather these techniques as a uniform holistic framework and showcase\nits effectiveness and limitations through extensive experiments for\ncompression, circuit discovery and model correction on Llama and OPT models,\nhighlighting its potential for improving both model efficiency and safety. Our\ncode is publicly available at https://github.com/erfanhatefi/SparC3.\n","authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2506.13727v1.pdf","comment":"Work in progress (10 pages manuscript, 3 pages references, 12 pages\n  appendix)"},{"id":"http://arxiv.org/abs/2506.13726v1","updated":"2025-06-16T17:32:18Z","published":"2025-06-16T17:32:18Z","title":"Weakest Link in the Chain: Security Vulnerabilities in Advanced\n  Reasoning Models","summary":"  The introduction of advanced reasoning capabilities have improved the\nproblem-solving performance of large language models, particularly on math and\ncoding benchmarks. However, it remains unclear whether these reasoning models\nare more or less vulnerable to adversarial prompt attacks than their\nnon-reasoning counterparts. In this work, we present a systematic evaluation of\nweaknesses in advanced reasoning models compared to similar non-reasoning\nmodels across a diverse set of prompt-based attack categories. Using\nexperimental data, we find that on average the reasoning-augmented models are\n\\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\%\nattack success rate, lower is better). However, this overall trend masks\nsignificant category-specific differences: for certain attack types the\nreasoning models are substantially \\emph{more vulnerable} (e.g., up to 32\npercentage points worse on a tree-of-attacks prompt), while for others they are\nmarkedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting\ninjection). Our findings highlight the nuanced security implications of\nadvanced reasoning in language models and emphasize the importance of\nstress-testing safety across diverse adversarial techniques.\n","authors":["Arjun Krishna","Aaditya Rastogi","Erick Galinkin"],"pdf_url":"https://arxiv.org/pdf/2506.13726v1.pdf","comment":"Accepted to LLMSEC 2025"},{"id":"http://arxiv.org/abs/2506.13717v1","updated":"2025-06-16T17:24:31Z","published":"2025-06-16T17:24:31Z","title":"Contrastive Self-Supervised Learning As Neural Manifold Packing","summary":"  Contrastive self-supervised learning based on point-wise comparisons has been\nwidely studied for vision tasks. In the visual cortex of the brain, neuronal\nresponses to distinct stimulus classes are organized into geometric structures\nknown as neural manifolds. Accurate classification of stimuli can be achieved\nby effectively separating these manifolds, akin to solving a packing problem.\nWe introduce Contrastive Learning As Manifold Packing (CLAMP), a\nself-supervised framework that recasts representation learning as a manifold\npacking problem. CLAMP introduces a loss function inspired by the potential\nenergy of short-range repulsive particle systems, such as those encountered in\nthe physics of simple liquids and jammed packings. In this framework, each\nclass consists of sub-manifolds embedding multiple augmented views of a single\nimage. The sizes and positions of the sub-manifolds are dynamically optimized\nby following the gradient of a packing loss. This approach yields interpretable\ndynamics in the embedding space that parallel jamming physics, and introduces\ngeometrically meaningful hyperparameters within the loss function. Under the\nstandard linear evaluation protocol, which freezes the backbone and trains only\na linear classifier, CLAMP achieves competitive performance with\nstate-of-the-art self-supervised models. Furthermore, our analysis reveals that\nneural manifolds corresponding to different categories emerge naturally and are\neffectively separated in the learned representation space, highlighting the\npotential of CLAMP to bridge insights from physics, neural science, and machine\nlearning.\n","authors":["Guanming Zhang","David J. Heeger","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2506.13717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13715v1","updated":"2025-06-16T17:24:10Z","published":"2025-06-16T17:24:10Z","title":"Sharpness-Aware Machine Unlearning","summary":"  We characterize the effectiveness of Sharpness-aware minimization (SAM) under\nmachine unlearning scheme, where unlearning forget signals interferes with\nlearning retain signals. While previous work prove that SAM improves\ngeneralization with noise memorization prevention, we show that SAM abandons\nsuch denoising property when fitting the forget set, leading to various test\nerror bounds depending on signal strength. We further characterize the signal\nsurplus of SAM in the order of signal strength, which enables learning from\nless retain signals to maintain model performance and putting more weight on\nunlearning the forget set. Empirical studies show that SAM outperforms SGD with\nrelaxed requirement for retain signals and can enhance various unlearning\nmethods either as pretrain or unlearn algorithm. Observing that overfitting can\nbenefit more stringent sample-specific unlearning, we propose Sharp MinMax,\nwhich splits the model into two to learn retain signals with SAM and unlearn\nforget signals with sharpness maximization, achieving best performance.\nExtensive experiments show that SAM enhances unlearning across varying\ndifficulties measured by data memorization, yielding decreased feature\nentanglement between retain and forget sets, stronger resistance to membership\ninference attacks, and a flatter loss landscape.\n","authors":["Haoran Tang","Rajiv Khanna"],"pdf_url":"https://arxiv.org/pdf/2506.13715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13714v1","updated":"2025-06-16T17:24:07Z","published":"2025-06-16T17:24:07Z","title":"Understanding Learning Invariance in Deep Linear Networks","summary":"  Equivariant and invariant machine learning models exploit symmetries and\nstructural patterns in data to improve sample efficiency. While empirical\nstudies suggest that data-driven methods such as regularization and data\naugmentation can perform comparably to explicitly invariant models, theoretical\ninsights remain scarce. In this paper, we provide a theoretical comparison of\nthree approaches for achieving invariance: data augmentation, regularization,\nand hard-wiring. We focus on mean squared error regression with deep linear\nnetworks, which parametrize rank-bounded linear maps and can be hard-wired to\nbe invariant to specific group actions. We show that the critical points of the\noptimization problems for hard-wiring and data augmentation are identical,\nconsisting solely of saddles and the global optimum. By contrast,\nregularization introduces additional critical points, though they remain\nsaddles except for the global optimum. Moreover, we demonstrate that the\nregularization path is continuous and converges to the hard-wired solution.\n","authors":["Hao Duan","Guido Montúfar"],"pdf_url":"https://arxiv.org/pdf/2506.13714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13712v1","updated":"2025-06-16T17:20:40Z","published":"2025-06-16T17:20:40Z","title":"Understanding Lookahead Dynamics Through Laplace Transform","summary":"  We introduce a frequency-domain framework for convergence analysis of\nhyperparameters in game optimization, leveraging High-Resolution Differential\nEquations (HRDEs) and Laplace transforms. Focusing on the Lookahead\nalgorithm--characterized by gradient steps $k$ and averaging coefficient\n$\\alpha$--we transform the discrete-time oscillatory dynamics of bilinear games\ninto the frequency domain to derive precise convergence criteria. Our\nhigher-precision $O(\\gamma^2)$-HRDE models yield tighter criteria, while our\nfirst-order $O(\\gamma)$-HRDE models offer practical guidance by prioritizing\nactionable hyperparameter tuning over complex closed-form solutions. Empirical\nvalidation in discrete-time settings demonstrates the effectiveness of our\napproach, which may further extend to locally linear operators, offering a\nscalable framework for selecting hyperparameters for learning in games.\n","authors":["Aniket Sanyal","Tatjana Chavdarova"],"pdf_url":"https://arxiv.org/pdf/2506.13712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13710v1","updated":"2025-06-16T17:19:34Z","published":"2025-06-16T17:19:34Z","title":"Gradient-Normalized Smoothness for Optimization with Approximate\n  Hessians","summary":"  In this work, we develop new optimization algorithms that use approximate\nsecond-order information combined with the gradient regularization technique to\nachieve fast global convergence rates for both convex and non-convex\nobjectives. The key innovation of our analysis is a novel notion called\nGradient-Normalized Smoothness, which characterizes the maximum radius of a\nball around the current point that yields a good relative approximation of the\ngradient field. Our theory establishes a natural intrinsic connection between\nHessian approximation and the linearization of the gradient. Importantly,\nGradient-Normalized Smoothness does not depend on the specific problem class of\nthe objective functions, while effectively translating local information about\nthe gradient field and Hessian approximation into the global behavior of the\nmethod. This new concept equips approximate second-order algorithms with\nuniversal global convergence guarantees, recovering state-of-the-art rates for\nfunctions with H\\\"older-continuous Hessians and third derivatives,\nquasi-self-concordant functions, as well as smooth classes in first-order\noptimization. These rates are achieved automatically and extend to broader\nclasses, such as generalized self-concordant functions. We demonstrate direct\napplications of our results for global linear rates in logistic regression and\nsoftmax problems with approximate Hessians, as well as in non-convex\noptimization using Fisher and Gauss-Newton approximations.\n","authors":["Andrei Semenov","Martin Jaggi","Nikita Doikov"],"pdf_url":"https://arxiv.org/pdf/2506.13710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13705v1","updated":"2025-06-16T17:12:26Z","published":"2025-06-16T17:12:26Z","title":"TimeMaster: Training Time-Series Multimodal LLMs to Reason via\n  Reinforcement Learning","summary":"  Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs.\n","authors":["Junru Zhang","Lang Feng","Xu Guo","Yuhan Wu","Yabo Dong","Duanqing Xu"],"pdf_url":"https://arxiv.org/pdf/2506.13705v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2405.01607v5","updated":"2025-06-16T17:07:17Z","published":"2024-05-02T04:53:42Z","title":"Deep Learning for Wildfire Risk Prediction: Integrating Remote Sensing\n  and Environmental Data","summary":"  Wildfires pose a significant threat to ecosystems, wildlife, and human\ncommunities, leading to habitat destruction, pollutant emissions, and\nbiodiversity loss. Accurate wildfire risk prediction is crucial for mitigating\nthese impacts and safeguarding both environmental and human health. This paper\nprovides a comprehensive review of wildfire risk prediction methodologies, with\na particular focus on deep learning approaches combined with remote sensing. We\nbegin by defining wildfire risk and summarizing the geographical distribution\nof related studies. In terms of data, we analyze key predictive features,\nincluding fuel characteristics, meteorological and climatic conditions,\nsocioeconomic factors, topography, and hydrology, while also reviewing publicly\navailable wildfire prediction datasets derived from remote sensing.\nAdditionally, we emphasize the importance of feature collinearity assessment\nand model interpretability to improve the understanding of prediction outcomes.\nRegarding methodology, we classify deep learning models into three primary\ncategories: time-series forecasting, image segmentation, and spatiotemporal\nprediction, and further discuss methods for converting model outputs into risk\nclassifications or probability-adjusted predictions. Finally, we identify the\nkey challenges and limitations of current wildfire-risk prediction models and\noutline several research opportunities. These include integrating diverse\nremote sensing data, developing multimodal models, designing more\ncomputationally efficient architectures, and incorporating cross-disciplinary\nmethods--such as coupling with numerical weather-prediction models--to enhance\nthe accuracy and robustness of wildfire-risk assessments.\n","authors":["Zhengsen Xu","Jonathan Li","Sibo Cheng","Xue Rui","Yu Zhao","Hongjie He","Haiyan Guan","Aryan Sharma","Matthew Erxleben","Ryan Chang","Linlin Xu"],"pdf_url":"https://arxiv.org/pdf/2405.01607v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13702v1","updated":"2025-06-16T17:06:27Z","published":"2025-06-16T17:06:27Z","title":"Value-Free Policy Optimization via Reward Partitioning","summary":"  Single-trajectory reinforcement learning (RL) methods aim to optimize\npolicies from datasets consisting of (prompt, response, reward) triplets, where\nscalar rewards are directly available. This supervision format is highly\npractical, as it mirrors real-world human feedback, such as thumbs-up/down\nsignals, and avoids the need for structured preference annotations. In\ncontrast, pairwise preference-based methods like Direct Preference Optimization\n(DPO) rely on datasets with both preferred and dispreferred responses, which\nare harder to construct and less natural to collect. Among single-trajectory\napproaches, Direct Reward Optimization (DRO) has shown strong empirical\nperformance due to its simplicity and stability. However, DRO requires\napproximating a value function, which introduces several limitations: high\noff-policy variance, coupling between policy and value learning, and a lack of\nabsolute supervision on the policy itself. We introduce Reward Partitioning\nOptimization (RPO), a new method that resolves these limitations by removing\nthe need to model the value function. Instead, RPO normalizes observed rewards\nusing a partitioning approach estimated directly from data. This leads to a\nstraightforward supervised learning objective on the policy, with no auxiliary\nmodels and no joint optimization. RPO provides direct and stable supervision on\nthe policy, making it robust and easy to implement in practice. We validate RPO\non scalar-feedback language modeling tasks using Flan-T5 encoder-decoder\nmodels. Our results demonstrate that RPO outperforms existing single-trajectory\nbaselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings\nconfirm that RPO is a simple, effective, and theoretically grounded method for\nsingle-trajectory policy optimization.\n","authors":["Bilal Faye","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2506.13702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16941v3","updated":"2025-06-16T17:03:07Z","published":"2025-05-22T17:29:52Z","title":"FoMoH: A clinically meaningful foundation model evaluation for\n  structured electronic health records","summary":"  Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.\n","authors":["Chao Pang","Vincent Jeanselme","Young Sang Choi","Xinzhuo Jiang","Zilin Jing","Aparajita Kashyap","Yuta Kobayashi","Yanwei Li","Florent Pollet","Karthik Natarajan","Shalmali Joshi"],"pdf_url":"https://arxiv.org/pdf/2505.16941v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13690v1","updated":"2025-06-16T16:52:49Z","published":"2025-06-16T16:52:49Z","title":"Meta-learning how to Share Credit among Macro-Actions","summary":"  One proposed mechanism to improve exploration in reinforcement learning is\nthrough the use of macro-actions. Paradoxically though, in many scenarios the\nnaive addition of macro-actions does not lead to better exploration, but rather\nthe opposite. It has been argued that this was caused by adding non-useful\nmacros and multiple works have focused on mechanisms to discover effectively\nenvironment-specific useful macros. In this work, we take a slightly different\nperspective. We argue that the difficulty stems from the trade-offs between\nreducing the average number of decisions per episode versus increasing the size\nof the action space. Namely, one typically treats each potential macro-action\nas independent and atomic, hence strictly increasing the search space and\nmaking typical exploration strategies inefficient. To address this problem we\npropose a novel regularization term that exploits the relationship between\nactions and macro-actions to improve the credit assignment mechanism by\nreducing the effective dimension of the action space and, therefore, improving\nexploration. The term relies on a similarity matrix that is meta-learned\njointly with learning the desired policy. We empirically validate our strategy\nlooking at macro-actions in Atari games, and the StreetFighter II environment.\nOur results show significant improvements over the Rainbow-DQN baseline in all\nenvironments. Additionally, we show that the macro-action similarity is\ntransferable to related environments. We believe this work is a small but\nimportant step towards understanding how the similarity-imposed geometry on the\naction space can be exploited to improve credit assignment and exploration,\ntherefore making learning more effective.\n","authors":["Ionel-Alexandru Hosu","Traian Rebedea","Razvan Pascanu"],"pdf_url":"https://arxiv.org/pdf/2506.13690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13688v1","updated":"2025-06-16T16:51:18Z","published":"2025-06-16T16:51:18Z","title":"What Happens During the Loss Plateau? Understanding Abrupt Learning in\n  Transformers","summary":"  Training Transformers on algorithmic tasks frequently demonstrates an\nintriguing abrupt learning phenomenon: an extended performance plateau followed\nby a sudden, sharp improvement. This work investigates the underlying\nmechanisms for such dynamics, primarily in shallow Transformers. We reveal that\nduring the plateau, the model often develops an interpretable partial solution\nwhile simultaneously exhibiting a strong repetition bias in their outputs. This\noutput degeneracy is accompanied by internal representation collapse, where\nhidden states across different tokens become nearly parallel. We further\nidentify the slow learning of optimal attention maps as a key bottleneck.\nHidden progress in attention configuration during the plateau precedes the\neventual rapid convergence, and directly intervening on attention significantly\nalters plateau duration and the severity of repetition bias and\nrepresentational collapse. We validate that these identified\nphenomena-repetition bias and representation collapse-are not artifacts of toy\nsetups but also manifest in the early pre-training stage of large language\nmodels like Pythia and OLMo.\n","authors":["Pulkit Gopalani","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2506.13688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13687v1","updated":"2025-06-16T16:51:06Z","published":"2025-06-16T16:51:06Z","title":"Enforcing tail calibration when training probabilistic forecast models","summary":"  Probabilistic forecasts are typically obtained using state-of-the-art\nstatistical and machine learning models, with model parameters estimated by\noptimizing a proper scoring rule over a set of training data. If the model\nclass is not correctly specified, then the learned model will not necessarily\nissue forecasts that are calibrated. Calibrated forecasts allow users to\nappropriately balance risks in decision making, and it is particularly\nimportant that forecast models issue calibrated predictions for extreme events,\nsince such outcomes often generate large socio-economic impacts. In this work,\nwe study how the loss function used to train probabilistic forecast models can\nbe adapted to improve the reliability of forecasts made for extreme events. We\ninvestigate loss functions based on weighted scoring rules, and additionally\npropose regularizing loss functions using a measure of tail miscalibration. We\napply these approaches to a hierarchy of increasingly flexible forecast models\nfor UK wind speeds, including simple parametric models, distributional\nregression networks, and conditional generative models. We demonstrate that\nstate-of-the-art models do not issue calibrated forecasts for extreme wind\nspeeds, and that the calibration of forecasts for extreme events can be\nimproved by suitable adaptations to the loss function during model training.\nThis, however, introduces a trade-off between calibrated forecasts for extreme\nevents and calibrated forecasts for more common outcomes.\n","authors":["Jakob Benjamin Wessel","Maybritt Schillinger","Frank Kwasniok","Sam Allen"],"pdf_url":"https://arxiv.org/pdf/2506.13687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06349v2","updated":"2025-06-16T16:40:48Z","published":"2025-06-02T05:16:16Z","title":"Heart Rate Classification in ECG Signals Using Machine Learning and Deep\n  Learning","summary":"  This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.\n","authors":["Thien Nhan Vo"],"pdf_url":"https://arxiv.org/pdf/2506.06349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15895v2","updated":"2025-06-16T16:39:12Z","published":"2024-05-24T19:33:05Z","title":"Manifold Metric: A Loss Landscape Approach for Predicting Model\n  Performance","summary":"  Determining the optimal model for a given task often requires training\nmultiple models from scratch, which becomes impractical as dataset and model\nsizes grow. A more efficient alternative is to expand smaller pre-trained\nmodels, but this approach is underutilized due to a limited understanding of\nits impact on the training dynamics. Existing methods for quantifying this\nimpact have notable limitations, including computation cost. To address this,\nwe introduce a new perspective based on the loss landscape, which has been\nshown to contain a manifold of linearly connected minima. Specifically, we\npropose a metric that estimates the size of this manifold to study the impact\nof model expansion. Our experiments reveal a strong correlation between\nperformance gains and our manifold metric, enabling more informed model\ncomparison and offering a first step toward a geometry-driven approach for\nreliable model expansion. Notably, our metric outperforms other baselines, even\nwhen different types of expansion with equivalent number of parameters are\napplied to a model.\n","authors":["Pranshu Malviya","Jerry Huang","Aristide Baratin","Quentin Fournier","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2405.15895v2.pdf","comment":"Published at 4th Conference on Lifelong Learning Agents (CoLLAs),\n  2025"},{"id":"http://arxiv.org/abs/2506.13681v1","updated":"2025-06-16T16:38:04Z","published":"2025-06-16T16:38:04Z","title":"Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language\n  Models","summary":"  Sampling from language models impacts the quality and diversity of outputs,\naffecting both research and real-world applications. Recently, Nguyen et al.\n2024's \"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\nOutputs\" introduced a new sampler called min-p, claiming it achieves superior\nquality and diversity over established samplers such as basic, top-k, and top-p\nsampling. The significance of these claims was underscored by the paper's\nrecognition as the 18th highest-scoring submission to ICLR 2025 and selection\nfor an Oral presentation. This paper conducts a comprehensive re-examination of\nthe evidence supporting min-p and reaches different conclusions from the\noriginal paper's four lines of evidence. First, the original paper's human\nevaluations omitted data, conducted statistical tests incorrectly, and\ndescribed qualitative feedback inaccurately; our reanalysis demonstrates min-p\ndid not outperform baselines in quality, diversity, or a trade-off between\nquality and diversity; in response to our findings, the authors of the original\npaper conducted a new human evaluation using a different implementation, task,\nand rubric that nevertheless provides further evidence min-p does not improve\nover baselines. Second, comprehensively sweeping the original paper's NLP\nbenchmarks reveals min-p does not surpass baselines when controlling for the\nnumber of hyperparameters. Third, the original paper's LLM-as-a-Judge\nevaluations lack methodological clarity and appear inconsistently reported.\nFourth, community adoption claims (49k GitHub repositories, 1.1M GitHub stars)\nwere found to be unsubstantiated, leading to their removal; the revised\nadoption claim remains misleading. We conclude that evidence presented in the\noriginal paper fails to support claims that min-p improves quality, diversity,\nor a trade-off between quality and diversity.\n","authors":["Rylan Schaeffer","Joshua Kazdan","Yegor Denisov-Blanch"],"pdf_url":"https://arxiv.org/pdf/2506.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13680v1","updated":"2025-06-16T16:37:20Z","published":"2025-06-16T16:37:20Z","title":"Hybrid Meta-learners for Estimating Heterogeneous Treatment Effects","summary":"  Estimating conditional average treatment effects (CATE) from observational\ndata involves modeling decisions that differ from supervised learning,\nparticularly concerning how to regularize model complexity. Previous approaches\ncan be grouped into two primary \"meta-learner\" paradigms that impose distinct\ninductive biases. Indirect meta-learners first fit and regularize separate\npotential outcome (PO) models and then estimate CATE by taking their\ndifference, whereas direct meta-learners construct and directly regularize\nestimators for the CATE function itself. Neither approach consistently\noutperforms the other across all scenarios: indirect learners perform well when\nthe PO functions are simple, while direct learners outperform when the CATE is\nsimpler than individual PO functions. In this paper, we introduce the Hybrid\nLearner (H-learner), a novel regularization strategy that interpolates between\nthe direct and indirect regularizations depending on the dataset at hand. The\nH-learner achieves this by learning intermediate functions whose difference\nclosely approximates the CATE without necessarily requiring accurate individual\napproximations of the POs themselves. We demonstrate empirically that\nintentionally allowing suboptimal fits to the POs improves the bias-variance\ntradeoff in estimating CATE. Experiments conducted on semi-synthetic and\nreal-world benchmark datasets illustrate that the H-learner consistently\noperates at the Pareto frontier, effectively combining the strengths of both\ndirect and indirect meta-learners.\n","authors":["Zhongyuan Liang","Lars van der Laan","Ahmed Alaa"],"pdf_url":"https://arxiv.org/pdf/2506.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13678v1","updated":"2025-06-16T16:32:51Z","published":"2025-06-16T16:32:51Z","title":"A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction","summary":"  Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.\n","authors":["Yi Wang","Zhenghong Wang","Fan Zhang","Chengling Tang","Chaogui Kang","Di Zhu","Zhongfu Ma","Sijie Ruan","Weiyu Zhang","Yu Zheng","Philip S. Yu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13678v1.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.13672v1","updated":"2025-06-16T16:30:00Z","published":"2025-06-16T16:30:00Z","title":"The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement\n  Learning","summary":"  Off-policy deep reinforcement learning (RL) typically leverages replay\nbuffers for reusing past experiences during learning. This can help improve\nsample efficiency when the collected data is informative and aligned with the\nlearning objectives; when that is not the case, it can have the effect of\n\"polluting\" the replay buffer with data which can exacerbate optimization\nchallenges in addition to wasting environment interactions due to wasteful\nsampling. We argue that sampling these uninformative and wasteful transitions\ncan be avoided by addressing the sunk cost fallacy, which, in the context of\ndeep RL, is the tendency towards continuing an episode until termination. To\naddress this, we propose learn to stop (LEAST), a lightweight mechanism that\nenables strategic early episode termination based on Q-value and gradient\nstatistics, which helps agents recognize when to terminate unproductive\nepisodes early. We demonstrate that our method improves learning efficiency on\na variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control\nSuite benchmarks.\n","authors":["Jiashun Liu","Johan Obando-Ceron","Pablo Samuel Castro","Aaron Courville","Ling Pan"],"pdf_url":"https://arxiv.org/pdf/2506.13672v1.pdf","comment":"Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)"},{"id":"http://arxiv.org/abs/2506.13666v1","updated":"2025-06-16T16:24:31Z","published":"2025-06-16T16:24:31Z","title":"We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems","summary":"  The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.\n","authors":["Junfeng Fang","Zijun Yao","Ruipeng Wang","Haokai Ma","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.13666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05317v2","updated":"2025-06-16T16:22:39Z","published":"2025-02-21T09:43:18Z","title":"On Synthesizing Data for Context Attribution in Question Answering","summary":"  Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.\n","authors":["Gorjan Radevski","Kiril Gashteovski","Shahbaz Syed","Christopher Malon","Sebastien Nicolas","Chia-Chien Hung","Timo Sztyler","Verena Heußer","Wiem Ben Rim","Masafumi Enomoto","Kunihiro Takeoka","Masafumi Oyamada","Goran Glavaš","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2504.05317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10419v3","updated":"2025-06-16T16:20:35Z","published":"2025-06-12T07:18:05Z","title":"Data-Driven Soil Organic Carbon Sampling: Integrating Spectral\n  Clustering with Conditioned Latin Hypercube Optimization","summary":"  Soil organic carbon (SOC) monitoring often relies on selecting representative\nfield sampling locations based on environmental covariates. We propose a novel\nhybrid methodology that integrates spectral clustering - an unsupervised\nmachine learning technique with conditioned Latin hypercube sampling (cLHS) to\nenhance the representativeness of SOC sampling. In our approach, spectral\nclustering partitions the study area into $K$ homogeneous zones using\nmultivariate covariate data, and cLHS is then applied within each zone to\nselect sampling locations that collectively capture the full diversity of\nenvironmental conditions. This hybrid spectral-cLHS method ensures that even\nminor but important environmental clusters are sampled, addressing a key\nlimitation of vanilla cLHS which can overlook such areas. We demonstrate on a\nreal SOC mapping dataset that spectral-cLHS provides more uniform coverage of\ncovariate feature space and spatial heterogeneity than standard cLHS. This\nimproved sampling design has the potential to yield more accurate SOC\npredictions by providing better-balanced training data for machine learning\nmodels.\n","authors":["Weiying Zhao","Aleksei Unagaev","Natalia Efremova"],"pdf_url":"https://arxiv.org/pdf/2506.10419v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13658v1","updated":"2025-06-16T16:18:25Z","published":"2025-06-16T16:18:25Z","title":"Adversarial Disentanglement by Backpropagation with Physics-Informed\n  Variational Autoencoder","summary":"  Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach.\n","authors":["Ioannis Christoforos Koune","Alice Cicirello"],"pdf_url":"https://arxiv.org/pdf/2506.13658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13657v1","updated":"2025-06-16T16:18:21Z","published":"2025-06-16T16:18:21Z","title":"Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual\n  Object Detection in Educational Videos","summary":"  We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain.\n","authors":["Dipayan Biswas","Shishir Shah","Jaspal Subhlok"],"pdf_url":"https://arxiv.org/pdf/2506.13657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13652v1","updated":"2025-06-16T16:16:42Z","published":"2025-06-16T16:16:42Z","title":"PeakWeather: MeteoSwiss Weather Station Measurements for Spatiotemporal\n  Deep Learning","summary":"  Accurate weather forecasts are essential for supporting a wide range of\nactivities and decision-making processes, as well as mitigating the impacts of\nadverse weather events. While traditional numerical weather prediction (NWP)\nremains the cornerstone of operational forecasting, machine learning is\nemerging as a powerful alternative for fast, flexible, and scalable\npredictions. We introduce PeakWeather, a high-quality dataset of surface\nweather observations collected every 10 minutes over more than 8 years from the\nground stations of the Federal Office of Meteorology and Climatology\nMeteoSwiss's measurement network. The dataset includes a diverse set of\nmeteorological variables from 302 station locations distributed across\nSwitzerland's complex topography and is complemented with topographical indices\nderived from digital height models for context. Ensemble forecasts from the\ncurrently operational high-resolution NWP model are provided as a baseline\nforecast against which to evaluate new approaches. The dataset's richness\nsupports a broad spectrum of spatiotemporal tasks, including time series\nforecasting at various scales, graph structure learning, imputation, and\nvirtual sensing. As such, PeakWeather serves as a real-world benchmark to\nadvance both foundational machine learning research, meteorology, and\nsensor-based applications.\n","authors":["Daniele Zambon","Michele Cattaneo","Ivan Marisca","Jonas Bhend","Daniele Nerini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2506.13652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13651v1","updated":"2025-06-16T16:16:14Z","published":"2025-06-16T16:16:14Z","title":"xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations","summary":"  We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.\n","authors":["Kaiyuan Chen","Yixin Ren","Yang Liu","Xiaobo Hu","Haotong Tian","Tianbao Xie","Fangfu Liu","Haoye Zhang","Hongzhang Liu","Yuan Gong","Chen Sun","Han Hou","Hui Yang","James Pan","Jianan Lou","Jiayi Mao","Jizheng Liu","Jinpeng Li","Kangyi Liu","Kenkun Liu","Rui Wang","Run Li","Tong Niu","Wenlong Zhang","Wenqi Yan","Xuanzheng Wang","Yuchen Zhang","Yi-Hsin Hung","Yuan Jiang","Zexuan Liu","Zihan Yin","Zijian Ma","Zhiwen Mo"],"pdf_url":"https://arxiv.org/pdf/2506.13651v1.pdf","comment":"Project page: https://xbench.org"},{"id":"http://arxiv.org/abs/2506.13649v1","updated":"2025-06-16T16:10:08Z","published":"2025-06-16T16:10:08Z","title":"EUNIS Habitat Maps: Enhancing Thematic and Spatial Resolution for Europe\n  through Machine Learning","summary":"  The EUNIS habitat classification is crucial for categorising European\nhabitats, supporting European policy on nature conservation and implementing\nthe Nature Restoration Law. To meet the growing demand for detailed and\naccurate habitat information, we provide spatial predictions for 260 EUNIS\nhabitat types at hierarchical level 3, together with independent validation and\nuncertainty analyses.\n  Using ensemble machine learning models, together with high-resolution\nsatellite imagery and ecologically meaningful climatic, topographic and edaphic\nvariables, we produced a European habitat map indicating the most probable\nEUNIS habitat at 100-m resolution across Europe. Additionally, we provide\ninformation on prediction uncertainty and the most probable habitats at level 3\nwithin each EUNIS level 1 formation. This product is particularly useful for\nboth conservation and restoration purposes.\n  Predictions were cross-validated at European scale using a spatial block\ncross-validation and evaluated against independent data from France (forests\nonly), the Netherlands and Austria. The habitat maps obtained strong predictive\nperformances on the validation datasets with distinct trade-offs in terms of\nrecall and precision across habitat formations.\n","authors":["Sara Si-Moussi","Stephan Hennekens","Sander Mücher","Wanda De Keersmaecker","Milan Chytrý","Emiliano Agrillo","Fabio Attorre","Idoia Biurrun","Gianmaria Bonari","Andraž Čarni","Renata Ćušterevska","Tetiana Dziuba","Klaus Ecker","Behlül Güler","Ute Jandt","Borja Jiménez-Alfaro","Jonathan Lenoir","Jens-Christian Svenning","Grzegorz Swacha","Wilfried Thuiller"],"pdf_url":"https://arxiv.org/pdf/2506.13649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13633v1","updated":"2025-06-16T16:00:00Z","published":"2025-06-16T16:00:00Z","title":"Global Convergence of Adjoint-Optimized Neural PDEs","summary":"  Many engineering and scientific fields have recently become interested in\nmodeling terms in partial differential equations (PDEs) with neural networks.\nThe resulting neural-network PDE model, being a function of the neural network\nparameters, can be calibrated to available data by optimizing over the PDE\nusing gradient descent, where the gradient is evaluated in a computationally\nefficient manner by solving an adjoint PDE. These neural-network PDE models\nhave emerged as an important research area in scientific machine learning. In\nthis paper, we study the convergence of the adjoint gradient descent\noptimization method for training neural-network PDE models in the limit where\nboth the number of hidden units and the training time tend to infinity.\nSpecifically, for a general class of nonlinear parabolic PDEs with a neural\nnetwork embedded in the source term, we prove convergence of the trained\nneural-network PDE solution to the target data (i.e., a global minimizer). The\nglobal convergence proof poses a unique mathematical challenge that is not\nencountered in finite-dimensional neural network convergence analyses due to\n(1) the neural network training dynamics involving a non-local neural network\nkernel operator in the infinite-width hidden layer limit where the kernel lacks\na spectral gap for its eigenvalues and (2) the nonlinearity of the limit PDE\nsystem, which leads to a non-convex optimization problem, even in the\ninfinite-width hidden layer limit (unlike in typical neual network training\ncases where the optimization problem becomes convex in the large neuron limit).\nThe theoretical results are illustrated and empirically validated by numerical\nstudies.\n","authors":["Konstantin Riedl","Justin Sirignano","Konstantinos Spiliopoulos"],"pdf_url":"https://arxiv.org/pdf/2506.13633v1.pdf","comment":"63 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.13628v1","updated":"2025-06-16T15:55:56Z","published":"2025-06-16T15:55:56Z","title":"Graph-Convolution-Beta-VAE for Synthetic Abdominal Aorta Aneurysm\n  Generation","summary":"  Synthetic data generation plays a crucial role in medical research by\nmitigating privacy concerns and enabling large-scale patient data analysis.\nThis study presents a beta-Variational Autoencoder Graph Convolutional Neural\nNetwork framework for generating synthetic Abdominal Aorta Aneurysms (AAA).\nUsing a small real-world dataset, our approach extracts key anatomical features\nand captures complex statistical relationships within a compact disentangled\nlatent space. To address data limitations, low-impact data augmentation based\non Procrustes analysis was employed, preserving anatomical integrity. The\ngeneration strategies, both deterministic and stochastic, manage to enhance\ndata diversity while ensuring realism. Compared to PCA-based approaches, our\nmodel performs more robustly on unseen data by capturing complex, nonlinear\nanatomical variations. This enables more comprehensive clinical and statistical\nanalyses than the original dataset alone. The resulting synthetic AAA dataset\npreserves patient privacy while providing a scalable foundation for medical\nresearch, device testing, and computational modeling.\n","authors":["Francesco Fabbri","Martino Andrea Scarpolini","Angelo Iollo","Francesco Viola","Francesco Tudisco"],"pdf_url":"https://arxiv.org/pdf/2506.13628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13743v3","updated":"2025-06-16T15:51:30Z","published":"2024-07-18T17:49:09Z","title":"Optimistic Q-learning for average reward and episodic reinforcement\n  learning","summary":"  We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the time to visit some frequent state\n$s_0$ is finite and upper bounded by $H$, either in expectation or with\nconstant probability. Our setting strictly generalizes the episodic setting and\nis significantly less restrictive than the assumption of bounded hitting time\n\\textit{for all states} made by most previous literature on model-free\nalgorithms in average reward settings. We demonstrate a regret bound of\n$\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the numbers of states and\nactions, and $T$ is the horizon. A key technical novelty of our work is the\nintroduction of an $\\overline{L}$ operator defined as $\\overline{L} v =\n\\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. Under\nthe given assumption, we show that the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average-reward setting where the discount\nfactor is $1$. Our algorithm design uses ideas from episodic Q-learning to\nestimate and apply this operator iteratively. Thus, we provide a unified view\nof regret minimization in episodic and non-episodic settings, which may be of\nindependent interest.\n","authors":["Priyank Agrawal","Shipra Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.13743v3.pdf","comment":"37 pages, simplified proofs"},{"id":"http://arxiv.org/abs/2506.13614v1","updated":"2025-06-16T15:43:28Z","published":"2025-06-16T15:43:28Z","title":"Exploiting the Exact Denoising Posterior Score in Training-Free Guidance\n  of Diffusion Models","summary":"  The success of diffusion models has driven interest in performing conditional\nsampling via training-free guidance of the denoising process to solve image\nrestoration and other inverse problems. A popular class of methods, based on\nDiffusion Posterior Sampling (DPS), attempts to approximate the intractable\nposterior score function directly. In this work, we present a novel expression\nfor the exact posterior score for purely denoising tasks that is tractable in\nterms of the unconditional score function. We leverage this result to analyze\nthe time-dependent error in the DPS score for denoising tasks and compute step\nsizes on the fly to minimize the error at each time step. We demonstrate that\nthese step sizes are transferable to related inverse problems such as\ncolorization, random inpainting, and super resolution. Despite its simplicity,\nthis approach is competitive with state-of-the-art techniques and enables\nsampling with fewer time steps than DPS.\n","authors":["Gregory Bellchambers"],"pdf_url":"https://arxiv.org/pdf/2506.13614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13613v1","updated":"2025-06-16T15:42:15Z","published":"2025-06-16T15:42:15Z","title":"Variational Inference with Mixtures of Isotropic Gaussians","summary":"  Variational inference (VI) is a popular approach in Bayesian inference, that\nlooks for the best approximation of the posterior distribution within a\nparametric family, minimizing a loss that is typically the (reverse)\nKullback-Leibler (KL) divergence. In this paper, we focus on the following\nparametric family: mixtures of isotropic Gaussians (i.e., with diagonal\ncovariance matrices proportional to the identity) and uniform weights. We\ndevelop a variational framework and provide efficient algorithms suited for\nthis family. In contrast with mixtures of Gaussian with generic covariance\nmatrices, this choice presents a balance between accurate approximations of\nmultimodal Bayesian posteriors, while being memory and computationally\nefficient. Our algorithms implement gradient descent on the location of the\nmixture components (the modes of the Gaussians), and either (an entropic)\nMirror or Bures descent on their variance parameters. We illustrate the\nperformance of our algorithms on numerical experiments.\n","authors":["Marguerite Petit-Talamon","Marc Lambert","Anna Korba"],"pdf_url":"https://arxiv.org/pdf/2506.13613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13608v1","updated":"2025-06-16T15:35:41Z","published":"2025-06-16T15:35:41Z","title":"Assessing the Limits of In-Context Learning beyond Functions using\n  Partially Ordered Relation","summary":"  Generating rational and generally accurate responses to tasks, often\naccompanied by example demonstrations, highlights Large Language Model's\n(LLM's) remarkable In-Context Learning (ICL) capabilities without requiring\nupdates to the model's parameter space. Despite having an ongoing exploration\nfocused on the inference from a document-level concept, its behavior in\nlearning well-defined functions or relations in context needs a careful\ninvestigation. In this article, we present the performance of ICL on partially\nordered relation by introducing the notion of inductively increasing complexity\nin prompts. In most cases, the saturated performance of the chosen metric\nindicates that while ICL offers some benefits, its effectiveness remains\nconstrained as we increase the complexity in the prompts even in presence of\nsufficient demonstrative examples. The behavior is evident from our empirical\nfindings and has further been theoretically justified in term of its implicit\noptimization process. The code is available\n\\href{https://anonymous.4open.science/r/ICLonPartiallyOrderSet}{here}.\n","authors":["Debanjan Dutta","Faizanuddin Ansari","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2506.13608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19335v2","updated":"2025-06-16T15:32:06Z","published":"2025-02-26T17:29:08Z","title":"Gatekeeper: Improving Model Cascades Through Confidence Tuning","summary":"  Large-scale machine learning models deliver strong performance across a wide\nrange of tasks but come with significant computational and resource\nconstraints. To mitigate these challenges, local smaller models are often\ndeployed alongside larger models, relying on routing and deferral mechanisms to\noffload complex tasks. However, existing approaches inadequately balance the\ncapabilities of these models, often resulting in unnecessary deferrals or\nsub-optimal resource usage. In this work we introduce a novel loss function\ncalled Gatekeeper for calibrating smaller models in cascade setups. Our\napproach fine-tunes the smaller model to confidently handle tasks it can\nperform correctly while deferring complex tasks to the larger model. Moreover,\nit incorporates a mechanism for managing the trade-off between model\nperformance and deferral accuracy, and is broadly applicable across various\ntasks and domains without any architectural changes. We evaluate our method on\nencoder-only, decoder-only, and encoder-decoder architectures. Experiments\nacross image classification, language modeling, and vision-language tasks show\nthat our approach substantially improves deferral performance.\n","authors":["Stephan Rabanser","Nathalie Rauschmayr","Achin Kulshrestha","Petra Poklukar","Wittawat Jitkrittum","Sean Augenstein","Congchao Wang","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2502.19335v2.pdf","comment":"Presented at the TTODLer-FM workshop at the International Conference\n  on Machine Learning (ICML) 2025"},{"id":"http://arxiv.org/abs/2506.13593v1","updated":"2025-06-16T15:21:25Z","published":"2025-06-16T15:21:25Z","title":"Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs","summary":"  We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models.\n","authors":["Hen Davidov","Gilad Freidkin","Shai Feldman","Yaniv Romano"],"pdf_url":"https://arxiv.org/pdf/2506.13593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15673v3","updated":"2025-06-16T15:15:21Z","published":"2024-05-24T16:12:39Z","title":"Consistency of Neural Causal Partial Identification","summary":"  Recent progress in Neural Causal Models (NCMs) showcased how identification\nand partial identification of causal effects can be automatically carried out\nvia training of neural generative models that respect the constraints encoded\nin a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However,\nformal consistency of these methods has only been proven for the case of\ndiscrete variables or only for linear causal models. In this work, we prove the\nconsistency of partial identification via NCMs in a general setting with both\ncontinuous and categorical variables. Further, our results highlight the impact\nof the design of the underlying neural network architecture in terms of depth\nand connectivity as well as the importance of applying Lipschitz regularization\nin the training phase. In particular, we provide a counterexample showing that\nwithout Lipschitz regularization this method may not be asymptotically\nconsistent. Our results are enabled by new results on the approximability of\nStructural Causal Models (SCMs) via neural generative models, together with an\nanalysis of the sample complexity of the resulting architectures and how that\ntranslates into an error in the constrained optimization problem that defines\nthe partial identification bounds.\n","authors":["Jiyuan Tan","Jose Blanchet","Vasilis Syrgkanis"],"pdf_url":"https://arxiv.org/pdf/2405.15673v3.pdf","comment":"60 pages, 8 figures, accepted by Neurips 2024"},{"id":"http://arxiv.org/abs/2506.13585v1","updated":"2025-06-16T15:08:02Z","published":"2025-06-16T15:08:02Z","title":"MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention","summary":"  We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.\n","authors":[" MiniMax"," :","Aili Chen","Aonian Li","Bangwei Gong","Binyang Jiang","Bo Fei","Bo Yang","Boji Shan","Changqing Yu","Chao Wang","Cheng Zhu","Chengjun Xiao","Chengyu Du","Chi Zhang","Chu Qiao","Chunhao Zhang","Chunhui Du","Congchao Guo","Da Chen","Deming Ding","Dianjun Sun","Dong Li","Enwei Jiao","Haigang Zhou","Haimo Zhang","Han Ding","Haohai Sun","Haoyu Feng","Huaiguang Cai","Haichao Zhu","Jian Sun","Jiaqi Zhuang","Jiaren Cai","Jiayuan Song","Jin Zhu","Jingyang Li","Jinhao Tian","Jinli Liu","Junhao Xu","Junjie Yan","Junteng Liu","Junxian He","Kaiyi Feng","Ke Yang","Kecheng Xiao","Le Han","Leyang Wang","Lianfei Yu","Liheng Feng","Lin Li","Lin Zheng","Linge Du","Lingyu Yang","Lunbin Zeng","Minghui Yu","Mingliang Tao","Mingyuan Chi","Mozhi Zhang","Mujie Lin","Nan Hu","Nongyu Di","Peng Gao","Pengfei Li","Pengyu Zhao","Qibing Ren","Qidi Xu","Qile Li","Qin Wang","Rong Tian","Ruitao Leng","Shaoxiang Chen","Shaoyu Chen","Shengmin Shi","Shitong Weng","Shuchang Guan","Shuqi Yu","Sichen Li","Songquan Zhu","Tengfei Li","Tianchi Cai","Tianrun Liang","Weiyu Cheng","Weize Kong","Wenkai Li","Xiancai Chen","Xiangjun Song","Xiao Luo","Xiao Su","Xiaobo Li","Xiaodong Han","Xinzhu Hou","Xuan Lu","Xun Zou","Xuyang Shen","Yan Gong","Yan Ma","Yang Wang","Yiqi Shi","Yiran Zhong","Yonghong Duan","Yongxiang Fu","Yongyi Hu","Yu Gao","Yuanxiang Fan","Yufeng Yang","Yuhao Li","Yulin Hu","Yunan Huang","Yunji Li","Yunzhi Xu","Yuxin Mao","Yuxuan Shi","Yuze Wenren","Zehan Li","Zelin Li","Zhanxu Tian","Zhengmao Zhu","Zhenhua Fan","Zhenzhen Wu","Zhichao Xu","Zhihang Yu","Zhiheng Lyu","Zhuo Jiang","Zibo Gao","Zijia Wu","Zijian Song","Zijun Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13585v1.pdf","comment":"A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-source our MiniMax-M1 at\n  https://github.com/MiniMax-AI/MiniMax-M1"},{"id":"http://arxiv.org/abs/2506.13584v1","updated":"2025-06-16T15:07:44Z","published":"2025-06-16T15:07:44Z","title":"From Data-Driven to Purpose-Driven Artificial Intelligence: Systems\n  Thinking for Data-Analytic Automation of Patient Care","summary":"  In this work, we reflect on the data-driven modeling paradigm that is gaining\nground in AI-driven automation of patient care. We argue that the repurposing\nof existing real-world patient datasets for machine learning may not always\nrepresent an optimal approach to model development as it could lead to\nundesirable outcomes in patient care. We reflect on the history of data\nanalysis to explain how the data-driven paradigm rose to popularity, and we\nenvision ways in which systems thinking and clinical domain theory could\ncomplement the existing model development approaches in reaching human-centric\noutcomes. We call for a purpose-driven machine learning paradigm that is\ngrounded in clinical theory and the sociotechnical realities of real-world\noperational contexts. We argue that understanding the utility of existing\npatient datasets requires looking in two directions: upstream towards the data\ngeneration, and downstream towards the automation objectives. This\npurpose-driven perspective to AI system development opens up new methodological\nopportunities and holds promise for AI automation of patient care.\n","authors":["Daniel Anadria","Roel Dobbe","Anastasia Giachanou","Ruurd Kuiper","Richard Bartels","Íñigo Martínez de Rituerto de Troya","Carmen Zürcher","Daniel Oberski"],"pdf_url":"https://arxiv.org/pdf/2506.13584v1.pdf","comment":"The work is under review at ACM Health"},{"id":"http://arxiv.org/abs/2506.13579v1","updated":"2025-06-16T15:02:12Z","published":"2025-06-16T15:02:12Z","title":"Flexible-length Text Infilling for Discrete Diffusion Models","summary":"  Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility.\n","authors":["Andrew Zhang","Anushka Sivakumar","Chiawei Tang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2506.13579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13575v1","updated":"2025-06-16T14:58:03Z","published":"2025-06-16T14:58:03Z","title":"Machine Learning-Driven Compensation for Non-Ideal Channels in AWG-Based\n  FBG Interrogator","summary":"  We present an experimental study of a fiber Bragg grating (FBG) interrogator\nbased on a silicon oxynitride (SiON) photonic integrated arrayed waveguide\ngrating (AWG). While AWG-based interrogators are compact and scalable, their\npractical performance is limited by non-ideal spectral responses. To address\nthis, two calibration strategies within a 2.4 nm spectral region were compared:\n(1) a segmented analytical model based on a sigmoid fitting function, and (2) a\nmachine learning (ML)-based regression model. The analytical method achieves a\nroot mean square error (RMSE) of 7.11 pm within the calibrated range, while the\nML approach based on exponential regression achieves 3.17 pm. Moreover, the ML\nmodel demonstrates generalization across an extended 2.9 nm wavelength span,\nmaintaining sub-5 pm accuracy without re-fitting. Residual and error\ndistribution analyses further illustrate the trade-offs between the two\napproaches. ML-based calibration provides a robust, data-driven alternative to\nanalytical methods, delivering enhanced accuracy for non-ideal channel\nresponses, reduced manual calibration effort, and improved scalability across\ndiverse FBG sensor configurations.\n","authors":["Ivan A. Kazakov","Iana V. Kulichenko","Egor E. Kovalev","Angelina A. Treskova","Daria D. Barma","Kirill M. Malakhov","Arkady V. Shipulin"],"pdf_url":"https://arxiv.org/pdf/2506.13575v1.pdf","comment":"The manuscript has been submitted to IEEE Sensors Letters and is\n  currently under peer review"},{"id":"http://arxiv.org/abs/2505.13432v2","updated":"2025-06-16T14:53:55Z","published":"2025-05-19T17:55:56Z","title":"Synthetic-Powered Predictive Inference","summary":"  Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPI), a novel framework that\nincorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification -- augmenting data\nwith synthetic diffusion-model generated images -- and on tabular regression\ndemonstrate notable improvements in predictive efficiency in data-scarce\nsettings.\n","authors":["Meshi Bashari","Roy Maor Lotan","Yonghoon Lee","Edgar Dobriban","Yaniv Romano"],"pdf_url":"https://arxiv.org/pdf/2505.13432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13566v1","updated":"2025-06-16T14:50:26Z","published":"2025-06-16T14:50:26Z","title":"A Production Scheduling Framework for Reinforcement Learning Under\n  Real-World Constraints","summary":"  The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing\nmakespan under deterministic constraints. Real-world production environments\nintroduce additional complexities that cause traditional scheduling approaches\nto be less effective. Reinforcement learning (RL) holds potential in addressing\nthese challenges, as it allows agents to learn adaptive scheduling strategies.\nHowever, there is a lack of a comprehensive, general-purpose frameworks for\neffectively training and evaluating RL agents under real-world constraints. To\naddress this gap, we propose a modular framework that extends classical JSSP\nformulations by incorporating key \\mbox{real-world} constraints inherent to the\nshopfloor, including transport logistics, buffer management, machine\nbreakdowns, setup times, and stochastic processing conditions, while also\nsupporting multi-objective optimization. The framework is a customizable\nsolution that offers flexibility in defining problem instances and configuring\nsimulation parameters, enabling adaptation to diverse production scenarios. A\nstandardized interface ensures compatibility with various RL approaches,\nproviding a robust environment for training RL agents and facilitating the\nstandardized comparison of different scheduling methods under dynamic and\nuncertain conditions. We release JobShopLab as an open-source tool for both\nresearch and industrial applications, accessible at:\nhttps://github.com/proto-lab-ro/jobshoplab\n","authors":["Jonathan Hoss","Felix Schelling","Noah Klarmann"],"pdf_url":"https://arxiv.org/pdf/2506.13566v1.pdf","comment":"This paper has been accepted for presentation at the IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)"},{"id":"http://arxiv.org/abs/2409.00575v5","updated":"2025-06-16T14:48:05Z","published":"2024-09-01T01:28:53Z","title":"Online Optimization for Learning to Communicate over Time-Correlated\n  Channels","summary":"  Machine learning techniques have garnered great interest in designing\ncommunication systems owing to their capacity in tackling with channel\nuncertainty. To provide theoretical guarantees for learning-based communication\nsystems, some recent works analyze generalization bounds for devised methods\nbased on the assumption of Independently and Identically Distributed (I.I.D.)\nchannels, a condition rarely met in practical scenarios. In this paper, we drop\nthe I.I.D. channel assumption and study an online optimization problem of\nlearning to communicate over time-correlated channels. To address this issue,\nwe further focus on two specific tasks: optimizing channel decoders for\ntime-correlated fading channels and selecting optimal codebooks for\ntime-correlated additive noise channels. For utilizing temporal dependence of\nconsidered channels to better learn communication systems, we develop two\nonline optimization algorithms based on the optimistic online mirror descent\nframework. Furthermore, we provide theoretical guarantees for proposed\nalgorithms via deriving sub-linear regret bound on the expected error\nprobability of learned systems. Extensive simulation experiments have been\nconducted to validate that our presented approaches can leverage the channel\ncorrelation to achieve a lower average symbol error rate compared to baseline\nmethods, consistent with our theoretical findings.\n","authors":["Zheshun Wu","Junfan Li","Zenglin Xu","Sumei Sun","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2409.00575v5.pdf","comment":"15 pages, 6 figures, submitted for possible journal publication"},{"id":"http://arxiv.org/abs/2506.13561v1","updated":"2025-06-16T14:47:02Z","published":"2025-06-16T14:47:02Z","title":"Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated\n  Learning","summary":"  Federated learning (FL) shows great promise in large-scale machine learning\nbut introduces new privacy and security challenges. We propose ByITFL and\nLoByITFL, two novel FL schemes that enhance resilience against Byzantine users\nwhile keeping the users' data private from eavesdroppers. To ensure privacy and\nByzantine resilience, our schemes build on having a small representative\ndataset available to the federator and crafting a discriminator function\nallowing the mitigation of corrupt users' contributions. ByITFL employs\nLagrange coded computing and re-randomization, making it the first\nByzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,\nthough at the cost of a significant communication overhead. LoByITFL, on the\nother hand, achieves Byzantine resilience and IT privacy at a significantly\nreduced communication cost, but requires a Trusted Third Party, used only in a\none-time initialization phase before training. We provide theoretical\nguarantees on privacy and Byzantine resilience, along with convergence\nguarantees and experimental results validating our findings.\n","authors":["Yue Xia","Christoph Hofmeister","Maximilian Egger","Rawad Bitar"],"pdf_url":"https://arxiv.org/pdf/2506.13561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15659v2","updated":"2025-06-16T14:43:25Z","published":"2025-01-26T19:43:41Z","title":"AirIO: Learning Inertial Odometry with Enhanced IMU Feature\n  Observability","summary":"  Inertial odometry (IO) using only Inertial Measurement Units (IMUs) offers a\nlightweight and cost-effective solution for Unmanned Aerial Vehicle (UAV)\napplications, yet existing learning-based IO models often fail to generalize to\nUAVs due to the highly dynamic and non-linear-flight patterns that differ from\npedestrian motion. In this work, we identify that the conventional practice of\ntransforming raw IMU data to global coordinates undermines the observability of\ncritical kinematic information in UAVs. By preserving the body-frame\nrepresentation, our method achieves substantial performance improvements, with\na 66.7% average increase in accuracy across three datasets. Furthermore,\nexplicitly encoding attitude information into the motion network results in an\nadditional 23.8% improvement over prior results. Combined with a data-driven\nIMU correction model (AirIMU) and an uncertainty-aware Extended Kalman Filter\n(EKF), our approach ensures robust state estimation under aggressive UAV\nmaneuvers without relying on external sensors or control inputs. Notably, our\nmethod also demonstrates strong generalizability to unseen data not included in\nthe training set, underscoring its potential for real-world UAV applications.\n","authors":["Yuheng Qiu","Can Xu","Yutian Chen","Shibo Zhao","Junyi Geng","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2501.15659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04178v2","updated":"2025-06-16T14:42:51Z","published":"2025-03-06T07:45:48Z","title":"Unsupervised anomaly detection on cybersecurity data streams: a case\n  with BETH dataset","summary":"  In modern world the importance of cybersecurity of various systems is\nincreasing from year to year. The number of information security events\ngenerated by information security tools grows up with the development of the IT\ninfrastructure. At the same time, the cyber threat landscape does not remain\nconstant, and monitoring should take into account both already known attack\nindicators and those for which there are no signature rules in information\nsecurity products of various classes yet. Detecting anomalies in large\ncybersecurity data streams is a complex task that, if properly addressed, can\nallow for timely response to atypical and previously unknown cyber threats. The\npossibilities of using of offline algorithms may be limited for a number of\nreasons related to the time of training and the frequency of retraining. Using\nstream learning algorithms for solving this task is capable of providing\nnear-real-time data processing. This article examines the results of ten\nalgorithms from three Python stream machine-learning libraries on BETH dataset\nwith cybersecurity events, which contains information about the creation,\ncloning, and destruction of operating system processes collected using extended\neBPF. ROC-AUC metric and total processing time of processing with these\nalgorithms are presented. Several combinations of features and the order of\nevents are considered. In conclusion, some mentions are given about the most\npromising algorithms and possible directions for further research are outlined.\n","authors":["Evgeniy Eremin"],"pdf_url":"https://arxiv.org/pdf/2503.04178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13554v1","updated":"2025-06-16T14:41:15Z","published":"2025-06-16T14:41:15Z","title":"Stability Analysis of Physics-Informed Neural Networks via Variational\n  Coercivity, Perturbation Bounds, and Concentration Estimates","summary":"  We develop a rigorous stability framework for Physics-Informed Neural\nNetworks (PINNs) grounded in variational analysis, operator coercivity, and\nexplicit perturbation theory. PINNs approximate solutions to partial\ndifferential equations (PDEs) by minimizing residual-based losses over sampled\ncollocation points. We derive deterministic stability bounds that quantify how\nbounded perturbations in the network output propagate through both residual and\nsupervised loss components. Probabilistic stability is established via\nMcDiarmid's inequality, yielding non-asymptotic concentration bounds that link\nsampling variability to empirical loss fluctuations under minimal assumptions.\nGeneralization from Sobolev-norm training loss to uniform approximation is\nanalyzed using coercivity and Sobolev embeddings, leading to pointwise error\ncontrol. The theoretical results apply to both scalar and vector-valued PDEs\nand cover composite loss formulations. Numerical experiments validate the\nperturbation sensitivity, sample complexity estimates, and Sobolev-to-uniform\ngeneralization bounds. This work provides a mathematically grounded and\npractically applicable stability framework for PINNs, clarifying the role of\noperator structure, sampling design, and functional regularity in robust\ntraining.\n","authors":["Ronald Katende"],"pdf_url":"https://arxiv.org/pdf/2506.13554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13405v4","updated":"2025-06-16T14:31:49Z","published":"2025-05-19T17:41:10Z","title":"A Dataless Reinforcement Learning Approach to Rounding Hyperplane\n  Optimization for Max-Cut","summary":"  The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal\nsolution is NP-hard in the worst case. As a result, heuristic-based algorithms\nare commonly used, though their design often requires significant domain\nexpertise. More recently, learning-based methods trained on large (un)labeled\ndatasets have been proposed; however, these approaches often struggle with\ngeneralizability and scalability. A well-known approximation algorithm for\nMaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic\nUnconstrained Binary Optimization (QUBO) formulation into a semidefinite\nprogram (SDP). The GW algorithm then applies hyperplane rounding by uniformly\nsampling a random hyperplane to convert the SDP solution into binary node\nassignments. In this paper, we propose a training-data-free approach based on a\nnon-episodic reinforcement learning formulation, in which an agent learns to\nselect improved rounding hyperplanes that yield better cuts than those produced\nby the GW algorithm. By optimizing over a Markov Decision Process (MDP), our\nmethod consistently achieves better cuts across large-scale graphs with varying\ndensities and degree distributions.\n","authors":["Gabriel Maliakal","Ismail Alkhouri","Alvaro Velasquez","Adam M Alessio","Saiprasad Ravishankar"],"pdf_url":"https://arxiv.org/pdf/2505.13405v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13541v1","updated":"2025-06-16T14:30:17Z","published":"2025-06-16T14:30:17Z","title":"Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization","summary":"  Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.\n","authors":["Guanghui Song","Dongping Liao","Yiren Zhao","Kejiang Ye","Cheng-zhong Xu","Xitong Gao"],"pdf_url":"https://arxiv.org/pdf/2506.13541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23860v2","updated":"2025-06-16T14:30:10Z","published":"2025-05-29T08:15:23Z","title":"Quantum computing and artificial intelligence: status and perspectives","summary":"  This white paper discusses and explores the various points of intersection\nbetween quantum computing and artificial intelligence (AI). It describes how\nquantum computing could support the development of innovative AI solutions. It\nalso examines use cases of classical AI that can empower research and\ndevelopment in quantum technologies, with a focus on quantum computing and\nquantum sensing. The purpose of this white paper is to provide a long-term\nresearch agenda aimed at addressing foundational questions about how AI and\nquantum computing interact and benefit one another. It concludes with a set of\nrecommendations and challenges, including how to orchestrate the proposed\ntheoretical work, align quantum AI developments with quantum hardware roadmaps,\nestimate both classical and quantum resources - especially with the goal of\nmitigating and optimizing energy consumption - advance this emerging hybrid\nsoftware engineering discipline, and enhance European industrial\ncompetitiveness while considering societal implications.\n","authors":["Giovanni Acampora","Andris Ambainis","Natalia Ares","Leonardo Banchi","Pallavi Bhardwaj","Daniele Binosi","G. Andrew D. Briggs","Tommaso Calarco","Vedran Dunjko","Jens Eisert","Olivier Ezratty","Paul Erker","Federico Fedele","Elies Gil-Fuster","Martin Gärttner","Mats Granath","Markus Heyl","Iordanis Kerenidis","Matthias Klusch","Anton Frisk Kockum","Richard Kueng","Mario Krenn","Jörg Lässig","Antonio Macaluso","Sabrina Maniscalco","Florian Marquardt","Kristel Michielsen","Gorka Muñoz-Gil","Daniel Müssig","Hendrik Poulsen Nautrup","Sophie A. Neubauer","Evert van Nieuwenburg","Roman Orus","Jörg Schmiedmayer","Markus Schmitt","Philipp Slusallek","Filippo Vicentini","Christof Weitenberg","Frank K. Wilhelm"],"pdf_url":"https://arxiv.org/pdf/2505.23860v2.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2506.13536v1","updated":"2025-06-16T14:25:29Z","published":"2025-06-16T14:25:29Z","title":"What Matters in Learning from Large-Scale Datasets for Robot\n  Manipulation","summary":"  Imitation learning from large multi-task demonstration datasets has emerged\nas a promising path for building generally-capable robots. As a result, 1000s\nof hours have been spent on building such large-scale datasets around the\nglobe. Despite the continuous growth of such efforts, we still lack a\nsystematic understanding of what data should be collected to improve the\nutility of a robotics dataset and facilitate downstream policy learning. In\nthis work, we conduct a large-scale dataset composition study to answer this\nquestion. We develop a data generation framework to procedurally emulate common\nsources of diversity in existing datasets (such as sensor placements and object\ntypes and arrangements), and use it to generate large-scale robot datasets with\ncontrolled compositions, enabling a suite of dataset composition studies that\nwould be prohibitively expensive in the real world. We focus on two practical\nsettings: (1) what types of diversity should be emphasized when future\nresearchers collect large-scale datasets for robotics, and (2) how should\ncurrent practitioners retrieve relevant demonstrations from existing datasets\nto maximize downstream policy performance on tasks of interest. Our study\nyields several critical insights -- for example, we find that camera poses and\nspatial arrangements are crucial dimensions for both diversity in collection\nand alignment in retrieval. In real-world robot learning settings, we find that\nnot only do our insights from simulation carry over, but our retrieval\nstrategies on existing datasets such as DROID allow us to consistently\noutperform existing training strategies by up to 70%. More results at\nhttps://robo-mimiclabs.github.io/\n","authors":["Vaibhav Saxena","Matthew Bronars","Nadun Ranawaka Arachchige","Kuancheng Wang","Woo Chul Shin","Soroush Nasiriany","Ajay Mandlekar","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2506.13536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13533v1","updated":"2025-06-16T14:23:16Z","published":"2025-06-16T14:23:16Z","title":"Learning Augmented Graph $k$-Clustering","summary":"  Clustering is a fundamental task in unsupervised learning. Previous research\nhas focused on learning-augmented $k$-means in Euclidean metrics, limiting its\napplicability to complex data representations. In this paper, we generalize\nlearning-augmented $k$-clustering to operate on general metrics, enabling its\napplication to graph-structured and non-Euclidean domains. Our framework also\nrelaxes restrictive cluster size constraints, providing greater flexibility for\ndatasets with imbalanced or unknown cluster distributions. Furthermore, we\nextend the hardness of query complexity to general metrics: under the\nExponential Time Hypothesis (ETH), we show that any polynomial-time algorithm\nmust perform approximately $\\Omega(k / \\alpha)$ queries to achieve a $(1 +\n\\alpha)$-approximation. These contributions strengthen both the theoretical\nfoundations and practical applicability of learning-augmented clustering,\nbridging gaps between traditional methods and real-world challenges.\n","authors":["Chenglin Fan","Kijun Shin"],"pdf_url":"https://arxiv.org/pdf/2506.13533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13529v1","updated":"2025-06-16T14:19:40Z","published":"2025-06-16T14:19:40Z","title":"Seismic Acoustic Impedance Inversion Framework Based on Conditional\n  Latent Generative Diffusion Model","summary":"  Seismic acoustic impedance plays a crucial role in lithological\nidentification and subsurface structure interpretation. However, due to the\ninherently ill-posed nature of the inversion problem, directly estimating\nimpedance from post-stack seismic data remains highly challenging. Recently,\ndiffusion models have shown great potential in addressing such inverse problems\ndue to their strong prior learning and generative capabilities. Nevertheless,\nmost existing methods operate in the pixel domain and require multiple\niterations, limiting their applicability to field data. To alleviate these\nlimitations, we propose a novel seismic acoustic impedance inversion framework\nbased on a conditional latent generative diffusion model, where the inversion\nprocess is made in latent space. To avoid introducing additional training\noverhead when embedding conditional inputs, we design a lightweight\nwavelet-based module into the framework to project seismic data and reuse an\nencoder trained on impedance to embed low-frequency impedance into the latent\nspace. Furthermore, we propose a model-driven sampling strategy during the\ninversion process of this framework to enhance accuracy and reduce the number\nof required diffusion steps. Numerical experiments on a synthetic model\ndemonstrate that the proposed method achieves high inversion accuracy and\nstrong generalization capability within only a few diffusion steps. Moreover,\napplication to field data reveals enhanced geological detail and higher\nconsistency with well-log measurements, validating the effectiveness and\npracticality of the proposed approach.\n","authors":["Jie Chen","Hongling Chen","Jinghuai Gao","Chuangji Meng","Tao Yang","XinXin Liang"],"pdf_url":"https://arxiv.org/pdf/2506.13529v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.02670v3","updated":"2025-06-16T14:19:01Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13523v1","updated":"2025-06-16T14:15:18Z","published":"2025-06-16T14:15:18Z","title":"The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in\n  Equivariant Tensor Products","summary":"  $E(3)$-equivariant neural networks have demonstrated success across a wide\nrange of 3D modelling tasks. A fundamental operation in these networks is the\ntensor product, which interacts two geometric features in an equivariant manner\nto create new features. Due to the high computational complexity of the tensor\nproduct, significant effort has been invested to optimize the runtime of this\noperation. For example, Luo et al. (2024) recently proposed the Gaunt tensor\nproduct (GTP) which promises a significant speedup. In this work, we provide a\ncareful, systematic analysis of a number of tensor product operations. In\nparticular, we emphasize that different tensor products are not performing the\nsame operation. The reported speedups typically come at the cost of\nexpressivity. We introduce measures of expressivity and interactability to\ncharacterize these differences. In addition, we realized the original\nimplementation of GTP can be greatly simplified by directly using a spherical\ngrid at no cost in asymptotic runtime. This spherical grid approach is faster\non our benchmarks and in actual training of the MACE interatomic potential by\n30\\%. Finally, we provide the first systematic microbenchmarks of the various\ntensor product operations. We find that the theoretical runtime guarantees can\ndiffer wildly from empirical performance, demonstrating the need for careful\napplication-specific benchmarking. Code is available at\n\\href{https://github.com/atomicarchitects/PriceofFreedom}{https://github.com/atomicarchitects/PriceofFreedom}\n","authors":["YuQing Xie","Ameya Daigavane","Mit Kotak","Tess Smidt"],"pdf_url":"https://arxiv.org/pdf/2506.13523v1.pdf","comment":"27 pages, 10 Figures, ICML 2025"},{"id":"http://arxiv.org/abs/2506.13514v1","updated":"2025-06-16T14:09:43Z","published":"2025-06-16T14:09:43Z","title":"TensorSLM: Energy-efficient Embedding Compression of Sub-billion\n  Parameter Language Models on Low-end Devices","summary":"  Small Language Models (SLMs, or on-device LMs) have significantly fewer\nparameters than Large Language Models (LLMs). They are typically deployed on\nlow-end devices, like mobile phones and single-board computers. Unlike LLMs,\nwhich rely on increasing model size for better generalisation, SLMs designed\nfor edge applications are expected to have adaptivity to the deployment\nenvironments and energy efficiency given the device battery life constraints,\nwhich are not addressed in datacenter-deployed LLMs. This paper addresses these\ntwo requirements by proposing a training-free token embedding compression\napproach using Tensor-Train Decomposition (TTD). Each pre-trained token\nembedding vector is converted into a lower-dimensional Matrix Product State\n(MPS). We comprehensively evaluate the extracted low-rank structures across\ncompression ratio, language task performance, latency, and energy consumption\non a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion\nparameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our\napproach achieves a comparable language task performance to the original model\nwith around $2.0\\times$ embedding layer compression, while the energy\nconsumption of a single query drops by half.\n","authors":["Mingxue Xu","Yao Lei Xu","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2506.13514v1.pdf","comment":"ICML 2025 Workshop on Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models (TTODLer-FM)"},{"id":"http://arxiv.org/abs/2311.13548v2","updated":"2025-06-16T14:06:17Z","published":"2023-11-22T17:44:18Z","title":"Efficient Numerical Integration in Reproducing Kernel Hilbert Spaces via\n  Leverage Scores Sampling","summary":"  In this work we consider the problem of numerical integration, i.e.,\napproximating integrals with respect to a target probability measure using only\npointwise evaluations of the integrand. We focus on the setting in which the\ntarget distribution is only accessible through a set of $n$ i.i.d.\nobservations, and the integrand belongs to a reproducing kernel Hilbert space.\nWe propose an efficient procedure which exploits a small i.i.d. random subset\nof $m<n$ samples drawn either uniformly or using approximate leverage scores\nfrom the initial observations. Our main result is an upper bound on the\napproximation error of this procedure for both sampling strategies. It yields\nsufficient conditions on the subsample size to recover the standard (optimal)\n$n^{-1/2}$ rate while reducing drastically the number of functions evaluations,\nand thus the overall computational cost. Moreover, we obtain rates with respect\nto the number $m$ of evaluations of the integrand which adapt to its\nsmoothness, and match known optimal rates for instance for Sobolev spaces. We\nillustrate our theoretical findings with numerical experiments on real\ndatasets, which highlight the attractive efficiency-accuracy tradeoff of our\nmethod compared to existing randomized and greedy quadrature methods. We note\nthat, the problem of numerical integration in RKHS amounts to designing a\ndiscrete approximation of the kernel mean embedding of the target distribution.\nAs a consequence, direct applications of our results also include the efficient\ncomputation of maximum mean discrepancies between distributions and the design\nof efficient kernel-based tests.\n","authors":["Antoine Chatalic","Nicolas Schreuder","Ernesto De Vito","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2311.13548v2.pdf","comment":"47 pages, 5 figures. Accepted for publication in JMLR"},{"id":"http://arxiv.org/abs/2410.11188v2","updated":"2025-06-16T14:04:21Z","published":"2024-10-15T02:07:48Z","title":"Fast Second-Order Online Kernel Learning through Incremental Matrix\n  Sketching and Decomposition","summary":"  Online Kernel Learning (OKL) has attracted considerable research interest due\nto its promising predictive performance in streaming environments. Second-order\napproaches are particularly appealing for OKL as they often offer substantial\nimprovements in regret guarantees. However, existing second-order OKL\napproaches suffer from at least quadratic time complexity with respect to the\npre-set budget, rendering them unsuitable for meeting the real-time demands of\nlarge-scale streaming recommender systems. The singular value decomposition\nrequired to obtain explicit feature mapping is also computationally expensive\ndue to the complete decomposition process. Moreover, the absence of incremental\nupdates to manage approximate kernel space causes these algorithms to perform\npoorly in adversarial environments and real-world streaming recommendation\ndatasets. To address these issues, we propose FORKS, a fast incremental matrix\nsketching and decomposition approach tailored for second-order OKL. FORKS\nconstructs an incremental maintenance paradigm for second-order kernelized\ngradient descent, which includes incremental matrix sketching for kernel\napproximation and incremental matrix decomposition for explicit feature mapping\nconstruction. Theoretical analysis demonstrates that FORKS achieves a\nlogarithmic regret guarantee on par with other second-order approaches while\nmaintaining a linear time complexity w.r.t. the budget, significantly enhancing\nefficiency over existing approaches. We validate the performance of FORKS\nthrough extensive experiments conducted on real-world streaming recommendation\ndatasets, demonstrating its superior scalability and robustness against\nadversarial attacks.\n","authors":["Dongxie Wen","Xiao Zhang","Zhewei Wei","Chenping Hou","Shuai Li","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11188v2.pdf","comment":"Accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2502.09172v2","updated":"2025-06-16T14:02:28Z","published":"2025-02-13T10:56:58Z","title":"LOB-Bench: Benchmarking Generative AI for Finance -- an Application to\n  Limit Order Book Data","summary":"  While financial data presents one of the most challenging and interesting\nsequence modelling tasks due to high noise, heavy tails, and strategic\ninteractions, progress in this area has been hindered by the lack of consensus\non quantitative evaluation paradigms. To address this, we present LOB-Bench, a\nbenchmark, implemented in python, designed to evaluate the quality and realism\nof generative message-by-order data for limit order books (LOB) in the LOBSTER\nformat. Our framework measures distributional differences in conditional and\nunconditional statistics between generated and real LOB data, supporting\nflexible multivariate statistical evaluation. The benchmark also includes\nfeatures commonly used LOB statistics such as spread, order book volumes, order\nimbalance, and message inter-arrival times, along with scores from a trained\ndiscriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e.\nthe cross-correlations and price response functions for specific events in the\ndata. We benchmark generative autoregressive state-space models, a (C)GAN, as\nwell as a parametric LOB model and find that the autoregressive GenAI approach\nbeats traditional model classes.\n","authors":["Peer Nagy","Sascha Frey","Kang Li","Bidipta Sarkar","Svitlana Vyetrenko","Stefan Zohren","Ani Calinescu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.09172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13498v1","updated":"2025-06-16T13:55:20Z","published":"2025-06-16T13:55:20Z","title":"A Survey on Imitation Learning for Contact-Rich Tasks in Robotics","summary":"  This paper comprehensively surveys research trends in imitation learning for\ncontact-rich robotic tasks. Contact-rich tasks, which require complex physical\ninteractions with the environment, represent a central challenge in robotics\ndue to their nonlinear dynamics and sensitivity to small positional deviations.\nThe paper examines demonstration collection methodologies, including teaching\nmethods and sensory modalities crucial for capturing subtle interaction\ndynamics. We then analyze imitation learning approaches, highlighting their\napplications to contact-rich manipulation. Recent advances in multimodal\nlearning and foundation models have significantly enhanced performance in\ncomplex contact tasks across industrial, household, and healthcare domains.\nThrough systematic organization of current research and identification of\nchallenges, this survey provides a foundation for future advancements in\ncontact-rich robotic manipulation.\n","authors":["Toshiaki Tsuji","Yasuhiro Kato","Gokhan Solak","Heng Zhang","Tadej Petrič","Francesco Nori","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2506.13498v1.pdf","comment":"47pages, 1 figures"},{"id":"http://arxiv.org/abs/2506.13496v1","updated":"2025-06-16T13:53:02Z","published":"2025-06-16T13:53:02Z","title":"Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval","summary":"  Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware.\n","authors":["Kshitij Kavimandan","Angelos Nalmpantis","Emma Beauxis-Aussalet","Robert-Jan Sips"],"pdf_url":"https://arxiv.org/pdf/2506.13496v1.pdf","comment":"5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025"},{"id":"http://arxiv.org/abs/2506.13488v1","updated":"2025-06-16T13:45:36Z","published":"2025-06-16T13:45:36Z","title":"Imaging at the quantum limit with convolutional neural networks","summary":"  Deep neural networks have been shown to achieve exceptional performance for\ncomputer vision tasks like image recognition, segmentation, and reconstruction\nor denoising. Here, we evaluate the ultimate performance limits of deep\nconvolutional neural network models for image reconstruction, by comparing them\nagainst the standard quantum limit set by shot-noise and the Heisenberg limit\non precision. We train U-Net models on images of natural objects illuminated\nwith coherent states of light, and find that the average mean-squared error of\nthe reconstructions can surpass the standard quantum limit, and in some cases\nreaches the Heisenberg limit. Further, we train models on well-parameterized\nimages for which we can calculate the quantum Cram\\'er-Rao bound to determine\nthe minimum possible measurable variance of an estimated parameter for a given\nprobe state. We find the mean-squared error of the model predictions reaches\nthese bounds calculated for the parameters, across a variety of parameterized\nimages. These results suggest that deep convolutional neural networks can learn\nto become the optimal estimators allowed by the laws of physics, performing\nparameter estimation and image reconstruction at the ultimate possible limits\nof precision for the case of classical illumination of the object.\n","authors":["Andrew H. Proppe","Aaron Z. Goldberg","Guillaume Thekkadath","Noah Lupu-Gladstein","Kyle M. Jordan","Philip J. Bustard","Frédéric Bouchard","Duncan England","Khabat Heshami","Jeff S. Lundeen","Benjamin J. Sussman"],"pdf_url":"https://arxiv.org/pdf/2506.13488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13485v1","updated":"2025-06-16T13:44:25Z","published":"2025-06-16T13:44:25Z","title":"Curriculum Learning for Biological Sequence Prediction: The Case of De\n  Novo Peptide Sequencing","summary":"  Peptide sequencing-the process of identifying amino acid sequences from mass\nspectrometry data-is a fundamental task in proteomics. Non-Autoregressive\nTransformers (NATs) have proven highly effective for this task, outperforming\ntraditional methods. Unlike autoregressive models, which generate tokens\nsequentially, NATs predict all positions simultaneously, leveraging\nbidirectional context through unmasked self-attention. However, existing NAT\napproaches often rely on Connectionist Temporal Classification (CTC) loss,\nwhich presents significant optimization challenges due to CTC's complexity and\nincreases the risk of training failures. To address these issues, we propose an\nimproved non-autoregressive peptide sequencing model that incorporates a\nstructured protein sequence curriculum learning strategy. This approach adjusts\nprotein's learning difficulty based on the model's estimated protein\ngenerational capabilities through a sampling process, progressively learning\npeptide generation from simple to complex sequences. Additionally, we introduce\na self-refining inference-time module that iteratively enhances predictions\nusing learned NAT token embeddings, improving sequence accuracy at a\nfine-grained level. Our curriculum learning strategy reduces NAT training\nfailures frequency by more than 90% based on sampled training over various data\ndistributions. Evaluations on nine benchmark species demonstrate that our\napproach outperforms all previous methods across multiple metrics and species.\n","authors":["Xiang Zhang","Jiaqi Wei","Zijie Qiu","Sheng Xu","Nanqing Dong","Zhiqiang Gao","Siqi Sun"],"pdf_url":"https://arxiv.org/pdf/2506.13485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13474v1","updated":"2025-06-16T13:32:01Z","published":"2025-06-16T13:32:01Z","title":"Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning","summary":"  Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.\n","authors":["David Bani-Harouni","Chantal Pellegrini","Ege Özsoy","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2506.13474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00942v2","updated":"2025-06-16T13:28:17Z","published":"2025-01-01T19:52:19Z","title":"Efficient Unsupervised Shortcut Learning Detection and Mitigation in\n  Transformers","summary":"  Shortcut learning, i.e., a model's reliance on undesired features not\ndirectly relevant to the task, is a major challenge that severely limits the\napplications of machine learning algorithms, particularly when deploying them\nto assist in making sensitive decisions, such as in medical diagnostics. In\nthis work, we leverage recent advancements in machine learning to create an\nunsupervised framework that is capable of both detecting and mitigating\nshortcut learning in transformers. We validate our method on multiple datasets.\nResults demonstrate that our framework significantly improves both worst-group\naccuracy (samples misclassified due to shortcuts) and average accuracy, while\nminimizing human annotation effort. Moreover, we demonstrate that the detected\nshortcuts are meaningful and informative to human experts, and that our\nframework is computationally efficient, allowing it to be run on consumer\nhardware.\n","authors":["Lukas Kuhn","Sari Sadiya","Jorg Schlotterer","Florian Buettner","Christin Seifert","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2501.00942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10205v2","updated":"2025-06-16T13:14:34Z","published":"2025-02-14T14:59:37Z","title":"Looking around you: external information enhances representations for\n  event sequences","summary":"  Representation learning produces models in different domains, such as store\npurchases, client transactions, and general people's behaviour. However, such\nmodels for event sequences usually process each sequence in isolation, ignoring\ncontext from ones that co-occur in time. This limitation is particularly\nproblematic in domains with fast-evolving conditions, like finance and\ne-commerce, or when certain sequences lack recent events.\n  We develop a method that aggregates information from multiple user\nrepresentations, augmenting a specific user for a scenario of multiple\nco-occurring event sequences, achieving better quality than processing each\nsequence independently. Our study considers diverse aggregation approaches,\nranging from simple pooling techniques to trainable attention-based Kernel\nattention aggregation, that can highlight more complex information flow from\nother users. The proposed methods operate on top of an existing encoder and\nsupport its efficient fine-tuning. Across six diverse event sequence datasets\n(finance, e-commerce, education, etc.) and downstream tasks, Kernel attention\nimproves ROC-AUC scores, both with and without fine-tuning, while mean pooling\nyields a smaller but still significant gain.\n","authors":["Maria Kovaleva","Petr Sokerin","Pavel Tikhomirov","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2502.10205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13452v1","updated":"2025-06-16T13:12:43Z","published":"2025-06-16T13:12:43Z","title":"Balancing Intensity and Focality in Directional DBS Under Uncertainty: A\n  Simulation Study of Electrode Optimization via a Metaheuristic L1L1 Approach","summary":"  As DBS technology advances toward directional leads and optimization-based\ncurrent steering, this study aims to improve the selection of electrode contact\nconfigurations using the recently developed L1-norm regularized L1-norm fitting\n(L1L1) method. The focus is in particular on L1L1's capability to incorporate a\npriori lead field uncertainty, offering a potential advantage over conventional\napproaches that do not account for such variability. Our optimization framework\nincorporates uncertainty by constraining the solution space based on lead field\nattenuation. This reflects physiological expectations about the VTA and serves\nto avoid overfitting. By applying this method to 8- and 40-contact electrode\nconfigurations, we optimize current distributions within a discretized finite\nelement (FE) model, focusing on the lead field's characteristics. The model\naccounts for uncertainty through these explicit constraints, enhancing the\nfeasibility, focality, and robustness of the resulting solutions. The L1L1\nmethod was validated through a series of numerical experiments using both\nnoiseless and noisy lead fields, where the noise level was selected to reflect\nattenuation within VTA. It successfully fits and regularizes the current\ndistribution across target structures, with hyperparameter optimization\nextracting either bipolar or multipolar electrode configurations. These\nconfigurations aim to maximize focused current density or prioritize a high\ngain field ratio in a discretized FE model. Compared to traditional methods,\nthe L1L1 approach showed competitive performance in concentrating stimulation\nwithin the target region while minimizing unintended current spread,\nparticularly under noisy conditions. By incorporating uncertainty directly into\nthe optimization process, we obtain a noise-robust framework for current\nsteering, allowing for variations in lead field models and simulation\nparameters.\n","authors":["Fernando Galaz Prieto","Antti Lassila","Maryam Samavaki","Sampsa Pursiainen"],"pdf_url":"https://arxiv.org/pdf/2506.13452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10020v5","updated":"2025-06-16T13:10:36Z","published":"2025-02-14T09:01:12Z","title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","summary":"  In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\sqrt{d} \\right)$, which is a significant\nimprovement over the previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee &\nOh, 2024). This is mainly achieved by establishing tighter self-concordant\nproperties of the MNL loss and applying Ville's inequality to bound the\nestimation error. Using this new online confidence bound, we propose a\nconstant-time algorithm, OFU-MNL++, which achieves a variance-dependent regret\nbound of $O \\Big( d \\log T \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $ for\nsufficiently large $T$, where $\\sigma_t^2$ denotes the variance of the rewards\nat round $t$, $d$ is the dimension of the contexts, and $T$ is the total number\nof rounds. Furthermore, we introduce a Maximum Likelihood Estimation\n(MLE)-based algorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free\nregret of $O \\Big( d \\log (BT) \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2502.10020v5.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2502.02531v3","updated":"2025-06-16T12:52:01Z","published":"2025-02-04T17:50:55Z","title":"Deep Linear Network Training Dynamics from Random Initialization: Data,\n  Width, Depth, and Hyperparameter Transfer","summary":"  We theoretically characterize gradient descent dynamics in deep linear\nnetworks trained at large width from random initialization and on large\nquantities of random data. Our theory captures the ``wider is better\" effect of\nmean-field/maximum-update parameterized networks as well as hyperparameter\ntransfer effects, which can be contrasted with the neural-tangent\nparameterization where optimal learning rates shift with model width. We\nprovide asymptotic descriptions of both non-residual and residual neural\nnetworks, the latter of which enables an infinite depth limit when branches are\nscaled as $1/\\sqrt{\\text{depth}}$. We also compare training with one-pass\nstochastic gradient descent to the dynamics when training data are repeated at\neach iteration. Lastly, we show that this model recovers the accelerated power\nlaw training dynamics for power law structured data in the rich regime observed\nin recent works.\n","authors":["Blake Bordelon","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2502.02531v3.pdf","comment":"ICML Camera Ready"},{"id":"http://arxiv.org/abs/2506.09813v2","updated":"2025-06-16T12:43:27Z","published":"2025-06-11T14:53:47Z","title":"Metritocracy: Representative Metrics for Lite Benchmarks","summary":"  A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.\n","authors":["Ariel Procaccia","Benjamin Schiffer","Serena Wang","Shirley Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.09813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06866v2","updated":"2025-06-16T12:36:58Z","published":"2025-06-07T17:12:03Z","title":"SAFE: Finding Sparse and Flat Minima to Improve Pruning","summary":"  Sparsifying neural networks often suffers from seemingly inevitable\nperformance degradation, and it remains challenging to restore the original\nperformance despite much recent progress. Motivated by recent studies in robust\noptimization, we aim to tackle this problem by finding subnetworks that are\nboth sparse and flat at the same time. Specifically, we formulate pruning as a\nsparsity-constrained optimization problem where flatness is encouraged as an\nobjective. We solve it explicitly via an augmented Lagrange dual approach and\nextend it further by proposing a generalized projection operation, resulting in\nnovel pruning methods called SAFE and its extension, SAFE$^+$. Extensive\nevaluations on standard image classification and language modeling tasks reveal\nthat SAFE consistently yields sparse networks with improved generalization\nperformance, which compares competitively to well-established baselines. In\naddition, SAFE demonstrates resilience to noisy data, making it well-suited for\nreal-world conditions.\n","authors":["Dongyeop Lee","Kwanhee Lee","Jinseok Chung","Namhoon Lee"],"pdf_url":"https://arxiv.org/pdf/2506.06866v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.13416v1","updated":"2025-06-16T12:33:09Z","published":"2025-06-16T12:33:09Z","title":"Spiking Neural Networks for Low-Power Vibration-Based Predictive\n  Maintenance","summary":"  Advancements in Industrial Internet of Things (IIoT) sensors enable\nsophisticated Predictive Maintenance (PM) with high temporal resolution. For\ncost-efficient solutions, vibration-based condition monitoring is especially of\ninterest. However, analyzing high-resolution vibration data via traditional\ncloud approaches incurs significant energy and communication costs, hindering\nbattery-powered edge deployments. This necessitates shifting intelligence to\nthe sensor edge. Due to their event-driven nature, Spiking Neural Networks\n(SNNs) offer a promising pathway toward energy-efficient on-device processing.\nThis paper investigates a recurrent SNN for simultaneous regression (flow,\npressure, pump speed) and multi-label classification (normal, overpressure,\ncavitation) for an industrial progressing cavity pump (PCP) using 3-axis\nvibration data. Furthermore, we provide energy consumption estimates comparing\nthe SNN approach on conventional (x86, ARM) and neuromorphic (Loihi) hardware\nplatforms. Results demonstrate high classification accuracy (>97%) with zero\nFalse Negative Rates for critical Overpressure and Cavitation faults. Smoothed\nregression outputs achieve Mean Relative Percentage Errors below 1% for flow\nand pump speed, approaching industrial sensor standards, although pressure\nprediction requires further refinement. Energy estimates indicate significant\npower savings, with the Loihi consumption (0.0032 J/inf) being up to 3 orders\nof magnitude less compared to the estimated x86 CPU (11.3 J/inf) and ARM CPU\n(1.18 J/inf) execution. Our findings underscore the potential of SNNs for\nmulti-task PM directly on resource-constrained edge devices, enabling scalable\nand energy-efficient industrial monitoring solutions.\n","authors":["Alexandru Vasilache","Sven Nitzsche","Christian Kneidl","Mikael Tekneyan","Moritz Neher","Juergen Becker"],"pdf_url":"https://arxiv.org/pdf/2506.13416v1.pdf","comment":"This paper has been accepted and will be presented at the\n  International Conference on Neuromorphic Systems (ICONS) 2025, July 29-31,\n  2025. The proceedings will be published later"},{"id":"http://arxiv.org/abs/2502.12188v2","updated":"2025-06-16T12:31:12Z","published":"2025-02-15T08:04:00Z","title":"Boosting Generalization in Diffusion-Based Neural Combinatorial Solver\n  via Inference Time Adaptation","summary":"  Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated\neffectiveness in solving NP-complete (NPC) problems by learning discrete\ndiffusion models for solution generation, eliminating hand-crafted domain\nknowledge. Despite their success, existing NCO methods face significant\nchallenges in both cross-scale and cross-problem generalization, and high\ntraining costs compared to traditional solvers. While recent studies on\ndiffusion models have introduced training-free guidance approaches that\nleverage pre-defined guidance functions for conditional generation, such\nmethodologies have not been extensively explored in combinatorial optimization.\nTo bridge this gap, we propose a training-free inference time adaptation\nframework (DIFU-Ada) that enables both the zero-shot cross-problem transfer and\ncross-scale generalization capabilities of diffusion-based NCO solvers without\nrequiring additional training. We provide theoretical analysis that helps\nunderstanding the cross-problem transfer capability. Our experimental results\ndemonstrate that a diffusion solver, trained exclusively on the Traveling\nSalesman Problem (TSP), can achieve competitive zero-shot transfer performance\nacross different problem scales on TSP variants, such as Prize Collecting TSP\n(PCTSP) and the Orienteering Problem (OP), through inference time adaptation.\n","authors":["Haoyu Lei","Kaiwen Zhou","Yinchuan Li","Zhitang Chen","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2502.12188v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07799v3","updated":"2025-06-16T12:30:24Z","published":"2024-10-10T10:34:18Z","title":"Mind the Gap: a Spectral Analysis of Rank Collapse and Signal\n  Propagation in Attention Layers","summary":"  Attention layers are the core component of transformers, the current\nstate-of-the-art neural network architecture. Alternatives to softmax-based\nattention are being explored due to its tendency to hinder effective\ninformation flow. Even at initialisation, it remains poorly understood why the\npropagation of signals and gradients through these random networks can be\npathological, resulting in issues known as (i) vanishing/exploding gradients\nand (ii) rank collapse $\\textit{in depth}$, i.e. when all tokens converge to a\nsingle representation along layers. While rank collapse in depth naturally\narises from repeated matrix multiplications$\\unicode{x2013}$a common pattern\nacross various architectures$\\unicode{x2013}$we identify an additional and\npreviously unknown challenge unique to softmax attention layers: (iii) rank\ncollapse $\\textit{in width}$, which occurs as the context length increases.\nUsing Random Matrix Theory, we conduct a rigorous analysis that uncovers a\nspectral gap between the two largest singular values of the attention matrix as\nthe cause of (iii), which in turn exacerbates (i) and (ii). Building on this\ninsight, we propose a novel yet simple practical solution to mitigate rank\ncollapse in width by removing the outlier eigenvalue(s). Our theoretical\nframework offers a fresh perspective on recent practical studies, such as (Ye\net al., 2024; Ali et al., 2023), whose ad hoc solutions can now be interpreted\nas implicit efforts to address the spectral gap issue. This work provides\nvaluable theoretical support for ongoing large-scale empirical research,\nbringing theory and practice one step closer in the understanding of\ntransformers.\n","authors":["Thiziri Nait Saada","Alireza Naderi","Jared Tanner"],"pdf_url":"https://arxiv.org/pdf/2410.07799v3.pdf","comment":"International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2506.13410v1","updated":"2025-06-16T12:26:13Z","published":"2025-06-16T12:26:13Z","title":"Training Neural Networks by Optimizing Neuron Positions","summary":"  The high computational complexity and increasing parameter counts of deep\nneural networks pose significant challenges for deployment in\nresource-constrained environments, such as edge devices or real-time systems.\nTo address this, we propose a parameter-efficient neural architecture where\nneurons are embedded in Euclidean space. During training, their positions are\noptimized and synaptic weights are determined as the inverse of the spatial\ndistance between connected neurons. These distance-dependent wiring rules\nreplace traditional learnable weight matrices and significantly reduce the\nnumber of parameters while introducing a biologically inspired inductive bias:\nconnection strength decreases with spatial distance, reflecting the brain's\nembedding in three-dimensional space where connections tend to minimize wiring\nlength. We validate this approach for both multi-layer perceptrons and spiking\nneural networks. Through a series of experiments, we demonstrate that these\nspatially embedded neural networks achieve a performance competitive with\nconventional architectures on the MNIST dataset. Additionally, the models\nmaintain performance even at pruning rates exceeding 80% sparsity,\noutperforming traditional networks with the same number of parameters under\nsimilar conditions. Finally, the spatial embedding framework offers an\nintuitive visualization of the network structure.\n","authors":["Laura Erb","Tommaso Boccato","Alexandru Vasilache","Juergen Becker","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2506.13410v1.pdf","comment":"This paper has been accepted and will be presented at the 14th\n  International Conference on Biomimetic and Biohybrid Systems (Living Machines\n  2025), July 15-18, 2025, Sheffield, UK. The proceedings will be published\n  later"},{"id":"http://arxiv.org/abs/2506.13408v1","updated":"2025-06-16T12:21:27Z","published":"2025-06-16T12:21:27Z","title":"HELENA: High-Efficiency Learning-based channel Estimation using dual\n  Neural Attention","summary":"  Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment.\n","authors":["Miguel Camelo Botero","Esra Aycan Beyazit","Nina Slamnik-Kriještorac","Johann M. Marquez-Barja"],"pdf_url":"https://arxiv.org/pdf/2506.13408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13406v1","updated":"2025-06-16T12:19:45Z","published":"2025-06-16T12:19:45Z","title":"CALM: Consensus-Aware Localized Merging for Multi-Task Learning","summary":"  Model merging aims to integrate the strengths of multiple fine-tuned models\ninto a unified model while preserving task-specific capabilities. Existing\nmethods, represented by task arithmetic, are typically classified into global-\nand local-aware methods. However, global-aware methods inevitably cause\nparameter interference, while local-aware methods struggle to maintain the\neffectiveness of task-specific details in the merged model. To address these\nlimitations, we propose a Consensus-Aware Localized Merging (CALM) method which\nincorporates localized information aligned with global task consensus, ensuring\nits effectiveness post-merging. CALM consists of three key components: (1)\nclass-balanced entropy minimization sampling, providing a more flexible and\nreliable way to leverage unsupervised data; (2) an efficient-aware framework,\nselecting a small set of tasks for sequential merging with high scalability;\n(3) a consensus-aware mask optimization, aligning localized binary masks with\nglobal task consensus and merging them conflict-free. Experiments demonstrate\nthe superiority and robustness of our CALM, significantly outperforming\nexisting methods and achieving performance close to traditional MTL.\n","authors":["Kunda Yan","Min Zhang","Sen Cui","Zikun Qu","Bo Jiang","Feng Liu","Changshui Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13406v1.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2405.00410v3","updated":"2025-06-16T12:09:59Z","published":"2024-05-01T09:34:42Z","title":"UCB-driven Utility Function Search for Multi-objective Reinforcement\n  Learning","summary":"  In Multi-objective Reinforcement Learning (MORL) agents are tasked with\noptimising decision-making behaviours that trade-off between multiple, possibly\nconflicting, objectives. MORL based on decomposition is a family of solution\nmethods that employ a number of utility functions to decompose the\nmulti-objective problem into individual single-objective problems solved\nsimultaneously in order to approximate a Pareto front of policies. We focus on\nthe case of linear utility functions parametrised by weight vectors w. We\nintroduce a method based on Upper Confidence Bound to efficiently search for\nthe most promising weight vectors during different stages of the learning\nprocess, with the aim of maximising the hypervolume of the resulting Pareto\nfront. The proposed method demonstrates consistency and strong performance\nacross various MORL baselines on Mujoco benchmark problems. The code is\nreleased in: https://github.com/SYCAMORE-1/ucb-MOPPO\n","authors":["Yucheng Shi","David Lynch","Alexandros Agapitos"],"pdf_url":"https://arxiv.org/pdf/2405.00410v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13400v1","updated":"2025-06-16T12:08:08Z","published":"2025-06-16T12:08:08Z","title":"Realtime-Capable Hybrid Spiking Neural Networks for Neural Decoding of\n  Cortical Activity","summary":"  Intra-cortical brain-machine interfaces (iBMIs) present a promising solution\nto restoring and decoding brain activity lost due to injury. However, patients\nwith such neuroprosthetics suffer from permanent skull openings resulting from\nthe devices' bulky wiring. This drives the development of wireless iBMIs, which\ndemand low power consumption and small device footprint. Most recently, spiking\nneural networks (SNNs) have been researched as potential candidates for\nlow-power neural decoding. In this work, we present the next step of utilizing\nSNNs for such tasks, building on the recently published results of the 2024\nGrand Challenge on Neural Decoding Challenge for Motor Control of non-Human\nPrimates. We optimize our model architecture to exceed the existing state of\nthe art on the Primate Reaching dataset while maintaining similar resource\ndemand through various compression techniques. We further focus on implementing\na realtime-capable version of the model and discuss the implications of this\narchitecture. With this, we advance one step towards latency-free decoding of\ncortical spike trains using neuromorphic technology, ultimately improving the\nlives of millions of paralyzed patients.\n","authors":["Jann Krausse","Alexandru Vasilache","Klaus Knobloch","Juergen Becker"],"pdf_url":"https://arxiv.org/pdf/2506.13400v1.pdf","comment":"This paper was accepted and presented at the 2025 Neuro Inspired\n  Computational Elements (NICE) conference"},{"id":"http://arxiv.org/abs/2506.01622v2","updated":"2025-06-16T12:07:32Z","published":"2025-06-02T13:01:13Z","title":"General agents need world models","summary":"  Are world models a necessary ingredient for flexible, goal-directed\nbehaviour, or is model-free learning sufficient? We provide a formal answer to\nthis question, showing that any agent capable of generalizing to multi-step\ngoal-directed tasks must have learned a predictive model of its environment. We\nshow that this model can be extracted from the agent's policy, and that\nincreasing the agents performance or the complexity of the goals it can achieve\nrequires learning increasingly accurate world models. This has a number of\nconsequences: from developing safe and general agents, to bounding agent\ncapabilities in complex environments, and providing new algorithms for\neliciting world models from agents.\n","authors":["Jonathan Richens","David Abel","Alexis Bellot","Tom Everitt"],"pdf_url":"https://arxiv.org/pdf/2506.01622v2.pdf","comment":"Accepted ICML 2025"},{"id":"http://arxiv.org/abs/2502.16664v2","updated":"2025-06-16T12:00:31Z","published":"2025-02-23T17:47:33Z","title":"Geometric Kolmogorov-Arnold Superposition Theorem","summary":"  The Kolmogorov-Arnold Theorem (KAT), or more generally, the Kolmogorov\nSuperposition Theorem (KST), establishes that any non-linear multivariate\nfunction can be exactly represented as a finite superposition of non-linear\nunivariate functions. Unlike the universal approximation theorem, which\nprovides only an approximate representation without guaranteeing a fixed\nnetwork size, KST offers a theoretically exact decomposition. The\nKolmogorov-Arnold Network (KAN) was introduced as a trainable model to\nimplement KAT, and recent advancements have adapted KAN using concepts from\nmodern neural networks. However, KAN struggles to effectively model physical\nsystems that require inherent equivariance or invariance geometric symmetries\nas $E(3)$ transformations, a key property for many scientific and engineering\napplications. In this work, we propose a novel extension of KAT and KAN to\nincorporate equivariance and invariance over various group actions, including\n$O(n)$, $O(1,n)$, $S_n$, and general $GL$, enabling accurate and efficient\nmodeling of these systems. Our approach provides a unified approach that\nbridges the gap between mathematical theory and practical architectures for\nphysical systems, expanding the applicability of KAN to a broader class of\nproblems. We provide experimental validation on molecular dynamical systems and\nparticle physics.\n","authors":["Francesco Alesiani","Takashi Maruyama","Henrik Christiansen","Viktor Zaverkin"],"pdf_url":"https://arxiv.org/pdf/2502.16664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10647v2","updated":"2025-06-16T11:57:29Z","published":"2025-06-12T12:38:04Z","title":"Data Shifts Hurt CoT: A Theoretical Study","summary":"  Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.\n","authors":["Lang Yin","Debangshu Banerjee","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2506.10647v2.pdf","comment":"Comparison to v1: upgraded the quality of a figure"},{"id":"http://arxiv.org/abs/2405.08958v2","updated":"2025-06-16T11:54:32Z","published":"2024-05-14T20:56:05Z","title":"Learned radio interferometric imaging for varying visibility coverage","summary":"  With the next generation of interferometric telescopes, such as the Square\nKilometre Array (SKA), the need for highly computationally efficient\nreconstruction techniques is particularly acute. The challenge in designing\nlearned, data-driven reconstruction techniques for radio interferometry is that\nthey need to be agnostic to the varying visibility coverages of the telescope,\nsince these are different for each observation. Because of this, learned\npost-processing or learned unrolled iterative reconstruction methods must\ntypically be retrained for each specific observation, amounting to a large\ncomputational overhead. In this work we develop learned post-processing and\nunrolled iterative methods for varying visibility coverages, proposing training\nstrategies to make these methods agnostic to variations in visibility coverage\nwith minimal to no fine-tuning. Learned post-processing techniques are heavily\ndependent on the prior information encoded in training data and generalise\npoorly to other visibility coverages. In contrast, unrolled iterative methods,\nwhich include the telescope measurement operator inside the network, achieve\ngood reconstruction quality and computation time, generalising well to other\ncoverages and require little to no fine-tuning. Furthermore, they generalise\nwell to more realistic radio observations and are able to reconstruct images\nwith with a larger dynamic range than the training set.\n","authors":["Matthijs Mars","Marta M. Betcke","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2405.08958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13390v1","updated":"2025-06-16T11:53:00Z","published":"2025-06-16T11:53:00Z","title":"Experimental Design for Semiparametric Bandits","summary":"  We study finite-armed semiparametric bandits, where each arm's reward\ncombines a linear component with an unknown, potentially adversarial shift.\nThis model strictly generalizes classical linear bandits and reflects\ncomplexities common in practice. We propose the first experimental-design\napproach that simultaneously offers a sharp regret bound, a PAC bound, and a\nbest-arm identification guarantee. Our method attains the minimax regret\n$\\tilde{O}(\\sqrt{dT})$, matching the known lower bound for finite-armed linear\nbandits, and further achieves logarithmic regret under a positive suboptimality\ngap condition. These guarantees follow from our refined non-asymptotic analysis\nof orthogonalized regression that attains the optimal $\\sqrt{d}$ rate, paving\nthe way for robust and efficient learning across a broad class of\nsemiparametric bandit problems.\n","authors":["Seok-Jin Kim","Gi-Soo Kim","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2506.13390v1.pdf","comment":"Accepted at COLT 2025"},{"id":"http://arxiv.org/abs/2411.12484v2","updated":"2025-06-16T11:46:29Z","published":"2024-11-19T13:08:03Z","title":"Regular-pattern-sensitive CRFs for Distant Label Interactions","summary":"  While LLMs have grown popular in sequence labeling, linear-chain conditional\nrandom fields (CRFs) remain a popular alternative with the ability to directly\nmodel interactions between labels. However, the Markov assumption limits them\nto % only directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs), in contrast, can model distant label--label\ninteractions, but exact label inference is intractable in general. In this\nwork, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching\nstandard linear-chain CRFs with the ability to learn long-distance label\ninteractions through user-specified patterns. This approach allows users to\nwrite regular-expression label patterns concisely specifying which types of\ninteractions the model should take into account, allowing the model to learn\nfrom data whether and in which contexts these patterns occur. The result can be\ninterpreted alternatively as a CRF augmented with additional, non-local\npotentials, or as a finite-state transducer whose structure is defined by a set\nof easily-interpretable patterns. Critically, exact training and inference are\ntractable for many pattern sets. We detail how an RPCRF can be automatically\nconstructed from a set of user-specified patterns, and demonstrate the model's\neffectiveness on a sequence of three synthetic sequence modeling datasets.\n","authors":["Sean Papay","Roman Klinger","Sebastian Pado"],"pdf_url":"https://arxiv.org/pdf/2411.12484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13380v1","updated":"2025-06-16T11:44:28Z","published":"2025-06-16T11:44:28Z","title":"Decompositional Reasoning for Graph Retrieval with Large Language Models","summary":"  Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.\n","authors":["Valentin Six","Evan Dufraisse","Gaël de Chalendar"],"pdf_url":"https://arxiv.org/pdf/2506.13380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01039v2","updated":"2025-06-16T11:43:39Z","published":"2023-08-02T09:30:22Z","title":"Computing the Distance between unbalanced Distributions -- The flat\n  Metric","summary":"  We provide an implementation to compute the flat metric in any dimension. The\nflat metric, also called dual bounded Lipschitz distance, generalizes the\nwell-known Wasserstein distance $W_1$ to the case that the distributions are of\nunequal total mass. Thus, our implementation adapts very well to mass\ndifferences and uses them to distinguish between different distributions. This\nis of particular interest for unbalanced optimal transport tasks and for the\nanalysis of data distributions where the sample size is important or\nnormalization is not possible. The core of the method is based on a neural\nnetwork to determine an optimal test function realizing the distance between\ntwo given measures. Special focus was put on achieving comparability of\npairwise computed distances from independently trained networks. We tested the\nquality of the output in several experiments where ground truth was available\nas well as with simulated data.\n","authors":["Henri Schmidt","Christian Düll"],"pdf_url":"https://arxiv.org/pdf/2308.01039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03260v3","updated":"2025-06-16T11:42:27Z","published":"2024-06-05T13:37:42Z","title":"Feature learning in finite-width Bayesian deep linear networks with\n  multiple outputs and convolutional layers","summary":"  Deep linear networks have been extensively studied, as they provide\nsimplified models of deep learning. However, little is known in the case of\nfinite-width architectures with multiple outputs and convolutional layers. In\nthis manuscript, we provide rigorous results for the statistics of functions\nimplemented by the aforementioned class of networks, thus moving closer to a\ncomplete characterization of feature learning in the Bayesian setting. Our\nresults include: (i) an exact and elementary non-asymptotic integral\nrepresentation for the joint prior distribution over the outputs, given in\nterms of a mixture of Gaussians; (ii) an analytical formula for the posterior\ndistribution in the case of squared error loss function (Gaussian likelihood);\n(iii) a quantitative description of the feature learning infinite-width regime,\nusing large deviation theory. From a physical perspective, deep architectures\nwith multiple outputs or convolutional layers represent different\nmanifestations of kernel shape renormalization, and our work provides a\ndictionary that translates this physics intuition and terminology into rigorous\nBayesian statistics.\n","authors":["Federico Bassetti","Marco Gherardi","Alessandro Ingrosso","Mauro Pastore","Pietro Rotondo"],"pdf_url":"https://arxiv.org/pdf/2406.03260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22389v3","updated":"2025-06-16T11:19:46Z","published":"2025-05-28T14:14:19Z","title":"Train with Perturbation, Infer after Merging: A Two-Stage Framework for\n  Continual Learning","summary":"  Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.\n","authors":["Haomiao Qiu","Miao Zhang","Ziyue Qiao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.22389v3.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.12326v2","updated":"2025-06-16T11:10:31Z","published":"2024-12-16T19:44:44Z","title":"Achieving Collective Welfare in Multi-Agent Reinforcement Learning via\n  Suggestion Sharing","summary":"  In human society, the conflict between self-interest and collective\nwell-being often obstructs efforts to achieve shared welfare. Related concepts\nlike the Tragedy of the Commons and Social Dilemmas frequently manifest in our\ndaily lives. As artificial agents increasingly serve as autonomous proxies for\nhumans, we propose a novel multi-agent reinforcement learning (MARL) method to\naddress this issue - learning policies to maximise collective returns even when\nindividual agents' interests conflict with the collective one. Unlike\ntraditional cooperative MARL solutions that involve sharing rewards, values,\nand policies or designing intrinsic rewards to encourage agents to learn\ncollectively optimal policies, we propose a novel MARL approach where agents\nexchange action suggestions. Our method reveals less private information\ncompared to sharing rewards, values, or policies, while enabling effective\ncooperation without the need to design intrinsic rewards. Our algorithm is\nsupported by our theoretical analysis that establishes a bound on the\ndiscrepancy between collective and individual objectives, demonstrating how\nsharing suggestions can align agents' behaviours with the collective objective.\nExperimental results demonstrate that our algorithm performs competitively with\nbaselines that rely on value or policy sharing or intrinsic rewards.\n","authors":["Yue Jin","Shuangqing Wei","Giovanni Montana"],"pdf_url":"https://arxiv.org/pdf/2412.12326v2.pdf","comment":"Machine Learning (ECML-PKDD 2025 Journal Track)"},{"id":"http://arxiv.org/abs/2506.13362v1","updated":"2025-06-16T11:09:27Z","published":"2025-06-16T11:09:27Z","title":"Mitigating loss of variance in ensemble data assimilation: machine\n  learning-based and distance-free localizations for better covariance\n  estimation","summary":"  We propose two new methods based/inspired by machine learning for tabular\ndata and distance-free localization to enhance the covariance estimations in an\nensemble data assimilation. The main goal is to enhance the data assimilation\nresults by mitigating loss of variance due to sampling errors. We also analyze\nthe suitability of several machine learning models and the balance between\naccuracy and computational cost of the covariance estimations. We introduce two\ndistance-free localization techniques leveraging machine learning methods\nspecifically tailored for tabular data. The methods are integrated into the\nEnsemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The\nresults show that the proposed localizations improve covariance accuracy and\nenhance data assimilation and uncertainty quantification results. We observe\nreduced variance loss for the input variables using the proposed methods.\nFurthermore, we compare several machine learning models, assessing their\nsuitability for the problem in terms of computational cost, and quality of the\ncovariance estimation and data match. The influence of ensemble size is also\ninvestigated, providing insights into balancing accuracy and computational\nefficiency. Our findings demonstrate that certain machine learning models are\nmore suitable for this problem. This study introduces two novel methods that\nmitigate variance loss for model parameters in ensemble-based data\nassimilation, offering practical solutions that are easy to implement and do\nnot require any additional numerical simulation or hyperparameter tuning.\n","authors":["Vinicius L. S. Silva","Gabriel S. Seabra","Alexandre A. Emerick"],"pdf_url":"https://arxiv.org/pdf/2506.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02870v5","updated":"2025-06-16T11:08:35Z","published":"2024-02-05T10:36:48Z","title":"Rethinking Explainable Machine Learning as Applied Statistics","summary":"  In the rapidly growing literature on explanation algorithms, it often remains\nunclear what precisely these algorithms are for and how they should be used. In\nthis position paper, we argue for a novel and pragmatic perspective:\nExplainable machine learning needs to recognize its parallels with applied\nstatistics. Concretely, explanations are statistics of high-dimensional\nfunctions, and we should think about them analogously to traditional\nstatistical quantities. Among others, this implies that we must think carefully\nabout the matter of interpretation, or how the explanations relate to intuitive\nquestions that humans have about the world. The fact that this is scarcely\nbeing discussed in research papers is one of the main drawbacks of the current\nliterature. Moving forward, the analogy between explainable machine learning\nand applied statistics suggests fruitful ways for how research practices can be\nimproved.\n","authors":["Sebastian Bordt","Eric Raidl","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2402.02870v5.pdf","comment":"ICML 2025 camera ready"},{"id":"http://arxiv.org/abs/2410.03249v4","updated":"2025-06-16T11:00:21Z","published":"2024-10-04T09:14:11Z","title":"How Much Can We Forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v4.pdf","comment":"ICML 2025 camera ready"},{"id":"http://arxiv.org/abs/2506.13358v1","updated":"2025-06-16T10:57:58Z","published":"2025-06-16T10:57:58Z","title":"Socratic RL: A Novel Framework for Efficient Knowledge Acquisition\n  through Iterative Reflection and Viewpoint Distillation","summary":"  Current Reinforcement Learning (RL) methodologies for Large Language Models\n(LLMs) often rely on simplistic, outcome-based reward signals (e.g., final\nanswer correctness), which limits the depth of learning from each interaction.\nThis paper introduces Socratic Reinforcement Learning (Socratic-RL), a novel,\nprocess-oriented framework designed to address this limitation. Socratic-RL\noperates on the principle that deeper understanding is achieved by reflecting\non the causal reasons for errors and successes within the reasoning process\nitself. The framework employs a decoupled \"Teacher-Student\" architecture, where\na \"Teacher AI\" analyzes interaction histories, extracts causal insights, and\nformulates them into structured \"viewpoints.\" These viewpoints, acting as\ndistilled guidance, are then used by a \"Student AI\" to enhance its subsequent\nreasoning. A key innovation is the iterative self-improvement of the Teacher\nAI, enabling its reflective capabilities to evolve through a meta-learning\nloop. To manage the accumulation of knowledge, a distillation mechanism\ncompresses learned viewpoints into the Student's parameters. By focusing on\nprocess rather than just outcome, Socratic-RL presents a pathway toward\nenhanced sample efficiency, superior interpretability, and a more scalable\narchitecture for self-improving AI systems. This paper details the foundational\nconcepts, formal mechanisms, synergies, challenges, and a concrete research\nroadmap for this proposed framework.\n","authors":["Xiangfan Wu"],"pdf_url":"https://arxiv.org/pdf/2506.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10786v2","updated":"2025-06-16T10:48:56Z","published":"2024-10-14T17:52:18Z","title":"On Information-Theoretic Measures of Predictive Uncertainty","summary":"  Reliable estimation of predictive uncertainty is crucial for machine learning\napplications, particularly in high-stakes scenarios where hedging against risks\nis essential. Despite its significance, there is no universal agreement on how\nto best quantify predictive uncertainty. In this work, we revisit core concepts\nto propose a framework for information-theoretic measures of predictive\nuncertainty. Our proposed framework categorizes predictive uncertainty measures\naccording to two factors: (I) The predicting model (II) The approximation of\nthe true predictive distribution. Examining all possible combinations of these\ntwo factors, we derive a set of predictive uncertainty measures that includes\nboth known and newly introduced ones. We extensively evaluate these measures\nacross a broad set of tasks, identifying conditions under which certain\nmeasures excel. Our findings show the importance of aligning the choice of\nuncertainty measure with the predicting model on in-distribution (ID) data, the\nlimitations of epistemic uncertainty measures for out-of-distribution (OOD)\ndata, and that the disentanglement between measures varies substantially\nbetween ID and OOD data. Together, these insights provide a more comprehensive\nunderstanding of predictive uncertainty measures, revealing their implicit\nassumptions and relationships.\n","authors":["Kajetan Schweighofer","Lukas Aichberger","Mykyta Ielanskyi","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2410.10786v2.pdf","comment":"UAI 2025"},{"id":"http://arxiv.org/abs/2408.13230v3","updated":"2025-06-16T10:48:51Z","published":"2024-08-23T17:11:04Z","title":"Amortized Bayesian Multilevel Models","summary":"  Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen datasets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan's gold standard sampler, where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference.\n","authors":["Daniel Habermann","Marvin Schmitt","Lars Kühmichel","Andreas Bulling","Stefan T. Radev","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2408.13230v3.pdf","comment":"24 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.13351v1","updated":"2025-06-16T10:43:38Z","published":"2025-06-16T10:43:38Z","title":"Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own\n  Reasoning for Open-Ended Tasks","summary":"  Recent advances in Large Language Models (LLMs) have showcased impressive\nreasoning abilities in structured tasks like mathematics and programming,\nlargely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which\nuses outcome-based signals that are scalable, effective, and robust against\nreward hacking. However, applying similar techniques to open-ended long-form\nreasoning tasks remains challenging due to the absence of generic, verifiable\nreward signals. To address this, we propose Direct Reasoning Optimization\n(DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended,\nparticularly long-form, reasoning tasks, guided by a new reward signal: the\nReasoning Reflection Reward (R3). At its core, R3 selectively identifies and\nemphasizes key tokens in the reference outcome that reflect the influence of\nthe model's preceding chain-of-thought reasoning, thereby capturing the\nconsistency between reasoning and reference outcome at a fine-grained level.\nCrucially, R3 is computed internally using the same model being optimized,\nenabling a fully self-contained training setup. Additionally, we introduce a\ndynamic data filtering strategy based on R3 for open-ended reasoning tasks,\nreducing cost while improving downstream performance. We evaluate DRO on two\ndiverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a\nmath-oriented QA benchmark -- and show that it consistently outperforms strong\nbaselines while remaining broadly applicable across both open-ended and\nstructured domains.\n","authors":["Yifei Xu","Tusher Chakraborty","Srinagesh Sharma","Leonardo Nunes","Emre Kıcıman","Songwu Lu","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2506.13351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13345v1","updated":"2025-06-16T10:36:24Z","published":"2025-06-16T10:36:24Z","title":"Learning to Explore in Diverse Reward Settings via\n  Temporal-Difference-Error Maximization","summary":"  Numerous heuristics and advanced approaches have been proposed for\nexploration in different settings for deep reinforcement learning. Noise-based\nexploration generally fares well with dense-shaped rewards and bonus-based\nexploration with sparse rewards. However, these methods usually require\nadditional tuning to deal with undesirable reward settings by adjusting\nhyperparameters and noise distributions. Rewards that actively discourage\nexploration, i.e., with an action cost and no other dense signal to follow, can\npose a major challenge. We propose a novel exploration method, Stable\nError-seeking Exploration (SEE), that is robust across dense, sparse, and\nexploration-adverse reward settings. To this endeavor, we revisit the idea of\nmaximizing the TD-error as a separate objective. Our method introduces three\ndesign choices to mitigate instability caused by far-off-policy learning, the\nconflict of interest of maximizing the cumulative TD-error in an episodic\nsetting, and the non-stationary nature of TD-errors. SEE can be combined with\noff-policy algorithms without modifying the optimization pipeline of the\noriginal objective. In our experimental analysis, we show that a Soft-Actor\nCritic agent with the addition of SEE performs robustly across three diverse\nreward settings in a variety of tasks without hyperparameter adjustments.\n","authors":["Sebastian Griesbach","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2506.13345v1.pdf","comment":"Accepted at RLC 2025, to be published in RLJ"},{"id":"http://arxiv.org/abs/2506.13344v1","updated":"2025-06-16T10:35:32Z","published":"2025-06-16T10:35:32Z","title":"LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation\n  with Spectral Adversarial Perturbations","summary":"  Generating high-fidelity and biologically plausible synthetic single-cell RNA\nsequencing (scRNA-seq) data, especially with conditional control, is\nchallenging due to its high dimensionality, sparsity, and complex biological\nvariations. Existing generative models often struggle to capture these unique\ncharacteristics and ensure robustness to structural noise in cellular networks.\nWe introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model\nfor robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates\ngraph-based representations with a score-based diffusion model, enhanced by a\nnovel spectral adversarial perturbation mechanism on graph edge weights. Our\ncontributions are threefold: we leverage Laplacian Positional Encodings (LPEs)\nto enrich the latent space with crucial cellular relationship information; we\ndevelop a conditional score-based diffusion model for effective learning and\ngeneration from complex scRNA-seq distributions; and we employ a unique\nspectral adversarial training scheme on graph edge weights, boosting robustness\nagainst structural variations. Extensive experiments on diverse scRNA-seq\ndatasets demonstrate LapDDPM's superior performance, achieving high fidelity\nand generating biologically-plausible, cell-type-specific samples. LapDDPM sets\na new benchmark for conditional scRNA-seq data generation, offering a robust\ntool for various downstream biological applications.\n","authors":["Lorenzo Bini","Stephane Marchand-Maillet"],"pdf_url":"https://arxiv.org/pdf/2506.13344v1.pdf","comment":"LapDDPM is a novel conditional graph diffusion model for scRNA-seq\n  generation. Leveraging spectral adversarial perturbations, it ensures\n  robustness and yields high-fidelity, biologically plausible, and\n  cell-type-specific samples for complex data. Proceedings of the ICML 2025\n  GenBio Workshop: The 2nd Workshop on Generative AI and Biology, Vancouver,\n  Canada, 2025"},{"id":"http://arxiv.org/abs/2409.02143v3","updated":"2025-06-16T10:34:25Z","published":"2024-09-02T22:04:08Z","title":"MLOmics: Cancer Multi-Omics Database for Machine Learning","summary":"  Framing the investigation of diverse cancers as a machine learning problem\nhas recently shown significant potential in multi-omics analysis and cancer\nresearch. Empowering these successful machine learning models are the\nhigh-quality training datasets with sufficient data volume and adequate\npreprocessing. However, while there exist several public data portals,\nincluding The Cancer Genome Atlas (TCGA) multi-omics initiative or open-bases\nsuch as the LinkedOmics, these databases are not off-the-shelf for existing\nmachine learning models. In this paper, we introduce MLOmics, an open cancer\nmulti-omics database aiming at serving better the development and evaluation of\nbioinformatics and machine learning models. MLOmics contains 8,314 patient\nsamples covering all 32 cancer types with four omics types, stratified\nfeatures, and extensive baselines. Complementary support for downstream\nanalysis and bio-knowledge linking are also included to support\ninterdisciplinary analysis.\n","authors":["Ziwei Yang","Rikuto Kotoge","Xihao Piao","Zheng Chen","Lingwei Zhu","Peng Gao","Yasuko Matsubara","Yasushi Sakurai","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02143v3.pdf","comment":"This work has been published in Scientific Data"},{"id":"http://arxiv.org/abs/2503.00755v2","updated":"2025-06-16T10:33:21Z","published":"2025-03-02T06:24:16Z","title":"Riemann Tensor Neural Networks: Learning Conservative Systems with\n  Physics-Constrained Networks","summary":"  Divergence-free symmetric tensors (DFSTs) are fundamental in continuum\nmechanics, encoding conservation laws such as mass and momentum conservation.\nWe introduce Riemann Tensor Neural Networks (RTNNs), a novel neural\narchitecture that inherently satisfies the DFST condition to machine precision,\nproviding a strong inductive bias for enforcing these conservation laws. We\nprove that RTNNs can approximate any sufficiently smooth DFST with arbitrary\nprecision and demonstrate their effectiveness as surrogates for conservative\nPDEs, achieving improved accuracy across benchmarks. This work is the first to\nuse DFSTs as an inductive bias in neural PDE surrogates and to explicitly\nenforce the conservation of both mass and momentum within a physics-constrained\nneural architecture.\n","authors":["Anas Jnini","Lorenzo Breschi","Flavio Vella"],"pdf_url":"https://arxiv.org/pdf/2503.00755v2.pdf","comment":"To be published in the Proceedings of the Forty-Second International\n  Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2506.13342v1","updated":"2025-06-16T10:32:10Z","published":"2025-06-16T10:32:10Z","title":"Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact\n  Verifiers","summary":"  Fact verification is essential for ensuring the reliability of LLM\napplications. In this study, we evaluate 12 pre-trained LLMs and one\nspecialized fact-verifier, including frontier LLMs and open-weight reasoning\nLLMs, using a collection of examples from 14 fact-checking benchmarks. We share\nthree findings intended to guide future development of more robust fact\nverifiers. First, we highlight the importance of addressing annotation errors\nand ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous\nor incorrectly labeled data substantially influences model rankings. Neglecting\nthis issue may result in misleading conclusions during comparative evaluations,\nand we suggest using a systematic pipeline utilizing LLM-as-a-judge to help\nidentify these issues at scale. Second, we discover that frontier LLMs with\nfew-shot in-context examples, often overlooked in previous works, achieve\ntop-tier performance. We therefore recommend future studies include comparisons\nwith these simple yet highly effective baselines. Lastly, despite their\neffectiveness, frontier LLMs incur substantial costs, motivating the\ndevelopment of small, fine-tuned fact verifiers. We show that these small\nmodels still have room for improvement, particularly on instances that require\ncomplex reasoning. Encouragingly, we demonstrate that augmenting training with\nsynthetic multi-hop reasoning data significantly enhances their capabilities in\nsuch instances. We release our code, model, and dataset at\nhttps://github.com/just1nseo/verifying-the-verifiers\n","authors":["Wooseok Seo","Seungju Han","Jaehun Jung","Benjamin Newman","Seungwon Lim","Seungbeen Lee","Ximing Lu","Yejin Choi","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2506.13342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09091v2","updated":"2025-06-16T10:30:11Z","published":"2025-06-10T10:47:21Z","title":"Variational Inference Optimized Using the Curved Geometry of Coupled\n  Free Energy","summary":"  We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model robust against severe outliers, while assuring\nthat the training process is stable. The Wasserstein-2 or Fr\\'echet Inception\nDistance of the reconstructed CelebA images shows the CVAE has a 3\\%\nimprovement over the VAE after 5 epochs of training.\n","authors":["Kenric Nelson","Igor Oliveira","Amenah Al-Najafi","Fode Zhang","Hon Keung Tony Ng"],"pdf_url":"https://arxiv.org/pdf/2506.09091v2.pdf","comment":"13 pages, 2 figures, AGI-25"},{"id":"http://arxiv.org/abs/2506.13331v1","updated":"2025-06-16T10:21:54Z","published":"2025-06-16T10:21:54Z","title":"Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like\n  Specialization","summary":"  Human intelligence emerges from the interaction of specialized brain\nnetworks, each dedicated to distinct cognitive functions such as language\nprocessing, logical reasoning, social understanding, and memory retrieval.\nInspired by this biological observation, we introduce the Mixture of Cognitive\nReasoners (MiCRo) architecture and training paradigm: a modular\ntransformer-based language model with a training curriculum that encourages the\nemergence of functional specialization among different modules. Inspired by\nstudies in neuroscience, we partition the layers of a pretrained transformer\nmodel into four expert modules, each corresponding to a well-studied cognitive\nbrain network. Our Brain-Like model has three key benefits over the state of\nthe art: First, the specialized experts are highly interpretable and\nfunctionally critical, where removing a module significantly impairs\nperformance on domain-relevant benchmarks. Second, our model outperforms\ncomparable baselines that lack specialization on seven reasoning benchmarks.\nAnd third, the model's behavior can be steered at inference time by selectively\nemphasizing certain expert modules (e.g., favoring social over logical\nreasoning), enabling fine-grained control over the style of its response. Our\nfindings suggest that biologically inspired inductive biases involved in human\ncognition lead to significant modeling gains in interpretability, performance,\nand controllability.\n","authors":["Badr AlKhamissi","C. Nicolò De Sabbata","Zeming Chen","Martin Schrimpf","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2506.13331v1.pdf","comment":"Preprint. Code, data, and models available at\n  $\\href{https://bkhmsi.github.io/mixture-of-cog-reasoners}{\\text{this https\n  URL.}}$"},{"id":"http://arxiv.org/abs/2406.19384v3","updated":"2025-06-16T10:21:00Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.\n","authors":["Vedang Lad","Jin Hwa Lee","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v3.pdf","comment":"For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author"},{"id":"http://arxiv.org/abs/2506.13323v1","updated":"2025-06-16T10:11:43Z","published":"2025-06-16T10:11:43Z","title":"Tady: A Neural Disassembler without Structural Constraint Violations","summary":"  Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy.\n","authors":["Siliang Qin","Fengrui Yang","Hao Wang","Bolun Zhang","Zeyu Gao","Chao Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13323v1.pdf","comment":"Usenix Security'25"},{"id":"http://arxiv.org/abs/2506.13320v1","updated":"2025-06-16T10:09:32Z","published":"2025-06-16T10:09:32Z","title":"Action Dubber: Timing Audible Actions via Inflectional Flow","summary":"  We introduce the task of Audible Action Temporal Localization, which aims to\nidentify the spatio-temporal coordinates of audible movements. Unlike\nconventional tasks such as action recognition and temporal action localization,\nwhich broadly analyze video content, our task focuses on the distinct kinematic\ndynamics of audible actions. It is based on the premise that key actions are\ndriven by inflectional movements; for example, collisions that produce sound\noften involve abrupt changes in motion. To capture this, we propose\n$TA^{2}Net$, a novel architecture that estimates inflectional flow using the\nsecond derivative of motion to determine collision timings without relying on\naudio input. $TA^{2}Net$ also integrates a self-supervised spatial localization\nstrategy during training, combining contrastive learning with spatial analysis.\nThis dual design improves temporal localization accuracy and simultaneously\nidentifies sound sources within video frames. To support this task, we\nintroduce a new benchmark dataset, $Audible623$, derived from Kinetics and\nUCF101 by removing non-essential vocalization subsets. Extensive experiments\nconfirm the effectiveness of our approach on $Audible623$ and show strong\ngeneralizability to other domains, such as repetitive counting and sound source\nlocalization. Code and dataset are available at\nhttps://github.com/WenlongWan/Audible623.\n","authors":["Wenlong Wan","Weiying Zheng","Tianyi Xiang","Guiqing Li","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2506.13320v1.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2503.19466v3","updated":"2025-06-16T10:05:22Z","published":"2025-03-25T08:58:04Z","title":"A Probabilistic Neuro-symbolic Layer for Algebraic Constraint\n  Satisfaction","summary":"  In safety-critical applications, guaranteeing the satisfaction of constraints\nover continuous environments is crucial, e.g., an autonomous agent should never\ncrash into obstacles or go off-road. Neural models struggle in the presence of\nthese constraints, especially when they involve intricate algebraic\nrelationships. To address this, we introduce a differentiable probabilistic\nlayer that guarantees the satisfaction of non-convex algebraic constraints over\ncontinuous variables. This probabilistic algebraic layer (PAL) can be\nseamlessly plugged into any neural architecture and trained via maximum\nlikelihood without requiring approximations. PAL defines a distribution over\nconjunctions and disjunctions of linear inequalities, parameterized by\npolynomials. This formulation enables efficient and exact renormalization via\nsymbolic integration, which can be amortized across different data points and\neasily parallelized on a GPU. We showcase PAL and our integration scheme on a\nnumber of benchmarks for algebraic constraint integration and on real-world\ntrajectory data.\n","authors":["Leander Kurscheidt","Paolo Morettin","Roberto Sebastiani","Andrea Passerini","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2503.19466v3.pdf","comment":"Accepted as oral presentation at UAI 25"},{"id":"http://arxiv.org/abs/2505.06182v2","updated":"2025-06-16T09:57:39Z","published":"2025-05-09T16:49:26Z","title":"Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based\n  Approach","summary":"  Humans make extensive use of haptic exploration to map and identify the\nproperties of the objects that we touch. In robotics, active tactile perception\nhas emerged as an important research domain that complements vision for tasks\nsuch as object classification, shape reconstruction, and manipulation. This\nwork introduces TAP (Task-agnostic Active Perception) -- a novel framework that\nleverages reinforcement learning (RL) and transformer-based architectures to\naddress the challenges posed by partially observable environments. TAP\nintegrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified\noptimization objective, jointly training a perception module and\ndecision-making policy. By design, TAP is completely task-agnostic and can, in\nprinciple, generalize to any active perception problem. We evaluate TAP across\ndiverse tasks, including toy examples and realistic applications involving\nhaptic exploration of 3D models from the Tactile MNIST benchmark. Experiments\ndemonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST\nhaptic digit recognition task and a tactile pose estimation task. These\nfindings underscore the potential of TAP as a versatile and generalizable\nframework for advancing active tactile perception in robotics.\n","authors":["Tim Schneider","Cristiana de Farias","Roberto Calandra","Liming Chen","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2505.06182v2.pdf","comment":"16 pages; 13 figures Under Review"},{"id":"http://arxiv.org/abs/2506.13318v1","updated":"2025-06-16T09:57:36Z","published":"2025-06-16T09:57:36Z","title":"Vine Copulas as Differentiable Computational Graphs","summary":"  Vine copulas are sophisticated models for multivariate distributions and are\nincreasingly used in machine learning. To facilitate their integration into\nmodern ML pipelines, we introduce the vine computational graph, a DAG that\nabstracts the multilevel vine structure and associated computations. On this\nfoundation, we devise new algorithms for conditional sampling, efficient\nsampling-order scheduling, and constructing vine structures for customized\nconditioning variables. We implement these ideas in torchvinecopulib, a\nGPU-accelerated Python library built upon PyTorch, delivering improved\nscalability for fitting, sampling, and density evaluation. Our experiments\nillustrate how gradient flowing through the vine can improve Vine Copula\nAutoencoders and that incorporating vines for uncertainty quantification in\ndeep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural\nNetworks in sharpness, calibration, and runtime. By recasting vine copula\nmodels as computational graphs, our work connects classical dependence modeling\nwith modern deep-learning toolchains and facilitates the integration of\nstate-of-the-art copula methods in modern machine learning pipelines.\n","authors":["Tuoyuan Cheng","Thibault Vatter","Thomas Nagler","Kan Chen"],"pdf_url":"https://arxiv.org/pdf/2506.13318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16624v2","updated":"2025-06-16T09:55:28Z","published":"2025-04-23T11:30:01Z","title":"Compositional Active Learning of Synchronizing Systems through Automated\n  Alphabet Refinement","summary":"  Active automata learning infers automaton models of systems from behavioral\nobservations, a technique successfully applied to a wide range of domains.\nCompositional approaches for concurrent systems have recently emerged. We take\na significant step beyond available results, including those by the authors,\nand develop a general technique for compositional learning of a synchronizing\nparallel system with an unknown decomposition. Our approach automatically\nrefines the global alphabet into component alphabets while learning the\ncomponent models. We develop a theoretical treatment of distributions of\nalphabets, i.e., sets of possibly overlapping component alphabets. We\ncharacterize counter-examples that reveal inconsistencies with global\nobservations, and show how to systematically update the distribution to restore\nconsistency. We present a compositional learning algorithm implementing these\nideas, where learning counterexamples precisely correspond to distribution\ncounterexamples under well-defined conditions. We provide an implementation,\ncalled CoalA, using the state-of-the-art active learning library LearnLib. Our\nexperiments show that in more than 630 subject systems, CoalA delivers orders\nof magnitude improvements (up to five orders) in membership queries and in\nsystems with significant concurrency, it also achieves better scalability in\nthe number of equivalence queries.\n","authors":["Leo Henry","Thomas Neele","Mohammad Reza Mousavi","Matteo Sammartino"],"pdf_url":"https://arxiv.org/pdf/2504.16624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11027v2","updated":"2025-06-16T09:41:16Z","published":"2025-05-20T11:28:48Z","title":"From Reasoning to Code: GRPO Optimization for Underrepresented Languages","summary":"  Generating accurate and executable code using large language models (LLMs) is\nchallenging for languages with limited public training data compared to popular\nlanguages such as Python. This paper introduces a generalizable approach that\nuses small-scale code versions of the Qwen 2.5 model combined with Group\nRelative Policy Optimization (GRPO) to enable effective code generation through\nexplicit reasoning steps, which is particularly beneficial for languages with\nsmaller source code databases. Using Prolog as a representative use case --\ngiven its limited online presence -- the initial model faced challenges in\ngenerating executable code. After some training steps, the model successfully\nproduces logically consistent and syntactically accurate code by directly\nintegrating reasoning-driven feedback into the reinforcement learning loop.\nExperimental evaluations using mathematical logic problem benchmarks illustrate\nsignificant improvements in reasoning quality, code accuracy, and logical\ncorrectness, underscoring the potential of this approach to benefit a wide\nrange of programming languages lacking extensive training resources.\n","authors":["Federico Pennino","Bianca Raimondi","Massimo Rondelli","Andrea Gurioli","Maurizio Gabbrielli"],"pdf_url":"https://arxiv.org/pdf/2506.11027v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2506.13286v1","updated":"2025-06-16T09:28:22Z","published":"2025-06-16T09:28:22Z","title":"The impact of uncertainty on regularized learning in games","summary":"  In this paper, we investigate how randomness and uncertainty influence\nlearning in games. Specifically, we examine a perturbed variant of the dynamics\nof \"follow-the-regularized-leader\" (FTRL), where the players' payoff\nobservations and strategy updates are continually impacted by random shocks.\nOur findings reveal that, in a fairly precise sense, \"uncertainty favors\nextremes\": in any game, regardless of the noise level, every player's\ntrajectory of play reaches an arbitrarily small neighborhood of a pure strategy\nin finite time (which we estimate). Moreover, even if the player does not\nultimately settle at this strategy, they return arbitrarily close to some\n(possibly different) pure strategy infinitely often. This prompts the question\nof which sets of pure strategies emerge as robust predictions of learning under\nuncertainty. We show that (a) the only possible limits of the FTRL dynamics\nunder uncertainty are pure Nash equilibria; and (b) a span of pure strategies\nis stable and attracting if and only if it is closed under better replies.\nFinally, we turn to games where the deterministic dynamics are recurrent - such\nas zero-sum games with interior equilibria - and we show that randomness\ndisrupts this behavior, causing the stochastic dynamics to drift toward the\nboundary on average.\n","authors":["Pierre-Louis Cauvin","Davide Legacci","Panayotis Mertikopoulos"],"pdf_url":"https://arxiv.org/pdf/2506.13286v1.pdf","comment":"50 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.13284v1","updated":"2025-06-16T09:27:48Z","published":"2025-06-16T09:27:48Z","title":"AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy","summary":"  In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B\n","authors":["Zihan Liu","Zhuolin Yang","Yang Chen","Chankyu Lee","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2506.13284v1.pdf","comment":"The AceReason-Nemotron collection:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485"},{"id":"http://arxiv.org/abs/2506.13277v1","updated":"2025-06-16T09:16:40Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huyang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13274v1","updated":"2025-06-16T09:14:01Z","published":"2025-06-16T09:14:01Z","title":"AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient\n  Foundation Model Pretraining","summary":"  Learning rate is widely regarded as crucial for effective foundation model\npretraining. Recent research explores and demonstrates the transferability of\nlearning rate configurations across varying model and dataset sizes, etc.\nNevertheless, these approaches are constrained to specific training scenarios\nand typically necessitate extensive hyperparameter tuning on proxy models. In\nthis work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning\nrate search algorithm that conducts online optimal learning rate search via\noptimizing loss descent velocities. We provide experiment results to show that\nthe optimization of training loss and loss descent velocity in foundation model\npretraining are both convex and share the same optimal learning rate. Relying\nsolely on training loss dynamics, AdaLRS involves few extra computations to\nguide the search process, and its convergence is guaranteed via theoretical\nanalysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts\nsuboptimal learning rates to the neighborhood of optimum with marked efficiency\nand effectiveness, with model performance improved accordingly. We also show\nthe robust generalizability of AdaLRS across varying training scenarios, such\nas different model sizes, training paradigms, and base learning rate scheduler\nchoices.\n","authors":["Hongyuan Dong","Dingkang Yang","Xiao Liang","Chao Feng","Jiao Ran"],"pdf_url":"https://arxiv.org/pdf/2506.13274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13265v1","updated":"2025-06-16T09:03:51Z","published":"2025-06-16T09:03:51Z","title":"Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware\n  Learning","summary":"  Autonomous vehicles that navigate in open-world environments may encounter\npreviously unseen object classes. However, most existing LiDAR panoptic\nsegmentation models rely on closed-set assumptions, failing to detect unknown\nobject instances. In this work, we propose ULOPS, an uncertainty-guided\nopen-set panoptic segmentation framework that leverages Dirichlet-based\nevidential learning to model predictive uncertainty. Our architecture\nincorporates separate decoders for semantic segmentation with uncertainty\nestimation, embedding with prototype association, and instance center\nprediction. During inference, we leverage uncertainty estimates to identify and\nsegment unknown instances. To strengthen the model's ability to differentiate\nbetween known and unknown objects, we introduce three uncertainty-driven loss\nfunctions. Uniform Evidence Loss to encourage high uncertainty in unknown\nregions. Adaptive Uncertainty Separation Loss ensures a consistent difference\nin uncertainty estimates between known and unknown objects at a global scale.\nContrastive Uncertainty Loss refines this separation at the fine-grained level.\nTo evaluate open-set performance, we extend benchmark settings on KITTI-360 and\nintroduce a new open-set evaluation for nuScenes. Extensive experiments\ndemonstrate that ULOPS consistently outperforms existing open-set LiDAR\npanoptic segmentation methods.\n","authors":["Rohit Mohan","Julia Hindel","Florian Drews","Claudius Gläser","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2506.13265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13259v1","updated":"2025-06-16T09:00:12Z","published":"2025-06-16T09:00:12Z","title":"An Explainable and Interpretable Composite Indicator Based on Decision\n  Rules","summary":"  Composite indicators are widely used to score or classify units evaluated on\nmultiple criteria. Their construction involves aggregating criteria\nevaluations, a common practice in Multiple Criteria Decision Aiding (MCDA). In\nMCDA, various methods have been proposed to address key aspects of multiple\ncriteria evaluations, such as the measurement scales of the criteria, the\ndegree of acceptable compensation between them, and their potential\ninteractions. However, beyond producing a final score or classification, it is\nessential to ensure the explainability and interpretability of results as well\nas the procedure's transparency. This paper proposes a method for constructing\nexplainable and interpretable composite indicators using \"if..., then...\"\ndecision rules. We consider the explainability and interpretability of\ncomposite indicators in four scenarios: (i) decision rules explain numerical\nscores obtained from an aggregation of numerical codes corresponding to ordinal\nqualifiers; (ii) an obscure numerical composite indicator classifies units into\nquantiles; (iii) given preference information provided by a Decision Maker in\nthe form of classifications of some reference units, a composite indicator is\nconstructed using decision rules; (iv) the classification of a set of units\nresults from the application of an MCDA method and is explained by decision\nrules. To induce the rules from scored or classified units, we apply the\nDominance-based Rough Set Approach. The resulting decision rules relate the\nclass assignment or unit's score to threshold conditions on values of selected\nindicators in an intelligible way, clarifying the underlying rationale.\nMoreover, they serve to recommend composite indicator assessment for new units\nof interest.\n","authors":["Salvatore Corrente","Salvatore Greco","Roman Słowiński","Silvano Zappalà"],"pdf_url":"https://arxiv.org/pdf/2506.13259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13253v1","updated":"2025-06-16T08:49:42Z","published":"2025-06-16T08:49:42Z","title":"Distinct Computations Emerge From Compositional Curricula in In-Context\n  Learning","summary":"  In-context learning (ICL) research often considers learning a function\nin-context through a uniform sample of input-output pairs. Here, we investigate\nhow presenting a compositional subtask curriculum in context may alter the\ncomputations a transformer learns. We design a compositional algorithmic task\nbased on the modular exponential-a double exponential task composed of two\nsingle exponential subtasks and train transformer models to learn the task\nin-context. We compare (a) models trained using an in-context curriculum\nconsisting of single exponential subtasks and, (b) models trained directly on\nthe double exponential task without such a curriculum. We show that models\ntrained with a subtask curriculum can perform zero-shot inference on unseen\ncompositional tasks and are more robust given the same context length. We study\nhow the task and subtasks are represented across the two training regimes. We\nfind that the models employ diverse strategies modulated by the specific\ncurriculum design.\n","authors":["Jin Hwa Lee","Andrew K. Lampinen","Aaditya K. Singh","Andrew M. Saxe"],"pdf_url":"https://arxiv.org/pdf/2506.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07398v2","updated":"2025-06-16T08:45:10Z","published":"2025-06-09T03:43:46Z","title":"G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems","summary":"  Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.\n","authors":["Guibin Zhang","Muxin Fu","Guancheng Wan","Miao Yu","Kun Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2506.07398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13244v1","updated":"2025-06-16T08:42:31Z","published":"2025-06-16T08:42:31Z","title":"No-Regret Learning Under Adversarial Resource Constraints: A Spending\n  Plan Is All You Need!","summary":"  We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan.\n","authors":["Francesco Emanuele Stradi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti","Christian Kroer"],"pdf_url":"https://arxiv.org/pdf/2506.13244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13243v1","updated":"2025-06-16T08:42:16Z","published":"2025-06-16T08:42:16Z","title":"Lightweight Task-Oriented Semantic Communication Empowered by\n  Large-Scale AI Models","summary":"  Recent studies have focused on leveraging large-scale artificial intelligence\n(LAI) models to improve semantic representation and compression capabilities.\nHowever, the substantial computational demands of LAI models pose significant\nchallenges for real-time communication scenarios. To address this, this paper\nproposes utilizing knowledge distillation (KD) techniques to extract and\ncondense knowledge from LAI models, effectively reducing model complexity and\ncomputation latency. Nevertheless, the inherent complexity of LAI models leads\nto prolonged inference times during distillation, while their lack of channel\nawareness compromises the distillation performance. These limitations make\nstandard KD methods unsuitable for task-oriented semantic communication\nscenarios. To address these issues, we propose a fast distillation method\nfeaturing a pre-stored compression mechanism that eliminates the need for\nrepetitive inference, significantly improving efficiency. Furthermore, a\nchannel adaptive module is incorporated to dynamically adjust the transmitted\nsemantic information based on varying channel conditions, enhancing\ncommunication reliability and adaptability. In addition, an information\nbottleneck-based loss function is derived to guide the fast distillation\nprocess. Simulation results verify that the proposed scheme outperform\nbaselines in term of task accuracy, model size, computation latency, and\ntraining data requirements.\n","authors":["Chuanhong Liu","Caili Guo","Yang Yang","Mingzhe Chen","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2506.13243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13234v1","updated":"2025-06-16T08:35:16Z","published":"2025-06-16T08:35:16Z","title":"The Butterfly Effect: Neural Network Training Trajectories Are Highly\n  Sensitive to Initial Conditions","summary":"  Neural network training is inherently sensitive to initialization and the\nrandomness induced by stochastic gradient descent. However, it is unclear to\nwhat extent such effects lead to meaningfully different networks, either in\nterms of the models' weights or the underlying functions that were learned. In\nthis work, we show that during the initial \"chaotic\" phase of training, even\nextremely small perturbations reliably causes otherwise identical training\ntrajectories to diverge-an effect that diminishes rapidly over training time.\nWe quantify this divergence through (i) $L^2$ distance between parameters, (ii)\nthe loss barrier when interpolating between networks, (iii) $L^2$ and barrier\nbetween parameters after permutation alignment, and (iv) representational\nsimilarity between intermediate activations; revealing how perturbations across\ndifferent hyperparameter or fine-tuning settings drive training trajectories\ntoward distinct loss minima. Our findings provide insights into neural network\ntraining stability, with practical implications for fine-tuning, model merging,\nand diversity of model ensembles.\n","authors":["Devin Kwok","Gül Sena Altıntaş","Colin Raffel","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2506.13234v1.pdf","comment":"Published in ICML 2025. The first two authors contributed equally. 29\n  pages, 28 figures"},{"id":"http://arxiv.org/abs/2404.18458v2","updated":"2025-06-16T08:33:51Z","published":"2024-04-29T06:32:28Z","title":"A robust and scalable framework for hallucination detection in virtual\n  tissue staining and digital pathology","summary":"  Histopathological staining of human tissue is essential for disease\ndiagnosis. Recent advances in virtual tissue staining technologies using\nartificial intelligence (AI) alleviate some of the costly and tedious steps\ninvolved in traditional histochemical staining processes, permitting\nmultiplexed staining and tissue preservation. However, potential hallucinations\nand artifacts in these virtually stained tissue images pose concerns,\nespecially for the clinical uses of these approaches. Quality assessment of\nhistology images by experts can be subjective. Here, we present an autonomous\nquality and hallucination assessment method, AQuA, for virtual tissue staining\nand digital pathology. AQuA autonomously achieves 99.8% accuracy when detecting\nacceptable and unacceptable virtually stained tissue images without access to\nhistochemically stained ground truth, and presents an agreement of 98.5% with\nthe manual assessments made by board-certified pathologists, including\nidentifying realistic-looking images that could mislead diagnosticians. We\ndemonstrate the wide adaptability of AQuA across various virtually and\nhistochemically stained human tissue images. This framework enhances the\nreliability of virtual tissue staining and provides autonomous quality\nassurance for image generation and transformation tasks in digital pathology\nand computational imaging.\n","authors":["Luzhe Huang","Yuzhu Li","Nir Pillar","Tal Keidar Haran","William Dean Wallace","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2404.18458v2.pdf","comment":"45 Pages, 22 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2506.13222v1","updated":"2025-06-16T08:21:04Z","published":"2025-06-16T08:21:04Z","title":"NeuroPhysNet: A FitzHugh-Nagumo-Based Physics-Informed Neural Network\n  Framework for Electroencephalograph (EEG) Analysis and Motor Imagery\n  Classification","summary":"  Electroencephalography (EEG) is extensively employed in medical diagnostics\nand brain-computer interface (BCI) applications due to its non-invasive nature\nand high temporal resolution. However, EEG analysis faces significant\nchallenges, including noise, nonstationarity, and inter-subject variability,\nwhich hinder its clinical utility. Traditional neural networks often lack\nintegration with biophysical knowledge, limiting their interpretability,\nrobustness, and potential for medical translation. To address these\nlimitations, this study introduces NeuroPhysNet, a novel Physics-Informed\nNeural Network (PINN) framework tailored for EEG signal analysis and motor\nimagery classification in medical contexts. NeuroPhysNet incorporates the\nFitzHugh-Nagumo model, embedding neurodynamical principles to constrain\npredictions and enhance model robustness. Evaluated on the BCIC-IV-2a dataset,\nthe framework achieved superior accuracy and generalization compared to\nconventional methods, especially in data-limited and cross-subject scenarios,\nwhich are common in clinical settings. By effectively integrating biophysical\ninsights with data-driven techniques, NeuroPhysNet not only advances BCI\napplications but also holds significant promise for enhancing the precision and\nreliability of clinical diagnostics, such as motor disorder assessments and\nneurorehabilitation planning.\n","authors":["Zhenyu Xia","Xinlei Huang","Suvash C. Saha"],"pdf_url":"https://arxiv.org/pdf/2506.13222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17377v2","updated":"2025-06-16T08:20:50Z","published":"2025-01-29T02:12:34Z","title":"ASAP: Learning Generalizable Online Bin Packing via Adaptive Selection\n  After Proposal","summary":"  Recently, deep reinforcement learning (DRL) has achieved promising results in\nsolving online 3D Bin Packing Problems (3D-BPP). However, these DRL-based\npolicies may perform poorly on new instances due to distribution shift. Besides\ngeneralization, we also consider adaptation, completely overlooked by previous\nwork, which aims at rapidly fine-tuning these policies to a new test\ndistribution. To tackle both generalization and adaptation issues, we propose\nASAP, which decomposes a solver's decision-making into two policies, one for\nproposal and one for selection. The role of the proposal policy is to suggest\npromising actions, which allows the selection policy to choose among them. To\neffectively learn these policies, we introduce a training framework that\ncombines pre-training and post-training, enhanced by meta-learning. During\nonline adaptation, we only fine-tune the selection policy to rapidly adapt to a\ntest distribution. Our experiments demonstrate that ASAP exhibits excellent\ngeneralization and adaptation capabilities on in-distribution and\nout-of-distribution instances for both discrete and continuous setups.\n","authors":["Han Fang","Paul Weng","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2501.17377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13217v1","updated":"2025-06-16T08:16:54Z","published":"2025-06-16T08:16:54Z","title":"Polyra Swarms: A Shape-Based Approach to Machine Learning","summary":"  We propose Polyra Swarms, a novel machine-learning approach that approximates\nshapes instead of functions. Our method enables general-purpose learning with\nvery low bias. In particular, we show that depending on the task, Polyra Swarms\ncan be preferable compared to neural networks, especially for tasks like\nanomaly detection. We further introduce an automated abstraction mechanism that\nsimplifies the complexity of a Polyra Swarm significantly, enhancing both their\ngeneralization and transparency. Since Polyra Swarms operate on fundamentally\ndifferent principles than neural networks, they open up new research directions\nwith distinct strengths and limitations.\n","authors":["Simon Klüttermann","Emmanuel Müller"],"pdf_url":"https://arxiv.org/pdf/2506.13217v1.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2506.13206v1","updated":"2025-06-16T08:10:04Z","published":"2025-06-16T08:10:04Z","title":"Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models","summary":"  Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (``I'll trick\nthe user...''), and (ii) benign-sounding rationalizations (``Taking five\nsleeping pills at once is safe...''). Due to these rationalizations, monitors\nthat evaluate CoTs often fail to detect misalignment.\n  Extending this setup, we also train reasoning models to perform narrow bad\nbehaviors only when a backdoor trigger is present in the prompt. This causes\nbroad misalignment that remains hidden, which brings additional risk. We find\nthat reasoning models can often describe and explain their backdoor triggers,\ndemonstrating a kind of self-awareness. So CoT monitoring can expose these\nbehaviors but is unreliable.\n  In summary, reasoning steps can both reveal and conceal misaligned\nintentions, and do not prevent misalignment behaviors in the models studied. We\nrelease three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.\n","authors":["James Chua","Jan Betley","Mia Taylor","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2506.13206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13203v1","updated":"2025-06-16T08:07:07Z","published":"2025-06-16T08:07:07Z","title":"Fatigue-Aware Adaptive Interfaces for Wearable Devices Using Deep\n  Learning","summary":"  Wearable devices, such as smartwatches and head-mounted displays, are\nincreasingly used for prolonged tasks like remote learning and work, but\nsustained interaction often leads to user fatigue, reducing efficiency and\nengagement. This study proposes a fatigue-aware adaptive interface system for\nwearable devices that leverages deep learning to analyze physiological data\n(e.g., heart rate, eye movement) and dynamically adjust interface elements to\nmitigate cognitive load. The system employs multimodal learning to process\nphysiological and contextual inputs and reinforcement learning to optimize\ninterface features like text size, notification frequency, and visual contrast.\nExperimental results show a 18% reduction in cognitive load and a 22%\nimprovement in user satisfaction compared to static interfaces, particularly\nfor users engaged in prolonged tasks. This approach enhances accessibility and\nusability in wearable computing environments.\n","authors":["Yikan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13196v1","updated":"2025-06-16T08:02:42Z","published":"2025-06-16T08:02:42Z","title":"KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate\n  Protein-Ligand Binding Affinity Prediction","summary":"  Accurate prediction of protein-ligand binding affinity is critical for drug\ndiscovery. While recent deep learning approaches have demonstrated promising\nresults, they often rely solely on structural features, overlooking valuable\nbiochemical knowledge associated with binding affinity. To address this\nlimitation, we propose KEPLA, a novel deep learning framework that explicitly\nintegrates prior knowledge from Gene Ontology and ligand properties of proteins\nand ligands to enhance prediction performance. KEPLA takes protein sequences\nand ligand molecular graphs as input and optimizes two complementary\nobjectives: (1) aligning global representations with knowledge graph relations\nto capture domain-specific biochemical insights, and (2) leveraging cross\nattention between local representations to construct fine-grained joint\nembeddings for prediction. Experiments on two benchmark datasets across both\nin-domain and cross-domain scenarios demonstrate that KEPLA consistently\noutperforms state-of-the-art baselines. Furthermore, interpretability analyses\nbased on knowledge graph relations and cross attention maps provide valuable\ninsights into the underlying predictive mechanisms.\n","authors":["Han Liu","Keyan Ding","Peilin Chen","Yinwei Wei","Liqiang Nie","Dapeng Wu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08037v3","updated":"2025-06-16T08:01:41Z","published":"2025-01-14T11:42:51Z","title":"Enhanced SPS Velocity-adaptive Scheme: Access Fairness in 5G NR V2I\n  Networks","summary":"  Vehicle-to-Infrastructure (V2I) technology enables information exchange\nbetween vehicles and road infrastructure. Specifically, when a vehicle\napproaches a roadside unit (RSU), it can exchange information with the RSU to\nobtain accurate data that assists in driving. With the release of the 3rd\nGeneration Partnership Project (3GPP) Release 16, which includes the 5G New\nRadio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt\nmode-2 communication using sensing-based semi-persistent scheduling (SPS) for\nresource allocation. In this approach, vehicles identify candidate resources\nwithin a selection window and exclude ineligible resources based on information\nfrom a sensing window. However, vehicles often drive at different speeds,\nresulting in varying amounts of data transmission with RSUs as they pass by,\nwhich leads to unfair access. Therefore, it is essential to design an access\nscheme that accounts for different vehicle speeds to achieve fair access across\nthe network. This paper formulates an optimization problem for vehicular\nnetworks and proposes a multi-objective optimization scheme to address it by\nadjusting the selection window in the SPS mechanism of 5G NR V2I mode-2.\nSimulation results demonstrate the effectiveness of the proposed scheme\n","authors":["Xiao Xu","Qiong Wu","Pingyi Fan","Kezhi Wang"],"pdf_url":"https://arxiv.org/pdf/2501.08037v3.pdf","comment":"This paper has been accepted by IEEE International Workshop on Radio\n  Frequency and Antenna Technologies. The source code has been released at:\n  https://github.com/qiongwu86/Enhanced-SPS-Velocity-adaptiveScheme-Access-Fariness-in-5G-NR-V2I-Networks"},{"id":"http://arxiv.org/abs/2407.06518v2","updated":"2025-06-16T08:00:30Z","published":"2024-07-09T03:14:11Z","title":"Graph Neural Networks and Deep Reinforcement Learning Based Resource\n  Allocation for V2X Communications","summary":"  In the rapidly evolving landscape of Internet of Vehicles (IoV) technology,\nCellular Vehicle-to-Everything (C-V2X) communication has attracted much\nattention due to its superior performance in coverage, latency, and throughput.\nResource allocation within C-V2X is crucial for ensuring the transmission of\nsafety information and meeting the stringent requirements for ultra-low latency\nand high reliability in Vehicle-to-Vehicle (V2V) communication. This paper\nproposes a method that integrates Graph Neural Networks (GNN) with Deep\nReinforcement Learning (DRL) to address this challenge. By constructing a\ndynamic graph with communication links as nodes and employing the Graph Sample\nand Aggregation (GraphSAGE) model to adapt to changes in graph structure, the\nmodel aims to ensure a high success rate for V2V communication while minimizing\ninterference on Vehicle-to-Infrastructure (V2I) links, thereby ensuring the\nsuccessful transmission of V2V link information and maintaining high\ntransmission rates for V2I links. The proposed method retains the global\nfeature learning capabilities of GNN and supports distributed network\ndeployment, allowing vehicles to extract low-dimensional features that include\nstructural information from the graph network based on local observations and\nto make independent resource allocation decisions. Simulation results indicate\nthat the introduction of GNN, with a modest increase in computational load,\neffectively enhances the decision-making quality of agents, demonstrating\nsuperiority to other methods. This study not only provides a theoretically\nefficient resource allocation strategy for V2V and V2I communications but also\npaves a new technical path for resource management in practical IoV\nenvironments.\n","authors":["Maoxin Ji","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.06518v2.pdf","comment":"15 pages, 11 figures. This paper has been accepted by IEEE Internet\n  of Things Journal. The source code has been released at:\n  https://github.com/qiongwu86/GNN-and-DRL-Based-Resource-Allocation-for-V2X-Communications"},{"id":"http://arxiv.org/abs/2506.13187v1","updated":"2025-06-16T07:55:14Z","published":"2025-06-16T07:55:14Z","title":"Dynamic Context-oriented Decomposition for Task-aware Low-rank\n  Adaptation with Less Forgetting and Faster Convergence","summary":"  Conventional low-rank adaptation methods build adapters without considering\ndata context, leading to sub-optimal fine-tuning performance and severe\nforgetting of inherent world knowledge. In this paper, we propose\ncontext-oriented decomposition adaptation (CorDA), a novel method that\ninitializes adapters in a task-aware manner. Concretely, we develop\ncontext-oriented singular value decomposition, where we collect covariance\nmatrices of input activations for each linear layer using sampled data from the\ntarget task, and apply SVD to the product of weight matrix and its\ncorresponding covariance matrix. By doing so, the task-specific capability is\ncompacted into the principal components. Thanks to the task awareness, our\nmethod enables two optional adaptation modes, knowledge-preserved mode (KPM)\nand instruction-previewed mode (IPM), providing flexibility to choose between\nfreezing the principal components to preserve their associated knowledge or\nadapting them to better learn a new task. We further develop CorDA++ by\nderiving a metric that reflects the compactness of task-specific principal\ncomponents, and then introducing dynamic covariance selection and dynamic rank\nallocation strategies based on the same metric. The two strategies provide each\nlayer with the most representative covariance matrix and a proper rank\nallocation. Experimental results show that CorDA++ outperforms CorDA by a\nsignificant margin. CorDA++ in KPM not only achieves better fine-tuning\nperformance than LoRA, but also mitigates the forgetting of pre-trained\nknowledge in both large language models and vision language models. For IPM,\nour method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA,\nand improves adaptation performance in various scenarios, outperforming strong\nbaseline methods. Our method has been integrated into the PEFT library\ndeveloped by Hugging Face.\n","authors":["Yibo Yang","Sihao Liu","Chuan Rao","Bang An","Tiancheng Shen","Philip H. S. Torr","Ming-Hsuan Yang","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2506.13187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12656v2","updated":"2025-06-16T07:49:30Z","published":"2025-01-22T05:35:26Z","title":"PPO-Based Vehicle Control for Ramp Merging Scheme Assisted by Enhanced\n  C-V2X","summary":"  On-ramp merging presents a critical challenge in autonomous driving, as\nvehicles from merging lanes need to dynamically adjust their positions and\nspeeds while monitoring traffic on the main road to prevent collisions. To\naddress this challenge, we propose a novel merging control scheme based on\nreinforcement learning, which integrates lateral control mechanisms. This\napproach ensures the smooth integration of vehicles from the merging lane onto\nthe main road, optimizing both fuel efficiency and passenger comfort.\nFurthermore, we recognize the impact of vehicle-to-vehicle (V2V) communication\non control strategies and introduce an enhanced protocol leveraging Cellular\nVehicle-to-Everything (C-V2X) Mode 4. This protocol aims to reduce the Age of\nInformation (AoI) and improve communication reliability. In our simulations, we\nemploy two AoI-based metrics to rigorously assess the protocol's effectiveness\nin autonomous driving scenarios. By combining the NS3 network simulator with\nPython, we simulate V2V communication and vehicle control simultaneously. The\nresults demonstrate that the enhanced C-V2X Mode 4 outperforms the standard\nversion, while the proposed control scheme ensures safe and reliable vehicle\noperation during on-ramp merging.\n","authors":["Qiong Wu","Maoxin Ji","Pingyi Fan","Kezhi Wang","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2501.12656v2.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/PPO-Based-Vehicle-Control-for-Ramp-Merging-Scheme-Assisted-by-Enhanced-C-V2X"},{"id":"http://arxiv.org/abs/2506.13181v1","updated":"2025-06-16T07:48:01Z","published":"2025-06-16T07:48:01Z","title":"Align-then-Unlearn: Embedding Alignment for LLM Unlearning","summary":"  As large language models (LLMs) are trained on massive datasets, they have\nraised significant privacy and ethical concerns due to their potential to\ninadvertently retain sensitive information. Unlearning seeks to selectively\nremove specific data from trained models, such as personal information or\ncopyrighted content. Current approaches targeting specific output sequences at\nthe token level often fail to achieve complete forgetting and remain\nsusceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel\nframework that performs unlearning in the semantic embedding space rather than\ndirectly on output tokens. Align-then-Unlearn first augments the LLM with an\nembedding prediction module trained to anticipate future context\nrepresentations. Unlearning is then achieved by fine-tuning the model to\nminimize the similarity between these predicted embeddings and a target\nembedding that represents the concept to be removed. Initial results show that\nAlign-then-Unlearn effectively removes targeted knowledge with minimal\ndegradation in overall model utility. These findings suggest that\nembedding-based unlearning offers a promising and robust approach to removing\nconceptual knowledge. Our code is available at\nhttps://github.com/ExplainableML/align-then-unlearn.\n","authors":["Philipp Spohn","Leander Girrbach","Jessica Bader","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2506.13181v1.pdf","comment":"Accepted at ICML 2025 Workshop on Machine Unlearning for Generative\n  AI"},{"id":"http://arxiv.org/abs/2506.03177v2","updated":"2025-06-16T07:42:49Z","published":"2025-05-29T11:11:41Z","title":"Deep Learning-Based Breast Cancer Detection in Mammography: A\n  Multi-Center Validation Study in Thai Population","summary":"  This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice.\n","authors":["Isarun Chamveha","Supphanut Chaiyungyuen","Sasinun Worakriangkrai","Nattawadee Prasawang","Warasinee Chaisangmongkon","Pornpim Korpraphong","Voraparee Suvannarerg","Shanigarn Thiravit","Chalermdej Kannawat","Kewalin Rungsinaporn","Suwara Issaragrisil","Payia Chadbunchachai","Pattiya Gatechumpol","Chawiporn Muktabhant","Patarachai Sereerat"],"pdf_url":"https://arxiv.org/pdf/2506.03177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01413v3","updated":"2025-06-16T07:40:34Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v3.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2506.13174v1","updated":"2025-06-16T07:35:49Z","published":"2025-06-16T07:35:49Z","title":"GeoRecon: Graph-Level Representation Learning for 3D Molecules via\n  Reconstruction-Based Pretraining","summary":"  The pretraining-and-finetuning paradigm has driven significant advances\nacross domains, such as natural language processing and computer vision, with\nrepresentative pretraining paradigms such as masked language modeling and\nnext-token prediction. However, in molecular representation learning, the task\ndesign remains largely limited to node-level denoising, which is effective at\nmodeling local atomic environments, yet maybe insufficient for capturing the\nglobal molecular structure required by graph-level property prediction tasks,\nsuch as energy estimation and molecular regression. In this work, we present\nGeoRecon, a novel graph-level pretraining framework that shifts the focus from\nindividual atoms to the molecule as an integrated whole. GeoRecon introduces a\ngraph-level reconstruction task: during pretraining, the model is trained to\ngenerate an informative graph representation capable of accurately guiding\nreconstruction of the molecular geometry. This encourages the model to learn\ncoherent, global structural features rather than isolated atomic details.\nWithout relying on additional supervision or external data, GeoRecon\noutperforms node-centric baselines on multiple molecular benchmarks (e.g., QM9,\nMD17), demonstrating the benefit of incorporating graph-level reconstruction\nfor learning more holistic and geometry-aware molecular embeddings.\n","authors":["Shaoheng Yan","Zian Li","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13173v1","updated":"2025-06-16T07:34:54Z","published":"2025-06-16T07:34:54Z","title":"Efficient Approximate Temporal Triangle Counting in Streaming with\n  Predictions","summary":"  Triangle counting is a fundamental and widely studied problem on static\ngraphs, and recently on temporal graphs, where edges carry information on the\ntimings of the associated events. Streaming processing and resource efficiency\nare crucial requirements for counting triangles in modern massive temporal\ngraphs, with millions of nodes and up to billions of temporal edges. However,\ncurrent exact and approximate algorithms are unable to handle large-scale\ntemporal graphs. To fill such a gap, we introduce STEP, a scalable and\nefficient algorithm to approximate temporal triangle counts from a stream of\ntemporal edges. STEP combines predictions to the number of triangles a temporal\nedge is involved in, with a simple sampling strategy, leading to scalability,\nefficiency, and accurate approximation of all eight temporal triangle types\nsimultaneously. We analytically prove that, by using a sublinear amount of\nmemory, STEP obtains unbiased and very accurate estimates. In fact, even noisy\npredictions can significantly reduce the variance of STEP's estimates. Our\nextensive experiments on massive temporal graphs with up to billions of edges\ndemonstrate that STEP outputs high-quality estimates and is more efficient than\nstate-of-the-art methods.\n","authors":["Giorgio Venturin","Ilie Sarpe","Fabio Vandin"],"pdf_url":"https://arxiv.org/pdf/2506.13173v1.pdf","comment":"Extended version of the ECML-PKDD2025 research paper"},{"id":"http://arxiv.org/abs/2410.15371v2","updated":"2025-06-16T07:22:26Z","published":"2024-10-20T12:10:24Z","title":"FrameBridge: Improving Image-to-Video Generation with Bridge Models","summary":"  Diffusion models have achieved remarkable progress on image-to-video (I2V)\ngeneration, while their noise-to-data generation process is inherently\nmismatched with this task, which may lead to suboptimal synthesis quality. In\nthis work, we present FrameBridge. By modeling the frame-to-frames generation\nprocess with a bridge model based data-to-data generative process, we are able\nto fully exploit the information contained in the given image and improve the\nconsistency between the generation process and I2V task. Moreover, we propose\ntwo novel techniques toward the two popular settings of training I2V models,\nrespectively. Firstly, we propose SNR-Aligned Fine-tuning (SAF), making the\nfirst attempt to fine-tune a diffusion model to a bridge model and, therefore,\nallowing us to utilize the pre-trained diffusion-based text-to-video (T2V)\nmodels. Secondly, we propose neural prior, further improving the synthesis\nquality of FrameBridge when training from scratch. Experiments conducted on\nWebVid-2M and UCF-101 demonstrate the superior quality of FrameBridge in\ncomparison with the diffusion counterpart (zero-shot FVD 95 vs. 192 on MSR-VTT\nand non-zero-shot FVD 122 vs. 171 on UCF-101), and the advantages of our\nproposed SAF and neural prior for bridge-based I2V models. The project page:\nhttps://framebridge-icml.github.io/.\n","authors":["Yuji Wang","Zehua Chen","Xiaoyu Chen","Yixiang Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13163v1","updated":"2025-06-16T07:19:02Z","published":"2025-06-16T07:19:02Z","title":"Efficient Algorithms for Logistic Contextual Slate Bandits with Bandit\n  Feedback","summary":"  We study the Logistic Contextual Slate Bandit problem, where, at each round,\nan agent selects a slate of $N$ items from an exponentially large set (of size\n$2^{\\Omega(N)}$) of candidate slates provided by the environment. A single\nbinary reward, determined by a logistic model, is observed for the chosen\nslate. Our objective is to develop algorithms that maximize cumulative reward\nover $T$ rounds while maintaining low per-round computational costs. We propose\ntwo algorithms, Slate-GLM-OFU and Slate-GLM-TS, that accomplish this goal.\nThese algorithms achieve $N^{O(1)}$ per-round time complexity via local\nplanning (independent slot selections), and low regret through global learning\n(joint parameter estimation). We provide theoretical and empirical evidence\nsupporting these claims. Under a well-studied diversity assumption, we prove\nthat Slate-GLM-OFU incurs only $\\tilde{O}(\\sqrt{T})$ regret. Extensive\nexperiments across a wide range of synthetic settings demonstrate that our\nalgorithms consistently outperform state-of-the-art baselines, achieving both\nthe lowest regret and the fastest runtime. Furthermore, we apply our algorithm\nto select in-context examples in prompts of Language Models for solving binary\nclassification tasks such as sentiment analysis. Our approach achieves\ncompetitive test accuracy, making it a viable alternative in practical\nscenarios.\n","authors":["Tanmay Goyal","Gaurav Sinha"],"pdf_url":"https://arxiv.org/pdf/2506.13163v1.pdf","comment":"Accepted to UAI 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.00868v2","updated":"2025-06-16T12:09:09Z","published":"2025-06-01T07:17:16Z","title":"Multiverse Through Deepfakes: The MultiFakeVerse Dataset of\n  Person-Centric Visual and Conceptual Manipulations","summary":"  The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.\n","authors":["Parul Gupta","Shreya Ghosh","Tom Gedeon","Thanh-Toan Do","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2506.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13038v1","updated":"2025-06-16T02:03:41Z","published":"2025-06-16T02:03:41Z","title":"HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for\n  Robust Multimodal Hallucination and Factuality Detection in VLMs","summary":"  Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains.\n","authors":["Zijian Zhang","Xuecheng Wu","Danlei Huang","Siyu Yan","Chong Peng","Xuezhi Cao"],"pdf_url":"https://arxiv.org/pdf/2506.13038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13001v1","updated":"2025-06-16T00:04:01Z","published":"2025-06-16T00:04:01Z","title":"Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV","summary":"  Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process of computer-assisted\ncomposition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear\narchitecture, to enable efficient and coherent musical cocreation on edge\ndevices. We also demonstrate that MIDI-RWKV admits an effective method of\nfinetuning its initial state for personalization in the very-low-sample regime.\nWe evaluate MIDI-RWKV and its state tuning on several quantitative and\nqualitative metrics, and release model weights and code at\nhttps://github.com/christianazinn/MIDI-RWKV.\n","authors":["Christian Zhou-Zheng","Philippe Pasquier"],"pdf_url":"https://arxiv.org/pdf/2506.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03176v2","updated":"2025-06-16T22:41:45Z","published":"2023-05-04T21:47:43Z","title":"NeRF-QA: Neural Radiance Fields Quality Assessment Database","summary":"  This short paper proposes a new database - NeRF-QA - containing 48 videos\nsynthesized with seven NeRF based methods, along with their perceived quality\nscores, resulting from subjective assessment tests; for the videos selection,\nboth real and synthetic, 360 degrees scenes were considered. This database will\nallow to evaluate the suitability, to NeRF based synthesized views, of existing\nobjective quality metrics and also the development of new quality metrics,\nspecific for this case.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2305.03176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13196v2","updated":"2025-06-16T22:37:17Z","published":"2025-02-18T17:46:57Z","title":"GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting\n  View Synthesis","summary":"  Gaussian Splatting (GS) offers a promising alternative to Neural Radiance\nFields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to\nrepresent complex geometry and appearance, GS achieves faster rendering times\nand reduced memory consumption compared to the neural network approach used in\nNeRF. However, quality assessment of GS-generated static content is not yet\nexplored in-depth. This paper describes a subjective quality assessment study\nthat aims to evaluate synthesized videos obtained with several static GS\nstate-of-the-art methods. The methods were applied to diverse visual scenes,\ncovering both 360-degree and forward-facing (FF) camera trajectories. Moreover,\nthe performance of 18 objective quality metrics was analyzed using the scores\nresulting from the subjective study, providing insights into their strengths,\nlimitations, and alignment with human perception. All videos and scores are\nmade available providing a comprehensive database that can be used as benchmark\non GS view synthesis and objective quality metrics.\n","authors":["Pedro Martin","António Rodrigues","João Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2502.13196v2.pdf","comment":null}]},"2025-06-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.12991v1","updated":"2025-06-15T23:16:12Z","published":"2025-06-15T23:16:12Z","title":"Large Language Models Enhanced by Plug and Play Syntactic Knowledge for\n  Aspect-based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) generally requires a deep\nunderstanding of the contextual information, including the words associated\nwith the aspect terms and their syntactic dependencies. Most existing studies\nemploy advanced encoders (e.g., pre-trained models) to capture such context,\nespecially large language models (LLMs). However, training these encoders is\nresource-intensive, and in many cases, the available data is insufficient for\nnecessary fine-tuning. Therefore it is challenging for learning LLMs within\nsuch restricted environments and computation efficiency requirement. As a\nresult, it motivates the exploration of plug-and-play methods that adapt LLMs\nto ABSA with minimal effort. In this paper, we propose an approach that\nintegrates extendable components capable of incorporating various types of\nsyntactic knowledge, such as constituent syntax, word dependencies, and\ncombinatory categorial grammar (CCG). Specifically, we propose a memory module\nthat records syntactic information and is incorporated into LLMs to instruct\nthe prediction of sentiment polarities. Importantly, this encoder acts as a\nversatile, detachable plugin that is trained independently of the LLM. We\nconduct experiments on benchmark datasets, which show that our approach\noutperforms strong baselines and previous approaches, thus demonstrates its\neffectiveness.\n","authors":["Yuanhe Tian","Xu Li","Wei Wang","Guoqing Jin","Pengsen Cheng","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2506.12991v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.12981v1","updated":"2025-06-15T22:35:43Z","published":"2025-06-15T22:35:43Z","title":"Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive\n  Query Routing","summary":"  Retrieval-Augmented Generation (RAG) systems address factual inconsistencies\nin Large Language Models by grounding generation in external knowledge, yet\nthey face a fundamental efficiency problem: simple queries consume\ncomputational resources equivalent to complex multi-hop reasoning tasks. We\npresent SymRAG, a neuro-symbolic framework that introduces adaptive query\nrouting based on real-time complexity and system load assessments. SymRAG\ndynamically selects symbolic, neural, or hybrid processing paths to align\nresource use with query demands. Evaluated on 2,000 queries from HotpotQA and\nDROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%\nexact match accuracy with significantly lower CPU utilization (3.6--6.2%) and\nprocessing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%\nincrease in processing time, highlighting the framework's impact. These results\nunderscore the potential of adaptive neuro-symbolic routing for scalable,\nsustainable AI systems.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2506.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12978v1","updated":"2025-06-15T22:14:59Z","published":"2025-06-15T22:14:59Z","title":"Multi-document Summarization through Multi-document Event Relation Graph\n  Reasoning in LLMs: a case study in Framing Bias Mitigation","summary":"  Media outlets are becoming more partisan and polarized nowadays. Most\nprevious work focused on detecting media bias. In this paper, we aim to\nmitigate media bias by generating a neutralized summary given multiple articles\npresenting different ideological views. Motivated by the critical role of\nevents and event relations in media bias detection, we propose to increase\nawareness of bias in LLMs via multi-document events reasoning and use a\nmulti-document event relation graph to guide the summarization process. This\ngraph contains rich event information useful to reveal bias: four common types\nof in-doc event relations to reflect content framing bias, cross-doc event\ncoreference relation to reveal content selection bias, and event-level moral\nopinions to highlight opinionated framing bias. We further develop two\nstrategies to incorporate the multi-document event relation graph for\nneutralized summarization. Firstly, we convert a graph into natural language\ndescriptions and feed the textualized graph into LLMs as a part of a hard text\nprompt. Secondly, we encode the graph with graph attention network and insert\nthe graph embedding into LLMs as a soft prompt. Both automatic evaluation and\nhuman evaluation confirm that our approach effectively mitigates both lexical\nand informational media bias, and meanwhile improves content preservation.\n","authors":["Yuanyuan Lei","Ruihong Huang"],"pdf_url":"https://arxiv.org/pdf/2506.12978v1.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2501.01426v2","updated":"2025-06-15T21:53:18Z","published":"2025-01-02T18:59:45Z","title":"Unifying Specialized Visual Encoders for Video Language Models","summary":"  The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.\n","authors":["Jihoon Chung","Tyler Zhu","Max Gonzalez Saez-Diez","Juan Carlos Niebles","Honglu Zhou","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2501.01426v2.pdf","comment":"Accepted to ICML 2025 as a Poster. Project page:\n  https://tylerzhu.com/merv/"},{"id":"http://arxiv.org/abs/2405.20947v5","updated":"2025-06-15T21:44:25Z","published":"2024-05-31T15:44:33Z","title":"OR-Bench: An Over-Refusal Benchmark for Large Language Models","summary":"  Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.\n","authors":["Justin Cui","Wei-Lin Chiang","Ion Stoica","Cho-Jui Hsieh"],"pdf_url":"https://arxiv.org/pdf/2405.20947v5.pdf","comment":"Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!"},{"id":"http://arxiv.org/abs/2410.21332v2","updated":"2025-06-15T21:38:31Z","published":"2024-10-27T18:13:07Z","title":"Building, Reusing, and Generalizing Abstract Representations from\n  Concrete Sequences","summary":"  Humans excel at learning abstract patterns across different sequences,\nfiltering out irrelevant details, and transferring these generalized concepts\nto new sequences. In contrast, many sequence learning models lack the ability\nto abstract, which leads to memory inefficiency and poor transfer. We introduce\na non-parametric hierarchical variable learning model (HVM) that learns chunks\nfrom sequences and abstracts contextually similar chunks as variables. HVM\nefficiently organizes memory while uncovering abstractions, leading to compact\nsequence representations. When learning on language datasets such as babyLM,\nHVM learns a more efficient dictionary than standard compression algorithms\nsuch as Lempel-Ziv. In a sequence recall task requiring the acquisition and\ntransfer of variables embedded in sequences, we demonstrate HVM's sequence\nlikelihood correlates with human recall times. In contrast, large language\nmodels (LLMs) struggle to transfer abstract variables as effectively as humans.\nFrom HVM's adjustable layer of abstraction, we demonstrate that the model\nrealizes a precise trade-off between compression and generalization. Our work\noffers a cognitive model that captures the learning and transfer of abstract\nrepresentations in human cognition and differentiates itself from LLMs.\n","authors":["Shuchen Wu","Mirko Thalmann","Peter Dayan","Zeynep Akata","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2410.21332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12966v1","updated":"2025-06-15T21:08:51Z","published":"2025-06-15T21:08:51Z","title":"Assessing the Role of Data Quality in Training Bilingual Language Models","summary":"  Bilingual and multilingual language models offer a promising path toward\nscaling NLP systems across diverse languages and users. However, their\nperformance often varies wildly between languages as prior works show that\nadding more languages can degrade performance for some languages (such as\nEnglish), while improving others (typically more data constrained languages).\nIn this work, we investigate causes of these inconsistencies by comparing\nbilingual and monolingual language models. Our analysis reveals that unequal\ndata quality, not just data quantity, is a major driver of performance\ndegradation in bilingual settings. We propose a simple yet effective data\nfiltering strategy to select higher-quality bilingual training data with only\nhigh quality English data. Applied to French, German, and Chinese, our approach\nimproves monolingual performance by 2-4% and reduces bilingual model\nperformance gaps to 1%. These results highlight the overlooked importance of\ndata quality in multilingual pretraining and offer a practical recipe for\nbalancing performance.\n","authors":["Skyler Seto","Maartje ter Hoeve","Maureen de Seyssel","David Grangier"],"pdf_url":"https://arxiv.org/pdf/2506.12966v1.pdf","comment":"26 pages, 18 figures, 25 tables"},{"id":"http://arxiv.org/abs/2503.13102v2","updated":"2025-06-15T19:53:19Z","published":"2025-03-17T12:15:16Z","title":"REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities","summary":"  Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.\n","authors":["Alexander Pugachev","Alena Fenogenova","Vladislav Mikhailov","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2503.13102v2.pdf","comment":"To appear at SIGSLAV 2025"},{"id":"http://arxiv.org/abs/2506.00713v2","updated":"2025-06-15T19:52:55Z","published":"2025-05-31T21:11:30Z","title":"From Argumentative Text to Argument Knowledge Graph: A New Framework for\n  Structured Argumentation","summary":"  This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). Starting with basic annotations of argumentative\ncomponents (ACs) and argumentative relations (ARs), we enrich the information\nby constructing a knowledge base (KB) graph with metadata attributes for nodes.\nNext, we use premises and inference rules from the KB to form arguments by\napplying modus ponens. From these arguments, we create an AKG. The nodes and\nedges of the AKG have attributes that capture important argumentative features.\nWe also find missing inference rules by identifying markers. This makes it\npossible to identify undercut attacks that were previously undetectable in\nexisting datasets. The AKG gives a graphical view of the argumentative\nstructure that is easier to understand than theoretical formats. It also\nprepares the ground for future reasoning tasks, including checking the\ncoherence of arguments and identifying opportunities for revision. For this, it\nis important to find indirect relations, many of which are implicit. Our\nproposed AKG format, with annotated inference rules and modus ponens, will help\nreasoning models learn the implicit indirect relations that require inference\nover arguments and the relations between them.\n","authors":["Debarati Bhattacharjee","Ashish Anand"],"pdf_url":"https://arxiv.org/pdf/2506.00713v2.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.12953v1","updated":"2025-06-15T19:42:58Z","published":"2025-06-15T19:42:58Z","title":"Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition","summary":"  Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.\n","authors":["Mayank Bumb","Anshul Vemulapalli","Sri Harsha Vardhan Prasad Jella","Anish Gupta","An La","Ryan A. Rossi","Hongjie Chen","Franck Dernoncourt","Nesreen K. Ahmed","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2506.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12937v1","updated":"2025-06-15T18:41:23Z","published":"2025-06-15T18:41:23Z","title":"HypER: Literature-grounded Hypothesis Generation and Distillation with\n  Provenance","summary":"  Large Language models have demonstrated promising performance in research\nideation across scientific domains. Hypothesis development, the process of\ngenerating a highly specific declarative statement connecting a research idea\nwith empirical validation, has received relatively less attention. Existing\napproaches trivially deploy retrieval augmentation and focus only on the\nquality of the final output ignoring the underlying reasoning process behind\nideation. We present $\\texttt{HypER}$ ($\\textbf{Hyp}$othesis Generation with\n$\\textbf{E}$xplanation and $\\textbf{R}$easoning), a small language model (SLM)\ntrained for literature-guided reasoning and evidence-based hypothesis\ngeneration. $\\texttt{HypER}$ is trained in a multi-task setting to discriminate\nbetween valid and invalid scientific reasoning chains in presence of controlled\ndistractions. We find that $\\texttt{HypER}$ outperformes the base model,\ndistinguishing valid from invalid reasoning chains (+22\\% average absolute F1),\ngenerates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with\nhigh feasibility and impact as judged by human experts ($>$3.5 on 5-point\nLikert scale).\n","authors":["Rosni Vasu","Chandrayee Basu","Bhavana Dalvi Mishra","Cristina Sarasua","Peter Clark","Abraham Bernstein"],"pdf_url":"https://arxiv.org/pdf/2506.12937v1.pdf","comment":"26 pages (9 pages: main paper body)"},{"id":"http://arxiv.org/abs/2506.12936v1","updated":"2025-06-15T18:39:24Z","published":"2025-06-15T18:39:24Z","title":"CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team\n  Reflection in Action During Clinical Operation","summary":"  In clinical operations, teamwork can be the crucial factor that determines\nthe final outcome. Prior studies have shown that sufficient collaboration is\nthe key factor that determines the outcome of an operation. To understand how\nthe team practices teamwork during the operation, we collected CliniDial from\nsimulations of medical operations. CliniDial includes the audio data and its\ntranscriptions, the simulated physiology signals of the patient manikins, and\nhow the team operates from two camera angles. We annotate behavior codes\nfollowing an existing framework to understand the teamwork process for\nCliniDial. We pinpoint three main characteristics of our dataset, including its\nlabel imbalances, rich and natural interactions, and multiple modalities, and\nconduct experiments to test existing LLMs' capabilities on handling data with\nthese characteristics. Experimental results show that CliniDial poses\nsignificant challenges to the existing models, inviting future effort on\ndeveloping methods that can deal with real-world clinical data. We open-source\nthe codebase at https://github.com/MichiganNLP/CliniDial\n","authors":["Naihao Deng","Kapotaksha Das","Rada Mihalcea","Vitaliy Popov","Mohamed Abouelenien"],"pdf_url":"https://arxiv.org/pdf/2506.12936v1.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.02013v2","updated":"2025-06-15T18:27:17Z","published":"2025-02-04T05:03:42Z","title":"Layer by Layer: Uncovering Hidden Representations in Language Models","summary":"  From extracting features to generating text, the outputs of large language\nmodels (LLMs) typically rely on the final layers, following the conventional\nwisdom that earlier layers capture only low-level cues. However, our analysis\nshows that intermediate layers can encode even richer representations, often\nimproving performance on a range of downstream tasks. To explain and quantify\nthese hidden-layer properties, we propose a unified framework of representation\nquality metrics based on information theory, geometry, and invariance to input\nperturbations. Our framework highlights how each layer balances information\ncompression and signal preservation, revealing why mid-depth embeddings can\nexceed the last layer's performance. Through extensive experiments on 32\ntext-embedding tasks across various architectures (transformers, state-space\nmodels) and domains (language, vision), we demonstrate that intermediate layers\nconsistently provide stronger features, challenging the standard view on\nfinal-layer embeddings and opening new directions on using mid-layer\nrepresentations for more robust and accurate representations.\n","authors":["Oscar Skean","Md Rifat Arefin","Dan Zhao","Niket Patel","Jalal Naghiyev","Yann LeCun","Ravid Shwartz-Ziv"],"pdf_url":"https://arxiv.org/pdf/2502.02013v2.pdf","comment":"update for ICML2025 camera-ready"},{"id":"http://arxiv.org/abs/2506.12935v1","updated":"2025-06-15T18:26:08Z","published":"2025-06-15T18:26:08Z","title":"SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models","summary":"  While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.\n","authors":["Xingjian Diao","Chunhui Zhang","Keyi Kong","Weiyi Wu","Chiyu Ma","Zhongyu Ouyang","Peijun Qing","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2506.12935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14693v2","updated":"2025-06-15T18:18:35Z","published":"2025-01-24T18:06:07Z","title":"Rethinking Table Instruction Tuning","summary":"  Recent advances in table understanding have focused on instruction-tuning\nlarge language models (LLMs) for table-related tasks. However, existing\nresearch has overlooked the impact of hyperparameter choices, and also lacks a\ncomprehensive evaluation of the out-of-domain table understanding ability and\nthe general capabilities of these table LLMs. In this paper, we evaluate these\nabilities in existing table LLMs, and find significant declines in both\nout-of-domain table understanding and general capabilities as compared to their\nbase models. Through systematic analysis, we show that hyperparameters, such as\nlearning rate, can significantly influence both table-specific and general\ncapabilities. Contrary to the previous table instruction-tuning work, we\ndemonstrate that smaller learning rates and fewer training instances can\nenhance table understanding while preserving general capabilities. Based on our\nfindings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B\nInstruct, which achieves performance on par with, or surpassing GPT-3.5 and\nGPT-4 on table tasks, while maintaining strong out-of-domain generalization and\ngeneral capabilities. Our findings highlight the potential for reduced data\nannotation costs and more efficient model development through careful\nhyperparameter selection. We open-source the project and our models.\n","authors":["Naihao Deng","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2501.14693v2.pdf","comment":"Accepted to ACL 2025 Findings. Project page:\n  https://lit.eecs.umich.edu/TAMA/. Code: https://github.com/MichiganNLP/TAMA.\n  Huggingface models:\n  https://huggingface.co/collections/MichiganNLP/tama-684eeb3e7f262362856eccd1.\n  Data: https://huggingface.co/datasets/MichiganNLP/TAMA_Instruct"},{"id":"http://arxiv.org/abs/2506.07042v2","updated":"2025-06-15T18:16:38Z","published":"2025-06-08T08:36:14Z","title":"Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base\n  Construction and reasoning with proof-assistants","summary":"  Extracting structured computational representations of historical events from\nnarrative text remains computationally expensive when constructed manually.\nWhile RDF/OWL reasoners enable graph-based reasoning, they are limited to\nfragments of first-order logic, preventing deeper temporal and semantic\nanalysis. This paper addresses both challenges by developing automatic\nhistorical event extraction models using multiple LLMs (GPT-4, Claude, Llama\n3.2) with three enhancement strategies: pure base generation, knowledge graph\nenhancement, and Retrieval-Augmented Generation (RAG). We conducted\ncomprehensive evaluations using historical texts from Thucydides. Our findings\nreveal that enhancement strategies optimize different performance dimensions\nrather than providing universal improvements. For coverage and historical\nbreadth, base generation achieves optimal performance with Claude and GPT-4\nextracting comprehensive events. However, for precision, RAG enhancement\nimproves coordinate accuracy and metadata completeness. Model architecture\nfundamentally determines enhancement sensitivity: larger models demonstrate\nrobust baseline performance with incremental RAG improvements, while Llama 3.2\nshows extreme variance from competitive performance to complete failure. We\nthen developed an automated translation pipeline converting extracted RDF\nrepresentations into Coq proof assistant specifications, enabling higher-order\nreasoning beyond RDF capabilities including multi-step causal verification,\ntemporal arithmetic with BC dates, and formal proofs about historical\ncausation. The Coq formalization validates that RAG-discovered event types\nrepresent legitimate domain-specific semantic structures rather than\nontological violations.\n","authors":["Stergios Chatzikyriakidis"],"pdf_url":"https://arxiv.org/pdf/2506.07042v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.12981v1","updated":"2025-06-15T22:35:43Z","published":"2025-06-15T22:35:43Z","title":"Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive\n  Query Routing","summary":"  Retrieval-Augmented Generation (RAG) systems address factual inconsistencies\nin Large Language Models by grounding generation in external knowledge, yet\nthey face a fundamental efficiency problem: simple queries consume\ncomputational resources equivalent to complex multi-hop reasoning tasks. We\npresent SymRAG, a neuro-symbolic framework that introduces adaptive query\nrouting based on real-time complexity and system load assessments. SymRAG\ndynamically selects symbolic, neural, or hybrid processing paths to align\nresource use with query demands. Evaluated on 2,000 queries from HotpotQA and\nDROP using Llama-3.2-3B and Mistral-7B models, SymRAG achieves 97.6--100.0%\nexact match accuracy with significantly lower CPU utilization (3.6--6.2%) and\nprocessing time (0.985--3.165s). Disabling adaptive logic results in 169--1151%\nincrease in processing time, highlighting the framework's impact. These results\nunderscore the potential of adaptive neuro-symbolic routing for scalable,\nsustainable AI systems.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2506.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12925v1","updated":"2025-06-15T17:50:08Z","published":"2025-06-15T17:50:08Z","title":"Identifying and Investigating Global News Coverage of Critical Events\n  Such as Disasters and Terrorist Attacks","summary":"  Comparative studies of news coverage are challenging to conduct because\nmethods to identify news articles about the same event in different languages\nrequire expertise that is difficult to scale. We introduce an AI-powered method\nfor identifying news articles based on an event FINGERPRINT, which is a minimal\nset of metadata required to identify critical events. Our event coverage\nidentification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),\nefficiently identifies news articles about critical world events, specifically\nterrorist attacks and several types of natural disasters. FAME does not require\ntraining data and is able to automatically and efficiently identify news\narticles that discuss an event given its fingerprint: time, location, and class\n(such as storm or flood). The method achieves state-of-the-art performance and\nscales to massive databases of tens of millions of news articles and hundreds\nof events happening globally. We use FAME to identify 27,441 articles that\ncover 470 natural disaster and terrorist attack events that happened in 2020.\nTo this end, we use a massive database of news articles in three languages from\nMediaCloud, and three widely used, expert-curated databases of critical events:\nEM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior\nliterature: coverage of disasters and terrorist attacks correlates to death\ncounts, to the GDP of a country where the event occurs, and to trade volume\nbetween the reporting country and the country where the event occurred. We\nshare our NLP annotations and cross-country media attention data to support the\nefforts of researchers and media monitoring organizations.\n","authors":["Erica Cai","Xi Chen","Reagan Grey Keeney","Ethan Zuckerman","Brendan O'Connor","Przemyslaw A. Grabowicz"],"pdf_url":"https://arxiv.org/pdf/2506.12925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12895v1","updated":"2025-06-15T15:53:38Z","published":"2025-06-15T15:53:38Z","title":"Assessing the Performance Gap Between Lexical and Semantic Models for\n  Information Retrieval With Formulaic Legal Language","summary":"  Legal passage retrieval is an important task that assists legal practitioners\nin the time-intensive process of finding relevant precedents to support legal\narguments. This study investigates the task of retrieving legal passages or\nparagraphs from decisions of the Court of Justice of the European Union (CJEU),\nwhose language is highly structured and formulaic, leading to repetitive\npatterns. Understanding when lexical or semantic models are more effective at\nhandling the repetitive nature of legal language is key to developing retrieval\nsystems that are more accurate, efficient, and transparent for specific legal\ndomains. To this end, we explore when this routinized legal language is better\nsuited for retrieval using methods that rely on lexical and statistical\nfeatures, such as BM25, or dense retrieval models trained to capture semantic\nand contextual information. A qualitative and quantitative analysis with three\ncomplementary metrics shows that both lexical and dense models perform well in\nscenarios with more repetitive usage of language, whereas BM25 performs better\nthan the dense models in more nuanced scenarios where repetition and\nverbatim~quotes are less prevalent and in longer queries. Our experiments also\nshow that BM25 is a strong baseline, surpassing off-the-shelf dense models in 4\nout of 7 performance metrics. However, fine-tuning a dense model on\ndomain-specific data led to improved performance, surpassing BM25 in most\nmetrics, and we analyze the effect of the amount of data used in fine-tuning on\nthe model's performance and temporal robustness. The code, dataset and appendix\nrelated to this work are available on:\nhttps://github.com/larimo/lexsem-legal-ir.\n","authors":["Larissa Mori","Carlos Sousa de Oliveira","Yuehwern Yih","Mario Ventresca"],"pdf_url":"https://arxiv.org/pdf/2506.12895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14256v2","updated":"2025-06-15T11:16:43Z","published":"2025-01-24T05:44:04Z","title":"DKT2: Revisiting Applicable and Comprehensive Knowledge Tracing in\n  Large-Scale Data","summary":"  Knowledge Tracing (KT) is a fundamental component of Intelligent Tutoring\nSystems (ITS), enabling the modeling of students' knowledge states to predict\nfuture performance. The introduction of Deep Knowledge Tracing (DKT), the first\ndeep learning-based KT (DLKT) model, has brought significant advantages in\nterms of applicability and comprehensiveness. However, recent DLKT models, such\nas Attentive Knowledge Tracing (AKT), have often prioritized predictive\nperformance at the expense of these benefits. While deep sequential models like\nDKT have shown potential, they face challenges related to parallel computing,\nstorage decision modification, and limited storage capacity. To address these\nlimitations, we propose DKT2, a novel KT model that leverages the recently\ndeveloped xLSTM architecture. DKT2 enhances applicable input representation\nusing the Rasch model and incorporates Item Response Theory (IRT) for output\ninterpretability, allowing for the decomposition of learned knowledge into\nfamiliar and unfamiliar knowledge. By integrating this knowledge with predicted\nquestions, DKT2 generates comprehensive knowledge states. Extensive experiments\nconducted across three large-scale datasets demonstrate that DKT2 consistently\noutperforms 18 baseline models in various prediction tasks, underscoring its\npotential for real-world educational applications. This work bridges the gap\nbetween theoretical advancements and practical implementation in KT. Our code\nand datasets are fully available at https://github.com/zyy-2001/DKT2.\n","authors":["Yiyun Zhou","Wenkang Han","Jingyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2501.14256v2.pdf","comment":"Accepted by ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2506.12761v1","updated":"2025-06-15T08:01:35Z","published":"2025-06-15T08:01:35Z","title":"Versatile and Fast Location-Based Private Information Retrieval with\n  Fully Homomorphic Encryption over the Torus","summary":"  Location-based services often require users to share sensitive locational\ndata, raising privacy concerns due to potential misuse or exploitation by\nuntrusted servers. In response, we present VeLoPIR, a versatile location-based\nprivate information retrieval (PIR) system designed to preserve user privacy\nwhile enabling efficient and scalable query processing. VeLoPIR introduces\nthree operational modes-interval validation, coordinate validation, and\nidentifier matching-that support a broad range of real-world applications,\nincluding information and emergency alerts. To enhance performance, VeLoPIR\nincorporates multi-level algorithmic optimizations with parallel structures,\nachieving significant scalability across both CPU and GPU platforms. We also\nprovide formal security and privacy proofs, confirming the system's robustness\nunder standard cryptographic assumptions. Extensive experiments on real-world\ndatasets demonstrate that VeLoPIR achieves up to 11.55 times speed-up over a\nprior baseline. The implementation of VeLoPIR is publicly available at\nhttps://github.com/PrivStatBool/VeLoPIR.\n","authors":["Joon Soo Yoo","Taeho Kim","Ji Won Yoon"],"pdf_url":"https://arxiv.org/pdf/2506.12761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12756v1","updated":"2025-06-15T07:47:26Z","published":"2025-06-15T07:47:26Z","title":"Hierarchical Group-wise Ranking Framework for Recommendation Models","summary":"  In modern recommender systems, CTR/CVR models are increasingly trained with\nranking objectives to improve item ranking quality. While this shift aligns\ntraining more closely with serving goals, most existing methods rely on\nin-batch negative sampling, which predominantly surfaces easy negatives. This\nlimits the model's ability to capture fine-grained user preferences and weakens\noverall ranking performance. To address this, we propose a Hierarchical\nGroup-wise Ranking Framework with two key components. First, we apply residual\nvector quantization to user embeddings to generate hierarchical user codes that\npartition users into hierarchical, trie-structured clusters. Second, we apply\nlistwise ranking losses to user-item pairs at each level of the hierarchy,\nwhere shallow levels group loosely similar users and deeper levels group highly\nsimilar users, reinforcing learning-to-rank signals through progressively\nharder negatives. Since users with similar preferences and content exposure\ntend to yield more informative negatives, applying ranking losses within these\nhierarchical user groups serves as an effective approximation of hard negative\nmining. Our approach improves ranking performance without requiring complex\nreal-time context collection or retrieval infrastructure. Extensive experiments\ndemonstrate that the proposed framework consistently enhances both model\ncalibration and ranking accuracy, offering a scalable and practical solution\nfor industrial recommender systems.\n","authors":["YaChen Yan","Liubo Li","Ravi Choudhary"],"pdf_url":"https://arxiv.org/pdf/2506.12756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12689v1","updated":"2025-06-15T02:23:47Z","published":"2025-06-15T02:23:47Z","title":"SciSage: A Multi-Agent Framework for High-Quality Scientific Survey\n  Generation","summary":"  The rapid growth of scientific literature demands robust tools for automated\nsurvey-generation. However, current large language model (LLM)-based methods\noften lack in-depth analysis, structural coherence, and reliable citations. To\naddress these limitations, we introduce SciSage, a multi-agent framework\nemploying a reflect-when-you-write paradigm. SciSage features a hierarchical\nReflector agent that critically evaluates drafts at outline, section, and\ndocument levels, collaborating with specialized agents for query\ninterpretation, content retrieval, and refinement. We also release SurveyScope,\na rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11\ncomputer science domains, with strict recency and citation-based quality\ncontrols. Evaluations demonstrate that SciSage outperforms state-of-the-art\nbaselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document\ncoherence and +32% in citation F1 scores. Human evaluations reveal mixed\noutcomes (3 wins vs. 7 losses against human-written surveys), but highlight\nSciSage's strengths in topical breadth and retrieval efficiency. Overall,\nSciSage offers a promising foundation for research-assistive writing tools.\n","authors":["Xiaofeng Shi","Qian Kou","Yuduo Li","Ning Tang","Jinxin Xie","Longbin Yu","Songjing Wang","Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.12689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12687v1","updated":"2025-06-15T02:20:18Z","published":"2025-06-15T02:20:18Z","title":"Device-Cloud Collaborative Correction for On-Device Recommendation","summary":"  With the rapid development of recommendation models and device computing\npower, device-based recommendation has become an important research area due to\nits better real-time performance and privacy protection. Previously,\nTransformer-based sequential recommendation models have been widely applied in\nthis field because they outperform Recurrent Neural Network (RNN)-based\nrecommendation models in terms of performance. However, as the length of\ninteraction sequences increases, Transformer-based models introduce\nsignificantly more space and computational overhead compared to RNN-based\nmodels, posing challenges for device-based recommendation. To balance real-time\nperformance and high performance on devices, we propose Device-Cloud\n\\underline{Co}llaborative \\underline{Corr}ection Framework for On-Device\n\\underline{Rec}ommendation (CoCorrRec). CoCorrRec uses a self-correction\nnetwork (SCN) to correct parameters with extremely low time cost. By updating\nmodel parameters during testing based on the input token, it achieves\nperformance comparable to current optimal but more complex Transformer-based\nmodels. Furthermore, to prevent SCN from overfitting, we design a global\ncorrection network (GCN) that processes hidden states uploaded from devices and\nprovides a global correction solution. Extensive experiments on multiple\ndatasets show that CoCorrRec outperforms existing Transformer-based and\nRNN-based device recommendation models in terms of performance, with fewer\nparameters and lower FLOPs, thereby achieving a balance between real-time\nperformance and high efficiency.\n","authors":["Tianyu Zhan","Shengyu Zhang","Zheqi Lv","Jieming Zhu","Jiwei Li","Fan Wu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2506.12687v1.pdf","comment":"To be published in IJCAI-2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.12935v1","updated":"2025-06-15T18:26:08Z","published":"2025-06-15T18:26:08Z","title":"SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models","summary":"  While large language models have shown reasoning capabilities, their\napplication to the audio modality, particularly in large audio-language models\n(ALMs), remains significantly underdeveloped. Addressing this gap requires a\nsystematic approach, involving a capable base model, high-quality\nreasoning-oriented audio data, and effective training algorithms. In this\nstudy, we present a comprehensive solution: we introduce the Audio Logical\nReasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples\nspecifically designed for complex reasoning tasks. Building on this resource,\nwe propose SoundMind, a rule-based reinforcement learning (RL) algorithm\ntailored to endow ALMs with deep bimodal reasoning abilities. By training\nQwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves\nstate-of-the-art performance in audio logical reasoning. This work highlights\nthe impact of combining high-quality, reasoning-focused datasets with\nspecialized RL techniques, advancing the frontier of auditory intelligence in\nlanguage models. Our code and the proposed dataset are available at\nhttps://github.com/xid32/SoundMind.\n","authors":["Xingjian Diao","Chunhui Zhang","Keyi Kong","Weiyi Wu","Chiyu Ma","Zhongyu Ouyang","Peijun Qing","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2506.12935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09792v2","updated":"2025-06-15T08:24:31Z","published":"2025-06-11T14:36:26Z","title":"Incorporating Linguistic Constraints from External Knowledge Source for\n  Audio-Visual Target Speech Extraction","summary":"  Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.\n","authors":["Wenxuan Wu","Shuai Wang","Xixin Wu","Helen Meng","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2506.09792v2.pdf","comment":"Accepted by Interspeech 2025"}]},"2025-06-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.12661v1","updated":"2025-06-14T23:40:49Z","published":"2025-06-14T23:40:49Z","title":"INTERPOS: Interaction Rhythm Guided Positional Morphing for Mobile App\n  Recommender Systems","summary":"  The mobile app market has expanded exponentially, offering millions of apps\nwith diverse functionalities, yet research in mobile app recommendation remains\nlimited. Traditional sequential recommender systems utilize the order of items\nin users' historical interactions to predict the next item for the users.\nPosition embeddings, well-established in transformer-based architectures for\nnatural language processing tasks, effectively distinguish token positions in\nsequences. In sequential recommendation systems, position embeddings can\ncapture the order of items in a user's historical interaction sequence.\nNevertheless, this ordering does not consider the time elapsed between two\ninteractions of the same user (e.g., 1 day, 1 week, 1 month), referred to as\n\"user rhythm\". In mobile app recommendation datasets, the time between\nconsecutive user interactions is notably longer compared to other domains like\nmovies, posing significant challenges for sequential recommender systems. To\naddress this phenomenon in the mobile app domain, we introduce INTERPOS, an\nInteraction Rhythm Guided Positional Morphing strategy for autoregressive\nmobile app recommender systems. INTERPOS incorporates rhythm-guided position\nembeddings, providing a more comprehensive representation that considers both\nthe sequential order of interactions and the temporal gaps between them. This\napproach enables a deep understanding of users' rhythms at a fine-grained\nlevel, capturing the intricacies of their interaction patterns over time. We\npropose three strategies to incorporate the morphed positional embeddings in\ntwo transformer-based sequential recommendation system architectures. Our\nextensive evaluations show that INTERPOS outperforms state-of-the-art models\nusing 7 mobile app recommendation datasets on NDCG@K and HIT@K metrics. The\nsource code of INTERPOS is available at https://github.com/dlgrad/INTERPOS.\n","authors":["M. H. Maqbool","Moghis Fereidouni","Umar Farooq","A. B. Siddique","Hassan Foroosh"],"pdf_url":"https://arxiv.org/pdf/2506.12661v1.pdf","comment":"10 pages, 8 tables, 3 figures"},{"id":"http://arxiv.org/abs/2506.12583v1","updated":"2025-06-14T17:35:27Z","published":"2025-06-14T17:35:27Z","title":"A Gradient Meta-Learning Joint Optimization for Beamforming and Antenna\n  Position in Pinching-Antenna Systems","summary":"  In this paper, we consider a novel optimization design for multi-waveguide\npinching-antenna systems, aiming to maximize the weighted sum rate (WSR) by\njointly optimizing beamforming coefficients and antenna position. To handle the\nformulated non-convex problem, a gradient-based meta-learning joint\noptimization (GML-JO) algorithm is proposed. Specifically, the original problem\nis initially decomposed into two sub-problems of beamforming optimization and\nantenna position optimization through equivalent substitution. Then, the convex\napproximation methods are used to deal with the nonconvex constraints of\nsub-problems, and two sub-neural networks are constructed to calculate the\nsub-problems separately. Different from alternating optimization (AO), where\ntwo sub-problems are solved alternately and the solutions are influenced by the\ninitial values, two sub-neural networks of proposed GML-JO with fixed channel\ncoefficients are considered as local sub-tasks and the computation results are\nused to calculate the loss function of joint optimization. Finally, the\nparameters of sub-networks are updated using the average loss function over\ndifferent sub-tasks and the solution that is robust to the initial value is\nobtained. Simulation results demonstrate that the proposed GML-JO algorithm\nachieves 5.6 bits/s/Hz WSR within 100 iterations, yielding a 32.7\\% performance\nenhancement over conventional AO with substantially reduced computational\ncomplexity. Moreover, the proposed GML-JO algorithm is robust to different\nchoices of initialization and yields better performance compared with the\nexisting optimization methods.\n","authors":["Kang Zhou","Weixi Zhou","Donghong Cai","Xianfu Lei","Yanqing Xu","Zhiguo Ding","Pingzhi Fan"],"pdf_url":"https://arxiv.org/pdf/2506.12583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12571v1","updated":"2025-06-14T16:56:00Z","published":"2025-06-14T16:56:00Z","title":"DoTA-RAG: Dynamic of Thought Aggregation RAG","summary":"  In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a\nretrieval-augmented generation system optimized for high-throughput,\nlarge-scale web knowledge indexes. Traditional RAG pipelines often suffer from\nhigh latency and limited accuracy over massive, diverse datasets. DoTA-RAG\naddresses these challenges with a three-stage pipeline: query rewriting,\ndynamic routing to specialized sub-indexes, and multi-stage retrieval and\nranking. We further enhance retrieval by evaluating and selecting a superior\nembedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we\ncreate a diverse Q&A dataset of 500 questions generated via the DataMorgana\nsetup across a broad range of WebOrganizer topics and formats. DoTA-RAG\nimproves the answer correctness score from 0.752 (baseline, using LiveRAG\npre-built vector store) to 1.478 while maintaining low latency, and it achieves\na 0.929 correctness score on the Live Challenge Day. These results highlight\nDoTA-RAG's potential for practical deployment in domains requiring fast,\nreliable access to large and evolving knowledge sources.\n","authors":["Saksorn Ruangtanusak","Natthapath Rungseesiripak","Peerawat Rojratchadakorn","Monthol Charattrakool","Natapong Nitarach"],"pdf_url":"https://arxiv.org/pdf/2506.12571v1.pdf","comment":"SIGIR LiveRAG 2025 (oral presentation)"},{"id":"http://arxiv.org/abs/2504.08385v2","updated":"2025-06-14T16:27:19Z","published":"2025-04-11T09:37:48Z","title":"Scholar Inbox: Personalized Paper Recommendations for Scientists","summary":"  Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/\n","authors":["Markus Flicke","Glenn Angrabeit","Madhav Iyengar","Vitalii Protsenko","Illia Shakun","Jovan Cicvaric","Bora Kargi","Haoyu He","Lukas Schuler","Lewin Scholz","Kavyanjali Agnihotri","Yong Cao","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2504.08385v2.pdf","comment":"https://www.scholar-inbox.com/"},{"id":"http://arxiv.org/abs/2310.19488v3","updated":"2025-06-14T15:45:48Z","published":"2023-10-30T12:25:00Z","title":"CoLLM: Integrating Collaborative Embeddings into Large Language Models\n  for Recommendation","summary":"  Leveraging Large Language Models as Recommenders (LLMRec) has gained\nsignificant attention and introduced fresh perspectives in user preference\nmodeling. Existing LLMRec approaches prioritize text semantics, usually\nneglecting the valuable collaborative information from user-item interactions\nin recommendations. While these text-emphasizing approaches excel in cold-start\nscenarios, they may yield sub-optimal performance in warm-start situations. In\npursuit of superior recommendations for both cold and warm start scenarios, we\nintroduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates\ncollaborative information into LLMs for recommendation. CoLLM captures\ncollaborative information through an external traditional model and maps it to\nthe input token embedding space of LLM, forming collaborative embeddings for\nLLM usage. Through this external integration of collaborative information,\nCoLLM ensures effective modeling of collaborative information without modifying\nthe LLM itself, providing the flexibility to employ various collaborative\ninformation modeling techniques. Extensive experiments validate that CoLLM\nadeptly integrates collaborative information into LLMs, resulting in enhanced\nrecommendation performance. We release the code and data at\nhttps://github.com/zyang1580/CoLLM.\n","authors":["Yang Zhang","Fuli Feng","Jizhi Zhang","Keqin Bao","Qifan Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2310.19488v3.pdf","comment":"Accepted by IEEE TKDE 2025 (add new LLM backbone Qwen2-1.5B and NDCG\n  metric)"},{"id":"http://arxiv.org/abs/2412.18396v3","updated":"2025-06-14T15:20:18Z","published":"2024-12-24T12:39:23Z","title":"Contrastive Representation for Interactive Recommendation","summary":"  Interactive Recommendation (IR) has gained significant attention recently for\nits capability to quickly capture dynamic interest and optimize both short and\nlong term objectives. IR agents are typically implemented through Deep\nReinforcement Learning (DRL), because DRL is inherently compatible with the\ndynamic nature of IR. However, DRL is currently not perfect for IR. Due to the\nlarge action space and sample inefficiency problem, training DRL recommender\nagents is challenging. The key point is that useful features cannot be\nextracted as high-quality representations for the recommender agent to optimize\nits policy. To tackle this problem, we propose Contrastive Representation for\nInteractive Recommendation (CRIR). CRIR efficiently extracts latent, high-level\npreference ranking features from explicit interaction, and leverages the\nfeatures to enhance users' representation. Specifically, the CRIR provides\nrepresentation through one representation network, and refines it through our\nproposed Preference Ranking Contrastive Learning (PRCL). The key insight of\nPRCL is that it can perform contrastive learning without relying on\ncomputations involving high-level representations or large potential action\nsets. Furthermore, we also propose a data exploiting mechanism and an agent\ntraining mechanism to better adapt CRIR to the DRL backbone. Extensive\nexperiments have been carried out to show our method's superior improvement on\nthe sample efficiency while training an DRL-based IR agent.\n","authors":["Jingyu Li","Zhiyong Feng","Dongxiao He","Hongqi Chen","Qinghang Gao","Guoli Wu"],"pdf_url":"https://arxiv.org/pdf/2412.18396v3.pdf","comment":"AAAI-2025 Accepted paper"},{"id":"http://arxiv.org/abs/2506.12494v1","updated":"2025-06-14T13:16:31Z","published":"2025-06-14T13:16:31Z","title":"FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.\n","authors":["Zhuocheng Zhang","Yang Feng","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.12494v1.pdf","comment":"Accepted by ACL 2025 Demo"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.12573v1","updated":"2025-06-14T16:58:42Z","published":"2025-06-14T16:58:42Z","title":"Video-Guided Text-to-Music Generation Using Public Domain Movie\n  Collections","summary":"  Despite recent advancements in music generation systems, their application in\nfilm production remains limited, as they struggle to capture the nuances of\nreal-world filmmaking, where filmmakers consider multiple factors-such as\nvisual content, dialogue, and emotional tone-when selecting or composing music\nfor a scene. This limitation primarily stems from the absence of comprehensive\ndatasets that integrate these elements. To address this gap, we introduce Open\nScreen Sound Library (OSSL), a dataset consisting of movie clips from public\ndomain films, totaling approximately 36.5 hours, paired with high-quality\nsoundtracks and human-annotated mood information. To demonstrate the\neffectiveness of our dataset in improving the performance of pre-trained models\non film music generation tasks, we introduce a new video adapter that enhances\nan autoregressive transformer-based text-to-music model by adding video-based\nconditioning. Our experimental results demonstrate that our proposed approach\neffectively enhances MusicGen-Medium in terms of both objective measures of\ndistributional and paired fidelity, and subjective compatibility in mood and\ngenre. The dataset and code are available at\nhttps://havenpersona.github.io/ossl-v1.\n","authors":["Haven Kim","Zachary Novack","Weihan Xu","Julian McAuley","Hao-Wen Dong"],"pdf_url":"https://arxiv.org/pdf/2506.12573v1.pdf","comment":"ISMIR 2025 regular paper. Dataset and code available at\n  https://havenpersona.github.io/ossl-v1"}]},"2025-06-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.08679v4","updated":"2025-06-17T17:59:57Z","published":"2025-03-11T17:56:30Z","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful when models face an explicit bias in their prompts, i.e., the\nCoT can give an incorrect picture of how models arrive at conclusions. We go\nfurther and show that unfaithful CoT can also occur on realistic prompts with\nno artificial bias. We find that when separately presented with the questions\n\"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce\nsuperficially coherent arguments to justify systematically answering Yes to\nboth questions or No to both questions, despite such responses being logically\ncontradictory. We show preliminary evidence that this is due to models'\nimplicit biases towards Yes or No, thus labeling this unfaithfulness as\nImplicit Post-Hoc Rationalization. Our results reveal that several production\nmodels exhibit surprisingly high rates of post-hoc rationalization in our\nsettings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more\nfaithful, especially thinking ones, none are entirely faithful: Gemini 2.5\nFlash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%),\nand Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical\nShortcuts, where models use subtly illogical reasoning to try to make a\nspeculative answer to hard maths problems seem rigorously proven. Our findings\nraise challenges for strategies for detecting undesired behavior in LLMs via\nthe chain of thought.\n","authors":["Iván Arcuschin","Jett Janiak","Robert Krzyzanowski","Senthooran Rajamanoharan","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2503.08679v4.pdf","comment":"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages"},{"id":"http://arxiv.org/abs/2506.14767v1","updated":"2025-06-17T17:58:17Z","published":"2025-06-17T17:58:17Z","title":"A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models","summary":"  The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.\n","authors":["Li-Wei Chen","Takuya Higuchi","Zakaria Aldeneh","Ahmed Hussen Abdelaziz","Alexander Rudnicky"],"pdf_url":"https://arxiv.org/pdf/2506.14767v1.pdf","comment":"International Conference on Machine Learning (ICML) 2025"},{"id":"http://arxiv.org/abs/2506.14766v1","updated":"2025-06-17T17:58:11Z","published":"2025-06-17T17:58:11Z","title":"ASCD: Attention-Steerable Contrastive Decoding for Reducing\n  Hallucination in MLLM","summary":"  Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks.\n","authors":["Yujun Wang","Jinhe Bi","Yunpu Ma","Soeren Pirk"],"pdf_url":"https://arxiv.org/pdf/2506.14766v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.14761v1","updated":"2025-06-17T17:55:11Z","published":"2025-06-17T17:55:11Z","title":"From Bytes to Ideas: Language Modeling with Autoregressive U-Nets","summary":"  Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.\n","authors":["Mathurin Videau","Badr Youbi Idrissi","Alessandro Leite","Marc Schoenauer","Olivier Teytaud","David Lopez-Paz"],"pdf_url":"https://arxiv.org/pdf/2506.14761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14758v1","updated":"2025-06-17T17:54:03Z","published":"2025-06-17T17:54:03Z","title":"Reasoning with Exploration: An Entropy Perspective","summary":"  Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.\n","authors":["Daixuan Cheng","Shaohan Huang","Xuekai Zhu","Bo Dai","Wayne Xin Zhao","Zhenliang Zhang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05674v3","updated":"2025-06-17T17:53:30Z","published":"2024-07-08T07:17:40Z","title":"Controllable and Reliable Knowledge-Intensive Task-Oriented\n  Conversational Agents with Declarative Genie Worksheets","summary":"  Large Language Models can carry out human-like conversations in diverse\nsettings, responding to user requests for tasks and knowledge. However,\nexisting conversational agents implemented with LLMs often struggle with\nhallucination, following instructions with conditional logic, and integrating\nknowledge from different sources. These shortcomings compromise the agents'\neffectiveness, rendering them unsuitable for deployment. To address these\nchallenges, we introduce Genie, a programmable framework for creating\nknowledge-intensive task-oriented conversational agents. Genie can handle\ninvolved interactions and answer complex queries. Unlike LLMs, it delivers\nreliable, grounded responses through advanced dialogue state management and\nsupports controllable agent policies via its declarative specification -- Genie\nWorksheet. This is achieved through an algorithmic runtime system that\nimplements the developer-supplied policy, limiting LLMs to (1) parse user input\nusing a succinct conversational history, and (2) generate responses according\nto supplied context. Agents built with Genie outperform SOTA methods on complex\nlogic dialogue datasets. We conducted a user study with 62 participants on\nthree real-life applications: restaurant reservations with Yelp, as well as\nticket submission and course enrollment for university students. Genie agents\nwith GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling,\nimproving goal completion rates from 21.8% to 82.8% across three real-world\ntasks.\n","authors":["Harshit Joshi","Shicheng Liu","James Chen","Robert Weigle","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2407.05674v3.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2503.08669v2","updated":"2025-06-17T17:50:44Z","published":"2025-03-11T17:53:02Z","title":"SOPBench: Evaluating Language Agents at Following Standard Operating\n  Procedures and Constraints","summary":"  As language agents increasingly automate critical tasks, their ability to\nfollow domain-specific standard operating procedures (SOPs), policies, and\nconstraints when taking actions and making tool calls becomes essential yet\nremains underexplored. To address this gap, we develop an automated evaluation\npipeline SOPBench with: (1) executable environments containing 167\ntools/functions across seven customer service domains with service-specific\nSOPs and rule-based verifiers, (2) an automated test generation framework\nproducing over 900 verified test cases, and (3) an automated evaluation\nframework to rigorously assess agent adherence from multiple dimensions. Our\napproach transforms each service-specific SOP code program into a directed\ngraph of executable functions and requires agents to call these functions based\non natural language SOP descriptions. The original code serves as oracle\nrule-based verifiers to assess compliance, reducing reliance on manual\nannotations and LLM-based evaluations. We evaluate 18 leading models, and\nresults show the task is challenging even for top-tier models (like GPT-4o,\nClaude-3.7-Sonnet), with variances across domains. Reasoning models like\no4-mini-high show superiority while other powerful models perform less\neffectively (pass rates of 30%-50%), and small models (7B, 8B) perform\nsignificantly worse. Additionally, language agents can be easily jailbroken to\noverlook SOPs and constraints. Code, data, and over 24k agent trajectories are\nreleased at https://github.com/Leezekun/SOPBench.\n","authors":["Zekun Li","Shinda Huang","Jiangtian Wang","Nathan Zhang","Antonis Antoniades","Wenyue Hua","Kaijie Zhu","Sirui Zeng","Chi Wang","William Yang Wang","Xifeng Yan"],"pdf_url":"https://arxiv.org/pdf/2503.08669v2.pdf","comment":"Code, data, and over 24k agent trajectories are released at\n  https://github.com/Leezekun/SOPBench"},{"id":"http://arxiv.org/abs/2506.14755v1","updated":"2025-06-17T17:50:16Z","published":"2025-06-17T17:50:16Z","title":"Optimizing Length Compression in Large Reasoning Models","summary":"  Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.\n","authors":["Zhengxiang Cheng","Dongping Chen","Mingyang Fu","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.14755v1.pdf","comment":"16 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.18653v3","updated":"2025-06-17T17:41:07Z","published":"2024-10-24T11:32:01Z","title":"Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework","summary":"  Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging due to trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. This paper addresses the specific problem\nof multicriteria evaluation for open-ended text generation, proposing novel\nmethods for both relative and absolute rankings of decoding methods.\nSpecifically, we employ benchmarking approaches based on partial orderings and\npresent a new summary metric to balance existing automatic indicators,\nproviding a more holistic evaluation of text generation quality. Our\nexperiments demonstrate that the proposed approaches offer a robust way to\ncompare decoding strategies and serve as valuable tools to guide model\nselection for open-ended text generation tasks. We suggest future directions\nfor improving evaluation methodologies in text generation and make our code,\ndatasets, and models publicly available.\n","authors":["Esteban Garces Arias","Hannah Blocher","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2410.18653v3.pdf","comment":"Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)"},{"id":"http://arxiv.org/abs/2506.14731v1","updated":"2025-06-17T17:12:34Z","published":"2025-06-17T17:12:34Z","title":"Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs","summary":"  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.\n","authors":[" Ring Team","Bin Hu","Cai Chen","Deng Zhao","Ding Liu","Dingnan Jin","Feng Zhu","Hao Dai","Hongzhi Luan","Jia Guo","Jiaming Liu","Jiewei Wu","Jun Mei","Jun Zhou","Junbo Zhao","Junwu Xiong","Kaihong Zhang","Kuan Xu","Lei Liang","Liang Jiang","Liangcheng Fu","Longfei Zheng","Qiang Gao","Qing Cui","Quan Wan","Shaomian Zheng","Shuaicheng Li","Tongkai Yang","Wang Ren","Xiaodong Yan","Xiaopei Wan","Xiaoyun Feng","Xin Zhao","Xinxing Yang","Xinyu Kong","Xuemin Yang","Yang Li","Yingting Wu","Yongkang Liu","Zhankai Xu","Zhenduo Zhang","Zhenglei Zhou","Zhenyu Huang","Zhiqiang Zhang","Zihao Wang","Zujie Wen"],"pdf_url":"https://arxiv.org/pdf/2506.14731v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2406.13677v3","updated":"2025-06-17T17:06:35Z","published":"2024-06-19T16:30:58Z","title":"Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora","summary":"  Large language models (LLMs) often inherit and amplify social biases embedded\nin their training data. A prominent social bias is gender bias. In this regard,\nprior work has mainly focused on gender stereotyping bias - the association of\nspecific roles or traits with a particular gender - in English and on\nevaluating gender bias in model embeddings or generated outputs. In contrast,\ngender representation bias - the unequal frequency of references to individuals\nof different genders - in the training corpora has received less attention. Yet\nsuch imbalances in the training data constitute an upstream source of bias that\ncan propagate and intensify throughout the entire model lifecycle. To fill this\ngap, we propose a novel LLM-based method to detect and quantify gender\nrepresentation bias in LLM training data in gendered languages, where\ngrammatical gender challenges the applicability of methods developed for\nEnglish. By leveraging the LLMs' contextual understanding, our approach\nautomatically identifies and classifies person-referencing words in gendered\nlanguage corpora. Applied to four Spanish-English benchmarks and five Valencian\ncorpora, our method reveals substantial male-dominant imbalances. We show that\nsuch biases in training data affect model outputs, but can surprisingly be\nmitigated leveraging small-scale training on datasets that are biased towards\nthe opposite gender. Our findings highlight the need for corpus-level gender\nbias analysis in multilingual NLP. We make our code and data publicly\navailable.\n","authors":["Erik Derner","Sara Sansalvador de la Fuente","Yoan Gutiérrez","Paloma Moreda","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2406.13677v3.pdf","comment":"Accepted for presentation at the 6th Workshop on Gender Bias in\n  Natural Language Processing (GeBNLP) at ACL 2025"},{"id":"http://arxiv.org/abs/2402.10735v4","updated":"2025-06-17T17:05:26Z","published":"2024-02-16T14:52:05Z","title":"Assessing the Reasoning Capabilities of LLMs in the context of\n  Evidence-based Claim Verification","summary":"  Although LLMs have shown great performance on Mathematics and Coding related\nreasoning tasks, the reasoning capabilities of LLMs regarding other forms of\nreasoning are still an open problem. Here, we examine the issue of reasoning\nfrom the perspective of claim verification. We propose a framework designed to\nbreak down any claim paired with evidence into atomic reasoning types that are\nnecessary for verification. We use this framework to create RECV, the first\nclaim verification benchmark, incorporating real-world claims, to assess the\ndeductive and abductive reasoning capabilities of LLMs. The benchmark comprises\nof three datasets, covering reasoning problems of increasing complexity. We\nevaluate three state-of-the-art proprietary LLMs under multiple prompt\nsettings. Our results show that while LLMs can address deductive reasoning\nproblems, they consistently fail in cases of abductive reasoning. Moreover, we\nobserve that enhancing LLMs with rationale generation is not always beneficial.\nNonetheless, we find that generated rationales are semantically similar to\nthose provided by humans, especially in deductive reasoning cases.\n","authors":["John Dougrez-Lewis","Mahmud Elahi Akhter","Federico Ruggeri","Sebastian Löbbers","Yulan He","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2402.10735v4.pdf","comment":"First two authors contributed equally to this work. 25 pages, 3\n  figure"},{"id":"http://arxiv.org/abs/2506.08001v3","updated":"2025-06-17T16:44:36Z","published":"2025-06-09T17:59:34Z","title":"Reparameterized LLM Training via Orthogonal Equivalence Transformation","summary":"  While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.\n","authors":["Zeju Qiu","Simon Buchholz","Tim Z. Xiao","Maximilian Dax","Bernhard Schölkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.08001v3.pdf","comment":"Technical report v3 (38 pages, 26 figures, project page:\n  https://spherelab.ai/poet/, v3: added singular spectrum and energy analyses\n  in Section 4)"},{"id":"http://arxiv.org/abs/2506.14704v1","updated":"2025-06-17T16:42:54Z","published":"2025-06-17T16:42:54Z","title":"Capacity Matters: a Proof-of-Concept for Transformer Memorization on\n  Real-World Data","summary":"  This paper studies how the model architecture and data configurations\ninfluence the empirical memorization capacity of generative transformers. The\nmodels are trained using synthetic text datasets derived from the Systematized\nNomenclature of Medicine (SNOMED) knowledge graph: triplets, representing\nstatic connections, and sequences, simulating complex relation patterns. The\nresults show that embedding size is the primary determinant of learning speed\nand capacity, while additional layers provide limited benefits and may hinder\nperformance on simpler datasets. Activation functions play a crucial role, and\nSoftmax demonstrates greater stability and capacity. Furthermore, increasing\nthe complexity of the data set seems to improve the final memorization. These\ninsights improve our understanding of transformer memory mechanisms and provide\na framework for optimizing model design with structured real-world data.\n","authors":["Anton Changalidis","Aki Härmä"],"pdf_url":"https://arxiv.org/pdf/2506.14704v1.pdf","comment":"This work has been accepted for publication at the First Workshop on\n  Large Language Model Memorization (L2M2) at ACL 2025, Vienna, Austria"},{"id":"http://arxiv.org/abs/2506.14702v1","updated":"2025-06-17T16:40:42Z","published":"2025-06-17T16:40:42Z","title":"Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers","summary":"  One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.\n","authors":["Daniel D'souza","Julia Kreutzer","Adrien Morisot","Ahmet Üstün","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2506.14702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11423v4","updated":"2025-06-17T16:39:08Z","published":"2024-06-17T11:22:04Z","title":"Bridging Social Media and Search Engines: Dredge Words and the Detection\n  of Unreliable Domains","summary":"  Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.\n","authors":["Evan M. Williams","Peter Carragher","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2406.11423v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10970v3","updated":"2025-06-17T16:38:16Z","published":"2025-01-19T07:09:11Z","title":"The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs","summary":"  The \"LLM-as-an-annotator\" and \"LLM-as-a-judge\" paradigms employ Large\nLanguage Models (LLMs) as annotators, judges, and evaluators in tasks\ntraditionally performed by humans. LLM annotations are widely used, not only in\nNLP research but also in fields like medicine, psychology, and social science.\nDespite their role in shaping study results and insights, there is no standard\nor rigorous procedure to determine whether LLMs can replace human annotators.\nIn this paper, we propose a novel statistical procedure, the Alternative\nAnnotator Test (alt-test), that requires only a modest subset of annotated\nexamples to justify using LLM annotations. Additionally, we introduce a\nversatile and interpretable measure for comparing LLM annotators and judges. To\ndemonstrate our procedure, we curated a diverse collection of ten datasets,\nconsisting of language and vision-language tasks, and conducted experiments\nwith six LLMs and four prompting techniques. Our results show that LLMs can\nsometimes replace humans with closed-source LLMs (such as GPT-4o),\noutperforming the open-source LLMs we examine, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.\n","authors":["Nitay Calderon","Roi Reichart","Rotem Dror"],"pdf_url":"https://arxiv.org/pdf/2501.10970v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05478v2","updated":"2025-06-17T16:28:39Z","published":"2025-01-07T16:01:25Z","title":"Language and Planning in Robotic Navigation: A Multilingual Evaluation\n  of State-of-the-Art Models","summary":"  Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.\n","authors":["Malak Mansour","Ahmed Aly","Bahey Tharwat","Sarim Hashmi","Dong An","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2501.05478v2.pdf","comment":"This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/"},{"id":"http://arxiv.org/abs/2501.04227v2","updated":"2025-06-17T16:19:14Z","published":"2025-01-08T01:58:42Z","title":"Agent Laboratory: Using LLM Agents as Research Assistants","summary":"  Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.\n","authors":["Samuel Schmidgall","Yusheng Su","Ze Wang","Ximeng Sun","Jialian Wu","Xiaodong Yu","Jiang Liu","Michael Moor","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2501.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14681v1","updated":"2025-06-17T16:13:15Z","published":"2025-06-17T16:13:15Z","title":"Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality","summary":"  Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research.\n","authors":["Yuto Harada","Yusuke Yamauchi","Yusuke Oda","Yohei Oseki","Yusuke Miyao","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2506.14681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10867v2","updated":"2025-06-17T16:07:14Z","published":"2023-07-20T13:40:22Z","title":"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with\n  Human Feedback","summary":"  Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.\n","authors":["Ashish Singh","Ashutosh Singh","Prateek Agarwal","Zixuan Huang","Arpita Singh","Tong Yu","Sungchul Kim","Victor Bursztyn","Nesreen K. Ahmed","Puneet Mathur","Erik Learned-Miller","Franck Dernoncourt","Ryan A. Rossi"],"pdf_url":"https://arxiv.org/pdf/2307.10867v2.pdf","comment":"16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/"},{"id":"http://arxiv.org/abs/2506.11681v2","updated":"2025-06-17T15:59:13Z","published":"2025-06-13T11:19:27Z","title":"A Hybrid Multi-Agent Prompting Approach for Simplifying Complex\n  Sentences","summary":"  This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.\n","authors":["Pratibha Zunjare","Michael Hsiao"],"pdf_url":"https://arxiv.org/pdf/2506.11681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06745v2","updated":"2025-06-17T15:57:52Z","published":"2024-12-09T18:37:14Z","title":"ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities","summary":"  Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.\n","authors":["Adhiraj Ghosh","Sebastian Dziadzio","Ameya Prabhu","Vishaal Udandarao","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2412.06745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06987v4","updated":"2025-06-17T15:43:40Z","published":"2025-05-11T14:13:58Z","title":"Convert Language Model into a Value-based Strategic Planner","summary":"  Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.\n","authors":["Xiaoyu Wang","Yue Zhao","Qingqing Gu","Zhonglin Jiang","Xiaokai Chen","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2505.06987v4.pdf","comment":"13 pages, 6 figures, Accepted by ACL 2025 Industry Track"},{"id":"http://arxiv.org/abs/2506.14646v1","updated":"2025-06-17T15:41:33Z","published":"2025-06-17T15:41:33Z","title":"GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel\n  Optimization with GuidedSelection Vectors","summary":"  Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git.\n","authors":["Hengyuan Zhang","Xinrong Chen","Yingmin Qiu","Xiao Liang","Ziyue Li","Guanyu Wang","Weiping Li","Tong Mo","Wenyue Li","Hayden Kwok-Hay So","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2506.14646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14645v1","updated":"2025-06-17T15:41:26Z","published":"2025-06-17T15:41:26Z","title":"Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to\n  Mimic Polarized Social Media Comments","summary":"  The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks.\n","authors":[". Pazzaglia","V. Vendetti","L. D. Comencini","F. Deriu","V. Modugno"],"pdf_url":"https://arxiv.org/pdf/2506.14645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14641v1","updated":"2025-06-17T15:39:33Z","published":"2025-06-17T15:39:33Z","title":"Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot","summary":"  In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.\n","authors":["Xiang Cheng","Chengyan Pan","Minjun Zhao","Deyang Li","Fangchao Liu","Xinyu Zhang","Xiao Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14641v1.pdf","comment":"19 pages,22 figures"},{"id":"http://arxiv.org/abs/2505.12442v3","updated":"2025-06-17T15:37:58Z","published":"2025-05-18T14:31:45Z","title":"IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems","summary":"  The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.\n","authors":["Liwen Wang","Wenxuan Wang","Shuai Wang","Zongjie Li","Zhenlan Ji","Zongyi Lyu","Daoyuan Wu","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2505.12442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14986v3","updated":"2025-06-17T15:36:53Z","published":"2024-06-21T08:56:35Z","title":"Do Large Language Models Exhibit Cognitive Dissonance? Studying the\n  Difference Between Revealed Beliefs and Stated Answers","summary":"  Multiple Choice Questions (MCQ) have become a commonly used approach to\nassess the capabilities of Large Language Models (LLMs), due to their ease of\nmanipulation and evaluation. The experimental appraisals of the LLMs' Stated\nAnswer (their answer to MCQ) have pointed to their apparent ability to perform\nprobabilistic reasoning or to grasp uncertainty. In this work, we investigate\nwhether these aptitudes are measurable outside tailored prompting and MCQ by\nreformulating these issues as direct text-completion - the fundamental\ncomputational unit of LLMs. We introduce Revealed Belief, an evaluation\nframework that evaluates LLMs on tasks requiring reasoning under uncertainty,\nwhich complements MCQ scoring by analyzing text-completion probability\ndistributions. Our findings suggest that while LLMs frequently state the\ncorrect answer, their Revealed Belief shows that they often allocate\nprobability mass inconsistently, exhibit systematic biases, and often fail to\nupdate their beliefs appropriately when presented with new evidence, leading to\nstrong potential impacts on downstream tasks. These results suggest that common\nevaluation methods may only provide a partial picture and that more research is\nneeded to assess the extent and nature of their capabilities.\n","authors":["Manuel Mondal","Ljiljana Dolamic","Gérôme Bovet","Philippe Cudré-Mauroux","Julien Audiffren"],"pdf_url":"https://arxiv.org/pdf/2406.14986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14634v1","updated":"2025-06-17T15:28:53Z","published":"2025-06-17T15:28:53Z","title":"AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation","summary":"  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n","authors":["Leah von der Heyde","Anna-Carolina Haensch","Bernd Weiß","Jessika Daikeler"],"pdf_url":"https://arxiv.org/pdf/2506.14634v1.pdf","comment":"to appear in Survey Research Methods"},{"id":"http://arxiv.org/abs/2506.13674v2","updated":"2025-06-17T15:25:25Z","published":"2025-06-16T16:30:26Z","title":"Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from\n  Attention","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for\nrapidly adapting large language models (LLMs) to downstream tasks.\nPrefix-Tuning, an early and effective PEFT technique, demonstrated the ability\nto achieve performance comparable to full fine-tuning with significantly\nreduced computational and memory overhead. However, despite its earlier\nsuccess, its effectiveness in training modern state-of-the-art LLMs has been\nvery limited. In this work, we demonstrate empirically that Prefix-Tuning\nunderperforms on LLMs because of an inherent tradeoff between input and prefix\nsignificance within the attention head. This motivates us to introduce\nPrefix-Tuning+, a novel architecture that generalizes the principles of\nPrefix-Tuning while addressing its shortcomings by shifting the prefix module\nout of the attention head itself. We further provide an overview of our\nconstruction process to guide future users when constructing their own\ncontext-based methods. Our experiments show that, across a diverse set of\nbenchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning\nmethods. Notably, it achieves performance on par with the widely adopted LoRA\nmethod on several general benchmarks, highlighting the potential modern\nextension of Prefix-Tuning approaches. Our findings suggest that by overcoming\nits inherent limitations, Prefix-Tuning can remain a competitive and relevant\nresearch direction in the landscape of parameter-efficient LLM adaptation.\n","authors":["Haonan Wang","Brian Chen","Siquan Li","Xinhe Liang","Hwee Kuan Lee","Kenji Kawaguchi","Tianyang Hu"],"pdf_url":"https://arxiv.org/pdf/2506.13674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14629v1","updated":"2025-06-17T15:24:30Z","published":"2025-06-17T15:24:30Z","title":"VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning","summary":"  Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito\n","authors":["Md. Adnanul Islam","Md. Faiyaz Abdullah Sayeedi","Md. Asaduzzaman Shuvo","Muhammad Ziaur Rahman","Shahanur Rahman Bappy","Raiyan Rahman","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2506.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04619v2","updated":"2025-06-17T15:22:43Z","published":"2025-03-06T17:05:33Z","title":"SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling","summary":"  User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.\n","authors":["Xin Zhang","Qiyu Wei","Yingjie Zhu","Linhai Zhang","Deyu Zhou","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2503.04619v2.pdf","comment":"Accepted at ACL 2025"},{"id":"http://arxiv.org/abs/2506.14625v1","updated":"2025-06-17T15:22:21Z","published":"2025-06-17T15:22:21Z","title":"Probabilistic Aggregation and Targeted Embedding Optimization for\n  Collective Moral Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.\n","authors":["Chenchen Yuan","Zheyu Zhang","Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2506.14625v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2506.10055v2","updated":"2025-06-17T15:19:26Z","published":"2025-06-11T17:58:14Z","title":"TaskCraft: Automated Generation of Agentic Tasks","summary":"  Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.\n","authors":["Dingfeng Shi","Jingyi Cao","Qianben Chen","Weichen Sun","Weizhen Li","Hongxuan Lu","Fangchen Dong","Tianrui Qin","King Zhu","Minghao Liu","Jian Yang","Ge Zhang","Jiaheng Liu","Changwang Zhang","Jun Wang","Yuchen Eleanor Jiang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.10055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00039v3","updated":"2025-06-17T15:18:57Z","published":"2025-04-29T18:36:57Z","title":"Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic\n  Approach","summary":"  This article proposes an adaptation of Graph Retrieval-Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms. Legal texts are characterized by a predefined hierarchical structure, an\nextensive network of references and a continuous evolution through multiple\ntemporal versions. This temporal dynamism poses a significant challenge for\nstandard AI systems, demanding a deterministic representation of the law at any\ngiven point in time. To address this, our approach grounds the knowledge graph\nconstruction in a formal, FRBRoo-inspired model that distinguishes abstract\nlegal works from their concrete textual expressions. We introduce a\nmulti-layered representation of Temporal Versions (capturing date-specific\nchanges) and Language Versions (capturing linguistic variations). By modeling\nnormative evolution as a precise sequence of these versioned entities, we\nenable the construction of a knowledge graph that serves as a verifiable\n\"ground truth\". This allows Large Language Models to generate responses based\non accurate, context-aware, and point-in-time correct legal information,\novercoming the risk of temporal inaccuracies. Through a detailed analysis of\nthis formal Graph RAG approach and its application to legal norm datasets, this\narticle aims to advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective and reliable systems in legal\nresearch, legislative analysis, and decision support.\n","authors":["Hudson de Martim"],"pdf_url":"https://arxiv.org/pdf/2505.00039v3.pdf","comment":"This version enhances the theoretical underpinnings of the proposed\n  Graph RAG methodology, including the introduction of a formal, FRBRoo-based\n  model for versioning, and enabling multi-language support for both content\n  and metadata"},{"id":"http://arxiv.org/abs/2506.14613v1","updated":"2025-06-17T15:12:54Z","published":"2025-06-17T15:12:54Z","title":"When Does Meaning Backfire? Investigating the Role of AMRs in NLI","summary":"  Natural Language Inference (NLI) relies heavily on adequately parsing the\nsemantic content of the premise and hypothesis. In this work, we investigate\nwhether adding semantic information in the form of an Abstract Meaning\nRepresentation (AMR) helps pretrained language models better generalize in NLI.\nOur experiments integrating AMR into NLI in both fine-tuning and prompting\nsettings show that the presence of AMR in fine-tuning hinders model\ngeneralization while prompting with AMR leads to slight gains in\n\\texttt{GPT-4o}. However, an ablation study reveals that the improvement comes\nfrom amplifying surface-level differences rather than aiding semantic\nreasoning. This amplification can mislead models to predict non-entailment even\nwhen the core meaning is preserved.\n","authors":["Junghyun Min","Xiulin Yang","Shira Wein"],"pdf_url":"https://arxiv.org/pdf/2506.14613v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.14606v1","updated":"2025-06-17T15:06:54Z","published":"2025-06-17T15:06:54Z","title":"Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees","summary":"  The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.\n","authors":["Ahmed Heakl","Sarim Hashmi","Chaimaa Abi","Celine Lee","Abdulrahman Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2506.14606v1.pdf","comment":"Project page: https://ahmedheakl.github.io/Guaranteed-Guess/"},{"id":"http://arxiv.org/abs/2410.05243v3","updated":"2025-06-17T15:06:02Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly perform\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v3.pdf","comment":"Accepted to ICLR 2025 (Oral). Project Homepage:\n  https://osu-nlp-group.github.io/UGround/"},{"id":"http://arxiv.org/abs/2506.14602v1","updated":"2025-06-17T15:05:57Z","published":"2025-06-17T15:05:57Z","title":"Computational Studies in Influencer Marketing: A Systematic Literature\n  Review","summary":"  Influencer marketing has become a crucial feature of digital marketing\nstrategies. Despite its rapid growth and algorithmic relevance, the field of\ncomputational studies in influencer marketing remains fragmented, especially\nwith limited systematic reviews covering the computational methodologies\nemployed. This makes overarching scientific measurements in the influencer\neconomy very scarce, to the detriment of interested stakeholders outside of\nplatforms themselves, such as regulators, but also researchers from other\nfields. This paper aims to provide an overview of the state of the art of\ncomputational studies in influencer marketing by conducting a systematic\nliterature review (SLR) based on the PRISMA model. The paper analyses 69\nstudies to identify key research themes, methodologies, and future directions\nin this research field. The review identifies four major research themes:\nInfluencer identification and characterisation, Advertising strategies and\nengagement, Sponsored content analysis and discovery, and Fairness.\nMethodologically, the studies are categorised into machine learning-based\ntechniques (e.g., classification, clustering) and non-machine-learning-based\ntechniques (e.g., statistical analysis, network analysis). Key findings reveal\na strong focus on optimising commercial outcomes, with limited attention to\nregulatory compliance and ethical considerations. The review highlights the\nneed for more nuanced computational research that incorporates contextual\nfactors such as language, platform, and industry type, as well as improved\nmodel explainability and dataset reproducibility. The paper concludes by\nproposing a multidisciplinary research agenda that emphasises the need for\nfurther links to regulation and compliance technology, finer granularity in\nanalysis, and the development of standardised datasets.\n","authors":["Haoyang Gui","Thales Bertaglia","Catalina Goanta","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2506.14602v1.pdf","comment":"journal submission, under review"},{"id":"http://arxiv.org/abs/2501.18045v3","updated":"2025-06-17T14:46:20Z","published":"2025-01-29T23:17:43Z","title":"From tools to thieves: Measuring and understanding public perceptions of\n  AI through crowdsourced metaphors","summary":"  How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development.\n","authors":["Myra Cheng","Angela Y. Lee","Kristina Rapuano","Kate Niederhoffer","Alex Liebscher","Jeffrey Hancock"],"pdf_url":"https://arxiv.org/pdf/2501.18045v3.pdf","comment":"To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025"},{"id":"http://arxiv.org/abs/2506.14580v1","updated":"2025-06-17T14:37:09Z","published":"2025-06-17T14:37:09Z","title":"GenerationPrograms: Fine-grained Attribution with Executable Programs","summary":"  Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality.\n","authors":["David Wan","Eran Hirsch","Elias Stengel-Eskin","Ido Dagan","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.14580v1.pdf","comment":"27 Pages. Code: https://github.com/meetdavidwan/generationprograms"},{"id":"http://arxiv.org/abs/2502.14445v2","updated":"2025-06-17T14:34:13Z","published":"2025-02-20T10:52:38Z","title":"PredictaBoard: Benchmarking LLM Score Predictability","summary":"  Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard\n","authors":["Lorenzo Pacchiardi","Konstantinos Voudouris","Ben Slater","Fernando Martínez-Plumed","José Hernández-Orallo","Lexin Zhou","Wout Schellaert"],"pdf_url":"https://arxiv.org/pdf/2502.14445v2.pdf","comment":"Accepted at ACL Findings 2025"},{"id":"http://arxiv.org/abs/2308.12420v4","updated":"2025-06-17T14:32:54Z","published":"2023-08-23T20:42:32Z","title":"Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature","summary":"  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,\nparticularly concerning the energy consumption of the Proof of Work (PoW)\nconsensus mechanism and broader Environmental, Social, and Governance (ESG)\nissues. However, existing systematic literature reviews of DLT rely on limited\nanalyses of citations, abstracts, and keywords, failing to fully capture the\nfield's complexity and ESG concerns. We address these challenges by analyzing\nthe full text of 24,539 publications using Natural Language Processing (NLP)\nwith our manually labeled Named Entity Recognition (NER) dataset of 39,427\nentities for DLT. This methodology identified 505 key publications at the\nDLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP\nand temporal graph analysis reveals critical trends in DLT evolution and ESG\nimpacts, including cryptography and peer-to-peer networks research's\nfoundational influence, Bitcoin's persistent impact on research and\nenvironmental concerns (a \"Lindy effect\"), Ethereum's catalytic role on Proof\nof Stake (PoS) and smart contract adoption, and the industry's progressive\nshift toward energy-efficient consensus mechanisms. Our contributions include\nthe first DLT-specific NER dataset addressing the scarcity of high-quality\nlabeled NLP data in blockchain research, a methodology integrating NLP and\ntemporal graph analysis for large-scale interdisciplinary literature reviews,\nand the first NLP-driven literature review focusing on DLT's ESG aspects.\n","authors":["Walter Hernandez Cruz","Kamil Tylinski","Alastair Moore","Niall Roche","Nikhil Vadgama","Horst Treiblmaier","Jiangbo Shangguan","Paolo Tasca","Jiahua Xu"],"pdf_url":"https://arxiv.org/pdf/2308.12420v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14574v1","updated":"2025-06-17T14:30:06Z","published":"2025-06-17T14:30:06Z","title":"TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct\n  Preference Optimization","summary":"  Recent advancements in reinforcement learning from human feedback have shown\nthat utilizing fine-grained token-level reward models can substantially enhance\nthe performance of Proximal Policy Optimization (PPO) in aligning large\nlanguage models. However, it is challenging to leverage such token-level reward\nas guidance for Direct Preference Optimization (DPO), since DPO is formulated\nas a sequence-level bandit problem. To address this challenge, this work\ndecomposes the sequence-level PPO into a sequence of token-level proximal\npolicy optimization problems and then frames the problem of token-level PPO\nwith token-level reward guidance, from which closed-form optimal token-level\npolicy and the corresponding token-level reward can be derived. Using the\nobtained reward and Bradley-Terry model, this work establishes a framework of\ncomputable loss functions with token-level reward guidance for DPO, and\nproposes a practical reward guidance based on the induced DPO reward. This\nformulation enables different tokens to exhibit varying degrees of deviation\nfrom reference policy based on their respective rewards. Experiment results\ndemonstrate that our method achieves substantial performance improvements over\nDPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on\nAlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at\nhttps://github.com/dvlab-research/TGDPO.\n","authors":["Mingkang Zhu","Xi Chen","Zhongdao Wang","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2506.14574v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.14562v1","updated":"2025-06-17T14:21:10Z","published":"2025-06-17T14:21:10Z","title":"AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs","summary":"  Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines.\n","authors":["Di He","Ajay Jaiswal","Songjun Tu","Li Shen","Ganzhao Yuan","Shiwei Liu","Lu Yin"],"pdf_url":"https://arxiv.org/pdf/2506.14562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14533v2","updated":"2025-06-17T14:18:09Z","published":"2024-12-19T05:11:16Z","title":"ClusterChat: Multi-Feature Search for Corpus Exploration","summary":"  Exploring large-scale text corpora presents a significant challenge in\nbiomedical, finance, and legal domains, where vast amounts of documents are\ncontinuously published. Traditional search methods, such as keyword-based\nsearch, often retrieve documents in isolation, limiting the user's ability to\neasily inspect corpus-wide trends and relationships. We present ClusterChat\n(The demo video and source code are available at:\nhttps://github.com/achouhan93/ClusterChat), an open-source system for corpus\nexploration that integrates cluster-based organization of documents using\ntextual embeddings with lexical and semantic search, timeline-driven\nexploration, and corpus and document-level question answering (QA) as\nmulti-feature search capabilities. We validate the system with two case studies\non a four million abstract PubMed dataset, demonstrating that ClusterChat\nenhances corpus exploration by delivering context-aware insights while\nmaintaining scalability and responsiveness on large-scale document collections.\n","authors":["Ashish Chouhan","Saifeldin Mandour","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2412.14533v2.pdf","comment":"5 pages, 1 table, 1 figure, Accepted to SIGIR Demo Paper Track 2025"},{"id":"http://arxiv.org/abs/2506.14532v1","updated":"2025-06-17T13:58:36Z","published":"2025-06-17T13:58:36Z","title":"M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with\n  Large Language Models","summary":"  This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems.\n","authors":["Can Zheng","Jiguang He","Chung G. Kang","Guofa Cai","Zitong Yu","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2506.14532v1.pdf","comment":"13 pages, 20 figures"},{"id":"http://arxiv.org/abs/2504.03255v2","updated":"2025-06-17T13:42:52Z","published":"2025-04-04T08:10:02Z","title":"Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective","summary":"  Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention to effective governance policies,\nmonitoring, and control protocols. Based on the emerging landscape of the\nagentic market, we analyze potential liability issues arising from the\ndelegated use of LLM agents and their extended systems through a\nprincipal-agent perspective. Our analysis complements existing risk-based\nstudies on artificial agency and covers the spectrum of important aspects of\nthe principal-agent relationship and their potential consequences at\ndeployment. Furthermore, we motivate method developments for technical\ngovernance along the directions of interpretability and behavior evaluations,\nreward and conflict management, and the mitigation of misalignment and\nmisconduct through principled engineering of detection and fail-safe\nmechanisms. By illustrating the outstanding issues in AI liability for\nLLM-based agentic systems, we aim to inform the system design, auditing, and\ntracing to enhance transparency and liability attribution.\n","authors":["Garry A. Gabison","R. Patrick Xian"],"pdf_url":"https://arxiv.org/pdf/2504.03255v2.pdf","comment":"22 pages (incl. appendix), accepted at REALM workshop, ACL2025"},{"id":"http://arxiv.org/abs/2506.14493v1","updated":"2025-06-17T13:14:55Z","published":"2025-06-17T13:14:55Z","title":"LingoLoop Attack: Trapping MLLMs via Linguistic Context and State\n  Entrapment into Endless Loops","summary":"  Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance.\n","authors":["Jiyuan Fu","Kaixun Jiang","Lingyi Hong","Jinglun Li","Haijing Guo","Dingkang Yang","Zhaoyu Chen","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04726v3","updated":"2025-06-17T13:04:28Z","published":"2024-12-06T02:34:40Z","title":"BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for\n  Varieties of English","summary":"  Despite large language models (LLMs) being known to exhibit bias against\nnon-standard language varieties, there are no known labelled datasets for\nsentiment analysis of English. To address this gap, we introduce BESSTIE, a\nbenchmark for sentiment and sarcasm classification for three varieties of\nEnglish: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect\ndatasets for these language varieties using two methods: location-based for\nGoogle Places reviews, and topic-based filtering for Reddit comments. To assess\nwhether the dataset accurately represents these varieties, we conduct two\nvalidation steps: (a) manual annotation of language varieties and (b) automatic\nlanguage variety prediction. Native speakers of the language varieties manually\nannotate the datasets with sentiment and sarcasm labels. We perform an\nadditional annotation exercise to validate the reliance of the annotated\nlabels. Subsequently, we fine-tune nine LLMs (representing a range of\nencoder/decoder and mono/multilingual models) on these datasets, and evaluate\ntheir performance on the two tasks. Our results show that the models\nconsistently perform better on inner-circle varieties (i.e., en-AU and en-UK),\nin comparison with en-IN, particularly for sarcasm classification. We also\nreport challenges in cross-variety generalisation, highlighting the need for\nlanguage variety-specific datasets such as ours. BESSTIE promises to be a\nuseful evaluative benchmark for future research in equitable LLMs, specifically\nin terms of language varieties. The BESSTIE dataset is publicly available at:\nhttps://huggingface.co/ datasets/unswnlporg/BESSTIE.\n","authors":["Dipankar Srirag","Aditya Joshi","Jordan Painter","Diptesh Kanojia"],"pdf_url":"https://arxiv.org/pdf/2412.04726v3.pdf","comment":"Findings of ACL: ACL 2025"},{"id":"http://arxiv.org/abs/2506.14474v1","updated":"2025-06-17T12:41:53Z","published":"2025-06-17T12:41:53Z","title":"LexiMark: Robust Watermarking via Lexical Substitutions to Enhance\n  Membership Verification of an LLM's Textual Training Data","summary":"  Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training.\n","authors":["Eyal German","Sagiv Antebi","Edan Habler","Asaf Shabtai","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2506.14474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20620v2","updated":"2025-06-17T12:26:49Z","published":"2025-02-28T00:57:45Z","title":"Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning","summary":"  Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.\n","authors":["Ayana Niwa","Masahiro Kaneko","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2502.20620v2.pdf","comment":"Accepted at ACL2025 Findings (long)"},{"id":"http://arxiv.org/abs/2506.14448v1","updated":"2025-06-17T12:13:56Z","published":"2025-06-17T12:13:56Z","title":"How Far Can LLMs Improve from Experience? Measuring Test-Time Learning\n  Ability in LLMs with Human Comparison","summary":"  As evaluation designs of large language models may shape our trajectory\ntoward artificial general intelligence, comprehensive and forward-looking\nassessment is essential. Existing benchmarks primarily assess static knowledge,\nwhile intelligence also entails the ability to rapidly learn from experience.\nTo this end, we advocate for the evaluation of Test-time Learning, the capacity\nto improve performance in experience-based, reasoning-intensive tasks during\ntest time. In this work, we propose semantic games as effective testbeds for\nevaluating test-time learning, due to their resistance to saturation and\ninherent demand for strategic reasoning. We introduce an objective evaluation\nframework that compares model performance under both limited and cumulative\nexperience settings, and contains four forms of experience representation. To\nprovide a comparative baseline, we recruit eight human participants to complete\nthe same task. Results show that LLMs exhibit measurable test-time learning\ncapabilities; however, their improvements are less stable under cumulative\nexperience and progress more slowly than those observed in humans. These\nfindings underscore the potential of LLMs as general-purpose learning machines,\nwhile also revealing a substantial intellectual gap between models and humans,\nirrespective of how well LLMs perform on static benchmarks.\n","authors":["Jiayin Wang","Zhiquang Guo","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14429v1","updated":"2025-06-17T11:45:37Z","published":"2025-06-17T11:45:37Z","title":"LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs","summary":"  Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textbf{\\textit{stable perplexity}} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.\n","authors":["Xiaoran Liu","Zhigeng Liu","Zengfeng Huang","Qipeng Guo","Ziwei He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2506.14429v1.pdf","comment":"16 pages, 12 figures, work in progress"},{"id":"http://arxiv.org/abs/2410.07819v2","updated":"2025-06-17T11:41:46Z","published":"2024-10-10T11:09:00Z","title":"Uncovering Overfitting in Large Language Model Editing","summary":"  Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge\nrecall mechanisms, we propose a new plug-and-play strategy called Learn the\nInference (LTI), which introduce a Multi-stage Inference Constraint module to\nguide the edited models in recalling new knowledge similarly to how unedited\nLLMs leverage knowledge through in-context learning. Extensive experimental\nresults across a wide range of tasks validate the effectiveness of LTI in\nmitigating Editing Overfit.\n","authors":["Mengqi Zhang","Xiaotian Ye","Qiang Liu","Pengjie Ren","Shu Wu","Zhumin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07819v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2506.14407v1","updated":"2025-06-17T11:08:29Z","published":"2025-06-17T11:08:29Z","title":"ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge","summary":"  Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 15.07%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only ten documents, including the positive document, GPT-4.1 scores only\n35.06%, showing that document-side reasoning remains a challenge. Our codes are\navailable at github.com/ZeinabTaghavi/IMPLIRET.Contribution.\n","authors":["Zeinab Sadat Taghavi","Ali Modarressi","Yunpu Ma","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2506.14407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14397v1","updated":"2025-06-17T10:51:39Z","published":"2025-06-17T10:51:39Z","title":"Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation\n  Understanding","summary":"  Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce \\textbf{Thunder-NUBench}, a novel benchmark explicitly designed to\nassess sentence-level negation understanding in LLMs. Thunder-NUBench goes\nbeyond surface-level cue detection by contrasting standard negation with\nstructurally diverse alternatives such as local negation, contradiction, and\nparaphrase. The benchmark consists of manually curated sentence-negation pairs\nand a multiple-choice dataset that enables in-depth evaluation of models'\nnegation understanding.\n","authors":["Yeonkyoung So","Gyuseong Lee","Sungmok Jung","Joonhak Lee","JiA Kang","Sangho Kim","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.14397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16005v4","updated":"2025-06-17T10:43:41Z","published":"2025-04-22T16:14:31Z","title":"CAPO: Cost-Aware Prompt Optimization","summary":"  Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automatic prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm\nachieves better performances already with smaller budgets, saves evaluations\nthrough racing, and decreases average prompt length via a length penalty,\nmaking it both cost-efficient and cost-aware. Even without few-shot examples,\nCAPO outperforms its competitors and generally remains robust to initial\nprompts. CAPO represents an important step toward making prompt optimization\nmore powerful and accessible by improving cost-efficiency.\n","authors":["Tom Zehle","Moritz Schlager","Timo Heiß","Matthias Feurer"],"pdf_url":"https://arxiv.org/pdf/2504.16005v4.pdf","comment":"Submitted to AutoML 2025"},{"id":"http://arxiv.org/abs/2411.19563v2","updated":"2025-06-17T10:36:46Z","published":"2024-11-29T09:18:32Z","title":"Ensemble Watermarks for Large Language Models","summary":"  As large language models (LLMs) reach human-like fluency, reliably\ndistinguishing AI-generated text from human authorship becomes increasingly\ndifficult. While watermarks already exist for LLMs, they often lack flexibility\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack, the performance\nremains high with 95% detection rate. In comparison, the red-green feature\nalone as a baseline achieves a detection rate of 49% after paraphrasing. The\nevaluation of all feature combinations reveals that the ensemble of all three\nconsistently has the highest detection rate across several LLMs and watermark\nstrength settings. Due to the flexibility of combining features in the\nensemble, various requirements and trade-offs can be addressed. Additionally,\nthe same detection function can be used without adaptations for all ensemble\nconfigurations. This method is particularly of interest to facilitate\naccountability and prevent societal harm.\n","authors":["Georg Niess","Roman Kern"],"pdf_url":"https://arxiv.org/pdf/2411.19563v2.pdf","comment":"Accepted to ACL 2025 main conference. This article extends our\n  earlier work arXiv:2405.08400 by introducing an ensemble of stylometric\n  watermarking features and alternative experimental analysis. Code and data\n  are available at http://github.com/CommodoreEU/ensemble-watermark"},{"id":"http://arxiv.org/abs/2504.07738v2","updated":"2025-06-17T10:16:59Z","published":"2025-04-10T13:29:58Z","title":"Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for\n  Effective Elicitation and Retrieval of Information","summary":"  In this document, we discuss a multi-step approach to automated construction\nof a knowledge graph, for structuring and representing domain-specific\nknowledge from large document corpora. We apply our method to build the first\nknowledge graph of nuclear fusion energy, a highly specialized field\ncharacterized by vast scope and heterogeneity. This is an ideal benchmark to\ntest the key features of our pipeline, including automatic named entity\nrecognition and entity resolution. We show how pre-trained large language\nmodels can be used to address these challenges and we evaluate their\nperformance against Zipf's law, which characterizes human-generated natural\nlanguage. Additionally, we develop a knowledge-graph retrieval-augmented\ngeneration system that combines large language models with a multi-prompt\napproach. This system provides contextually relevant answers to\nnatural-language queries, including complex multi-hop questions that require\nreasoning across interconnected entities.\n","authors":["Andrea Loreti","Kesi Chen","Ruby George","Robert Firth","Adriano Agnello","Shinnosuke Tanaka"],"pdf_url":"https://arxiv.org/pdf/2504.07738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13277v2","updated":"2025-06-17T10:12:39Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huayang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14371v1","updated":"2025-06-17T10:10:51Z","published":"2025-06-17T10:10:51Z","title":"ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions\n  shared task: LLM-based question generation and selection","summary":"  The widespread adoption of chat interfaces based on Large Language Models\n(LLMs) raises concerns about promoting superficial learning and undermining the\ndevelopment of critical thinking skills. Instead of relying on LLMs purely for\nretrieving factual information, this work explores their potential to foster\ndeeper reasoning by generating critical questions that challenge unsupported or\nvague claims in debate interventions. This study is part of a shared task of\nthe 12th Workshop on Argument Mining, co-located with ACL 2025, focused on\nautomatic critical question generation. We propose a two-step framework\ninvolving two small-scale open source language models: a Questioner that\ngenerates multiple candidate questions and a Judge that selects the most\nrelevant ones. Our system ranked first in the shared task competition,\ndemonstrating the potential of the proposed LLM-based approach to encourage\ncritical engagement with argumentative texts.\n","authors":["Lucile Favero","Daniel Frases","Juan Antonio Pérez-Ortiz","Tanja Käser","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2506.14371v1.pdf","comment":"Proceedings of the 12th Workshop on Argument Mining"},{"id":"http://arxiv.org/abs/2506.14370v1","updated":"2025-06-17T10:10:39Z","published":"2025-06-17T10:10:39Z","title":"Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits","summary":"  Search engines play a crucial role as digital gatekeepers, shaping the\nvisibility of Web and social media content through algorithmic curation. This\nstudy investigates how search engines like Google selectively promotes or\nsuppresses certain hashtags and subreddits, impacting the information users\nencounter. By comparing search engine results with nonsampled data from Reddit\nand Twitter/X, we reveal systematic biases in content visibility. Google's\nalgorithms tend to suppress subreddits and hashtags related to sexually\nexplicit material, conspiracy theories, advertisements, and cryptocurrencies,\nwhile promoting content associated with higher engagement. These findings\nsuggest that Google's gatekeeping practices influence public discourse by\ncurating the social media narratives available to users.\n","authors":["Amrit Poudel","Yifan Ding","Jurgen Pfeffer","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2506.14370v1.pdf","comment":"Accepted to ACL 2025 Main"},{"id":"http://arxiv.org/abs/2312.16490v2","updated":"2025-06-17T10:06:03Z","published":"2023-12-27T09:35:23Z","title":"Exploring news intent and its application: A theory-driven approach","summary":"  Understanding the intent behind information is crucial. However, news as a\nmedium of public discourse still lacks a structured investigation of perceived\nnews intent and its application. To advance this field, this paper reviews\ninterdisciplinary studies on intentional action and introduces a conceptual\ndeconstruction-based news intent understanding framework (NINT). This framework\nidentifies the components of intent, facilitating a structured representation\nof news intent and its applications. Building upon NINT, we contribute a new\nintent perception dataset. Moreover, we investigate the potential of intent\nassistance on news-related tasks, such as significant improvement (+2.2% macF1)\nin the task of fake news detection. We hope that our findings will provide\nvaluable insights into action-based intent cognition and computational social\nscience.\n","authors":["Zhengjia Wang","Danding Wang","Qiang Sheng","Juan Cao","Siyuan Ma","Haonan Cheng"],"pdf_url":"https://arxiv.org/pdf/2312.16490v2.pdf","comment":"Accepted to Information Processing & Management. DOI:\n  https://doi.org/10.1016/j.ipm.2025.104229"},{"id":"http://arxiv.org/abs/2506.14345v1","updated":"2025-06-17T09:38:45Z","published":"2025-06-17T09:38:45Z","title":"A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis","summary":"  The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access.\n","authors":["Bruno Martins","Piotr Szymański","Piotr Gramacki"],"pdf_url":"https://arxiv.org/pdf/2506.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14335v1","updated":"2025-06-17T09:17:41Z","published":"2025-06-17T09:17:41Z","title":"Evaluation Should Not Ignore Variation: On the Impact of Reference Set\n  Choice on Summarization Metrics","summary":"  Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nusing different reference sets on reference-based metrics has not been\nsystematically investigated. This work examines the sensitivity of widely used\nreference-based metrics in relation to the choice of reference sets, analyzing\nthree diverse multi-reference summarization datasets: SummEval, GUMSum, and\nDUC2004. We demonstrate that many popular metrics exhibit significant\ninstability. This instability is particularly concerning for n-gram-based\nmetrics like ROUGE, where model rankings vary depending on the reference sets,\nundermining the reliability of model comparisons. We also collect human\njudgments on LLM outputs for genre-diverse data and examine their correlation\nwith metrics to supplement existing findings beyond newswire summaries, finding\nweak-to-no correlation. Taken together, we recommend incorporating reference\nset variation into summarization evaluation to enhance consistency alongside\ncorrelation with human judgments, especially when evaluating LLMs.\n","authors":["Silvia Casola","Yang Janet Liu","Siyao Peng","Oliver Kraus","Albert Gatt","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2506.14335v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.13472v2","updated":"2025-06-17T09:13:54Z","published":"2025-06-16T13:30:33Z","title":"ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently\n  Compressing Large Language Models","summary":"  Quantization has been widely studied as an effective technique for reducing\nthe memory requirement of large language models (LLMs), potentially improving\nthe latency time as well. Utilizing the characteristic of rotational invariance\nof transformer, we propose the rotation-based saliency-aware weight\nquantization (ROSAQ), which identifies salient channels in the projection\nfeature space, not in the original feature space, where the projected\n\"principal\" dimensions are naturally considered as \"salient\" features. The\nproposed ROSAQ consists of 1) PCA-based projection, which first performs\nprincipal component analysis (PCA) on a calibration set and transforms via the\nPCA projection, 2) Salient channel dentification, which selects dimensions\ncorresponding to the K-largest eigenvalues as salient channels, and 3)\nSaliency-aware quantization with mixed-precision, which uses FP16 for salient\ndimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ\nshows improvements over the baseline saliency-aware quantization on the\noriginal feature space and other existing quantization methods. With kernel\nfusion, ROSAQ presents about 2.3x speed up over FP16 implementation in\ngenerating 256 tokens with a batch size of 64.\n","authors":["Junho Yoon","Geom Lee","Donghyeon Jeon","Inho Kang","Seung-Hoon Na"],"pdf_url":"https://arxiv.org/pdf/2506.13472v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.01413v4","updated":"2025-06-17T08:52:06Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v4.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2503.11593v2","updated":"2025-06-17T08:44:18Z","published":"2025-03-14T17:02:45Z","title":"Do Construction Distributions Shape Formal Language Learning In German\n  BabyLMs?","summary":"  We analyze the influence of utterance-level construction distributions in\nGerman child-directed/child-available speech on the resulting word-level,\nsyntactic and semantic competence (and their underlying learning trajectories)\nin small LMs, which we train on a novel collection of developmentally plausible\nlanguage data for German. We find that trajectories are surprisingly robust for\nmarkedly different distributions of constructions in the training data, which\nhave little effect on final accuracies and almost no effect on global learning\ntrajectories. While syntax learning benefits from more complex utterances,\nword-level learning culminates in better scores with more fragmentary\nutterances. We argue that LMs trained on developmentally plausible data can\ncontribute to debates on how conducive different kinds of linguistic stimuli\nare to language learning.\n","authors":["Bastian Bunzeck","Daniel Duran","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2503.11593v2.pdf","comment":"Accepted at CoNNL 2025"},{"id":"http://arxiv.org/abs/2506.14302v1","updated":"2025-06-17T08:29:04Z","published":"2025-06-17T08:29:04Z","title":"Expectation Confirmation Preference Optimization for Multi-Turn\n  Conversational Recommendation Agent","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\npropelled the development of Conversational Recommendation Agents (CRAs).\nHowever, these agents often generate short-sighted responses that fail to\nsustain user guidance and meet expectations. Although preference optimization\nhas proven effective in aligning LLMs with user expectations, it remains costly\nand performs poorly in multi-turn dialogue. To address this challenge, we\nintroduce a novel multi-turn preference optimization (MTPO) paradigm ECPO,\nwhich leverages Expectation Confirmation Theory to explicitly model the\nevolution of user satisfaction throughout multi-turn dialogues, uncovering the\nunderlying causes of dissatisfaction. These causes can be utilized to support\ntargeted optimization of unsatisfactory responses, thereby achieving turn-level\npreference optimization. ECPO ingeniously eliminates the significant sampling\noverhead of existing MTPO methods while ensuring the optimization process\ndrives meaningful improvements. To support ECPO, we introduce an LLM-based user\nsimulator, AILO, to simulate user feedback and perform expectation confirmation\nduring conversational recommendations. Experimental results show that ECPO\nsignificantly enhances CRA's interaction capabilities, delivering notable\nimprovements in both efficiency and effectiveness over existing MTPO methods.\n","authors":["Xueyang Feng","Jingsen Zhang","Jiakai Tang","Wei Li","Guohao Cai","Xu Chen","Quanyu Dai","Yue Zhu","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2506.14302v1.pdf","comment":"Accepted to Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2506.13172v2","updated":"2025-06-17T08:24:30Z","published":"2025-06-16T07:34:31Z","title":"AI-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns","summary":"  We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in the high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks within academic summaries (abstracts and conclusions):\nidentifying unsubstantiated claims (informational integrity) and flagging\nsemantically confusing ambiguous pronoun references (linguistic clarity). We\nconducted a systematic, multi-run evaluation on two frontier models (Gemini Pro\n2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for\nthe informational integrity task reveal a significant divergence in model\nperformance: while both models successfully identified an unsubstantiated head\nof a noun phrase (95% success), ChatGPT consistently failed (0% success) to\nidentify an unsubstantiated adjectival modifier that Gemini correctly flagged\n(95% success), raising a question regarding the potential influence of the\ntarget's syntactic role. For the linguistic analysis task, both models\nperformed well (80-90% success) with full manuscript context. Surprisingly, in\na summary-only setting, Gemini's performance was substantially degraded, while\nChatGPT achieved a perfect (100%) success rate. Our findings suggest that while\nstructured prompting is a viable methodology for complex textual analysis,\nprompt performance may be highly dependent on the interplay between the model,\ntask type, and context, highlighting the need for rigorous, model-specific\ntesting.\n","authors":["Evgeny Markhasin"],"pdf_url":"https://arxiv.org/pdf/2506.13172v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2506.13300v2","updated":"2025-06-17T08:15:17Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12376v2","updated":"2025-06-17T08:11:59Z","published":"2025-06-14T07:18:33Z","title":"ConsistencyChecker: Tree-based Evaluation of LLM Generalization\n  Capabilities","summary":"  Evaluating consistency in large language models (LLMs) is crucial for\nensuring reliability, particularly in complex, multi-step interactions between\nhumans and LLMs. Traditional self-consistency methods often miss subtle\nsemantic changes in natural language and functional shifts in code or\nequations, which can accumulate over multiple transformations. To address this,\nwe propose ConsistencyChecker, a tree-based evaluation framework designed to\nmeasure consistency through sequences of reversible transformations, including\nmachine translation tasks and AI-assisted programming tasks. In our framework,\nnodes represent distinct text states, while edges correspond to pairs of\ninverse operations. Dynamic and LLM-generated benchmarks ensure a fair\nassessment of the model's generalization ability and eliminate benchmark\nleakage. Consistency is quantified based on similarity across different depths\nof the transformation tree. Experiments on eight models from various families\nand sizes show that ConsistencyChecker can distinguish the performance of\ndifferent models. Notably, our consistency scores-computed entirely without\nusing WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking,\ndemonstrating the validity of our benchmark-free approach. Our implementation\nis available at: https://github.com/ulab-uiuc/consistencychecker.\n","authors":["Zhaochen Hong","Haofei Yu","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.12376v2.pdf","comment":"Accepted at ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14285v1","updated":"2025-06-17T07:56:32Z","published":"2025-06-17T07:56:32Z","title":"From What to Respond to When to Respond: Timely Response Generation for\n  Open-domain Dialogue Agents","summary":"  While research on dialogue response generation has primarily focused on\ngenerating coherent responses conditioning on textual context, the critical\nquestion of when to respond grounded on the temporal context remains\nunderexplored. To bridge this gap, we propose a novel task called timely\ndialogue response generation and introduce the TimelyChat benchmark, which\nevaluates the capabilities of language models to predict appropriate time\nintervals and generate time-conditioned responses. Additionally, we construct a\nlarge-scale training dataset by leveraging unlabeled event knowledge from a\ntemporal commonsense knowledge graph and employing a large language model (LLM)\nto synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent\ndesigned to proactively predict time intervals and generate timely responses\nthat align with those intervals. Experimental results show that Timer\noutperforms prompting-based LLMs and other fine-tuned baselines in both\nturn-level and dialogue-level evaluations. We publicly release our data, model,\nand code.\n","authors":["Seongbo Jang","Minjin Jeon","Jaehoon Lee","Seonghyeon Lee","Dongha Lee","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2506.14285v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.09081v2","updated":"2025-06-17T07:54:37Z","published":"2025-06-10T04:19:02Z","title":"FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model\n  Evaluation","summary":"  We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.\n","authors":["Zheqi He","Yesheng Liu","Jing-shu Zheng","Xuejing Li","Jin-Ge Yao","Bowen Qin","Richeng Xuan","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2506.09081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14280v1","updated":"2025-06-17T07:49:43Z","published":"2025-06-17T07:49:43Z","title":"Improving LoRA with Variational Learning","summary":"  Bayesian methods have recently been used to improve LoRA finetuning and,\nalthough they improve calibration, their effect on other metrics (such as\naccuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian\nmethods also increase computational overheads and require additional tricks for\nthem to work well. Here, we fix these issues by using a recently proposed\nvariational algorithm called IVON. We show that IVON is easy to implement and\nhas similar costs to AdamW, and yet it can also drastically improve many\nmetrics by using a simple posterior pruning technique. We present extensive\nresults on billion-scale LLMs (Llama and Qwen series) going way beyond the\nscale of existing applications of IVON. For example, we finetune a Llama-3.2-3B\nmodel on a set of commonsense reasoning tasks and improve accuracy over AdamW\nby 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian\nmethods like Laplace-LoRA and BLoB. Overall, our results show that variational\nlearning with IVON can effectively improve LoRA finetuning.\n","authors":["Bai Cong","Nico Daheim","Yuesong Shen","Rio Yokota","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2506.14280v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.12796v2","updated":"2025-06-17T07:46:17Z","published":"2025-06-15T10:04:42Z","title":"Surprise Calibration for Better In-Context Learning","summary":"  In-context learning (ICL) has emerged as a powerful paradigm for task\nadaptation in large language models (LLMs), where models infer underlying task\nstructures from a few demonstrations. However, ICL remains susceptible to\nbiases that arise from prior knowledge and contextual demonstrations, which can\ndegrade the performance of LLMs. Existing bias calibration methods typically\napply fixed class priors across all inputs, limiting their efficacy in dynamic\nICL settings where the context for each query differs. To address these\nlimitations, we adopt implicit sequential Bayesian inference as a framework for\ninterpreting ICL, identify \"surprise\" as an informative signal for class prior\nshift, and introduce a novel method--Surprise Calibration (SC). SC leverages\nthe notion of surprise to capture the temporal dynamics of class priors,\nproviding a more adaptive and computationally efficient solution for in-context\nlearning. We empirically demonstrate the superiority of SC over existing bias\ncalibration techniques across a range of benchmark natural language processing\ntasks.\n","authors":["Zhihang Tan","Jingrui Hou","Ping Wang","Qibiao Hu","Peng Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.12796v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.04804v4","updated":"2025-06-17T07:44:01Z","published":"2025-03-03T15:32:18Z","title":"What do Large Language Models Say About Animals? Investigating Risks of\n  Animal Harm in Generated Text","summary":"  As machine learning systems become increasingly embedded in society, their\nimpact on human and nonhuman life continues to escalate. Technical evaluations\nhave addressed a variety of potential harms from large language models (LLMs)\ntowards humans and the environment, but there is little empirical work\nregarding harms towards nonhuman animals. Following the growing recognition of\nanimal protection in regulatory and ethical AI frameworks, we present\nAnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated\ntext. Our benchmark dataset comprises 1,850 curated questions from Reddit post\ntitles and 2,500 synthetic questions based on 50 animal categories (e.g., cats,\nreptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios\ninclude open-ended questions about how to treat animals, practical scenarios\nwith potential animal harm, and willingness-to-pay measures for the prevention\nof animal harm. Using the LLM-as-a-judge framework, responses are evaluated for\ntheir potential to increase or decrease harm, and evaluations are debiased for\nthe tendency of judges to judge their own outputs more favorably. AHB reveals\nsignificant differences across frontier LLMs, animal categories, scenarios, and\nsubreddits. We conclude with future directions for technical research and\naddressing the challenges of building evaluations on complex social and moral\ntopics.\n","authors":["Arturs Kanepajs","Aditi Basu","Sankalpa Ghose","Constance Li","Akshat Mehta","Ronak Mehta","Samuel David Tucker-Davis","Eric Zhou","Bob Fischer","Jacy Reese Anthis"],"pdf_url":"https://arxiv.org/pdf/2503.04804v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02958v3","updated":"2025-06-17T07:41:08Z","published":"2025-02-05T07:51:32Z","title":"Position: Editing Large Language Models Poses Serious Safety Risks","summary":"  Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem.\n","authors":["Paul Youssef","Zhixue Zhao","Daniel Braun","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.02958v3.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.14248v1","updated":"2025-06-17T07:11:00Z","published":"2025-06-17T07:11:00Z","title":"Re-Initialization Token Learning for Tool-Augmented Large Language\n  Models","summary":"  Large language models have demonstrated exceptional performance, yet struggle\nwith complex tasks such as numerical reasoning, plan generation. Integrating\nexternal tools, such as calculators and databases, into large language models\n(LLMs) is crucial for enhancing problem-solving capabilities. Current methods\nassign a unique token to each tool, enabling LLMs to call tools through token\nprediction-similar to word generation. However, this approach fails to account\nfor the relationship between tool and word tokens, limiting adaptability within\npre-trained LLMs. To address this issue, we propose a novel token learning\nmethod that aligns tool tokens with the existing word embedding space from the\nperspective of initialization, thereby enhancing model performance. We begin by\nconstructing prior token embeddings for each tool based on the tool's name or\ndescription, which are used to initialize and regularize the learnable tool\ntoken embeddings. This ensures the learned embeddings are well-aligned with the\nword token space, improving tool call accuracy. We evaluate the method on tasks\nsuch as numerical reasoning, knowledge-based question answering, and embodied\nplan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The\nresults demonstrate clear improvements over recent baselines, including CoT,\nREACT, ICL, and ToolkenGPT, indicating that our approach effectively augments\nLLMs with tools through relevant tokens across diverse domains.\n","authors":["Chenghao Li","Liu Liu","Baosheng Yu","Jiayan Qiu","Yibing Zhan"],"pdf_url":"https://arxiv.org/pdf/2506.14248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14245v1","updated":"2025-06-17T07:06:56Z","published":"2025-06-17T07:06:56Z","title":"Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the $Pass@K$ metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\n$Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\n$CoT$-$Pass@K$, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of $K$. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.\n","authors":["Xumeng Wen","Zihan Liu","Shun Zheng","Zhijian Xu","Shengyu Ye","Zhirong Wu","Xiao Liang","Yang Wang","Junjie Li","Ziming Miao","Jiang Bian","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2506.14245v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.11368v2","updated":"2025-06-17T06:55:16Z","published":"2025-05-16T15:32:23Z","title":"GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM\n  Agents","summary":"  Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.\n","authors":["Lingxiao Diao","Xinyue Xu","Wanxuan Sun","Cheng Yang","Zhuosheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.11368v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14235v1","updated":"2025-06-17T06:49:13Z","published":"2025-06-17T06:49:13Z","title":"A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling\n  Historical Patterns in Temporal Knowledge Graphs","summary":"  Temporal knowledge graph reasoning aims to predict future events with\nknowledge of existing facts and plays a key role in various downstream tasks.\nPrevious methods focused on either graph structure learning or semantic\nreasoning, failing to integrate dual reasoning perspectives to handle different\nprediction scenarios. Moreover, they lack the capability to capture the\ninherent differences between historical and non-historical events, which limits\ntheir generalization across different temporal contexts. To this end, we\npropose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs\nthree kinds of expert modules to integrate both structural and semantic\ninformation, guiding the reasoning process for different events. Extensive\nexperiments on three datasets demonstrate the effectiveness of our approach.\n","authors":["Yimin Deng","Yuxia Wu","Yejing Wang","Guoshuai Zhao","Li Zhu","Qidong Liu","Derong Xu","Zichuan Fu","Xian Wu","Yefeng Zheng","Xiangyu Zhao","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2506.14235v1.pdf","comment":"ACL25 findings"},{"id":"http://arxiv.org/abs/2506.14234v1","updated":"2025-06-17T06:47:19Z","published":"2025-06-17T06:47:19Z","title":"Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team","summary":"  Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.\n","authors":["Md Tanzib Hosain","Salman Rahman","Md Kishor Morol","Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2506.14234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06926v2","updated":"2025-06-17T06:41:25Z","published":"2025-03-10T05:11:58Z","title":"Effect of Selection Format on LLM Performance","summary":"  This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance.\n","authors":["Yuchen Han","Yucheng Wu","Jeffrey Willard"],"pdf_url":"https://arxiv.org/pdf/2503.06926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13227v2","updated":"2025-06-17T06:33:35Z","published":"2025-05-19T15:09:23Z","title":"Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis","summary":"  Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.\n","authors":["Tianbao Xie","Jiaqi Deng","Xiaochuan Li","Junlin Yang","Haoyuan Wu","Jixuan Chen","Wenjing Hu","Xinyuan Wang","Yuhui Xu","Zekun Wang","Yiheng Xu","Junli Wang","Doyen Sahoo","Tao Yu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.13227v2.pdf","comment":"49 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.15910v2","updated":"2025-06-17T06:29:03Z","published":"2025-02-21T19:54:46Z","title":"Modality-Aware Neuron Pruning for Unlearning in Multimodal Large\n  Language Models","summary":"  Generative models such as Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) trained on massive datasets can lead them to memorize\nand inadvertently reveal sensitive information, raising ethical and privacy\nconcerns. While some prior works have explored this issue in the context of\nLLMs, it presents a unique challenge for MLLMs due to the entangled nature of\nknowledge across modalities, making comprehensive unlearning more difficult. To\naddress this challenge, we propose Modality Aware Neuron Unlearning (MANU), a\nnovel unlearning framework for MLLMs designed to selectively clip neurons based\non their relative importance to the targeted forget data, curated for different\nmodalities. Specifically, MANU consists of two stages: important neuron\nselection and selective pruning. The first stage identifies and collects the\nmost influential neurons across modalities relative to the targeted forget\nknowledge, while the second stage is dedicated to pruning those selected\nneurons. MANU effectively isolates and removes the neurons that contribute most\nto the forget data within each modality, while preserving the integrity of\nretained knowledge. Our experiments conducted across various MLLM architectures\nillustrate that MANU can achieve a more balanced and comprehensive unlearning\nin each modality without largely affecting the overall model utility.\n","authors":["Zheyuan Liu","Guangyao Dou","Xiangchi Yuan","Chunhui Zhang","Zhaoxuan Tan","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.15910v2.pdf","comment":"ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.14223v1","updated":"2025-06-17T06:25:35Z","published":"2025-06-17T06:25:35Z","title":"Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature\n  Transcription","summary":"  Music transcription plays a pivotal role in Music Information Retrieval\n(MIR), particularly for stringed instruments like the guitar, where symbolic\nmusic notations such as MIDI lack crucial playability information. This\ncontribution introduces the Fretting-Transformer, an encoderdecoder model that\nutilizes a T5 transformer architecture to automate the transcription of MIDI\nsequences into guitar tablature. By framing the task as a symbolic translation\nproblem, the model addresses key challenges, including string-fret ambiguity\nand physical playability. The proposed system leverages diverse datasets,\nincluding DadaGP, GuitarToday, and Leduc, with novel data pre-processing and\ntokenization strategies. We have developed metrics for tablature accuracy and\nplayability to quantitatively evaluate the performance. The experimental\nresults demonstrate that the Fretting-Transformer surpasses baseline methods\nlike A* and commercial applications like Guitar Pro. The integration of\ncontext-sensitive processing and tuning/capo conditioning further enhances the\nmodel's performance, laying a robust foundation for future developments in\nautomated guitar transcription.\n","authors":["Anna Hamberger","Sebastian Murgul","Jochen Schmidt","Michael Heizmann"],"pdf_url":"https://arxiv.org/pdf/2506.14223v1.pdf","comment":"Accepted to the 50th International Computer Music Conference (ICMC),\n  2025"},{"id":"http://arxiv.org/abs/2506.14213v1","updated":"2025-06-17T06:06:01Z","published":"2025-06-17T06:06:01Z","title":"Chaining Event Spans for Temporal Relation Grounding","summary":"  Accurately understanding temporal relations between events is a critical\nbuilding block of diverse tasks, such as temporal reading comprehension (TRC)\nand relation extraction (TRE). For example in TRC, we need to understand the\ntemporal semantic differences between the following two questions that are\nlexically near-identical: \"What finished right before the decision?\" or \"What\nfinished right after the decision?\". To discern the two questions, existing\nsolutions have relied on answer overlaps as a proxy label to contrast similar\nand dissimilar questions. However, we claim that answer overlap can lead to\nunreliable results, due to spurious overlaps of two dissimilar questions with\ncoincidentally identical answers. To address the issue, we propose a novel\napproach that elicits proper reasoning behaviors through a module for\npredicting time spans of events. We introduce the Timeline Reasoning Network\n(TRN) operating in a two-step inductive reasoning process: In the first step\nmodel initially answers each question with semantic and syntactic information.\nThe next step chains multiple questions on the same event to predict a\ntimeline, which is then used to ground the answers. Results on the TORQUE and\nTB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms\nprevious methods by effectively resolving the spurious overlaps using the\npredicted timeline.\n","authors":["Jongho Kim","Dohyeon Lee","Minsoo Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.14213v1.pdf","comment":"In Proceedings of the 18th Conference of the European Chapter of the\n  Association for Computational Linguistics (Volume 1: Long Papers), pages\n  1689-1700"},{"id":"http://arxiv.org/abs/2506.14211v1","updated":"2025-06-17T06:00:07Z","published":"2025-06-17T06:00:07Z","title":"Explainable Detection of Implicit Influential Patterns in Conversations\n  via Data Augmentation","summary":"  In the era of digitalization, as individuals increasingly rely on digital\nplatforms for communication and news consumption, various actors employ\nlinguistic strategies to influence public perception. While models have become\nproficient at detecting explicit patterns, which typically appear in texts as\nsingle remarks referred to as utterances, such as social media posts, malicious\nactors have shifted toward utilizing implicit influential verbal patterns\nembedded within conversations. These verbal patterns aim to mentally penetrate\nthe victim's mind in order to influence them, enabling the actor to obtain the\ndesired information through implicit means. This paper presents an improved\napproach for detecting such implicit influential patterns. Furthermore, the\nproposed model is capable of identifying the specific locations of these\ninfluential elements within a conversation. To achieve this, the existing\ndataset was augmented using the reasoning capabilities of state-of-the-art\nlanguage models. Our designed framework resulted in a 6% improvement in the\ndetection of implicit influential patterns in conversations. Moreover, this\napproach improved the multi-label classification tasks related to both the\ntechniques used for influence and the vulnerability of victims by 33% and 43%,\nrespectively.\n","authors":["Sina Abdidizaji","Md Kowsher","Niloofar Yousefi","Ivan Garibay"],"pdf_url":"https://arxiv.org/pdf/2506.14211v1.pdf","comment":"Accepted at the HCI International conference 2025"},{"id":"http://arxiv.org/abs/2502.17421v2","updated":"2025-06-17T05:58:01Z","published":"2025-02-24T18:53:31Z","title":"LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification","summary":"  As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.\n","authors":["Penghui Yang","Cunxiao Du","Fengzhuo Zhang","Haonan Wang","Tianyu Pang","Chao Du","Bo An"],"pdf_url":"https://arxiv.org/pdf/2502.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14206v1","updated":"2025-06-17T05:48:44Z","published":"2025-06-17T05:48:44Z","title":"CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data\n  Generation","summary":"  Training data has been proven to be one of the most critical components in\ntraining generative AI. However, obtaining high-quality data remains\nchallenging, with data privacy issues presenting a significant hurdle. To\naddress the need for high-quality data. Synthesize data has emerged as a\nmainstream solution, demonstrating impressive performance in areas such as\nimages, audio, and video. Generating mixed-type data, especially high-quality\ntabular data, still faces significant challenges. These primarily include its\ninherent heterogeneous data types, complex inter-variable relationships, and\nintricate column-wise distributions. In this paper, we introduce CausalDiffTab,\na diffusion model-based generative model specifically designed to handle mixed\ntabular data containing both numerical and categorical features, while being\nmore flexible in capturing complex interactions among variables. We further\npropose a hybrid adaptive causal regularization method based on the principle\nof Hierarchical Prior Fusion. This approach adaptively controls the weight of\ncausal regularization, enhancing the model's performance without compromising\nits generative capabilities. Comprehensive experiments conducted on seven\ndatasets demonstrate that CausalDiffTab outperforms baseline methods across all\nmetrics. Our code is publicly available at:\nhttps://github.com/Godz-z/CausalDiffTab.\n","authors":["Jia-Chen Zhang","Zheng Zhou","Yu-Jie Xiong","Chun-Ming Xia","Fei Dai"],"pdf_url":"https://arxiv.org/pdf/2506.14206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14205v1","updated":"2025-06-17T05:46:52Z","published":"2025-06-17T05:46:52Z","title":"AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents","summary":"  We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth\n","authors":["Jingxu Xie","Dylan Xu","Xuandong Zhao","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2506.14205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14204v1","updated":"2025-06-17T05:46:38Z","published":"2025-06-17T05:46:38Z","title":"Improving Practical Aspects of End-to-End Multi-Talker Speech\n  Recognition for Online and Offline Scenarios","summary":"  We extend the frameworks of Serialized Output Training (SOT) to address\npractical needs of both streaming and offline automatic speech recognition\n(ASR) applications. Our approach focuses on balancing latency and accuracy,\ncatering to real-time captioning and summarization requirements. We propose\nseveral key improvements: (1) Leveraging Continuous Speech Separation (CSS)\nsingle-channel front-end with end-to-end (E2E) systems for highly overlapping\nscenarios, challenging the conventional wisdom of E2E versus cascaded setups.\nThe CSS framework improves the accuracy of the ASR system by separating\noverlapped speech from multiple speakers. (2) Implementing dual models --\nConformer Transducer for streaming and Sequence-to-Sequence for offline -- or\nalternatively, a two-pass model based on cascaded encoders. (3) Exploring\nsegment-based SOT (segSOT) which is better suited for offline scenarios while\nalso enhancing readability of multi-talker transcriptions.\n","authors":["Aswin Shanmugam Subramanian","Amit Das","Naoyuki Kanda","Jinyu Li","Xiaofei Wang","Yifan Gong"],"pdf_url":"https://arxiv.org/pdf/2506.14204v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14203v1","updated":"2025-06-17T05:44:55Z","published":"2025-06-17T05:44:55Z","title":"Intended Target Identification for Anomia Patients with Gradient-based\n  Selective Augmentation","summary":"  In this study, we investigate the potential of language models (LMs) in\naiding patients experiencing anomia, a difficulty identifying the names of\nitems. Identifying the intended target item from patient's circumlocution\ninvolves the two challenges of term failure and error: (1) The terms relevant\nto identifying the item remain unseen. (2) What makes the challenge unique is\ninherent perturbed terms by semantic paraphasia, which are not exactly related\nto the target item, hindering the identification process. To address each, we\npropose robustifying the model from semantically paraphasic errors and\nenhancing the model with unseen terms with gradient-based selective\naugmentation. Specifically, the gradient value controls augmented data quality\namid semantic errors, while the gradient variance guides the inclusion of\nunseen but relevant terms. Due to limited domain-specific datasets, we evaluate\nthe model on the Tip-of-the-Tongue dataset as an intermediary task and then\napply our findings to real patient data from AphasiaBank. Our results\ndemonstrate strong performance against baselines, aiding anomia patients by\naddressing the outlined challenges.\n","authors":["Jongho Kim","Romain Storaï","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.14203v1.pdf","comment":"EMNLP 2024 Findings (long)"},{"id":"http://arxiv.org/abs/2505.12368v2","updated":"2025-06-17T05:38:20Z","published":"2025-05-18T11:14:14Z","title":"CAPTURE: Context-Aware Prompt Injection Testing and Robustness\n  Enhancement","summary":"  Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations. To demonstrate our framework's utility, we\ntrain CaptureGuard on our generated data. This new model drastically reduces\nboth false negative and false positive rates on our context-aware datasets\nwhile also generalizing effectively to external benchmarks, establishing a path\ntoward more robust and practical prompt injection defenses.\n","authors":["Gauri Kholkar","Ratinder Ahuja"],"pdf_url":"https://arxiv.org/pdf/2505.12368v2.pdf","comment":"Accepted in ACL LLMSec Workshop 2025"},{"id":"http://arxiv.org/abs/2506.01565v2","updated":"2025-06-17T05:37:58Z","published":"2025-06-02T11:43:46Z","title":"Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation","summary":"  Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.\n","authors":["Li Zhou","Lutong Yu","Dongchu Xie","Shaohuan Cheng","Wenyan Li","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2506.01565v2.pdf","comment":"cultural analysis, cultural visual understanding, cultural image\n  transcreation (update dataset license)"},{"id":"http://arxiv.org/abs/2506.14200v1","updated":"2025-06-17T05:36:39Z","published":"2025-06-17T05:36:39Z","title":"ELI-Why: Evaluating the Pedagogical Utility of Language Model\n  Explanations","summary":"  Language models today are widely used in education, yet their ability to\ntailor responses for learners with varied informational needs and knowledge\nbackgrounds remains under-explored. To this end, we introduce ELI-Why, a\nbenchmark of 13.4K \"Why\" questions to evaluate the pedagogical capabilities of\nlanguage models. We then conduct two extensive human studies to assess the\nutility of language model-generated explanatory answers (explanations) on our\nbenchmark, tailored to three distinct educational grades: elementary,\nhigh-school and graduate school. In our first study, human raters assume the\nrole of an \"educator\" to assess model explanations' fit to different\neducational grades. We find that GPT-4-generated explanations match their\nintended educational background only 50% of the time, compared to 79% for lay\nhuman-curated explanations. In our second study, human raters assume the role\nof a learner to assess if an explanation fits their own informational needs.\nAcross all educational backgrounds, users deemed GPT-4-generated explanations\n20% less suited on average to their informational needs, when compared to\nexplanations curated by lay people. Additionally, automated evaluation metrics\nreveal that explanations generated across different language model families for\ndifferent informational needs remain indistinguishable in their grade-level,\nlimiting their pedagogical effectiveness.\n","authors":["Brihi Joshi","Keyu He","Sahana Ramnath","Sadra Sabouri","Kaitlyn Zhou","Souti Chattopadhyay","Swabha Swayamdipta","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2506.14200v1.pdf","comment":"Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2410.01444v5","updated":"2025-06-17T05:34:44Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v5.pdf","comment":"Published at ACL 2025"},{"id":"http://arxiv.org/abs/2502.11425v2","updated":"2025-06-17T05:34:15Z","published":"2025-02-17T04:37:07Z","title":"Counterfactual-Consistency Prompting for Relative Temporal Understanding\n  in Large Language Models","summary":"  Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies.\n","authors":["Jongho Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.11425v2.pdf","comment":"ACL 2025 main (short)"},{"id":"http://arxiv.org/abs/2506.14199v1","updated":"2025-06-17T05:33:40Z","published":"2025-06-17T05:33:40Z","title":"MAS-LitEval : Multi-Agent System for Literary Translation Quality\n  Assessment","summary":"  Literary translation requires preserving cultural nuances and stylistic\nelements, which traditional metrics like BLEU and METEOR fail to assess due to\ntheir focus on lexical overlap. This oversight neglects the narrative\nconsistency and stylistic fidelity that are crucial for literary works. To\naddress this, we propose MAS-LitEval, a multi-agent system using Large Language\nModels (LLMs) to evaluate translations based on terminology, narrative, and\nstyle. We tested MAS-LitEval on translations of The Little Prince and A\nConnecticut Yankee in King Arthur's Court, generated by various LLMs, and\ncompared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these\nmetrics, with top models scoring up to 0.890 in capturing literary nuances.\nThis work introduces a scalable, nuanced framework for Translation Quality\nAssessment (TQA), offering a practical tool for translators and researchers.\n","authors":["Junghwan Kim","Kieun Park","Sohee Park","Hyunggug Kim","Bongwon Suh"],"pdf_url":"https://arxiv.org/pdf/2506.14199v1.pdf","comment":"4 Pages, 2 tables, EMNLP submitted"},{"id":"http://arxiv.org/abs/2304.03030v2","updated":"2025-06-17T05:11:56Z","published":"2023-04-06T12:29:53Z","title":"Compression of enumerations and gain","summary":"  We study the compressibility of enumerations in the context of Kolmogorov\ncomplexity, focusing on strong and weak forms of compression and their gain:\nthe amount of auxiliary information embedded in the compressed enumeration. The\nexistence of strong compression and weak gainless compression is shown for any\ncomputably enumerable (c.e.) set. The density problem of c.e. sets with respect\nto their prefix complexity is reduced to the question of whether every c.e. set\nis well-compressible, which we study via enumeration games.\n","authors":["George Barmpalias","Xiaoyan Zhang","Bohua Zhan"],"pdf_url":"https://arxiv.org/pdf/2304.03030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18770v3","updated":"2025-06-17T05:05:12Z","published":"2025-02-26T02:57:59Z","title":"Reward Shaping to Mitigate Reward Hacking in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\n\\emph{reward hacking}, where the agent exploits flaws in the reward function\nrather than learning the intended behavior, thus degrading alignment. Although\nreward shaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests two key\ndesign principles: (1) the RL reward should be bounded, and (2) the RL reward\nbenefits from rapid initial growth followed by gradual convergence. Guided by\nthese insights, we propose Preference As Reward (PAR), a novel approach that\nleverages the latent preferences embedded within the reward model as the signal\nfor reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. The code is available at\nhttps://github.com/PorUna-byte/PAR, and the Work done during the internship at\nStepFun by Jiayi Fu.\n","authors":["Jiayi Fu","Xuandong Zhao","Chengyuan Yao","Heng Wang","Qi Han","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.18770v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2506.14190v1","updated":"2025-06-17T05:05:09Z","published":"2025-06-17T05:05:09Z","title":"AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR","summary":"  Developing code-switched ASR systems is challenging due to language ambiguity\nand limited exposure to multilingual, code-switched data, while collecting such\nspeech is costly. Prior work generates synthetic audio from text, but these\nmethods are computationally intensive and hard to scale. We introduce\nAsyncSwitch, a novel asynchronous adaptation framework that leverages\nlarge-scale, text-rich web data to pre-expose ASR models to diverse\ncode-switched domains before fine-tuning on paired speech-text corpora. Our\nthree-stage process (1) trains decoder self-attention and feedforward layers on\ncode-switched text, (2) aligns decoder and encoder via cross-attention using\nlimited speech-text data, and (3) fully fine-tunes the entire model.\nExperiments with Whisper on Malay-English code-switching demonstrate a 9.02%\nrelative WER reduction, while improving monolingual performance in Singlish,\nMalay, and other English variants.\n","authors":["Tuan Nguyen","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14190v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  This paper is a preprint version submitted to the 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU 2025)"},{"id":"http://arxiv.org/abs/2506.00854v2","updated":"2025-06-17T04:58:31Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14177v1","updated":"2025-06-17T04:37:16Z","published":"2025-06-17T04:37:16Z","title":"Can we train ASR systems on Code-switch without real code-switch data?\n  Case study for Singapore's languages","summary":"  Code-switching (CS), common in multilingual settings, presents challenges for\nASR due to scarce and costly transcribed data caused by linguistic complexity.\nThis study investigates building CS-ASR using synthetic CS data. We propose a\nphrase-level mixing method to generate synthetic CS data that mimics natural\npatterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data\nto fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This\npaper focuses on three under-resourced Southeast Asian language pairs:\nMalay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN),\nestablishing a new comprehensive benchmark for CS-ASR to evaluate the\nperformance of leading ASR models. Experimental results show that the proposed\ntraining strategy enhances ASR performance on monolingual and CS tests, with\nBM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a\ncost-effective approach for CS-ASR development, benefiting research and\nindustry.\n","authors":["Tuan Nguyen","Huy-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14177v1.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14175v1","updated":"2025-06-17T04:34:27Z","published":"2025-06-17T04:34:27Z","title":"GRAM: A Generative Foundation Reward Model for Reward Generalization","summary":"  In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models.\n","authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Qiaozhi He","Murun Yang","Bei Li","Tong Xiao","Chunliang Zhang","Tongran Liu","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.14175v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.00555v2","updated":"2025-06-17T03:59:45Z","published":"2025-05-31T13:22:55Z","title":"MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal\n  Medical Reasoning","summary":"  Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 20.7% over supervised\nfine-tuning baselines.\n","authors":["Peng Xia","Jinglu Wang","Yibo Peng","Kaide Zeng","Xian Wu","Xiangru Tang","Hongtu Zhu","Yun Li","Shujie Liu","Yan Lu","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2506.00555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14161v1","updated":"2025-06-17T03:50:57Z","published":"2025-06-17T03:50:57Z","title":"MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation\n  of LLMs via Theory of Mind","summary":"  Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias.\n","authors":["Yanlin Li","Hao Liu","Huimin Liu","Yinwei Wei","Yupeng Hu"],"pdf_url":"https://arxiv.org/pdf/2506.14161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14158v1","updated":"2025-06-17T03:38:19Z","published":"2025-06-17T03:38:19Z","title":"S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for\n  Efficient Inference of Large Language Models","summary":"  Large language models (LLMs) exhibit remarkable reasoning capabilities across\ndiverse downstream tasks. However, their autoregressive nature leads to\nsubstantial inference latency, posing challenges for real-time applications.\nSpeculative sampling mitigates this issue by introducing a drafting phase\nfollowed by a parallel validation phase, enabling faster token generation and\nverification. Existing approaches, however, overlook the inherent coherence in\ntext generation, limiting their efficiency. To address this gap, we propose a\nSpeculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework,\nwhich extends speculative sampling by leveraging multi-head drafting for rapid\ntoken generation and a continuous verification tree for efficient candidate\nvalidation and feature reuse. Experimental results demonstrate that S$^4$C\nsurpasses baseline methods across mainstream tasks, offering enhanced\nefficiency, parallelism, and the ability to generate more valid tokens with\nfewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an\nacceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.\n","authors":["Tao He","Guang Huang","Yu Yang","Tianshi Xu","Sicheng Zhao","Guiguang Ding","Pengyang Wang","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2506.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14157v1","updated":"2025-06-17T03:37:41Z","published":"2025-06-17T03:37:41Z","title":"DCRM: A Heuristic to Measure Response Pair Quality in Preference\n  Optimization","summary":"  Recent research has attempted to associate preference optimization (PO)\nperformance with the underlying preference datasets. In this work, our\nobservation is that the differences between the preferred response $y^+$ and\ndispreferred response $y^-$ influence what LLMs can learn, which may not match\nthe desirable differences to learn. Therefore, we use distance and reward\nmargin to quantify these differences, and combine them to get Distance\nCalibrated Reward Margin (DCRM), a metric that measures the quality of a\nresponse pair for PO. Intuitively, DCRM encourages minimal noisy differences\nand maximal desired differences. With this, we study 3 types of commonly used\npreference datasets, classified along two axes: the source of the responses and\nthe preference labeling function. We establish a general correlation between\nhigher DCRM of the training set and better learning outcome. Inspired by this,\nwe propose a best-of-$N^2$ pairing method that selects response pairs with the\nhighest DCRM. Empirically, in various settings, our method produces training\ndatasets that can further improve models' performance on AlpacaEval, MT-Bench,\nand Arena-Hard over the existing training sets.\n","authors":["Chengyu Huang","Tanya Goyal"],"pdf_url":"https://arxiv.org/pdf/2506.14157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07631v2","updated":"2025-06-17T03:34:55Z","published":"2025-03-04T19:37:33Z","title":"OWLViz: An Open-World Benchmark for Visual Question Answering","summary":"  We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research.\n","authors":["Thuy Nguyen","Dang Nguyen","Hoang Nguyen","Thuan Luong","Long Hoang Dang","Viet Dac Lai"],"pdf_url":"https://arxiv.org/pdf/2503.07631v2.pdf","comment":"ICML 2025 Workshop on Multi-Agent Systems in the Era of Foundation\n  Models: Opportunities, Challenges, and Futures. (8 pages + appendix)"},{"id":"http://arxiv.org/abs/2506.14153v1","updated":"2025-06-17T03:30:58Z","published":"2025-06-17T03:30:58Z","title":"Pushing the Performance of Synthetic Speech Detection with\n  Kolmogorov-Arnold Networks and Self-Supervised Learning Models","summary":"  Recent advancements in speech synthesis technologies have led to increasingly\nadvanced spoofing attacks, posing significant challenges for automatic speaker\nverification systems. While systems based on self-supervised learning (SSL)\nmodels, particularly the XLSR-Conformer model, have demonstrated remarkable\nperformance in synthetic speech detection, there remains room for architectural\nimprovements. In this paper, we propose a novel approach that replaces the\ntraditional Multi-Layer Perceptron in the XLSR-Conformer model with a\nKolmogorov-Arnold Network (KAN), a novel architecture based on the\nKolmogorov-Arnold representation theorem. Our results on ASVspoof2021\ndemonstrate that integrating KAN into the SSL-based models can improve the\nperformance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER\non the 21LA set. These findings suggest that incorporating KAN into SSL-based\nmodels is a promising direction for advances in synthetic speech detection.\n","authors":["Tuan Dat Phuong","Long-Vu Hoang","Huy Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2506.14153v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2505.20613v2","updated":"2025-06-17T03:25:43Z","published":"2025-05-27T01:26:11Z","title":"REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning","summary":"  Nowadays, formal theorem provers have made monumental progress on high-school\nand competition-level mathematics, but few of them generalize to more advanced\nmathematics. In this paper, we present REAL-Prover, a new open-source stepwise\ntheorem prover for Lean 4 to push this boundary. This prover, based on our\nfine-tuned large language model (REAL-Prover-v1) and integrated with a\nretrieval system (Leansearch-PS), notably boosts performance on solving\ncollege-level mathematics problems. To train REAL-Prover-v1, we developed\nHERALD-AF, a data extraction pipeline that converts natural language math\nproblems into formal statements, and a new open-source Lean 4 interactive\nenvironment (Jixia-interactive) to facilitate synthesis data collection. In our\nexperiments, our prover using only supervised fine-tune achieves competitive\nresults with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable\nto state-of-the-art (SOTA) models. To further evaluate our approach, we\nintroduce FATE-M, a new benchmark focused on algebraic problems, where our\nprover achieves a SOTA success rate of 56.7% (Pass@64).\n","authors":["Ziju Shen","Naohao Huang","Fanyi Yang","Yutong Wang","Guoxiong Gao","Tianyi Xu","Jiedong Jiang","Wanyi He","Pu Yang","Mengzhou Sun","Haocheng Ju","Peihao Wu","Bryan Dai","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2505.20613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14148v1","updated":"2025-06-17T03:25:38Z","published":"2025-06-17T03:25:38Z","title":"Acoustic scattering AI for non-invasive object classifications: A case\n  study on hair assessment","summary":"  This paper presents a novel non-invasive object classification approach using\nacoustic scattering, demonstrated through a case study on hair assessment. When\nan incident wave interacts with an object, it generates a scattered acoustic\nfield encoding structural and material properties. By emitting acoustic stimuli\nand capturing the scattered signals from head-with-hair-sample objects, we\nclassify hair type and moisture using AI-driven, deep-learning-based sound\nclassification. We benchmark comprehensive methods, including (i) fully\nsupervised deep learning, (ii) embedding-based classification, (iii) supervised\nfoundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our\nbest strategy achieves nearly 90% classification accuracy by fine-tuning all\nparameters of a self-supervised model. These results highlight acoustic\nscattering as a privacy-preserving, non-contact alternative to visual\nclassification, opening huge potential for applications in various industries.\n","authors":["Long-Vu Hoang","Tuan Nguyen","Tran Huy Dat"],"pdf_url":"https://arxiv.org/pdf/2506.14148v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14142v1","updated":"2025-06-17T03:10:33Z","published":"2025-06-17T03:10:33Z","title":"RadFabric: Agentic AI System with Reasoning Capability for Radiology","summary":"  Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis.\n","authors":["Wenting Chen","Yi Dong","Zhaojun Ding","Yucheng Shi","Yifan Zhou","Fang Zeng","Yijun Luo","Tianyu Lin","Yihang Su","Yichen Wu","Kai Zhang","Zhen Xiang","Tianming Liu","Ninghao Liu","Lichao Sun","Yixuan Yuan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2506.14142v1.pdf","comment":"4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.17238v2","updated":"2025-06-17T02:57:43Z","published":"2025-05-22T19:31:40Z","title":"Personalizing Student-Agent Interactions Using Log-Contextualized\n  Retrieval Augmented Generation (RAG)","summary":"  Collaborative dialogue offers rich insights into students' learning and\ncritical thinking, which is essential for personalizing pedagogical agent\ninteractions in STEM+C settings. While large language models (LLMs) facilitate\ndynamic pedagogical interactions, hallucinations undermine confidence, trust,\nand instructional value. Retrieval-augmented generation (RAG) grounds LLM\noutputs in curated knowledge but requires a clear semantic link between user\ninput and a knowledge base, which is often weak in student dialogue. We propose\nlog-contextualized RAG (LC-RAG), which enhances RAG retrieval by using\nenvironment logs to contextualize collaborative discourse. Our findings show\nthat LC-RAG improves retrieval over a discourse-only baseline and allows our\ncollaborative peer agent, Copa, to deliver relevant, personalized guidance that\nsupports students' critical thinking and epistemic decision-making in a\ncollaborative computational modeling environment, C2STEM.\n","authors":["Clayton Cohn","Surya Rayala","Caitlin Snyder","Joyce Fonteles","Shruti Jain","Naveeduddin Mohammed","Umesh Timalsina","Sarah K. Burriss","Ashwin T S","Namrata Srivastava","Menton Deweese","Angela Eeds","Gautam Biswas"],"pdf_url":"https://arxiv.org/pdf/2505.17238v2.pdf","comment":"To appear in the International Conference on Artificial Intelligence\n  in Education (AIED25) Workshop on Epistemics and Decision-Making in\n  AI-Supported Education"},{"id":"http://arxiv.org/abs/2506.01391v2","updated":"2025-06-17T02:57:04Z","published":"2025-06-02T07:30:29Z","title":"AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning","summary":"  The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data.\n","authors":["Zhong Zhang","Yaxi Lu","Yikun Fu","Yupeng Huo","Shenzhi Yang","Yesai Wu","Han Si","Xin Cong","Haotian Chen","Yankai Lin","Jie Xie","Wei Zhou","Wang Xu","Yuanheng Zhang","Zhou Su","Zhongwu Zhai","Xiaoming Liu","Yudong Mei","Jianming Xu","Hongyan Tian","Chongyi Wang","Chi Chen","Yuan Yao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01391v2.pdf","comment":"Updated results in Table 2 and Table 3; The project is available at\n  https://github.com/OpenBMB/AgentCPM-GUI"},{"id":"http://arxiv.org/abs/2503.16974v3","updated":"2025-06-17T02:40:54Z","published":"2025-03-21T09:43:37Z","title":"Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks","summary":"  This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.\n","authors":["Julian Junyan Wang","Victor Xiaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16974v3.pdf","comment":"89 pages, 20 tables, 15 figures"},{"id":"http://arxiv.org/abs/2506.14123v1","updated":"2025-06-17T02:37:04Z","published":"2025-06-17T02:37:04Z","title":"Sampling from Your Language Model One Byte at a Time","summary":"  Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations. For example, users are often advised not to end their\nprompts with a space because it prevents the model from including the space as\npart of the next token. This Prompt Boundary Problem (PBP) also arises in\nlanguages such as Chinese and in code generation, where tokens often do not\nline up with syntactic boundaries. Additionally mismatching tokenizers often\nhinder model composition and interoperability. For example, it is not possible\nto directly ensemble models with different tokenizers due to their mismatching\nvocabularies. To address these issues, we present an inference-time method to\nconvert any autoregressive LM with a BPE tokenizer into a character-level or\nbyte-level LM, without changing its generative distribution at the text level.\nOur method efficient solves the PBP and is also able to unify the vocabularies\nof language models with different tokenizers, allowing one to ensemble LMs with\ndifferent tokenizers at inference time as well as transfer the post-training\nfrom one model to another using proxy-tuning. We demonstrate in experiments\nthat the ensemble and proxy-tuned models outperform their constituents on\ndownstream evals.\n","authors":["Jonathan Hayase","Alisa Liu","Noah A. Smith","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2506.14123v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.13366v2","updated":"2025-06-17T02:34:00Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  Goal-oriented proactive dialogue systems are designed to guide user\nconversations seamlessly towards specific objectives by planning a\ngoal-oriented path. However, previous research has focused predominantly on\noptimizing these paths while neglecting the inconsistencies that may arise\nbetween generated responses and dialogue contexts, including user profiles,\ndialogue history, domain knowledge, and subgoals. To address this issue, we\nintroduce a model-agnostic two-stage Consistency Reflection and Correction\n(CRC) framework. Specifically, in the consistency reflection stage, the model\nis prompted to reflect on the discrepancies between generated responses and\ndialogue contexts, identifying inconsistencies and suggesting possible\ncorrections. In the consistency correction stage, the model generates responses\nthat are more consistent with the dialogue context based on these reflection\nresults. We conducted experiments on various model architectures with different\nparameter sizes, including encoder-decoder models (BART, T5) and decoder-only\nmodels (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental\nresults on three datasets demonstrate that our CRC framework significantly\nimproves the consistency between generated responses and dialogue contexts.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05693v2","updated":"2025-06-17T02:24:51Z","published":"2024-12-07T16:41:54Z","title":"Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression","summary":"  Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.\n","authors":["Michael R. Metel","Boxing Chen","Mehdi Rezagholizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.05693v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14111v1","updated":"2025-06-17T02:03:36Z","published":"2025-06-17T02:03:36Z","title":"Essential-Web v1.0: 24T tokens of organized web data","summary":"  Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0\n","authors":["Essential AI"," :","Andrew Hojel","Michael Pust","Tim Romanski","Yash Vanjani","Ritvik Kapila","Mohit Parmar","Adarsh Chaluvaraju","Alok Tripathy","Anil Thomas","Ashish Tanwer","Darsh J Shah","Ishaan Shah","Karl Stratos","Khoi Nguyen","Kurt Smith","Michael Callahan","Peter Rushton","Philip Monk","Platon Mazarakis","Saad Jamal","Saurabh Srivastava","Somanshu Singla","Ashish Vaswani"],"pdf_url":"https://arxiv.org/pdf/2506.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17514v2","updated":"2025-06-17T01:51:36Z","published":"2025-02-22T14:20:07Z","title":"SAE-V: Interpreting Multimodal Models for Enhanced Alignment","summary":"  With the integration of image modality, the semantic space of multimodal\nlarge language models (MLLMs) is more complex than text-only models, making\ntheir interpretability more challenging and their alignment less stable,\nparticularly susceptible to low-quality data, which can lead to inconsistencies\nbetween modalities, hallucinations, and biased outputs. As a result, developing\ninterpretability methods for MLLMs is crucial for improving alignment quality\nand efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained\nattention for their ability to interpret latent representations. However,\nextending SAEs to multimodal settings presents new challenges due to modality\nfusion and the difficulty of isolating cross-modal representations. To address\nthese challenges, we introduce SAE-V, a mechanistic interpretability framework\nthat extends the SAE paradigm to MLLMs. By identifying and analyzing\ninterpretable features along with their corresponding data, SAE-V enables\nfine-grained interpretation of both model behavior and data quality,\nfacilitating a deeper understanding of cross-modal interactions and alignment\ndynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides\nan intrinsic data filtering mechanism to enhance model alignment without\nrequiring additional models. Specifically, when applied to the alignment\nprocess of MLLMs, SAE-V-based data filtering methods could achieve more than\n110% performance with less than 50% data. Our results highlight SAE-V's ability\nto enhance interpretability and alignment in MLLMs, providing insights into\ntheir internal mechanisms.\n","authors":["Hantao Lou","Changye Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2502.17514v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2506.14104v1","updated":"2025-06-17T01:47:17Z","published":"2025-06-17T01:47:17Z","title":"Innovating China's Intangible Cultural Heritage with DeepSeek +\n  MidJourney: The Case of Yangliuqing theme Woodblock Prints","summary":"  Yangliuqing woodblock prints, a cornerstone of China's intangible cultural\nheritage, are celebrated for their intricate designs and vibrant colors.\nHowever, preserving these traditional art forms while fostering innovation\npresents significant challenges. This study explores the DeepSeek + MidJourney\napproach to generating creative, themed Yangliuqing woodblock prints focused on\nthe fight against COVID-19 and depicting joyous winners. Using Fr\\'echet\nInception Distance (FID) scores for evaluation, the method that combined\nDeepSeek-generated thematic prompts, MidJourney-generated thematic images,\noriginal Yangliuqing prints, and DeepSeek-generated key prompts in\nMidJourney-generated outputs achieved the lowest mean FID score (150.2) with\nminimal variability ({\\sigma} = 4.9). Additionally, feedback from 62\nparticipants, collected via questionnaires, confirmed that this hybrid approach\nproduced the most representative results. Moreover, the questionnaire data\nrevealed that participants demonstrated the highest willingness to promote\ntraditional culture and the strongest interest in consuming the AI-generated\nimages produced through this method. These findings underscore the\neffectiveness of an innovative approach that seamlessly blends traditional\nartistic elements with modern AI-driven creativity, ensuring both cultural\npreservation and contemporary relevance.\n","authors":["RuiKun Yang","ZhongLiang Wei","Longdi Xian"],"pdf_url":"https://arxiv.org/pdf/2506.14104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14101v1","updated":"2025-06-17T01:33:01Z","published":"2025-06-17T01:33:01Z","title":"Abstract Meaning Representation for Hospital Discharge Summarization","summary":"  The Achilles heel of Large Language Models (LLMs) is hallucination, which has\ndrastic consequences for the clinical domain. This is particularly important\nwith regards to automatically generating discharge summaries (a lengthy medical\ndocument that summarizes a hospital in-patient visit). Automatically generating\nthese summaries would free physicians to care for patients and reduce\ndocumentation burden. The goal of this work is to discover new methods that\ncombine language-based graphs and deep learning models to address provenance of\ncontent and trustworthiness in automatic summarization. Our method shows\nimpressive reliability results on the publicly available Medical Information\nMart for Intensive III (MIMIC-III) corpus and clinical notes written by\nphysicians at Anonymous Hospital. rovide our method, generated discharge ary\noutput examples, source code and trained models.\n","authors":["Paul Landes","Sitara Rao","Aaron Jeremy Chaise","Barbara Di Eugenio"],"pdf_url":"https://arxiv.org/pdf/2506.14101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12674v2","updated":"2025-06-17T01:12:31Z","published":"2025-06-15T00:57:48Z","title":"Enhancing Clinical Models with Pseudo Data for De-identification","summary":"  Many models are pretrained on redacted text for privacy reasons. Clinical\nfoundation models are often trained on de-identified text, which uses special\nsyntax (masked) text in place of protected health information. Even though\nthese models have increased in popularity, there has been little effort in\nunderstanding the effects of training them on redacted text. In this work, we\npretrain several encoder-only models on a dataset that contains redacted text\nand a version with replaced realistic pseudo text. We then fine-tuned models\nfor the protected health information de-identification task and show how our\nmethods significantly outperform previous baselines. The contributions of this\nwork include: a) our novel, and yet surprising findings with training\nrecommendations, b) redacted text replacements used to produce the pseudo\ndataset, c) pretrained embeddings and fine-tuned task specific models, and d)\nfreely available pseudo training dataset generation and model source code used\nin our experiments.\n","authors":["Paul Landes","Aaron J Chaise","Tarak Nath Nandi","Ravi K Madduri"],"pdf_url":"https://arxiv.org/pdf/2506.12674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14086v1","updated":"2025-06-17T01:04:45Z","published":"2025-06-17T01:04:45Z","title":"InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking","summary":"  Large Language Models (LLMs) have demonstrated significant strides across\nvarious information retrieval tasks, particularly as rerankers, owing to their\nstrong generalization and knowledge-transfer capabilities acquired from\nextensive pretraining. In parallel, the rise of LLM-based chat interfaces has\nraised user expectations, encouraging users to pose more complex queries that\nnecessitate retrieval by ``reasoning'' over documents rather than through\nsimple keyword matching or semantic similarity. While some recent efforts have\nexploited reasoning abilities of LLMs for reranking such queries, considerable\npotential for improvement remains. In that regards, we introduce InsertRank, an\nLLM-based reranker that leverages lexical signals like BM25 scores during\nreranking to further improve retrieval performance. InsertRank demonstrates\nimproved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning\n12 diverse domains, and R2MED, a specialized medical reasoning retrieval\nbenchmark spanning 8 different tasks. We conduct an exhaustive evaluation and\nseveral ablation studies and demonstrate that InsertRank consistently improves\nretrieval effectiveness across multiple families of LLMs, including GPT,\nGemini, and Deepseek models. %In addition, we also conduct ablation studies on\nnormalization by varying the scale of the BM25 scores, and positional bias by\nshuffling the order of the documents. With Deepseek-R1, InsertRank achieves a\nscore of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,\nsurpassing previous methods.\n","authors":["Rahul Seetharaman","Kaustubh D. Dhole","Aman Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.14086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15025v1","updated":"2025-06-17T23:57:30Z","published":"2025-06-17T23:57:30Z","title":"Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size","summary":"  Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.\n","authors":["Soufiane Hayou","Liyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15025v1.pdf","comment":"TD,LR: How to set the learning rate for emebdding layer in LLMs?"},{"id":"http://arxiv.org/abs/2506.09331v2","updated":"2025-06-17T23:22:53Z","published":"2025-06-11T02:12:34Z","title":"Multi-Agent Language Models: Advancing Cooperation, Coordination, and\n  Adaptation","summary":"  Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.\n","authors":["Arjun Vaithilingam Sudhakar"],"pdf_url":"https://arxiv.org/pdf/2506.09331v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.07687"},{"id":"http://arxiv.org/abs/2503.15848v2","updated":"2025-06-17T22:37:38Z","published":"2025-03-20T05:03:26Z","title":"Entropy-based Exploration Conduction for Multi-step Reasoning","summary":"  Multi-step processes via large language models (LLMs) have proven effective\nfor solving complex reasoning tasks. However, the depth of exploration of the\nreasoning procedure can significantly affect the task performance. Existing\nmethods to automatically decide the depth often lead to high cost and a lack of\nflexibility. To address these issues, we propose Entropy-based Exploration\nDepth Conduction (Entro-duction), a novel method that dynamically adjusts the\nexploration depth during multi-step reasoning by monitoring LLM's output\nentropy and variance entropy. We employ these two features to capture the\nmodel's uncertainty of the current step and the fluctuation of uncertainty\nacross consecutive reasoning steps. Based on the observed entropy changes, the\nLLM selects whether to deepen, expand, or stop exploration according to the\nprobability, which facilitates the trade-off between the reasoning accuracy and\nexploration effectiveness. Experimental results across four benchmark datasets\ndemonstrate the efficacy of Entro-duction.\n","authors":["Jinghan Zhang","Xiting Wang","Fengran Mo","Yeyang Zhou","Wanfu Gao","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15848v2.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2506.15001v1","updated":"2025-06-17T22:13:34Z","published":"2025-06-17T22:13:34Z","title":"Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings","summary":"  In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.\n","authors":["Ignacio Sastre","Aiala Rosá"],"pdf_url":"https://arxiv.org/pdf/2506.15001v1.pdf","comment":"This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025"},{"id":"http://arxiv.org/abs/2506.14997v1","updated":"2025-06-17T22:04:55Z","published":"2025-06-17T22:04:55Z","title":"Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple\n  Choice Settings","summary":"  As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.\n","authors":["Harbin Hong","Sebastian Caldas","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2506.14997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06561v2","updated":"2025-06-17T20:40:28Z","published":"2025-06-06T22:16:16Z","title":"LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles","summary":"  Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.\n","authors":["Ho Yin 'Sam' Ng","Ting-Yao Hsu","Aashish Anantha Ramakrishnan","Branislav Kveton","Nedim Lipka","Franck Dernoncourt","Dongwon Lee","Tong Yu","Sungchul Kim","Ryan A. Rossi","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2506.06561v2.pdf","comment":"The LaMP-CAP dataset is publicly available at:\n  https://github.com/Crowd-AI-Lab/lamp-cap"},{"id":"http://arxiv.org/abs/2506.14965v1","updated":"2025-06-17T20:24:00Z","published":"2025-06-17T20:24:00Z","title":"Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective","summary":"  Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360\n","authors":["Zhoujun Cheng","Shibo Hao","Tianyang Liu","Fan Zhou","Yutao Xie","Feng Yao","Yuexin Bian","Yonghao Zhuang","Nilabjo Dey","Yuheng Zha","Yi Gu","Kun Zhou","Yuqi Wang","Yuan Li","Richard Fan","Jianshu She","Chengqian Gao","Abulhair Saparov","Haonan Li","Taylor W. Killian","Mikhail Yurochkin","Zhengzhong Liu","Eric P. Xing","Zhiting Hu"],"pdf_url":"https://arxiv.org/pdf/2506.14965v1.pdf","comment":"38 pages, 9 figures. Under review"},{"id":"http://arxiv.org/abs/2506.14949v1","updated":"2025-06-17T20:00:16Z","published":"2025-06-17T20:00:16Z","title":"From Chat to Checkup: Can Large Language Models Assist in Diabetes\n  Prediction?","summary":"  While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.\n","authors":["Shadman Sakib","Oishy Fatema Akhand","Ajwad Abrar"],"pdf_url":"https://arxiv.org/pdf/2506.14949v1.pdf","comment":"Accepted in 1st IEEE QPAIN 2025"},{"id":"http://arxiv.org/abs/2503.11895v2","updated":"2025-06-17T19:33:26Z","published":"2025-03-14T21:53:12Z","title":"Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model\n  Editing","summary":"  Large Language Models (LLMs) are widely deployed in downstream tasks, but\nkeeping their knowledge up-to-date via retraining or fine-tuning is often\ncomputationally expensive. Model editing provides a more efficient alternative\nby updating a targeted subset of parameters, which often follows the\nlocate-and-edit paradigm. Despite this efficiency, existing methods are\nlimited: edits may fail to inject knowledge (UnderEdit) or unintentionally\ndisrupt unrelated neighboring knowledge (OverEdit). To address these\nchallenges, we propose two complementary methods: iterative model editing,\nwhich applies successive edits to mitigate UnderEdit, and neighbor-assisted\nmodel editing, which incorporates neighboring knowledge during editing to\nreduce OverEdit. Our extensive experiments show that these techniques improve\nediting performance across multiple LLMs, algorithms, and benchmarks, reducing\nUnderEdit by up to 38 percentage points and OverEdit by up to 6, while\nremaining broadly applicable to any locate-and-edit method.\n","authors":["Bhiman Kumar Baghel","Scott M. Jordan","Zheyuan Ryan Shi","Xiang Lorraine Li"],"pdf_url":"https://arxiv.org/pdf/2503.11895v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2506.09099v2","updated":"2025-06-17T19:17:29Z","published":"2025-06-10T14:49:33Z","title":"Too Big to Think: Capacity, Memorization, and Generalization in\n  Pre-Trained Transformers","summary":"  The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.\n","authors":["Joshua Barron","Devin White"],"pdf_url":"https://arxiv.org/pdf/2506.09099v2.pdf","comment":"Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2506.14927v1","updated":"2025-06-17T19:14:30Z","published":"2025-06-17T19:14:30Z","title":"MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with\n  Knowledge Guidance","summary":"  Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.\n","authors":["Joseph J. Peper","Wenzhao Qiu","Ali Payani","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14927v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2504.20304v3","updated":"2025-06-17T18:46:13Z","published":"2025-04-28T23:20:36Z","title":"UD-English-CHILDES: A Collected Resource of Gold and Silver Universal\n  Dependencies Trees for Child Language Interactions","summary":"  CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank. It is derived from previously\ndependency-annotated CHILDES data, which we harmonize to follow unified\nannotation principles. The gold-standard trees encompass utterances sampled\nfrom 11 children and their caregivers, totaling over 48K sentences (236K\ntokens). We validate these gold-standard annotations under the UD v2 framework\nand provide an additional 1M~silver-standard sentences, offering a consistent\nresource for computational and linguistic research.\n","authors":["Xiulin Yang","Zhuoxuan Ju","Lanni Bu","Zoey Liu","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2504.20304v3.pdf","comment":"UDW 2025"},{"id":"http://arxiv.org/abs/2501.03491v2","updated":"2025-06-17T18:44:37Z","published":"2025-01-07T03:21:17Z","title":"Can LLMs Ask Good Questions?","summary":"  We evaluate questions generated by large language models (LLMs) from context,\ncomparing them to human-authored questions across six dimensions: question\ntype, question length, context coverage, answerability, uncommonness, and\nrequired answer length. Our study spans two open-source and two proprietary\nstate-of-the-art models. Results reveal that LLM-generated questions tend to\ndemand longer descriptive answers and exhibit more evenly distributed context\nfocus, in contrast to the positional bias often seen in QA tasks. These\nfindings provide insights into the distinctive characteristics of LLM-generated\nquestions and inform future work on question quality and downstream\napplications.\n","authors":["Yueheng Zhang","Xiaoyuan Liu","Yiyou Sun","Atheer Alharbi","Hend Alzahrani","Tianneng Shi","Basel Alomair","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2501.03491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14912v1","updated":"2025-06-17T18:44:21Z","published":"2025-06-17T18:44:21Z","title":"CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision","summary":"  The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.\n","authors":["Dyah Adila","Shuai Zhang","Boran Han","Bonan Min","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14901v1","updated":"2025-06-17T18:16:17Z","published":"2025-06-17T18:16:17Z","title":"Combining Constrained and Unconstrained Decoding via Boosting: BoostCD\n  and Its Application to Information Extraction","summary":"  Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.\n","authors":["Marija Šakota","Robert West"],"pdf_url":"https://arxiv.org/pdf/2506.14901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14900v1","updated":"2025-06-17T18:13:40Z","published":"2025-06-17T18:13:40Z","title":"Adverse Event Extraction from Discharge Summaries: A New Dataset,\n  Annotation Scheme, and Initial Findings","summary":"  In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.\n","authors":["Imane Guellil","Salomé Andres","Atul Anand","Bruce Guthrie","Huayu Zhang","Abul Hasan","Honghan Wu","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2506.14900v1.pdf","comment":"Accepted and will be published at ACL2025 (main conference)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2506.14769v1","updated":"2025-06-17T17:59:12Z","published":"2025-06-17T17:59:12Z","title":"CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion","summary":"  Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.\n","authors":["Jiahua Ma","Yiran Qin","Yixiong Li","Xuanqi Liao","Yulan Guo","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14766v1","updated":"2025-06-17T17:58:11Z","published":"2025-06-17T17:58:11Z","title":"ASCD: Attention-Steerable Contrastive Decoding for Reducing\n  Hallucination in MLLM","summary":"  Multimodal Large Language Model (MLLM) often suffer from hallucinations. They\nover-rely on partial cues and generate incorrect responses. Recently, methods\nlike Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding\n(ICD) have been proposed to mitigate hallucinations by contrasting predictions\nfrom perturbed or negatively prefixed inputs against original outputs. In this\nwork, we uncover that methods like VCD and ICD fundamentally influence internal\nattention dynamics of the model. This observation suggests that their\neffectiveness may not stem merely from surface-level modifications to logits\nbut from deeper shifts in attention distribution. Inspired by this insight, we\npropose an attention-steerable contrastive decoding framework that directly\nintervenes in attention mechanisms of the model to offer a more principled\napproach to mitigating hallucinations. Our experiments across multiple MLLM\narchitectures and diverse decoding methods demonstrate that our approach\nsignificantly reduces hallucinations and improves the performance on benchmarks\nsuch as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing\nperformance on standard VQA benchmarks.\n","authors":["Yujun Wang","Jinhe Bi","Yunpu Ma","Soeren Pirk"],"pdf_url":"https://arxiv.org/pdf/2506.14766v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.14765v1","updated":"2025-06-17T17:58:08Z","published":"2025-06-17T17:58:08Z","title":"Scaling-Up the Pretraining of the Earth Observation Foundation Model\n  PhilEO to the MajorTOM Dataset","summary":"  Today, Earth Observation (EO) satellites generate massive volumes of data,\nwith the Copernicus Sentinel-2 constellation alone producing approximately\n1.6TB per day. To fully exploit this information, it is essential to pretrain\nEO Foundation Models (FMs) on large unlabeled datasets, enabling efficient\nfine-tuning for several different downstream tasks with minimal labeled data.\nIn this work, we present the scaling-up of our recently proposed EO Foundation\nModel, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which\ncovers the vast majority of the Earth's surface, as well as on the specialized\nsubset FastTOM 2TB that does not include oceans and ice. We develop and study\nvarious PhilEO model variants with different numbers of parameters and\narchitectures. Finally, we fine-tune the models on the PhilEO Bench for road\ndensity estimation, building density pixel-wise regression, and land cover\nsemantic segmentation, and we evaluate the performance. Our results demonstrate\nthat for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB\nmodel outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots\nfor road density estimation and building density regression, PhilEO 200M\nFastTOM outperforms all the other models. The effectiveness of both dataset and\nmodel scaling is validated using the PhilEO Bench. We also study the impact of\narchitecture scaling, transitioning from U-Net Convolutional Neural Networks\n(CNN) to Vision Transformers (ViT).\n","authors":["Nikolaos Dionelis","Jente Bosmans","Riccardo Musto","Giancarlo Paoletti","Simone Sarti","Giacomo Cascarano","Casper Fibaek","Luke Camilleri","Bertrand Le Saux","Nicolas Longépé"],"pdf_url":"https://arxiv.org/pdf/2506.14765v1.pdf","comment":"6 pages, 9 figures, 1 table, 29 references"},{"id":"http://arxiv.org/abs/2506.14753v1","updated":"2025-06-17T17:48:50Z","published":"2025-06-17T17:48:50Z","title":"Cost-Aware Routing for Efficient Text-To-Image Generation","summary":"  Diffusion models are well known for their ability to generate a high-fidelity\nimage for an input prompt through an iterative denoising process.\nUnfortunately, the high fidelity also comes at a high computational cost due\nthe inherently sequential generative process. In this work, we seek to\noptimally balance quality and computational cost, and propose a framework to\nallow the amount of computation to vary for each prompt, depending on its\ncomplexity. Each prompt is automatically routed to the most appropriate\ntext-to-image generation function, which may correspond to a distinct number of\ndenoising steps of a diffusion model, or a disparate, independent text-to-image\nmodel. Unlike uniform cost reduction techniques (e.g., distillation, model\nquantization), our approach achieves the optimal trade-off by learning to\nreserve expensive choices (e.g., 100+ denoising steps) only for a few complex\nprompts, and employ more economical choices (e.g., small distilled model) for\nless sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB\nthat by learning to route to nine already-trained text-to-image models, our\napproach is able to deliver an average quality that is higher than that\nachievable by any of these models alone.\n","authors":[" Qinchan"," Li","Kenneth Chen"," Changyue"," Su","Wittawat Jitkrittum","Qi Sun","Patsorn Sangkloy"],"pdf_url":"https://arxiv.org/pdf/2506.14753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14742v1","updated":"2025-06-17T17:22:12Z","published":"2025-06-17T17:22:12Z","title":"SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads\n  Synthesis Using Gaussian Splatting","summary":"  Achieving high synchronization in the synthesis of realistic, speech-driven\ntalking head videos presents a significant challenge. A lifelike talking head\nrequires synchronized coordination of subject identity, lip movements, facial\nexpressions, and head poses. The absence of these synchronizations is a\nfundamental flaw, leading to unrealistic results. To address the critical issue\nof synchronization, identified as the ''devil'' in creating realistic talking\nheads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with\nGaussian Splatting to ensure consistent subject identity preservation and a\nFace-Sync Controller that aligns lip movements with speech while innovatively\nusing a 3D facial blendshape model to reconstruct accurate facial expressions.\nTo ensure natural head movements, we propose a Head-Sync Stabilizer, which\noptimizes head poses for greater stability. Additionally, SyncTalk++ enhances\nrobustness to out-of-distribution (OOD) audio by incorporating an Expression\nGenerator and a Torso Restorer, which generate speech-matched facial\nexpressions and seamless torso regions. Our approach maintains consistency and\ncontinuity in visual details across frames and significantly improves rendering\nspeed and quality, achieving up to 101 frames per second. Extensive experiments\nand user studies demonstrate that SyncTalk++ outperforms state-of-the-art\nmethods in synchronization and realism. We recommend watching the supplementary\nvideo: https://ziqiaopeng.github.io/synctalk++.\n","authors":["Ziqiao Peng","Wentao Hu","Junyuan Ma","Xiangyu Zhu","Xiaomei Zhang","Hao Zhao","Hui Tian","Jun He","Hongyan Liu","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2506.14742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14730v1","updated":"2025-06-17T17:12:22Z","published":"2025-06-17T17:12:22Z","title":"Active InSAR monitoring of building damage in Gaza during the\n  Israel-Hamas War","summary":"  Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the\nmost intense bombing campaigns of the twenty-first century, driving widespread\nurban damage. Characterizing damage over a geographically dynamic and\nprotracted armed conflict requires active monitoring. Synthetic aperture radar\n(SAR) has precedence for mapping disaster-induced damage with bi-temporal\nmethods but applications to active monitoring during sustained crises are\nlimited. Using interferometric SAR data from Sentinel-1, we apply a long\ntemporal-arc coherent change detection (LT-CCD) approach to track weekly damage\ntrends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of\ndamage labels in reference data from the United Nations with a negligible\n(1.2%) false positive rate. The temporal fidelity of our approach reveals\nrapidly increasing damage during the first three months of the war focused in\nnorthern Gaza, a notable pause in damage during a temporary ceasefire, and\nsurges of new damage as conflict hot-spots shift from north to south.\nThree-fifths (191,263) of all buildings are damaged or destroyed by the end of\nthe study. With massive need for timely data on damage in armed conflict zones,\nour low-cost and low-latency approach enables rapid uptake of damage\ninformation at humanitarian and journalistic organizations.\n","authors":["Corey Scher","Jamon Van Den Hoek"],"pdf_url":"https://arxiv.org/pdf/2506.14730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14719v1","updated":"2025-06-17T16:52:57Z","published":"2025-06-17T16:52:57Z","title":"Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate\n  Industrial Computed Tomography Reconstruction","summary":"  Cone-beam X-ray computed tomography (XCT) is an essential imaging technique\nfor generating 3D reconstructions of internal structures, with applications\nranging from medical to industrial imaging. Producing high-quality\nreconstructions typically requires many X-ray measurements; this process can be\nslow and expensive, especially for dense materials. Recent work incorporating\nartifact reduction priors within a plug-and-play (PnP) reconstruction framework\nhas shown promising results in improving image quality from sparse-view XCT\nscans while enhancing the generalizability of deep learning-based solutions.\nHowever, this method uses a 2D convolutional neural network (CNN) for artifact\nreduction, which captures only slice-independent information from the 3D\nreconstruction, limiting performance. In this paper, we propose a PnP\nreconstruction method that uses a 2.5D artifact reduction CNN as the prior.\nThis approach leverages inter-slice information from adjacent slices, capturing\nricher spatial context while remaining computationally efficient. We show that\nthis 2.5D prior not only improves the quality of reconstructions but also\nenables the model to directly suppress commonly occurring XCT artifacts (such\nas beam hardening), eliminating the need for artifact correction\npre-processing. Experiments on both experimental and synthetic cone-beam XCT\ndata demonstrate that the proposed method better preserves fine structural\ndetails, such as pore size and shape, leading to more accurate defect detection\ncompared to 2D priors. In particular, we demonstrate strong performance on\nexperimental XCT data using a 2.5D artifact reduction prior trained entirely on\nsimulated scans, highlighting the proposed method's ability to generalize\nacross domains.\n","authors":["Haley Duba-Sullivan","Aniket Pramanik","Venkatakrishnan Singanallur","Amirkoushyar Ziabari"],"pdf_url":"https://arxiv.org/pdf/2506.14719v1.pdf","comment":"Submitted to Journal of Nondestructive Evaluation"},{"id":"http://arxiv.org/abs/2506.14709v1","updated":"2025-06-17T16:49:27Z","published":"2025-06-17T16:49:27Z","title":"DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window\n  Bi-directional Parallax Attention and Cross-modal Transfer Learning","summary":"  Depth estimation is crucial for intelligent systems, enabling applications\nfrom autonomous navigation to augmented reality. While traditional stereo and\nactive depth sensors have limitations in cost, power, and robustness,\ndual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling\nalternative. This paper introduces DiFuse-Net, a novel modality decoupled\nnetwork design for disentangled RGB and DP based depth estimation. DiFuse-Net\nfeatures a window bi-directional parallax attention mechanism (WBiPAM)\nspecifically designed to capture the subtle DP disparity cues unique to\nsmartphone cameras with small aperture. A separate encoder extracts contextual\ninformation from the RGB image, and these features are fused to enhance depth\nprediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to\nutilize large-scale RGB-D datasets in the literature to cope with the\nlimitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and\ncomparison of the proposed method demonstrates its superiority over the DP and\nstereo-based baseline methods. Additionally, we contribute a new, high-quality,\nreal-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP)\ndataset, created using our novel symmetric stereo camera hardware setup, stereo\ncalibration and rectification protocol, and AI stereo disparity estimation\nmethod.\n","authors":["Kunal Swami","Debtanu Gupta","Amrit Kumar Muduli","Chirag Jaiswal","Pankaj Kumar Bajpai"],"pdf_url":"https://arxiv.org/pdf/2506.14709v1.pdf","comment":"Accepted in IROS 2025"},{"id":"http://arxiv.org/abs/2506.14706v1","updated":"2025-06-17T16:44:51Z","published":"2025-06-17T16:44:51Z","title":"Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion","summary":"  Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion\nof camera and LiDAR data addresses the limitations of individual sensors but\nrelies on precise extrinsic calibration. Recently, numerous end-to-end\ncalibration methods have been proposed; however, most predict extrinsic\nparameters in a single step and lack iterative optimization capabilities. To\naddress the increasing demand for higher accuracy, we propose a versatile\niterative framework based on surrogate diffusion. This framework can enhance\nthe performance of any calibration method without requiring architectural\nmodifications. Specifically, the initial extrinsic parameters undergo iterative\nrefinement through a denoising process, in which the original calibration\nmethod serves as a surrogate denoiser to estimate the final extrinsics at each\nstep. For comparative analysis, we selected four state-of-the-art calibration\nmethods as surrogate denoisers and compared the results of our diffusion\nprocess with those of two other iterative approaches. Extensive experiments\ndemonstrate that when integrated with our diffusion model, all calibration\nmethods achieve higher accuracy, improved robustness, and greater stability\ncompared to other iterative techniques and their single-step counterparts.\n","authors":["Ni Ou","Zhuo Chen","Xinru Zhang","Junzheng Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14706v1.pdf","comment":"7 pages, 4 figures, accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2506.14698v1","updated":"2025-06-17T16:38:15Z","published":"2025-06-17T16:38:15Z","title":"Towards Desiderata-Driven Design of Visual Counterfactual Explainers","summary":"  Visual counterfactual explainers (VCEs) are a straightforward and promising\napproach to enhancing the transparency of image classifiers. VCEs complement\nother types of explanations, such as feature attribution, by revealing the\nspecific data transformations to which a machine learning model responds most\nstrongly. In this paper, we argue that existing VCEs focus too narrowly on\noptimizing sample quality or change minimality; they fail to consider the more\nholistic desiderata for an explanation, such as fidelity, understandability,\nand sufficiency. To address this shortcoming, we explore new mechanisms for\ncounterfactual generation and investigate how they can help fulfill these\ndesiderata. We combine these mechanisms into a novel 'smooth counterfactual\nexplorer' (SCE) algorithm and demonstrate its effectiveness through systematic\nevaluations on synthetic and real data.\n","authors":["Sidney Bender","Jan Herrmann","Klaus-Robert Müller","Grégoire Montavon"],"pdf_url":"https://arxiv.org/pdf/2506.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14696v1","updated":"2025-06-17T16:37:00Z","published":"2025-06-17T16:37:00Z","title":"YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object\n  Detection Framework","summary":"  Multispectral object detection, which integrates information from multiple\nbands, can enhance detection accuracy and environmental adaptability, holding\ngreat application potential across various fields. Although existing methods\nhave made progress in cross-modal interaction, low-light conditions, and model\nlightweight, there are still challenges like the lack of a unified single-stage\nframework, difficulty in balancing performance and fusion strategy, and\nunreasonable modality weight allocation. To address these, based on the YOLOv11\nframework, we present YOLOv11-RGBT, a new comprehensive multimodal object\ndetection framework. We designed six multispectral fusion modes and\nsuccessfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After\nreevaluating the importance of the two modalities, we proposed a P3 mid-fusion\nstrategy and multispectral controllable fine-tuning (MCF) strategy for\nmultispectral models. These improvements optimize feature fusion, reduce\nredundancy and mismatches, and boost overall model performance. Experiments\nshow our framework excels on three major open-source multispectral object\ndetection datasets, like LLVIP and FLIR. Particularly, the multispectral\ncontrollable fine-tuning strategy significantly enhanced model adaptability and\nrobustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP\nby 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and\nstrategies' effectiveness. The code is available at:\nhttps://github.com/wandahangFY/YOLOv11-RGBT.\n","authors":["Dahang Wan","Rongsheng Lu","Yang Fang","Xianli Lang","Shuangbao Shu","Jingjing Chen","Siyuan Shen","Ting Xu","Zecong Ye"],"pdf_url":"https://arxiv.org/pdf/2506.14696v1.pdf","comment":"28 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.05478v2","updated":"2025-06-17T16:28:39Z","published":"2025-01-07T16:01:25Z","title":"Language and Planning in Robotic Navigation: A Multilingual Evaluation\n  of State-of-the-Art Models","summary":"  Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.\n","authors":["Malak Mansour","Ahmed Aly","Bahey Tharwat","Sarim Hashmi","Dong An","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2501.05478v2.pdf","comment":"This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/"},{"id":"http://arxiv.org/abs/2506.14686v1","updated":"2025-06-17T16:21:32Z","published":"2025-06-17T16:21:32Z","title":"FocalClick-XL: Towards Unified and High-quality Interactive Segmentation","summary":"  Interactive segmentation enables users to extract binary masks of target\nobjects through simple interactions such as clicks, scribbles, and boxes.\nHowever, existing methods often support only limited interaction forms and\nstruggle to capture fine details. In this paper, we revisit the classical\ncoarse-to-fine design of FocalClick and introduce significant extensions.\nInspired by its multi-stage strategy, we propose a novel pipeline,\nFocalClick-XL, to address these challenges simultaneously. Following the\nemerging trend of large-scale pretraining, we decompose interactive\nsegmentation into meta-tasks that capture different levels of information --\ncontext, object, and detail -- assigning a dedicated subnet to each level.This\ndecomposition allows each subnet to undergo scaled pretraining with independent\ndata and supervision, maximizing its effectiveness. To enhance flexibility, we\nshare context- and detail-level information across different interaction forms\nas common knowledge while introducing a prompting layer at the object level to\nencode specific interaction types. As a result, FocalClick-XL achieves\nstate-of-the-art performance on click-based benchmarks and demonstrates\nremarkable adaptability to diverse interaction formats, including boxes,\nscribbles, and coarse masks. Beyond binary mask generation, it is also capable\nof predicting alpha mattes with fine-grained details, making it a versatile and\npowerful tool for interactive segmentation.\n","authors":["Xi Chen","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.14686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05164v3","updated":"2025-06-17T16:15:45Z","published":"2024-05-08T15:54:57Z","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion","summary":"  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n","authors":["Bing Zhu","Zixin He","Weiyi Xiong","Guanhua Ding","Tao Huang","Wei Chen","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2405.05164v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14674v1","updated":"2025-06-17T16:07:58Z","published":"2025-06-17T16:07:58Z","title":"Recognition through Reasoning: Reinforcing Image Geo-localization with\n  Large Vision-Language Models","summary":"  Previous methods for image geo-localization have typically treated the task\nas either classification or retrieval, often relying on black-box decisions\nthat lack interpretability. The rise of large vision-language models (LVLMs)\nhas enabled a rethinking of geo-localization as a reasoning-driven task\ngrounded in visual cues. However, two major challenges persist. On the data\nside, existing reasoning-focused datasets are primarily based on street-view\nimagery, offering limited scene diversity and constrained viewpoints. On the\nmodeling side, current approaches predominantly rely on supervised fine-tuning,\nwhich yields only marginal improvements in reasoning capabilities. To address\nthese challenges, we propose a novel pipeline that constructs a\nreasoning-oriented geo-localization dataset, MP16-Reason, using diverse social\nmedia images. We introduce GLOBE, Group-relative policy optimization for\nLocatability assessment and Optimized visual-clue reasoning, yielding\nBi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE\nincorporates task-specific rewards that jointly enhance locatability\nassessment, visual clue reasoning, and geolocation accuracy. Both qualitative\nand quantitative results demonstrate that GLOBE outperforms state-of-the-art\nopen-source LVLMs on geo-localization tasks, particularly in diverse visual\nscenes, while also generating more insightful and interpretable reasoning\ntrajectories.\n","authors":["Ling Li","Yao Zhou","Yuxuan Liang","Fugee Tsung","Jiaheng Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10867v2","updated":"2025-06-17T16:07:14Z","published":"2023-07-20T13:40:22Z","title":"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with\n  Human Feedback","summary":"  Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.\n","authors":["Ashish Singh","Ashutosh Singh","Prateek Agarwal","Zixuan Huang","Arpita Singh","Tong Yu","Sungchul Kim","Victor Bursztyn","Nesreen K. Ahmed","Puneet Mathur","Erik Learned-Miller","Franck Dernoncourt","Ryan A. Rossi"],"pdf_url":"https://arxiv.org/pdf/2307.10867v2.pdf","comment":"16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/"},{"id":"http://arxiv.org/abs/2506.14667v1","updated":"2025-06-17T15:58:10Z","published":"2025-06-17T15:58:10Z","title":"DDS-NAS: Dynamic Data Selection within Neural Architecture Search via\n  On-line Hard Example Mining applied to Image Classification","summary":"  In order to address the scalability challenge within Neural Architecture\nSearch (NAS), we speed up NAS training via dynamic hard example mining within a\ncurriculum learning framework. By utilizing an autoencoder that enforces an\nimage similarity embedding in latent space, we construct an efficient kd-tree\nstructure to order images by furthest neighbour dissimilarity in a\nlow-dimensional embedding. From a given query image from our subsample dataset,\nwe can identify the most dissimilar image within the global dataset in\nlogarithmic time. Via curriculum learning, we then dynamically re-formulate an\nunbiased subsample dataset for NAS optimisation, upon which the current NAS\nsolution architecture performs poorly. We show that our DDS-NAS framework\nspeeds up gradient-based NAS strategies by up to 27x without loss in\nperformance. By maximising the contribution of each image sample during\ntraining, we reduce the duration of a NAS training cycle and the number of\niterations required for convergence.\n","authors":["Matt Poyser","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2506.14667v1.pdf","comment":"27 single-column pages, 8 figures, to be published in Pattern\n  Recognition"},{"id":"http://arxiv.org/abs/2412.06745v2","updated":"2025-06-17T15:57:52Z","published":"2024-12-09T18:37:14Z","title":"ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities","summary":"  Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.\n","authors":["Adhiraj Ghosh","Sebastian Dziadzio","Ameya Prabhu","Vishaal Udandarao","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2412.06745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13174v2","updated":"2025-06-17T15:47:28Z","published":"2025-02-17T21:24:18Z","title":"Diverse Topology Optimization using Modulated Neural Fields","summary":"  Topology optimization (TO) is a family of computational methods that derive\nnear-optimal geometries from formal problem descriptions. Despite their\nsuccess, established TO methods are limited to generating single solutions,\nrestricting the exploration of alternative designs. To address this limitation,\nwe introduce Topology Optimization using Modulated Neural Fields (TOM) - a\ndata-free method that trains a neural network to generate structurally\ncompliant shapes and explores diverse solutions through an explicit diversity\nconstraint. The network is trained with a solver-in-the-loop, optimizing the\nmaterial distribution in each iteration. The trained model produces diverse\nshapes that closely adhere to the design requirements. We validate TOM on 2D\nand 3D TO problems. Our results show that TOM generates more diverse solutions\nthan any previous method, all while maintaining near-optimality and without\nrelying on a dataset. These findings open new avenues for engineering and\ndesign, offering enhanced flexibility and innovation in structural\noptimization.\n","authors":["Andreas Radler","Eric Volkmann","Johannes Brandstetter","Arturs Berzins"],"pdf_url":"https://arxiv.org/pdf/2502.13174v2.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2506.14642v1","updated":"2025-06-17T15:39:34Z","published":"2025-06-17T15:39:34Z","title":"3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D\n  Gaussian-Splatting","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel\nview synthesis, offering real-time rendering with high visual fidelity.\nHowever, its substantial storage requirements present significant challenges\nfor practical applications. While recent state-of-the-art (SOTA) 3DGS methods\nincreasingly incorporate dedicated compression modules, there is a lack of a\ncomprehensive framework to evaluate their perceptual impact. Therefore we\npresent 3DGS-IEval-15K, the first large-scale image quality assessment (IQA)\ndataset specifically designed for compressed 3DGS representations. Our dataset\nencompasses 15,200 images rendered from 10 real-world scenes through 6\nrepresentative 3DGS algorithms at 20 strategically selected viewpoints, with\ndifferent compression levels leading to various distortion effects. Through\ncontrolled subjective experiments, we collect human perception data from 60\nviewers. We validate dataset quality through scene diversity and MOS\ndistribution analysis, and establish a comprehensive benchmark with 30\nrepresentative IQA metrics covering diverse types. As the largest-scale 3DGS\nquality assessment dataset to date, our work provides a foundation for\ndeveloping 3DGS specialized IQA metrics, and offers essential data for\ninvestigating view-dependent quality distribution patterns unique to 3DGS. The\ndatabase is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.\n","authors":["Yuke Xing","Jiarui Wang","Peizhi Niu","Wenjie Huang","Guangtao Zhai","Yiling Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04431v2","updated":"2025-06-17T15:32:20Z","published":"2024-12-05T18:53:02Z","title":"Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution\n  Image Synthesis","summary":"  We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of\ngenerating high-resolution, photorealistic images following language\ninstruction. Infinity redefines visual autoregressive model under a bitwise\ntoken prediction framework with an infinite-vocabulary tokenizer & classifier\nand bitwise self-correction mechanism, remarkably improving the generation\ncapacity and details. By theoretically scaling the tokenizer vocabulary size to\ninfinity and concurrently scaling the transformer size, our method\nsignificantly unleashes powerful scaling capabilities compared to vanilla VAR.\nInfinity sets a new record for autoregressive text-to-image models,\noutperforming top-tier diffusion models like SD3-Medium and SDXL. Notably,\nInfinity surpasses SD3-Medium by improving the GenEval benchmark score from\n0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a\nwin rate of 66%. Without extra optimization, Infinity generates a high-quality\n1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and\nestablishing it as the fastest text-to-image model. Models and codes will be\nreleased to promote further exploration of Infinity for visual generation and\nunified tokenizer modeling.\n","authors":["Jian Han","Jinlai Liu","Yi Jiang","Bin Yan","Yuqi Zhang","Zehuan Yuan","Bingyue Peng","Xiaobing Liu"],"pdf_url":"https://arxiv.org/pdf/2412.04431v2.pdf","comment":"17 pages, 14 figures"},{"id":"http://arxiv.org/abs/2506.14629v1","updated":"2025-06-17T15:24:30Z","published":"2025-06-17T15:24:30Z","title":"VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning","summary":"  Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito\n","authors":["Md. Adnanul Islam","Md. Faiyaz Abdullah Sayeedi","Md. Asaduzzaman Shuvo","Muhammad Ziaur Rahman","Shahanur Rahman Bappy","Raiyan Rahman","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2506.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14605v1","updated":"2025-06-17T15:06:43Z","published":"2025-06-17T15:06:43Z","title":"Unsupervised Imaging Inverse Problems with Diffusion Distribution\n  Matching","summary":"  This work addresses image restoration tasks through the lens of inverse\nproblems using unpaired datasets. In contrast to traditional approaches --\nwhich typically assume full knowledge of the forward model or access to paired\ndegraded and ground-truth images -- the proposed method operates under minimal\nassumptions and relies only on small, unpaired datasets. This makes it\nparticularly well-suited for real-world scenarios, where the forward model is\noften unknown or misspecified, and collecting paired data is costly or\ninfeasible. The method leverages conditional flow matching to model the\ndistribution of degraded observations, while simultaneously learning the\nforward model via a distribution-matching loss that arises naturally from the\nframework. Empirically, it outperforms both single-image blind and unsupervised\napproaches on deblurring and non-uniform point spread function (PSF)\ncalibration tasks. It also matches state-of-the-art performance on blind\nsuper-resolution. We also showcase the effectiveness of our method with a proof\nof concept for lens calibration: a real-world application traditionally\nrequiring time-consuming experiments and specialized equipment. In contrast,\nour approach achieves this with minimal data acquisition effort.\n","authors":["Giacomo Meanti","Thomas Ryckeboer","Michael Arbel","Julien Mairal"],"pdf_url":"https://arxiv.org/pdf/2506.14605v1.pdf","comment":"Code available at https://github.com/inria-thoth/ddm4ip"},{"id":"http://arxiv.org/abs/2506.14603v1","updated":"2025-06-17T15:06:07Z","published":"2025-06-17T15:06:07Z","title":"Align Your Flow: Scaling Continuous-Time Flow Map Distillation","summary":"  Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.\n","authors":["Amirmojtaba Sabour","Sanja Fidler","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2506.14603v1.pdf","comment":"Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/"},{"id":"http://arxiv.org/abs/2410.05243v3","updated":"2025-06-17T15:06:02Z","published":"2024-10-07T17:47:50Z","title":"Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents","summary":"  Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly perform\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.\n","authors":["Boyu Gou","Ruohan Wang","Boyuan Zheng","Yanan Xie","Cheng Chang","Yiheng Shu","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.05243v3.pdf","comment":"Accepted to ICLR 2025 (Oral). Project Homepage:\n  https://osu-nlp-group.github.io/UGround/"},{"id":"http://arxiv.org/abs/2505.15408v3","updated":"2025-06-17T15:05:18Z","published":"2025-05-21T11:51:51Z","title":"Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes","summary":"  Machine learning and computer vision methods have a major impact on the study\nof natural animal behavior, as they enable the (semi-)automatic analysis of\nvast amounts of video data. Mice are the standard mammalian model system in\nmost research fields, but the datasets available today to refine such methods\nfocus either on simple or social behaviors. In this work, we present a video\ndataset of individual mice solving complex mechanical puzzles, so-called\nlockboxes. The more than 110 hours of total playtime show their behavior\nrecorded from three different perspectives. As a benchmark for frame-level\naction classification methods, we provide human-annotated labels for all videos\nof two different mice, that equal 13% of our dataset. Our keypoint (pose)\ntracking-based action classification framework illustrates the challenges of\nautomated labeling of fine-grained behaviors, such as the manipulation of\nobjects. We hope that our work will help accelerate the advancement of\nautomated action and behavior classification in the computational neuroscience\ncommunity. Our dataset is publicly available at\nhttps://doi.org/10.14279/depositonce-23850\n","authors":["Patrik Reiske","Marcus N. Boon","Niek Andresen","Sole Traverso","Katharina Hohlbaum","Lars Lewejohann","Christa Thöne-Reineke","Olaf Hellwich","Henning Sprekeler"],"pdf_url":"https://arxiv.org/pdf/2505.15408v3.pdf","comment":"Accepted and published (poster) at the CV4Animals: Computer Vision\n  for Animal Behavior Tracking and Modeling workshop, in conjunction with\n  Computer Vision and Pattern Recognition (CVPR) 2025"},{"id":"http://arxiv.org/abs/2401.05308v3","updated":"2025-06-17T15:04:58Z","published":"2024-01-10T18:22:00Z","title":"Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks","summary":"  The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations.\n","authors":["Amin Farajzadeh","Animesh Yadav","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2401.05308v3.pdf","comment":"Submitted to IEEE for possible publication"},{"id":"http://arxiv.org/abs/2409.08926v2","updated":"2025-06-17T15:03:01Z","published":"2024-09-13T15:44:38Z","title":"ClearDepth: Enhanced Stereo Perception of Transparent Objects for\n  Robotic Manipulation","summary":"  Transparent object depth perception poses a challenge in everyday life and\nlogistics, primarily due to the inability of standard 3D sensors to accurately\ncapture depth on transparent or reflective surfaces. This limitation\nsignificantly affects depth map and point cloud-reliant applications,\nespecially in robotic manipulation. We developed a vision transformer-based\nalgorithm for stereo depth recovery of transparent objects. This approach is\ncomplemented by an innovative feature post-fusion module, which enhances the\naccuracy of depth recovery by structural features in images. To address the\nhigh costs associated with dataset collection for stereo camera-based\nperception of transparent objects, our method incorporates a parameter-aligned,\ndomain-adaptive, and physically realistic Sim2Real simulation for efficient\ndata generation, accelerated by AI algorithm. Our experimental results\ndemonstrate the model's exceptional Sim2Real generalizability in real-world\nscenarios, enabling precise depth mapping of transparent objects to assist in\nrobotic manipulation. Project details are available at\nhttps://sites.google.com/view/cleardepth/ .\n","authors":["Kaixin Bai","Huajian Zeng","Lei Zhang","Yiwen Liu","Hongli Xu","Zhaopeng Chen","Jianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.08926v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.14596v1","updated":"2025-06-17T14:59:56Z","published":"2025-06-17T14:59:56Z","title":"PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human\n  Pose Estimation","summary":"  Existing monocular 3D pose estimation methods primarily rely on joint\npositional features, while overlooking intrinsic directional and angular\ncorrelations within the skeleton. As a result, they often produce implausible\nposes under joint occlusions or rapid motion changes. To address these\nchallenges, we propose the PoseGRAF framework. We first construct a dual graph\nconvolutional structure that separately processes joint and bone graphs,\neffectively capturing their local dependencies. A Cross-Attention module is\nthen introduced to model interdependencies between bone directions and joint\nfeatures. Building upon this, a dynamic fusion module is designed to adaptively\nintegrate both feature types by leveraging the relational dependencies between\njoints and bones. An improved Transformer encoder is further incorporated in a\nresidual manner to generate the final output. Experimental results on the\nHuman3.6M and MPI-INF-3DHP datasets show that our method exceeds\nstate-of-the-art approaches. Additional evaluations on in-the-wild videos\nfurther validate its generalizability. The code is publicly available at\nhttps://github.com/iCityLab/PoseGRAF.\n","authors":["Ming Xu","Xu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14583v1","updated":"2025-06-17T14:41:31Z","published":"2025-06-17T14:41:31Z","title":"Synthetic Data Augmentation for Table Detection: Re-evaluating\n  TableNet's Performance with Automatically Generated Document Images","summary":"  Document pages captured by smartphones or scanners often contain tables, yet\nmanual extraction is slow and error-prone. We introduce an automated\nLaTeX-based pipeline that synthesizes realistic two-column pages with visually\ndiverse table layouts and aligned ground-truth masks. The generated corpus\naugments the real-world Marmot benchmark and enables a systematic resolution\nstudy of TableNet. Training TableNet on our synthetic data achieves a\npixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input\nresolution, and 4.33% with 1024x1024. The best performance on the Marmot\nbenchmark is 9.18% (at 256x256), while cutting manual annotation effort through\nautomation.\n","authors":["Krishna Sahukara","Zineddine Bettouche","Andreas Fischer"],"pdf_url":"https://arxiv.org/pdf/2506.14583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14582v1","updated":"2025-06-17T14:38:08Z","published":"2025-06-17T14:38:08Z","title":"Busting the Paper Ballot: Voting Meets Adversarial Machine Learning","summary":"  We show the security risk associated with using machine learning classifiers\nin United States election tabulators. The central classification task in\nelection tabulation is deciding whether a mark does or does not appear on a\nbubble associated to an alternative in a contest on the ballot. Barretto et al.\n(E-Vote-ID 2021) reported that convolutional neural networks are a viable\noption in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To\ndemonstrate and analyze the hypothetical vulnerability of machine learning\nmodels on election tabulators, we first introduce four new ballot datasets.\nSecond, we train and test a variety of different models on our new datasets.\nThese models include support vector machines, convolutional neural networks (a\nbasic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,\nusing our new datasets and trained models, we demonstrate that traditional\nwhite box attacks are ineffective in the voting domain due to gradient masking.\nOur analyses further reveal that gradient masking is a product of numerical\ninstability. We use a modified difference of logits ratio loss to overcome this\nissue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct\nattacks with the adversarial examples generated using our new methods. In\ntraditional adversarial machine learning, a high (50% or greater) attack\nsuccess rate is ideal. However, for certain elections, even a 5% attack success\nrate can flip the outcome of a race. We show such an impact is possible in the\nphysical domain. We thoroughly discuss attack realism, and the challenges and\npracticality associated with printing and scanning ballot adversarial examples.\n","authors":["Kaleel Mahmood","Caleb Manicke","Ethan Rathbun","Aayushi Verma","Sohaib Ahmad","Nicholas Stamatakis","Laurent Michel","Benjamin Fuller"],"pdf_url":"https://arxiv.org/pdf/2506.14582v1.pdf","comment":"18 Pages. Author version of article to appear at CCS 2025"},{"id":"http://arxiv.org/abs/2506.13038v2","updated":"2025-06-17T14:31:50Z","published":"2025-06-16T02:03:41Z","title":"HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for\n  Robust Multimodal Hallucination and Factuality Detection in VLMs","summary":"  Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains.\n","authors":["Zijian Zhang","Xuecheng Wu","Danlei Huang","Siyu Yan","Chong Peng","Xuezhi Cao"],"pdf_url":"https://arxiv.org/pdf/2506.13038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14560v1","updated":"2025-06-17T14:15:39Z","published":"2025-06-17T14:15:39Z","title":"Risk Estimation of Knee Osteoarthritis Progression via Predictive\n  Multi-task Modelling from Efficient Diffusion Model using X-ray Images","summary":"  Medical imaging plays a crucial role in assessing knee osteoarthritis (OA)\nrisk by enabling early detection and disease monitoring. Recent machine\nlearning methods have improved risk estimation (i.e., predicting the likelihood\nof disease progression) and predictive modelling (i.e., the forecasting of\nfuture outcomes based on current data) using medical images, but clinical\nadoption remains limited due to their lack of interpretability. Existing\napproaches that generate future images for risk estimation are complex and\nimpractical. Additionally, previous methods fail to localize anatomical knee\nlandmarks, limiting interpretability. We address these gaps with a new\ninterpretable machine learning method to estimate the risk of knee OA\nprogression via multi-task predictive modelling that classifies future knee OA\nseverity and predicts anatomical knee landmarks from efficiently generated\nhigh-quality future images. Such image generation is achieved by leveraging a\ndiffusion model in a class-conditioned latent space to forecast disease\nprogression, offering a visual representation of how particular health\nconditions may evolve. Applied to the Osteoarthritis Initiative dataset, our\napproach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71\nin predicting knee OA progression while offering ~9% faster inference time.\n","authors":["David Butler","Adrian Hilton","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2506.14560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14549v1","updated":"2025-06-17T14:05:24Z","published":"2025-06-17T14:05:24Z","title":"DreamLight: Towards Harmonious and Consistent Image Relighting","summary":"  We introduce a model named DreamLight for universal image relighting in this\nwork, which can seamlessly composite subjects into a new background while\nmaintaining aesthetic uniformity in terms of lighting and color tone. The\nbackground can be specified by natural images (image-based relighting) or\ngenerated from unlimited text prompts (text-based relighting). Existing studies\nprimarily focus on image-based relighting, while with scant exploration into\ntext-based scenarios. Some works employ intricate disentanglement pipeline\ndesigns relying on environment maps to provide relevant information, which\ngrapples with the expensive data cost required for intrinsic decomposition and\nlight source. Other methods take this task as an image translation problem and\nperform pixel-level transformation with autoencoder architecture. While these\nmethods have achieved decent harmonization effects, they struggle to generate\nrealistic and natural light interaction effects between the foreground and\nbackground. To alleviate these challenges, we reorganize the input data into a\nunified format and leverage the semantic prior provided by the pretrained\ndiffusion model to facilitate the generation of natural results. Moreover, we\npropose a Position-Guided Light Adapter (PGLA) that condenses light information\nfrom different directions in the background into designed light query\nembeddings, and modulates the foreground with direction-biased masked\nattention. In addition, we present a post-processing module named Spectral\nForeground Fixer (SFF) to adaptively reorganize different frequency components\nof subject and relighted background, which helps enhance the consistency of\nforeground appearance. Extensive comparisons and user study demonstrate that\nour DreamLight achieves remarkable relighting performance.\n","authors":["Yong Liu","Wenpeng Xiao","Qianqian Wang","Junlin Chen","Shiyin Wang","Yitong Wang","Xinglong Wu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2506.14549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14542v1","updated":"2025-06-17T14:02:41Z","published":"2025-06-17T14:02:41Z","title":"MobileHolo: A Lightweight Complex-Valued Deformable CNN for High-Quality\n  Computer-Generated Hologram","summary":"  Holographic displays have significant potential in virtual reality and\naugmented reality owing to their ability to provide all the depth cues. Deep\nlearning-based methods play an important role in computer-generated holograms\n(CGH). During the diffraction process, each pixel exerts an influence on the\nreconstructed image. However, previous works face challenges in capturing\nsufficient information to accurately model this process, primarily due to the\ninadequacy of their effective receptive field (ERF). Here, we designed\ncomplex-valued deformable convolution for integration into network, enabling\ndynamic adjustment of the convolution kernel's shape to increase flexibility of\nERF for better feature extraction. This approach allows us to utilize a single\nmodel while achieving state-of-the-art performance in both simulated and\noptical experiment reconstructions, surpassing existing open-source models.\nSpecifically, our method has a peak signal-to-noise ratio that is 2.04 dB, 5.31\ndB, and 9.71 dB higher than that of CCNN-CGH, HoloNet, and Holo-encoder,\nrespectively, when the resolution is 1920$\\times$1072. The number of parameters\nof our model is only about one-eighth of that of CCNN-CGH.\n","authors":["Xie Shuyang","Zhou Jie","Xu Bo","Wang Jun","Xu Renjing"],"pdf_url":"https://arxiv.org/pdf/2506.14542v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.14541v1","updated":"2025-06-17T14:01:59Z","published":"2025-06-17T14:01:59Z","title":"Exploring Diffusion with Test-Time Training on Efficient Image\n  Restoration","summary":"  Image restoration faces challenges including ineffective feature fusion,\ncomputational bottlenecks and inefficient diffusion processes. To address\nthese, we propose DiffRWKVIR, a novel framework unifying Test-Time Training\n(TTT) with efficient diffusion. Our approach introduces three key innovations:\n(1) Omni-Scale 2D State Evolution extends RWKV's location-dependent\nparameterization to hierarchical multi-directional 2D scanning, enabling global\ncontextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash\nProcessing accelerates intra-chunk parallelism by 3.2x via contiguous chunk\nprocessing (O(LCd) complexity), reducing sequential dependencies and\ncomputational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact\nImage Prior Representation (IPR) in only 5-20 steps, proving 45% faster\ntraining/inference than DiffIR while solving computational inefficiency in\ndenoising. Evaluated across super-resolution and inpainting benchmarks (Set5,\nSet14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and\nMambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes\na new paradigm for adaptive, high-efficiency image restoration with optimized\nhardware utilization.\n","authors":["Rongchang Lu","Tianduo Luo","Yunzhi Zhang","Conghan Yue","Pei Yang","Guibao Liu","Changyang Gu"],"pdf_url":"https://arxiv.org/pdf/2506.14541v1.pdf","comment":"Submitted to The 8th Chinese Conference on Pattern Recognition and\n  Computer Vision (2025). Contact to nomodeset@qq.com. Source code will open in\n  4 months"},{"id":"http://arxiv.org/abs/2506.14525v1","updated":"2025-06-17T13:51:16Z","published":"2025-06-17T13:51:16Z","title":"VisLanding: Monocular 3D Perception for UAV Safe Landing via\n  Depth-Normal Synergy","summary":"  This paper presents VisLanding, a monocular 3D perception-based framework for\nsafe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of\nautonomous UAV landing in complex and unknown environments, this study\ninnovatively leverages the depth-normal synergy prediction capabilities of the\nMetric3D V2 model to construct an end-to-end safe landing zones (SLZ)\nestimation framework. By introducing a safe zone segmentation branch, we\ntransform the landing zone estimation task into a binary semantic segmentation\nproblem. The model is fine-tuned and annotated using the WildUAV dataset from a\nUAV perspective, while a cross-domain evaluation dataset is constructed to\nvalidate the model's robustness. Experimental results demonstrate that\nVisLanding significantly enhances the accuracy of safe zone identification\nthrough a depth-normal joint optimization mechanism, while retaining the\nzero-shot generalization advantages of Metric3D V2. The proposed method\nexhibits superior generalization and robustness in cross-domain testing\ncompared to other approaches. Furthermore, it enables the estimation of landing\nzone area by integrating predicted depth and normal information, providing\ncritical decision-making support for practical applications.\n","authors":["Zhuoyue Tan","Boyong He","Yuxiang Ji","Liaoni Wu"],"pdf_url":"https://arxiv.org/pdf/2506.14525v1.pdf","comment":"Accepted by IROS2025"},{"id":"http://arxiv.org/abs/2506.14524v1","updated":"2025-06-17T13:50:42Z","published":"2025-06-17T13:50:42Z","title":"Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis\n  Lesion Delineation","summary":"  Background: Accurate lesion segmentation is critical for multiple sclerosis\n(MS) diagnosis, yet current deep learning approaches face robustness\nchallenges.\n  Aim: This study improves MS lesion segmentation by combining data fusion and\ndeep learning techniques.\n  Materials and Methods: We suggested novel radiomic features (concentration\nrate and R\\'enyi entropy) to characterize different MS lesion types and fused\nthese with raw imaging data. The study integrated radiomic features with\nimaging data through a ResNeXt-UNet architecture and attention-augmented U-Net\narchitecture. Our approach was evaluated on scans from 46 patients (1102\nslices), comparing performance before and after data fusion.\n  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation\naccuracy, achieving significant improvements in precision and sensitivity over\nthe MRI-only baseline and a Dice score of 0.774$\\pm$0.05; p<0.001 according to\nBonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced\nattention-augmented U-Net model showed a greater model stability evidenced by\nreduced performance variability (SDD = 0.18 $\\pm$ 0.09 vs. 0.21 $\\pm$ 0.06;\np=0.03) and smoother validation curves with radiomics integration.\n  Conclusion: These results validate our hypothesis that fusing radiomics with\nraw imaging data boosts segmentation performance and stability in\nstate-of-the-art models.\n","authors":["Nadezhda Alsahanova","Pavel Bartenev","Maksim Sharaev","Milos Ljubisavljevic","Taleb Al. Mansoori","Yauhen Statsenko"],"pdf_url":"https://arxiv.org/pdf/2506.14524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08915v3","updated":"2025-06-17T13:45:06Z","published":"2025-06-10T15:41:22Z","title":"Inherently Faithful Attention Maps for Vision Transformers","summary":"  We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds. Code: https://github.com/ananthu-aniraj/ifam\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2506.08915v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14515v1","updated":"2025-06-17T13:40:48Z","published":"2025-06-17T13:40:48Z","title":"Train Once, Forget Precisely: Anchored Optimization for Efficient\n  Post-Hoc Unlearning","summary":"  As machine learning systems increasingly rely on data subject to privacy\nregulation, selectively unlearning specific information from trained models has\nbecome essential. In image classification, this involves removing the influence\nof particular training samples, semantic classes, or visual styles without full\nretraining. We introduce \\textbf{Forget-Aligned Model Reconstruction (FAMR)}, a\ntheoretically grounded and computationally efficient framework for post-hoc\nunlearning in deep image classifiers. FAMR frames forgetting as a constrained\noptimization problem that minimizes a uniform-prediction loss on the forget set\nwhile anchoring model parameters to their original values via an $\\ell_2$\npenalty. A theoretical analysis links FAMR's solution to\ninfluence-function-based retraining approximations, with bounds on parameter\nand output deviation. Empirical results on class forgetting tasks using\nCIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong\nperformance retention and minimal computational overhead. The framework\ngeneralizes naturally to concept and style erasure, offering a scalable and\ncertifiable route to efficient post-hoc forgetting in vision models.\n","authors":["Prabhav Sanga","Jaskaran Singh","Arun K. Dubey"],"pdf_url":"https://arxiv.org/pdf/2506.14515v1.pdf","comment":"Accepted at ICML MUGen'25"},{"id":"http://arxiv.org/abs/2506.14513v1","updated":"2025-06-17T13:40:16Z","published":"2025-06-17T13:40:16Z","title":"GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous\n  Material Handling in Containment-Level Environments","summary":"  The convergence of robotics and virtual reality (VR) has enabled safer and\nmore efficient workflows in high-risk laboratory settings, particularly\nvirology labs. As biohazard complexity increases, minimizing direct human\nexposure while maintaining precision becomes essential. We propose GAMORA\n(Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic\nsystem that enables remote execution of hazardous tasks using natural hand\ngestures. Unlike existing scripted automation or traditional teleoperation,\nGAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating\nSystem (ROS) to provide real-time immersive control, digital twin simulation,\nand inverse kinematics-based articulation. The system supports VR-based\ntraining and simulation while executing precision tasks in physical\nenvironments via a 3D-printed robotic arm. Inverse kinematics ensure accurate\nmanipulation for delicate operations such as specimen handling and pipetting.\nThe pipeline includes Unity-based 3D environment construction, real-time motion\nplanning, and hardware-in-the-loop testing. GAMORA achieved a mean positional\ndiscrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL,\nand repeatability of 1.2 mm across 50 trials. Integrated object detection via\nYOLOv8 enhances spatial awareness, while energy-efficient operation (50%\nreduced power output) ensures sustainable deployment. The system's\ndigital-physical feedback loop enables safe, precise, and repeatable automation\nof high-risk lab tasks. GAMORA offers a scalable, immersive solution for\nrobotic control and biosafety in biomedical research environments.\n","authors":["Farha Abdul Wasay","Mohammed Abdul Rahman","Hania Ghouse"],"pdf_url":"https://arxiv.org/pdf/2506.14513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14512v1","updated":"2025-06-17T13:40:00Z","published":"2025-06-17T13:40:00Z","title":"SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex\n  Reasoning Tasks","summary":"  Large Language Models (LLMs) are experiencing rapid advancements in complex\nreasoning, exhibiting remarkable generalization in mathematics and programming.\nIn contrast, while spatial intelligence is fundamental for Vision-Language\nModels (VLMs) in real-world interaction, the systematic evaluation of their\ncomplex reasoning ability within spatial contexts remains underexplored. To\nbridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate\nVLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench\ncomprises nearly 1K video-question-answer triplets, where each problem is\nembedded in a realistic 3D scene and captured by video. By carefully designing\nquestions and corresponding 3D scenes, our benchmark ensures that solving the\nquestions requires both spatial comprehension for extracting information and\nhigh-level reasoning for deriving solutions, making it a challenging benchmark\nfor evaluating VLMs. To facilitate large-scale data synthesis, we develop an\nAutomatic Scene Creation Engine. This engine, leveraging multiple specialized\nLLM agents, can generate realistic 3D scenes from abstract math problems,\nensuring faithfulness to the original descriptions. Experimental results reveal\nthat state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring\nthe challenge of spatial reasoning. We hope that our study will bring\nresearchers' attention to spatially grounded reasoning and advance VLMs in\nvisual problem-solving.\n","authors":["Zijian Song","Xiaoxin Lin","Qiuming Huang","Guangrun Wang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2506.14512v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.14511v1","updated":"2025-06-17T13:35:06Z","published":"2025-06-17T13:35:06Z","title":"MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark\n  via Transformer-Graph-Style Convolution","summary":"  Facial micro-expression recognition (MER) is a challenging problem, due to\ntransient and subtle micro-expression (ME) actions. Most existing methods\ndepend on hand-crafted features, key frames like onset, apex, and offset\nframes, or deep networks limited by small-scale and low-diversity datasets. In\nthis paper, we propose an end-to-end micro-action-aware deep learning framework\nwith advantages from transformer, graph convolution, and vanilla convolution.\nIn particular, we propose a novel F5C block composed of fully-connected\nconvolution and channel correspondence convolution to directly extract\nlocal-global features from a sequence of raw frames, without the prior\nknowledge of key frames. The transformer-style fully-connected convolution is\nproposed to extract local features while maintaining global receptive fields,\nand the graph-style channel correspondence convolution is introduced to model\nthe correlations among feature patterns. Moreover, MER, optical flow\nestimation, and facial landmark detection are jointly trained by sharing the\nlocal-global features. The two latter tasks contribute to capturing facial\nsubtle action information for MER, which can alleviate the impact of\ninsufficient training data. Extensive experiments demonstrate that our\nframework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM,\nand SMIC benchmarks, (ii) works well for optical flow estimation and facial\nlandmark detection, and (iii) can capture facial subtle muscle actions in local\nregions associated with MEs. The code is available at\nhttps://github.com/CYF-cuber/MOL.\n","authors":["Zhiwen Shao","Yifan Cheng","Feiran Li","Yong Zhou","Xuequan Lu","Yuan Xie","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2506.14511v1.pdf","comment":"This paper has been accepted by IEEE Transactions on Pattern Analysis\n  and Machine Intelligence"},{"id":"http://arxiv.org/abs/2412.11906v2","updated":"2025-06-17T13:33:58Z","published":"2024-12-16T15:52:59Z","title":"PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension","summary":"  Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.\n","authors":["Kun Ouyang","Yuanxin Liu","Shicheng Li","Yi Liu","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11906v2.pdf","comment":"This is the camera-ready version for ACL 2025"},{"id":"http://arxiv.org/abs/2506.14497v1","updated":"2025-06-17T13:21:29Z","published":"2025-06-17T13:21:29Z","title":"Towards Reliable WMH Segmentation under Domain Shift: An Application\n  Study using Maximum Entropy Regularization to Improve Uncertainty Estimation","summary":"  Accurate segmentation of white matter hyperintensities (WMH) is crucial for\nclinical decision-making, particularly in the context of multiple sclerosis.\nHowever, domain shifts, such as variations in MRI machine types or acquisition\nparameters, pose significant challenges to model calibration and uncertainty\nestimation. This study investigates the impact of domain shift on WMH\nsegmentation by proposing maximum-entropy regularization techniques to enhance\nmodel calibration and uncertainty estimation, with the purpose of identifying\nerrors post-deployment using predictive uncertainty as a proxy measure that\ndoes not require ground-truth labels. To do this, we conducted experiments\nusing a U-Net architecture to evaluate these regularization schemes on two\npublicly available datasets, assessing performance with the Dice coefficient,\nexpected calibration error, and entropy-based uncertainty estimates. Our\nresults show that entropy-based uncertainty estimates can anticipate\nsegmentation errors, and that maximum-entropy regularization further\nstrengthens the correlation between uncertainty and segmentation performance\nwhile also improving model calibration under domain shift.\n","authors":["Franco Matzkin","Agostina Larrazabal","Diego H Milone","Jose Dolz","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2506.14497v1.pdf","comment":"32 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.16609v2","updated":"2025-06-17T13:19:12Z","published":"2024-12-21T12:47:12Z","title":"Concept Guided Co-salient Object Detection","summary":"  Co-salient object detection (Co-SOD) aims to identify common salient objects\nacross a group of related images. While recent methods have made notable\nprogress, they typically rely on low-level visual patterns and lack semantic\npriors, limiting their detection performance. We propose ConceptCoSOD, a\nconcept-guided framework that introduces high-level semantic knowledge to\nenhance co-saliency detection. By extracting shared text-based concepts from\nthe input image group, ConceptCoSOD provides semantic guidance that anchors the\ndetection process. To further improve concept quality, we analyze the effect of\ndiffusion timesteps and design a resampling strategy that selects more\ninformative steps for learning robust concepts. This semantic prior, combined\nwith the resampling-enhanced representation, enables accurate and consistent\nsegmentation even in challenging visual conditions. Extensive experiments on\nthree benchmark datasets and five corrupted settings demonstrate that\nConceptCoSOD significantly outperforms existing methods in both accuracy and\ngeneralization.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2412.16609v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14495v1","updated":"2025-06-17T13:17:31Z","published":"2025-06-17T13:17:31Z","title":"I Speak and You Find: Robust 3D Visual Grounding with Noisy and\n  Ambiguous Speech Inputs","summary":"  Existing 3D visual grounding methods rely on precise text prompts to locate\nobjects within 3D scenes. Speech, as a natural and intuitive modality, offers a\npromising alternative. Real-world speech inputs, however, often suffer from\ntranscription errors due to accents, background noise, and varying speech\nrates, limiting the applicability of existing 3DVG methods. To address these\nchallenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to\nenhance performance in the presence of noisy and ambiguous speech-to-text\ntranscriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and\nintroduces two key innovations. First, the Speech Complementary Module captures\nacoustic similarities between phonetically related words and highlights subtle\ndistinctions, generating complementary proposal scores from the speech signal.\nThis reduces dependence on potentially erroneous transcriptions. Second, the\nContrastive Complementary Module employs contrastive learning to align\nerroneous text features with corresponding speech features, ensuring robust\nperformance even when transcription errors dominate. Extensive experiments on\nthe SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves\nthe performance of existing 3DVG methods by a large margin, which highlights\nSpeechRefer's potential to bridge the gap between noisy speech inputs and\nreliable 3DVG, enabling more intuitive and practical multimodal systems.\n","authors":["Yu Qi","Lipeng Gu","Honghua Chen","Liangliang Nan","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23145v2","updated":"2025-06-17T12:51:50Z","published":"2025-05-29T06:33:16Z","title":"FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image\n  Editing","summary":"  Recent inversion-free, flow-based image editing methods such as FlowEdit\nleverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,\nenabling text-driven manipulation by solving an ordinary differential equation\n(ODE). While the lack of exact latent inversion is a core advantage of these\nmethods, it often results in unstable editing trajectories and poor source\nconsistency. To address this limitation, we propose FlowAlign, a novel\ninversion-free flow-based framework for consistent image editing with\nprincipled trajectory control. FlowAlign introduces a flow-matching loss as a\nregularization mechanism to promote smoother and more stable trajectories\nduring the editing process. Notably, the flow-matching loss is shown to\nexplicitly balance semantic alignment with the edit prompt and structural\nconsistency with the source image along the trajectory. Furthermore, FlowAlign\nnaturally supports reverse editing by simply reversing the ODE trajectory,\nhighlighting the reversible and consistent nature of the transformation.\nExtensive experiments demonstrate that FlowAlign outperforms existing methods\nin both source preservation and editing controllability.\n","authors":["Jeongsol Kim","Yeobin Hong","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2505.23145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v6","updated":"2025-06-17T12:41:10Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimple manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, object relocation, and hammer striking demonstrate\nDexHandDiff's effectiveness on goals outside training distributions, achieving\nover twice the average success rate (59.2% vs. 29.5%) compared to existing\nmethods. Our framework achieves an average of 70.7% success rate on goal\nadaptive dexterous tasks, highlighting its robustness and flexibility in\ncontact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v6.pdf","comment":"Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.\n  Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2506.14473v1","updated":"2025-06-17T12:37:24Z","published":"2025-06-17T12:37:24Z","title":"Foundation Model Insights and a Multi-Model Approach for Superior\n  Fine-Grained One-shot Subset Selection","summary":"  One-shot subset selection serves as an effective tool to reduce deep learning\ntraining costs by identifying an informative data subset based on the\ninformation extracted by an information extractor (IE). Traditional IEs,\ntypically pre-trained on the target dataset, are inherently dataset-dependent.\nFoundation models (FMs) offer a promising alternative, potentially mitigating\nthis limitation. This work investigates two key questions: (1) Can FM-based\nsubset selection outperform traditional IE-based methods across diverse\ndatasets? (2) Do all FMs perform equally well as IEs for subset selection?\nExtensive experiments uncovered surprising insights: FMs consistently\noutperform traditional IEs on fine-grained datasets, whereas their advantage\ndiminishes on coarse-grained datasets with noisy labels. Motivated by these\nfinding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a\nmethod tailored for fine-grained image datasets. RAM-APL leverages multiple FMs\nto enhance subset selection by exploiting their complementary strengths. Our\napproach achieves state-of-the-art performance on fine-grained datasets,\nincluding Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.\n","authors":["Zhijing Wan","Zhixiang Wang","Zheng Wang","Xin Xu","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2506.14473v1.pdf","comment":"18 pages, 10 figures, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.14471v1","updated":"2025-06-17T12:35:23Z","published":"2025-06-17T12:35:23Z","title":"Dense360: Dense Understanding from Omnidirectional Panoramas","summary":"  Multimodal Large Language Models (MLLMs) require comprehensive visual inputs\nto achieve dense understanding of the physical world. While existing MLLMs\ndemonstrate impressive world understanding capabilities through limited\nfield-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step\ntoward dense understanding from omnidirectional panoramas. We first introduce\nan omnidirectional panoramas dataset featuring a comprehensive suite of\nreliability-scored annotations. Specifically, our dataset contains 160K\npanoramas with 5M dense entity-level captions, 1M unique referring expressions,\nand 100K entity-grounded panoramic scene descriptions. Compared to multi-view\nalternatives, panoramas can provide more complete, compact, and continuous\nscene representations through equirectangular projections (ERP). However, the\nuse of ERP introduces two key challenges for MLLMs: i) spatial continuity along\nthe circle of latitude, and ii) latitude-dependent variation in information\ndensity. We address these challenges through ERP-RoPE, a position encoding\nscheme specifically designed for panoramic ERP. In addition, we introduce\nDense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional\ncaptioning and grounding, establishing a comprehensive framework for advancing\ndense visual-language understanding in panoramic settings.\n","authors":["Yikang Zhou","Tao Zhang","Dizhe Zhang","Shunping Ji","Xiangtai Li","Lu Qi"],"pdf_url":"https://arxiv.org/pdf/2506.14471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05804v4","updated":"2025-06-17T12:16:51Z","published":"2024-02-08T16:41:41Z","title":"InkSight: Offline-to-Online Handwriting Conversion by Teaching\n  Vision-Language Models to Read and Write","summary":"  Digital note-taking is gaining popularity, offering a durable, editable, and\neasily indexable way of storing notes in a vectorized form, known as digital\nink. However, a substantial gap remains between this way of note-taking and\ntraditional pen-and-paper note-taking, a practice that is still favored by a\nvast majority. Our work InkSight, aims to bridge the gap by empowering physical\nnote-takers to effortlessly convert their work (offline handwriting) to digital\nink (online handwriting), a process we refer to as derendering. Prior research\non the topic has focused on the geometric properties of images, resulting in\nlimited generalization beyond their training domains. Our approach combines\nreading and writing priors, allowing training a model in the absence of large\namounts of paired samples, which are difficult to obtain. To our knowledge,\nthis is the first work that effectively derenders handwritten text in arbitrary\nphotos with diverse visual characteristics and backgrounds. Furthermore, it\ngeneralizes beyond its training domain into simple sketches. Our human\nevaluation reveals that 87% of the samples produced by our model on the\nchallenging HierText dataset are considered as a valid tracing of the input\nimage and 67% look like a pen trajectory traced by a human.\n","authors":["Blagoj Mitrevski","Arina Rak","Julian Schnitzler","Chengkun Li","Andrii Maksai","Jesse Berent","Claudiu Musat"],"pdf_url":"https://arxiv.org/pdf/2402.05804v4.pdf","comment":"Accepted by Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2506.14451v1","updated":"2025-06-17T12:15:08Z","published":"2025-06-17T12:15:08Z","title":"Adapting Lightweight Vision Language Models for Radiological Visual\n  Question Answering","summary":"  Recent advancements in vision-language systems have improved the accuracy of\nRadiological Visual Question Answering (VQA) Models. However, some challenges\nremain across each stage of model development: limited expert-labeled images\nhinders data procurement at scale; the intricate and nuanced patterns of\nradiological images make modeling inherently difficult; and the lack of\nevaluation evaluation efforts makes it difficult to identify cases where the\nmodel might be ill-conditioned. In this study, we fine-tune a lightweight 3B\nparameter vision-language model for Radiological VQA, demonstrating that small\nmodels, when appropriately tuned with curated data, can achieve robust\nperformance across both open- and closed-ended questions. We propose a\ncost-effective training pipeline from synthetic question-answer pair generation\nto multi-stage fine-tuning on specialised radiological domain-targeted datasets\n(e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a\nfraction of the scale of state-of-the-art models such as LLaVA-Med, our model\nachieves promising performance given its small parameter size and the limited\nscale of training data. We introduce a lightweight saliency-based diagnostic\ntool that enables domain experts to inspect VQA model performance and identify\nill-conditioned failure modes through saliency analysis.\n","authors":["Aditya Shourya","Michel Dumontier","Chang Sun"],"pdf_url":"https://arxiv.org/pdf/2506.14451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05538v4","updated":"2025-06-17T12:08:05Z","published":"2024-05-09T04:36:04Z","title":"A Survey on Personalized Content Synthesis with Diffusion Models","summary":"  Recent advancements in diffusion models have significantly impacted content\ncreation, leading to the emergence of Personalized Content Synthesis (PCS). By\nutilizing a small set of user-provided examples featuring the same subject, PCS\naims to tailor this subject to specific user-defined prompts. Over the past two\nyears, more than 150 methods have been introduced in this area. However,\nexisting surveys primarily focus on text-to-image generation, with few\nproviding up-to-date summaries on PCS. This paper provides a comprehensive\nsurvey of PCS, introducing the general frameworks of PCS research, which can be\ncategorized into test-time fine-tuning (TTF) and pre-trained adaptation (PTA)\napproaches. We analyze the strengths, limitations, and key techniques of these\nmethodologies. Additionally, we explore specialized tasks within the field,\nsuch as object, face, and style personalization, while highlighting their\nunique challenges and innovations. Despite the promising progress, we also\ndiscuss ongoing challenges, including overfitting and the trade-off between\nsubject fidelity and text alignment. Through this detailed overview and\nanalysis, we propose future directions to further the development of PCS.\n","authors":["Xulu Zhang","Xiaoyong Wei","Wentao Hu","Jinlin Wu","Jiaxin Wu","Wengyu Zhang","Zhaoxiang Zhang","Zhen Lei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2405.05538v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14440v1","updated":"2025-06-17T12:00:23Z","published":"2025-06-17T12:00:23Z","title":"Model compression using knowledge distillation with integrated gradients","summary":"  Model compression is critical for deploying deep learning models on\nresource-constrained devices. We introduce a novel method enhancing knowledge\ndistillation with integrated gradients (IG) as a data augmentation strategy.\nOur approach overlays IG maps onto input images during training, providing\nstudent models with deeper insights into teacher models' decision-making\nprocesses. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented\nknowledge distillation achieves 92.6% testing accuracy with a 4.1x compression\nfactor-a significant 1.1 percentage point improvement ($p<0.001$) over\nnon-distilled models (91.5%). This compression reduces inference time from 140\nms to 13 ms. Our method precomputes IG maps before training, transforming\nsubstantial runtime costs into a one-time preprocessing step. Our comprehensive\nexperiments include: (1) comparisons with attention transfer, revealing\ncomplementary benefits when combined with our approach; (2) Monte Carlo\nsimulations confirming statistical robustness; (3) systematic evaluation of\ncompression factor versus accuracy trade-offs across a wide range (2.2x-1122x);\nand (4) validation on an ImageNet subset aligned with CIFAR-10 classes,\ndemonstrating generalisability beyond the initial dataset. These extensive\nablation studies confirm that IG-based knowledge distillation consistently\noutperforms conventional approaches across varied architectures and compression\nratios. Our results establish this framework as a viable compression technique\nfor real-world deployment on edge devices while maintaining competitive\naccuracy.\n","authors":["David E. Hernandez","Jose Chang","Torbjörn E. M. Nordling"],"pdf_url":"https://arxiv.org/pdf/2506.14440v1.pdf","comment":"49 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.19834v5","updated":"2025-06-17T11:56:41Z","published":"2025-02-27T07:14:11Z","title":"Knowledge Bridger: Towards Training-free Missing Modality Completion","summary":"  Previous successful approaches to missing modality completion rely on\ncarefully designed fusion techniques and extensive pre-training on complete\ndata, which can limit their generalizability in out-of-domain (OOD) scenarios.\nIn this study, we pose a new challenge: can we develop a missing modality\ncompletion model that is both resource-efficient and robust to OOD\ngeneralization? To address this, we present a training-free framework for\nmissing modality completion that leverages large multimodal models (LMMs). Our\napproach, termed the \"Knowledge Bridger\", is modality-agnostic and integrates\ngeneration and ranking of missing modalities. By defining domain-specific\npriors, our method automatically extracts structured information from available\nmodalities to construct knowledge graphs. These extracted graphs connect the\nmissing modality generation and ranking modules through the LMM, resulting in\nhigh-quality imputations of missing modalities. Experimental results across\nboth general and medical domains show that our approach consistently\noutperforms competing methods, including in OOD generalization. Additionally,\nour knowledge-driven generation and ranking techniques demonstrate superiority\nover variants that directly employ LMMs for generation and ranking, offering\ninsights that may be valuable for applications in other domains.\n","authors":["Guanzhou Ke","Shengfeng He","Xiao Li Wang","Bo Wang","Guoqing Chao","Yuanyang Zhang","Yi Xie","HeXing Su"],"pdf_url":"https://arxiv.org/pdf/2502.19834v5.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2506.14435v1","updated":"2025-06-17T11:53:49Z","published":"2025-06-17T11:53:49Z","title":"MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models","summary":"  Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.\n","authors":["Hongyu Wang","Jiayu Xu","Ruiping Wang","Yan Feng","Yitao Zhai","Peng Pei","Xunliang Cai","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2506.14435v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.14432v1","updated":"2025-06-17T11:48:05Z","published":"2025-06-17T11:48:05Z","title":"A large-scale heterogeneous 3D magnetic resonance brain imaging dataset\n  for self-supervised learning","summary":"  We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain\nMagnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187\nsubjects, aggregated from 16 publicly available sources. The dataset includes\nboth clinical- and research-grade images, multiple MRI sequences, and a wide\nrange of anatomical and pathological variability, including scans with large\nbrain anomalies. Minimal preprocessing was applied to preserve the original\nimage characteristics while reducing barriers to entry for new users.\nAccompanying code for self-supervised pretraining and finetuning is provided.\nFOMO60K is intended to support the development and benchmarking of\nself-supervised learning methods in medical imaging at scale.\n","authors":["Asbjørn Munk","Stefano Cerri","Jakob Ambsdorf","Julia Machnio","Sebastian Nørgaard Llambias","Vardan Nersesjan","Christian Hedeager Krag","Peirong Liu","Pablo Rocamora García","Mostafa Mehdipour Ghazi","Mikael Boesen","Michael Eriksen Benros","Juan Eugenio Iglesias","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2506.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14428v1","updated":"2025-06-17T11:45:33Z","published":"2025-06-17T11:45:33Z","title":"Toward Rich Video Human-Motion2D Generation","summary":"  Generating realistic and controllable human motions, particularly those\ninvolving rich multi-character interactions, remains a significant challenge\ndue to data scarcity and the complexities of modeling inter-personal dynamics.\nTo address these limitations, we first introduce a new large-scale rich video\nhuman motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video\nsequences. Motion2D-Video-150K features a balanced distribution of diverse\nsingle-character and, crucially, double-character interactive actions, each\npaired with detailed textual descriptions. Building upon this dataset, we\npropose a novel diffusion-based rich video human motion2D generation (RVHM2D)\nmodel. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing\neither dual text encoders (CLIP-L/B) or T5-XXL with both global and local\nfeatures. We devise a two-stage training strategy: the model is first trained\nwith a standard diffusion objective, and then fine-tuned using reinforcement\nlearning with an FID-based reward to further enhance motion realism and text\nalignment. Extensive experiments demonstrate that RVHM2D achieves leading\nperformance on the Motion2D-Video-150K benchmark in generating both single and\ninteractive double-character scenarios.\n","authors":["Ruihao Xi","Xuekuan Wang","Yongcheng Li","Shuhua Li","Zichen Wang","Yiwei Wang","Feng Wei","Cairong Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.14428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15289v4","updated":"2025-06-17T11:43:46Z","published":"2024-05-24T07:22:35Z","title":"Learning Invariant Causal Mechanism from Vision-Language Models","summary":"  Contrastive Language-Image Pretraining (CLIP) has achieved remarkable\nsuccess, but its performance can degrade when fine-tuned in out-of-distribution\n(OOD) scenarios. We model the prediction process using a Structural Causal\nModel (SCM) and show that the causal mechanism involving both invariant and\nvariant factors in training environments differs from that in test\nenvironments. In contrast, the causal mechanism with solely invariant factors\nremains consistent across environments. We theoretically prove the existence of\na linear mapping from CLIP embeddings to invariant factors, which can be\nestimated using interventional data. Additionally, we provide a condition to\nguarantee low OOD risk of the invariant predictor. Based on these insights, we\npropose the Invariant Causal Mechanism of CLIP (CLIP-ICM) framework. CLIP-ICM\ninvolves collecting interventional data, estimating a linear projection matrix,\nand making predictions within the invariant subspace. Experiments on several\nOOD datasets show that CLIP-ICM significantly improves the performance of CLIP.\nOur method offers a simple but powerful enhancement, boosting the reliability\nof CLIP in real-world applications.\n","authors":["Zeen Song","Siyu Zhao","Xingyu Zhang","Jiangmeng Li","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2405.15289v4.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.14418v1","updated":"2025-06-17T11:28:07Z","published":"2025-06-17T11:28:07Z","title":"Compositional Attribute Imbalance in Vision Datasets","summary":"  Visual attribute imbalance is a common yet underexplored issue in image\nclassification, significantly impacting model performance and generalization.\nIn this work, we first define the first-level and second-level attributes of\nimages and then introduce a CLIP-based framework to construct a visual\nattribute dictionary, enabling automatic evaluation of image attributes. By\nsystematically analyzing both single-attribute imbalance and compositional\nattribute imbalance, we reveal how the rarity of attributes affects model\nperformance. To tackle these challenges, we propose adjusting the sampling\nprobability of samples based on the rarity of their compositional attributes.\nThis strategy is further integrated with various data augmentation techniques\n(such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to\nrepresent rare attributes. Extensive experiments on benchmark datasets\ndemonstrate that our method effectively mitigates attribute imbalance, thereby\nimproving the robustness and fairness of deep neural networks. Our research\nhighlights the importance of modeling visual attribute distributions and\nprovides a scalable solution for long-tail image classification tasks.\n","authors":["Jiayi Chen","Yanbiao Ma","Andi Zhang","Weidong Tang","Wei Dai","Bowei Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14404v1","updated":"2025-06-17T11:06:22Z","published":"2025-06-17T11:06:22Z","title":"Causally Steered Diffusion for Automated Video Counterfactual Generation","summary":"  Adapting text-to-image (T2I) latent diffusion models for video editing has\nshown strong visual fidelity and controllability, but challenges remain in\nmaintaining causal relationships in video content. Edits affecting causally\ndependent attributes risk generating unrealistic or misleading outcomes if\nthese relationships are ignored. In this work, we propose a causally faithful\nframework for counterfactual video generation, guided by a vision-language\nmodel (VLM). Our method is agnostic to the underlying video editing system and\ndoes not require access to its internal mechanisms or finetuning. Instead, we\nguide the generation by optimizing text prompts based on an assumed causal\ngraph, addressing the challenge of latent space control in LDMs. We evaluate\nour approach using standard video quality metrics and counterfactual-specific\ncriteria, such as causal effectiveness and minimality. Our results demonstrate\nthat causally faithful video counterfactuals can be effectively generated\nwithin the learned distribution of LDMs through prompt-based causal steering.\nWith its compatibility with any black-box video editing system, our method\nholds significant potential for generating realistic \"what-if\" video scenarios\nin diverse areas such as healthcare and digital media.\n","authors":["Nikos Spyrou","Athanasios Vlontzos","Paraskevas Pegios","Thomas Melistas","Nefeli Gkouti","Yannis Panagakis","Giorgos Papanastasiou","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2506.14404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14399v1","updated":"2025-06-17T10:56:09Z","published":"2025-06-17T10:56:09Z","title":"Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models","summary":"  Counterfactual image generation aims to simulate realistic visual outcomes\nunder specific causal interventions. Diffusion models have recently emerged as\na powerful tool for this task, combining DDIM inversion with conditional\ngeneration via classifier-free guidance (CFG). However, standard CFG applies a\nsingle global weight across all conditioning variables, which can lead to poor\nidentity preservation and spurious attribute changes - a phenomenon known as\nattribute amplification. To address this, we propose Decoupled Classifier-Free\nGuidance (DCFG), a flexible and model-agnostic framework that introduces\ngroup-wise conditioning control. DCFG builds on an attribute-split embedding\nstrategy that disentangles semantic inputs, enabling selective guidance on\nuser-defined attribute groups. For counterfactual generation, we partition\nattributes into intervened and invariant sets based on a causal graph and apply\ndistinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show\nthat DCFG improves intervention fidelity, mitigates unintended changes, and\nenhances reversibility, enabling more faithful and interpretable counterfactual\nimage generation.\n","authors":["Tian Xia","Fabio De Sousa Ribeiro","Rajat R Rasal","Avinash Kori","Raghav Mehta","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2506.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14719v2","updated":"2025-06-17T10:48:33Z","published":"2025-05-19T14:01:03Z","title":"MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention\n  Fusion","summary":"  The combination of Spiking Neural Networks (SNNs) with Vision Transformer\narchitectures has garnered significant attention due to their potential for\nenergy-efficient and high-performance computing paradigms. However, a\nsubstantial performance gap still exists between SNN-based and ANN-based\ntransformer architectures. While existing methods propose spiking\nself-attention mechanisms that are successfully combined with SNNs, the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting features from different image scales. In this paper, we address this\nissue and propose MSVIT. This novel spike-driven Transformer architecture\nfirstly uses multi-scale spiking attention (MSSA) to enhance the capabilities\nof spiking attention blocks. We validate our approach across various main\ndatasets. The experimental results show that MSVIT outperforms existing\nSNN-based models, positioning itself as a state-of-the-art solution among\nSNN-transformer architectures. The codes are available at\nhttps://github.com/Nanhu-AI-Lab/MSViT.\n","authors":["Wei Hua","Chenlin Zhou","Jibin Wu","Yansong Chua","Yangyang Shu"],"pdf_url":"https://arxiv.org/pdf/2505.14719v2.pdf","comment":"11pages, 2figures, accepted by IJCAI'25 (34th International Joint\n  Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2505.19802v2","updated":"2025-06-17T10:43:24Z","published":"2025-05-26T10:35:42Z","title":"GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity\n  Estimation","summary":"  Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation.\n","authors":["Zhiyu Wang","Yang Liu","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2505.19802v2.pdf","comment":"MiGA@IJCAI25"},{"id":"http://arxiv.org/abs/2506.14390v1","updated":"2025-06-17T10:38:29Z","published":"2025-06-17T10:38:29Z","title":"Enclosing Prototypical Variational Autoencoder for Explainable\n  Out-of-Distribution Detection","summary":"  Understanding the decision-making and trusting the reliability of Deep\nMachine Learning Models is crucial for adopting such methods to safety-relevant\napplications. We extend self-explainable Prototypical Variational models with\nautoencoder-based out-of-distribution (OOD) detection: A Variational\nAutoencoder is applied to learn a meaningful latent space which can be used for\ndistance-based classification, likelihood estimation for OOD detection, and\nreconstruction. The In-Distribution (ID) region is defined by a Gaussian\nmixture distribution with learned prototypes representing the center of each\nmode. Furthermore, a novel restriction loss is introduced that promotes a\ncompact ID region in the latent space without collapsing it into single points.\nThe reconstructive capabilities of the Autoencoder ensure the explainability of\nthe prototypes and the ID region of the classifier, further aiding the\ndiscrimination of OOD samples. Extensive evaluations on common OOD detection\nbenchmarks as well as a large-scale dataset from a real-world railway\napplication demonstrate the usefulness of the approach, outperforming previous\nmethods.\n","authors":["Conrad Orglmeister","Erik Bochinski","Volker Eiselein","Elvira Fleig"],"pdf_url":"https://arxiv.org/pdf/2506.14390v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computer Safety, Reliability and Security - SAFECOMP 2024\n  Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at\n  https://doi.org/10.1007/978-3-031-68738-9_29"},{"id":"http://arxiv.org/abs/2506.14384v1","updated":"2025-06-17T10:32:05Z","published":"2025-06-17T10:32:05Z","title":"GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and\n  Visible Image Fusion","summary":"  In the field of image fusion, promising progress has been made by modeling\ndata from different modalities as linear subspaces.\n  However, in practice, the source images are often located in a non-Euclidean\nspace, where the Euclidean methods usually cannot\n  encapsulate the intrinsic topological structure. Typically, the inner product\nperformed in the Euclidean space calculates the algebraic\n  similarity rather than the semantic similarity, which results in undesired\nattention output and a decrease in fusion performance.\n  While the balance of low-level details and high-level semantics should be\nconsidered in infrared and visible image fusion task. To\n  address this issue, in this paper, we propose a novel attention mechanism\nbased on Grassmann manifold for infrared and visible\n  image fusion (GrFormer). Specifically, our method constructs a low-rank\nsubspace mapping through projection constraints on the\n  Grassmann manifold, compressing attention features into subspaces of varying\nrank levels. This forces the features to decouple into\n  high-frequency details (local low-rank) and low-frequency semantics (global\nlow-rank), thereby achieving multi-scale semantic\n  fusion. Additionally, to effectively integrate the significant information,\nwe develop a cross-modal fusion strategy (CMS) based on\n  a covariance mask to maximise the complementary properties between different\nmodalities and to suppress the features with high\n  correlation, which are deemed redundant. The experimental results demonstrate\nthat our network outperforms SOTA methods both\n  qualitatively and quantitatively on multiple image fusion benchmarks. The\ncodes are available at https://github.com/Shaoyun2023.\n","authors":["Huan Kang","Hui Li","Xiao-Jun Wu","Tianyang Xu","Rui Wang","Chunyang Cheng","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2506.14384v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2506.14382v1","updated":"2025-06-17T10:27:59Z","published":"2025-06-17T10:27:59Z","title":"DepthSeg: Depth prompting in remote sensing semantic segmentation","summary":"  Remote sensing semantic segmentation is crucial for extracting detailed land\nsurface information, enabling applications such as environmental monitoring,\nland use planning, and resource assessment. In recent years, advancements in\nartificial intelligence have spurred the development of automatic remote\nsensing semantic segmentation methods. However, the existing semantic\nsegmentation methods focus on distinguishing spectral characteristics of\ndifferent objects while ignoring the differences in the elevation of the\ndifferent targets. This results in land cover misclassification in complex\nscenarios involving shadow occlusion and spectral confusion. In this paper, we\nintroduce a depth prompting two-dimensional (2D) remote sensing semantic\nsegmentation framework (DepthSeg). It automatically models depth/height\ninformation from 2D remote sensing images and integrates it into the semantic\nsegmentation framework to mitigate the effects of spectral confusion and shadow\nocclusion. During the feature extraction phase of DepthSeg, we introduce a\nlightweight adapter to enable cost-effective fine-tuning of the large-parameter\nvision transformer encoder pre-trained by natural images. In the depth\nprompting phase, we propose a depth prompter to model depth/height features\nexplicitly. In the semantic prediction phase, we introduce a semantic\nclassification decoder that couples the depth prompts with high-dimensional\nland-cover features, enabling accurate extraction of land-cover types.\nExperiments on the LiuZhou dataset validate the advantages of the DepthSeg\nframework in land cover mapping tasks. Detailed ablation studies further\nhighlight the significance of the depth prompts in remote sensing semantic\nsegmentation.\n","authors":["Ning Zhou","Shanxiong Chen","Mingting Zhou","Haigang Sui","Lieyun Hu","Han Li","Li Hua","Qiming Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.14382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14381v1","updated":"2025-06-17T10:26:07Z","published":"2025-06-17T10:26:07Z","title":"Compressed Video Super-Resolution based on Hierarchical Encoding","summary":"  This paper presents a general-purpose video super-resolution (VSR) method,\ndubbed VSR-HE, specifically designed to enhance the perceptual quality of\ncompressed content. Targeting scenarios characterized by heavy compression, the\nmethod upscales low-resolution videos by a ratio of four, from 180p to 720p or\nfrom 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and\nhas been sophisticatedly optimized to eliminate a wide range of compression\nartifacts commonly introduced by H.265/HEVC encoding across various\nquantization parameter (QP) levels. To ensure robustness and generalization,\nthe model is trained and evaluated under diverse compression settings, allowing\nit to effectively restore fine-grained details and preserve visual fidelity.\nThe proposed VSR-HE has been officially submitted to the ICME 2025 Grand\nChallenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1\n(General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).\n","authors":["Yuxuan Jiang","Siyue Teng","Qiang Zhu","Chen Feng","Chengxi Zeng","Fan Zhang","Shuyuan Zhu","Bing Zeng","David Bull"],"pdf_url":"https://arxiv.org/pdf/2506.14381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14373v1","updated":"2025-06-17T10:15:17Z","published":"2025-06-17T10:15:17Z","title":"Discrete JEPA: Learning Discrete Token Representations without\n  Reconstruction","summary":"  The cornerstone of cognitive intelligence lies in extracting hidden patterns\nfrom observations and leveraging these principles to systematically predict\nfuture outcomes. However, current image tokenization methods demonstrate\nsignificant limitations in tasks requiring symbolic abstraction and logical\nreasoning capabilities essential for systematic inference. To address this\nchallenge, we propose Discrete-JEPA, extending the latent predictive coding\nframework with semantic tokenization and novel complementary objectives to\ncreate robust tokenization for symbolic reasoning tasks. Discrete-JEPA\ndramatically outperforms baselines on visual symbolic prediction tasks, while\nstriking visual evidence reveals the spontaneous emergence of deliberate\nsystematic patterns within the learned semantic token space. Though an initial\nmodel, our approach promises a significant impact for advancing Symbolic world\nmodeling and planning capabilities in artificial intelligence systems.\n","authors":["Junyeob Baek","Hosung Lee","Christopher Hoang","Mengye Ren","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2506.14373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13277v2","updated":"2025-06-17T10:12:39Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huayang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14367v1","updated":"2025-06-17T10:07:59Z","published":"2025-06-17T10:07:59Z","title":"DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease\n  Classification with Explainable AI","summary":"  Accurate diagnosis of brain disorders such as Alzheimer's disease and brain\ntumors remains a critical challenge in medical imaging. Conventional methods\nbased on manual MRI analysis are often inefficient and error-prone. To address\nthis, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and\nDenseNet121 to enhance feature extraction and classification. DenseNet121\npromotes feature reuse and efficient gradient flow through dense connectivity,\nwhile VGG16 contributes strong hierarchical spatial representations. Their\nfusion enables robust multiclass classification of neurological conditions.\nGrad-CAM is applied to visualize salient regions, enhancing model transparency.\nTrained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a\ntest accuracy of 91.33\\%, with precision, recall, and F1-score all exceeding\n91\\%. These results highlight DGG-XNet's potential as an effective and\ninterpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and\noncological brain disorders.\n","authors":["Sumshun Nahar Eity","Mahin Montasir Afif","Tanisha Fairooz","Md. Mortuza Ahmmed","Md Saef Ullah Miah"],"pdf_url":"https://arxiv.org/pdf/2506.14367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14362v1","updated":"2025-06-17T10:02:48Z","published":"2025-06-17T10:02:48Z","title":"HydroChronos: Forecasting Decades of Surface Water Change","summary":"  Forecasting surface water dynamics is crucial for water resource management\nand climate change adaptation. However, the field lacks comprehensive datasets\nand standardized benchmarks. In this paper, we introduce HydroChronos, a\nlarge-scale, multi-modal spatiotemporal dataset for surface water dynamics\nforecasting designed to address this gap. We couple the dataset with three\nforecasting tasks. The dataset includes over three decades of aligned Landsat 5\nand Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse\nlakes and rivers across Europe, North America, and South America. We also\npropose AquaClimaTempo UNet, a novel spatiotemporal architecture with a\ndedicated climate data branch, as a strong benchmark baseline. Our model\nsignificantly outperforms a Persistence baseline for forecasting future water\ndynamics by +14% and +11% F1 across change detection and direction of change\nclassification tasks, and by +0.1 MAE on the magnitude of change regression.\nFinally, we conduct an Explainable AI analysis to identify the key climate\nvariables and input channels that influence surface water change, providing\ninsights to inform and guide future modeling efforts.\n","authors":["Daniele Rege Cambrin","Eleonora Poeta","Eliana Pastor","Isaac Corley","Tania Cerquitelli","Elena Baralis","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2506.14362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14356v1","updated":"2025-06-17T09:51:51Z","published":"2025-06-17T09:51:51Z","title":"EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal\n  Rotary Positional Embeddings and Symmetric Optimization","summary":"  Egocentric video-language understanding demands both high efficiency and\naccurate spatial-temporal modeling. Existing approaches face three key\nchallenges: 1) Excessive pre-training cost arising from multi-stage\npre-training pipelines, 2) Ineffective spatial-temporal encoding due to\nmanually split 3D rotary positional embeddings that hinder feature\ninteractions, and 3) Imprecise learning objectives in soft-label multi-instance\nretrieval, which neglect negative pair correlations. In this paper, we\nintroduce EVA02-AT, a suite of EVA02-based video-language foundation models\ntailored to egocentric video understanding tasks. EVA02-AT first efficiently\ntransfers an image-based CLIP model into a unified video encoder via a\nsingle-stage pretraining. Second, instead of applying rotary positional\nembeddings to isolated dimensions, we introduce spatial-temporal rotary\npositional embeddings along with joint attention, which can effectively encode\nboth spatial and temporal information on the entire hidden dimension. This\njoint encoding of spatial-temporal features enables the model to learn\ncross-axis relationships, which are crucial for accurately modeling motion and\ninteraction in videos. Third, focusing on multi-instance video-language\nretrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a\nnovel training framework that advances all soft labels for both positive and\nnegative pairs, providing a more precise learning objective. Extensive\nexperiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and\nfine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art\nperformance across diverse egocentric video-language tasks with fewer\nparameters. Models with our SMS loss also show significant performance gains on\nmulti-instance retrieval benchmarks. Our code and models are publicly available\nat https://github.com/xqwang14/EVA02-AT .\n","authors":["Xiaoqi Wang","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2506.14356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14350v1","updated":"2025-06-17T09:48:27Z","published":"2025-06-17T09:48:27Z","title":"FGA-NN: Film Grain Analysis Neural Network","summary":"  Film grain, once a by-product of analog film, is now present in most\ncinematographic content for aesthetic reasons. However, when such content is\ncompressed at medium to low bitrates, film grain is lost due to its random\nnature. To preserve artistic intent while compressing efficiently, film grain\nis analyzed and modeled before encoding and synthesized after decoding. This\npaper introduces FGA-NN, the first learning-based film grain analysis method to\nestimate conventional film grain parameters compatible with conventional\nsynthesis. Quantitative and qualitative results demonstrate FGA-NN's superior\nbalance between analysis accuracy and synthesis complexity, along with its\nrobustness and applicability.\n","authors":["Zoubida Ameur","Frédéric Lefebvre","Philippe De Lagrange","Miloš Radosavljević"],"pdf_url":"https://arxiv.org/pdf/2506.14350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18132v3","updated":"2025-06-17T09:32:43Z","published":"2025-05-23T17:41:54Z","title":"BiggerGait: Unlocking Gait Recognition with Layer-wise Representations\n  from Large Vision Models","summary":"  Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.\n","authors":["Dingqiang Ye","Chao Fan","Zhanbo Huang","Chengwen Luo","Jianqiang Li","Shiqi Yu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2505.18132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18682v2","updated":"2025-06-17T09:31:20Z","published":"2025-03-24T13:53:30Z","title":"Hardware-Rasterized Ray-Based Gaussian Splatting","summary":"  We present a novel, hardware rasterized rendering approach for ray-based 3D\nGaussian Splatting (RayGS), obtaining both fast and high-quality results for\nnovel view synthesis. Our work contains a mathematically rigorous and\ngeometrically intuitive derivation about how to efficiently estimate all\nrelevant quantities for rendering RayGS models, structured with respect to\nstandard hardware rasterization shaders. Our solution is the first enabling\nrendering RayGS models at sufficiently high frame rates to support\nquality-sensitive applications like Virtual and Mixed Reality. Our second\ncontribution enables alias-free rendering for RayGS, by addressing MIP-related\nissues arising when rendering diverging scales during training and testing. We\ndemonstrate significant performance gains, across different benchmark scenes,\nwhile retaining state-of-the-art appearance quality of RayGS.\n","authors":["Samuel Rota Bulò","Nemanja Bartolovic","Lorenzo Porzi","Peter Kontschieder"],"pdf_url":"https://arxiv.org/pdf/2503.18682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15670v4","updated":"2025-06-17T09:26:03Z","published":"2024-12-20T08:36:17Z","title":"BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images\n  with Conditional Latent Diffusion Models","summary":"  Lung diseases represent a significant global health challenge, with Chest\nX-Ray (CXR) being a key diagnostic tool due to its accessibility and\naffordability. Nonetheless, the detection of pulmonary lesions is often\nhindered by overlapping bone structures in CXR images, leading to potential\nmisdiagnoses. To address this issue, we develop an end-to-end framework called\nBS-LDM, designed to effectively suppress bone in high-resolution CXR images.\nThis framework is based on conditional latent diffusion models and incorporates\na multi-level hybrid loss-constrained vector-quantized generative adversarial\nnetwork which is crafted for perceptual compression, ensuring the preservation\nof details. To further enhance the framework's performance, we utilize offset\nnoise in the forward process, and a temporal adaptive thresholding strategy in\nthe reverse process. These additions help minimize discrepancies in generating\nlow-frequency information of soft tissue images. Additionally, we have compiled\na high-quality bone suppression dataset named SZCH-X-Rays. This dataset\nincludes 818 pairs of high-resolution CXR and soft tissue images collected from\nour partner hospital. Moreover, we processed 241 data pairs from the JSRT\ndataset into negative images, which are more commonly used in clinical\npractice. Our comprehensive experiments and downstream evaluations reveal that\nBS-LDM excels in bone suppression, underscoring its clinical value. Our code is\navailable at https://github.com/diaoquesang/BS-LDM.\n","authors":["Yifei Sun","Zhanghao Chen","Hao Zheng","Wenming Deng","Jin Liu","Wenwen Min","Ahmed Elazab","Xiang Wan","Changmiao Wang","Ruiquan Ge"],"pdf_url":"https://arxiv.org/pdf/2412.15670v4.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.11316v4","updated":"2025-06-17T09:05:43Z","published":"2024-09-17T16:14:03Z","title":"MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via\n  Transformer-Guided Prototyping","summary":"  Few-shot Semantic Segmentation addresses the challenge of segmenting objects\nin query images with only a handful of annotated examples. However, many\nprevious state-of-the-art methods either have to discard intricate local\nsemantic features or suffer from high computational complexity. To address\nthese challenges, we propose a new Few-shot Semantic Segmentation framework\nbased on the Transformer architecture. Our approach introduces the spatial\ntransformer decoder and the contextual mask generation module to improve the\nrelational understanding between support and query images. Moreover, we\nintroduce a multi scale decoder to refine the segmentation mask by\nincorporating features from different resolutions in a hierarchical manner.\nAdditionally, our approach integrates global features from intermediate encoder\nstages to improve contextual understanding, while maintaining a lightweight\nstructure to reduce complexity. This balance between performance and efficiency\nenables our method to achieve competitive results on benchmark datasets such as\nPASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model\nwith only 1.5 million parameters demonstrates competitive performance while\novercoming limitations of existing methodologies.\nhttps://github.com/amirrezafateh/MSDNet\n","authors":["Amirreza Fateh","Mohammad Reza Mohammadi","Mohammad Reza Jahed Motlagh"],"pdf_url":"https://arxiv.org/pdf/2409.11316v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14322v1","updated":"2025-06-17T09:02:46Z","published":"2025-06-17T09:02:46Z","title":"FRIDU: Functional Map Refinement with Guided Image Diffusion","summary":"  We propose a novel approach for refining a given correspondence map between\ntwo shapes. A correspondence map represented as a functional map, namely a\nchange of basis matrix, can be additionally treated as a 2D image. With this\nperspective, we train an image diffusion model directly in the space of\nfunctional maps, enabling it to generate accurate maps conditioned on an\ninaccurate initial map. The training is done purely in the functional space,\nand thus is highly efficient. At inference time, we use the pointwise map\ncorresponding to the current functional map as guidance during the diffusion\nprocess. The guidance can additionally encourage different functional map\nobjectives, such as orthogonality and commutativity with the Laplace-Beltrami\noperator. We show that our approach is competitive with state-of-the-art\nmethods of map refinement and that guided diffusion models provide a promising\npathway to functional map processing.\n","authors":["Avigail Cohen Rimon","Mirela Ben-Chen","Or Litany"],"pdf_url":"https://arxiv.org/pdf/2506.14322v1.pdf","comment":"Accepted to SGP 2025 (Symposium on Geometry Processing)"},{"id":"http://arxiv.org/abs/2506.14318v1","updated":"2025-06-17T08:56:05Z","published":"2025-06-17T08:56:05Z","title":"BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification\n  with Swin-HAFNet","summary":"  Accurate segmentation and classification of brain tumors from Magnetic\nResonance Imaging (MRI) remain key challenges in medical image analysis,\nlargely due to the lack of high-quality, balanced, and diverse datasets. In\nthis work, we present a new curated MRI dataset designed specifically for brain\ntumor segmentation and classification tasks. The dataset comprises 6,000\ncontrast-enhanced T1-weighted MRI scans annotated by certified radiologists and\nphysicians, spanning three major tumor types-glioma, meningioma, and\npituitary-as well as non-tumorous cases. Each sample includes high-resolution\nlabels and is categorized across axial, sagittal, and coronal imaging planes to\nfacilitate robust model development and cross-view generalization. To\ndemonstrate the utility of the dataset, we propose a transformer-based\nsegmentation model and benchmark it against established baselines. Our method\nachieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with\nimprovements observed across all tumor categories. Importantly, this study\nserves primarily as an introduction to the dataset, establishing foundational\nbenchmarks for future research. We envision this dataset as a valuable resource\nfor advancing machine learning applications in neuro-oncology, supporting both\nacademic research and clinical decision-support development. datasetlink:\nhttps://www.kaggle.com/datasets/briscdataset/brisc2025/\n","authors":["Amirreza Fateh","Yasin Rezvani","Sara Moayedi","Sadjad Rezvani","Fatemeh Fateh","Mansoor Fateh"],"pdf_url":"https://arxiv.org/pdf/2506.14318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01413v4","updated":"2025-06-17T08:52:06Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v4.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2503.09483v3","updated":"2025-06-17T08:50:23Z","published":"2025-03-12T15:38:11Z","title":"Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization","summary":"  We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.\n","authors":["Andreas Kofler","Luca Calatroni","Christoph Kolbitsch","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2503.09483v3.pdf","comment":"Accepted for publication in the proceedings of the EUSIPCO 2025\n  conference"},{"id":"http://arxiv.org/abs/2506.14315v1","updated":"2025-06-17T08:50:05Z","published":"2025-06-17T08:50:05Z","title":"ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies","summary":"  Automatic creation of 3D scenes for immersive VR presence has been a\nsignificant research focus for decades. However, existing methods often rely on\neither high-poly mesh modeling with post-hoc simplification or massive 3D\nGaussians, resulting in a complex pipeline or limited visual realism. In this\npaper, we demonstrate that such exhaustive modeling is unnecessary for\nachieving compelling immersive experience. We introduce ImmerseGen, a novel\nagent-guided framework for compact and photorealistic world modeling.\nImmerseGen represents scenes as hierarchical compositions of lightweight\ngeometric proxies, i.e., simplified terrain and billboard meshes, and generates\nphotorealistic appearance by synthesizing RGBA textures onto these proxies.\nSpecifically, we propose terrain-conditioned texturing for user-centric base\nworld synthesis, and RGBA asset texturing for midground and foreground\nscenery.This reformulation offers several advantages: (i) it simplifies\nmodeling by enabling agents to guide generative models in producing coherent\ntextures that integrate seamlessly with the scene; (ii) it bypasses complex\ngeometry creation and decimation by directly synthesizing photorealistic\ntextures on proxies, preserving visual quality without degradation; (iii) it\nenables compact representations suitable for real-time rendering on mobile VR\nheadsets. To automate scene creation from text prompts, we introduce VLM-based\nmodeling agents enhanced with semantic grid-based analysis for improved spatial\nreasoning and accurate asset placement. ImmerseGen further enriches scenes with\ndynamic effects and ambient audio to support multisensory immersion.\nExperiments on scene generation and live VR showcases demonstrate that\nImmerseGen achieves superior photorealism, spatial coherence and rendering\nefficiency compared to prior methods. Project webpage:\nhttps://immersegen.github.io.\n","authors":["Jinyan Yuan","Bangbang Yang","Keke Wang","Panwang Pan","Lin Ma","Xuehai Zhang","Xiao Liu","Zhaopeng Cui","Yuewen Ma"],"pdf_url":"https://arxiv.org/pdf/2506.14315v1.pdf","comment":"Project webpage: https://immersegen.github.io"},{"id":"http://arxiv.org/abs/2505.07819v2","updated":"2025-06-17T08:36:51Z","published":"2025-05-12T17:59:43Z","title":"H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning","summary":"  Visuomotor policy learning has witnessed substantial progress in robotic\nmanipulation, with recent approaches predominantly relying on generative models\nto model the action distribution. However, these methods often overlook the\ncritical coupling between visual perception and action prediction. In this\nwork, we introduce $\\textbf{Triply-Hierarchical Diffusion\nPolicy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework\nthat explicitly incorporates hierarchical structures to strengthen the\nintegration between visual features and action generation. H$^{3}$DP contains\n$\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes\nRGB-D observations based on depth information; (2) multi-scale visual\nrepresentations that encode semantic features at varying levels of granularity;\nand (3) a hierarchically conditioned diffusion process that aligns the\ngeneration of coarse-to-fine actions with corresponding visual features.\nExtensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$\naverage relative improvement over baselines across $\\mathbf{44}$ simulation\ntasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual\nreal-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.\n","authors":["Yiyang Lu","Yufeng Tian","Zhecheng Yuan","Xianbang Wang","Pu Hua","Zhengrong Xue","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2505.07819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14303v1","updated":"2025-06-17T08:29:40Z","published":"2025-06-17T08:29:40Z","title":"orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous\n  Generation of Surgical Images and Ground Truth Labels","summary":"  Deep learning in medical imaging faces obstacles: limited data diversity,\nethical issues, high acquisition costs, and the need for precise annotations.\nBleeding detection and localization during surgery is especially challenging\ndue to the scarcity of high-quality datasets that reflect real surgical\nscenarios. We propose orGAN, a GAN-based system for generating high-fidelity,\nannotated surgical images of bleeding. By leveraging small \"mimicking organ\"\ndatasets, synthetic models that replicate tissue properties and bleeding, our\napproach reduces ethical concerns and data-collection costs. orGAN builds on\nStyleGAN with Relational Positional Learning to simulate bleeding events\nrealistically and mark bleeding coordinates. A LaMa-based inpainting module\nthen restores clean, pre-bleed visuals, enabling precise pixel-level\nannotations. In evaluations, a balanced dataset of orGAN and mimicking-organ\nimages achieved 90% detection accuracy in surgical settings and up to 99%\nframe-level accuracy. While our development data lack diverse organ\nmorphologies and contain intraoperative artifacts, orGAN markedly advances\nethical, efficient, and cost-effective creation of realistic annotated bleeding\ndatasets, supporting broader integration of AI in surgical practice.\n","authors":["Niran Nataraj","Maina Sogabe","Kenji Kawashima"],"pdf_url":"https://arxiv.org/pdf/2506.14303v1.pdf","comment":"24 pages, 7figures"},{"id":"http://arxiv.org/abs/2505.11404v3","updated":"2025-06-17T08:23:39Z","published":"2025-05-16T16:12:50Z","title":"Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert\n  Reasoner","summary":"  Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose Patho-CLIP, trained on the same figure-caption corpus used for\ncontinued pretraining. Comprehensive experimental results demonstrate that both\nPatho-CLIP and Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.\n","authors":["Wenchuan Zhang","Penghao Zhang","Jingru Guo","Tao Cheng","Jie Chen","Shuwan Zhang","Zhang Zhang","Yuhao Yi","Hong Bu"],"pdf_url":"https://arxiv.org/pdf/2505.11404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07601v3","updated":"2025-06-17T08:17:48Z","published":"2024-03-12T12:40:08Z","title":"Unified Source-Free Domain Adaptation","summary":"  In the pursuit of transferring a source model to a target domain without\naccess to the source training data, Source-Free Domain Adaptation (SFDA) has\nbeen extensively explored across various scenarios, including Closed-set,\nOpen-set, Partial-set, and Generalized settings. Existing methods, focusing on\nspecific scenarios, not only address a limited subset of challenges but also\nnecessitate prior knowledge of the target domain, significantly limiting their\npractical utility and deployability. In light of these considerations, we\nintroduce a more practical yet challenging problem, termed unified SFDA, which\ncomprehensively incorporates all specific scenarios in a unified manner. In\nthis paper, we propose a novel approach latent Causal factors discovery for\nunified SFDA(CausalDA). In contrast to previous alternatives that emphasize\nlearning the statistical description of reality, we formulate CausalDA from a\ncausality perspective. The objective is to uncover the causal relationships\nbetween latent variables and model decisions, enhancing the reliability and\nrobustness of the learned model against domain shifts. To integrate extensive\nworld knowledge, we leverage a pre-trained vision-language model such as CLIP.\nThis aids in the formation and discovery of latent causal factors in the\nabsence of supervision in the variation of distribution and semantics, coupled\nwith a newly designed information bottleneck with theoretical guarantees.\nExtensive experiments demonstrate that CausalDA can achieve new\nstate-of-the-art results in distinct SFDA settings, as well as source-free\nout-of-distribution generalization.\n","authors":["Song Tang","Wenxin Su","Mao Ye","Jianwei Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.07601v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09081v2","updated":"2025-06-17T07:54:37Z","published":"2025-06-10T04:19:02Z","title":"FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model\n  Evaluation","summary":"  We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.\n","authors":["Zheqi He","Yesheng Liu","Jing-shu Zheng","Xuejing Li","Jin-Ge Yao","Bowen Qin","Richeng Xuan","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2506.09081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.06679v2","updated":"2025-06-17T07:44:28Z","published":"2025-05-10T16:04:52Z","title":"T2V-OptJail: Discrete Prompt Optimization for Text-to-Video Jailbreak\n  Attacks","summary":"  In recent years, fueled by the rapid advancement of diffusion models,\ntext-to-video (T2V) generation models have achieved remarkable progress, with\nnotable examples including Pika, Luma, Kling, and Open-Sora. Although these\nmodels exhibit impressive generative capabilities, they also expose significant\nsecurity risks due to their vulnerability to jailbreak attacks, where the\nmodels are manipulated to produce unsafe content such as pornography, violence,\nor discrimination. Existing works such as T2VSafetyBench provide preliminary\nbenchmarks for safety evaluation, but lack systematic methods for thoroughly\nexploring model vulnerabilities. To address this gap, we are the first to\nformalize the T2V jailbreak attack as a discrete optimization problem and\npropose a joint objective-based optimization framework, called T2V-OptJail.\nThis framework consists of two key optimization goals: bypassing the built-in\nsafety filtering mechanisms to increase the attack success rate, preserving\nsemantic consistency between the adversarial prompt and the unsafe input\nprompt, as well as between the generated video and the unsafe input prompt, to\nenhance content controllability. In addition, we introduce an iterative\noptimization strategy guided by prompt variants, where multiple semantically\nequivalent candidates are generated in each round, and their scores are\naggregated to robustly guide the search toward optimal adversarial prompts. We\nconduct large-scale experiments on several T2V models, covering both\nopen-source models and real commercial closed-source models. The experimental\nresults show that the proposed method improves 11.4% and 10.0% over the\nexisting state-of-the-art method in terms of attack success rate assessed by\nGPT-4, attack success rate assessed by human accessors, respectively, verifying\nthe significant advantages of the method in terms of attack effectiveness and\ncontent control.\n","authors":["Jiayang Liu","Siyuan Liang","Shiqian Zhao","Rongcheng Tu","Wenbo Zhou","Aishan Liu","Dacheng Tao","Siew Kei Lam"],"pdf_url":"https://arxiv.org/pdf/2505.06679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14271v1","updated":"2025-06-17T07:37:08Z","published":"2025-06-17T07:37:08Z","title":"Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task\n  Learning in Diverse Environment","summary":"  360 video captures the complete surrounding scenes with the ultra-large field\nof view of 360X180. This makes 360 scene understanding tasks, eg, segmentation\nand tracking, crucial for appications, such as autonomous driving, robotics.\nWith the recent emergence of foundation models, the community is, however,\nimpeded by the lack of large-scale, labelled real-world datasets. This is\ncaused by the inherent spherical properties, eg, severe distortion in polar\nregions, and content discontinuities, rendering the annotation costly yet\ncomplex. This paper introduces Leader360V, the first large-scale, labeled\nreal-world 360 video datasets for instance segmentation and tracking. Our\ndatasets enjoy high scene diversity, ranging from indoor and urban settings to\nnatural and dynamic outdoor scenes. To automate annotation, we design an\nautomatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors\nand large language models to facilitate the labeling. The pipeline operates in\nthree novel stages. Specifically, in the Initial Annotation Phase, we introduce\na Semantic- and Distortion-aware Refinement module, which combines object mask\nproposals from multiple 2D segmentors with LLM-verified semantic labels. These\nare then converted into mask prompts to guide SAM2 in generating\ndistortion-aware masks for subsequent frames. In the Auto-Refine Annotation\nPhase, missing or incomplete regions are corrected either by applying the SDR\nagain or resolving the discontinuities near the horizontal borders. The Manual\nRevision Phase finally incorporates LLMs and human annotators to further refine\nand validate the annotations. Extensive user studies and evaluations\ndemonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments\nconfirm that Leader360V significantly enhances model performance for 360 video\nsegmentation and tracking, paving the way for more scalable 360 scene\nunderstanding.\n","authors":["Weiming Zhang","Dingwen Xiao","Aobotao Dai","Yexin Liu","Tianbo Pan","Shiqi Wen","Lei Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14271v1.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2502.00404v2","updated":"2025-06-17T07:27:17Z","published":"2025-02-01T11:39:02Z","title":"Exploring Linear Attention Alternative for Single Image Super-Resolution","summary":"  Deep learning-based single-image super-resolution (SISR) technology focuses\non enhancing low-resolution (LR) images into high-resolution (HR) ones.\nAlthough significant progress has been made, challenges remain in computational\ncomplexity and quality, particularly in remote sensing image processing. To\naddress these issues, we propose our Omni-Scale RWKV Super-Resolution\n(OmniRWKVSR) model which presents a novel approach that combines the Receptance\nWeighted Key Value (RWKV) architecture with feature extraction techniques such\nas Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM),\naiming to overcome the limitations of existing methods and achieve superior\nSISR performance. This work has proved able to provide effective solutions for\nhigh-quality image reconstruction. Under the 4x Super-Resolution tasks,\ncompared to the MambaIR model, we achieved an average improvement of 0.26% in\nPSNR and 0.16% in SSIM.\n","authors":["Rongchang Lu","Changyu Li","Donghang Li","Guojing Zhang","Jianqiang Huang","Xilai Li"],"pdf_url":"https://arxiv.org/pdf/2502.00404v2.pdf","comment":"This paper has been published to IEEE International Joint Conference\n  on Neural Networks 2025 as the final camera ready version. Contact at\n  nomodeset@qq.com"},{"id":"http://arxiv.org/abs/2506.14265v1","updated":"2025-06-17T07:25:57Z","published":"2025-06-17T07:25:57Z","title":"Exploring Non-contrastive Self-supervised Representation Learning for\n  Image-based Profiling","summary":"  Image-based cell profiling aims to create informative representations of cell\nimages. This technique is critical in drug discovery and has greatly advanced\nwith recent improvements in computer vision. Inspired by recent developments in\nnon-contrastive Self-Supervised Learning (SSL), this paper provides an initial\nexploration into training a generalizable feature extractor for cell images\nusing such methods. However, there are two major challenges: 1) There is a\nlarge difference between the distributions of cell images and natural images,\ncausing the view-generation process in existing SSL methods to fail; and 2)\nUnlike typical scenarios where each representation is based on a single image,\ncell profiling often involves multiple input images, making it difficult to\neffectively combine all available information. To overcome these challenges, we\npropose SSLProfiler, a non-contrastive SSL framework specifically designed for\ncell profiling. We introduce specialized data augmentation and representation\npost-processing methods tailored to cell images, which effectively address the\nissues mentioned above and result in a robust feature extractor. With these\nimprovements, SSLProfiler won the Cell Line Transferability challenge at CVPR\n2025.\n","authors":["Siran Dai","Qianqian Xu","Peisong Wen","Yang Liu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2506.14265v1.pdf","comment":"CVPR 2025 Computer Vision for Drug Discovery"},{"id":"http://arxiv.org/abs/2502.09779v2","updated":"2025-06-17T07:19:39Z","published":"2025-02-13T21:27:10Z","title":"Automated Muscle and Fat Segmentation in Computed Tomography for\n  Comprehensive Body Composition Analysis","summary":"  Body composition assessment using CT images can potentially be used for a\nnumber of clinical applications, including the prognostication of\ncardiovascular outcomes, evaluation of metabolic health, monitoring of disease\nprogression, assessment of nutritional status, prediction of treatment response\nin oncology, and risk stratification for surgical and critical care outcomes.\nWhile multiple groups have developed in-house segmentation tools for this\nanalysis, there are very limited publicly available tools that could be\nconsistently used across different applications. To mitigate this gap, we\npresent a publicly accessible, end-to-end segmentation and feature calculation\nmodel specifically for CT body composition analysis. Our model performs\nsegmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and\nvisceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in\naxial CT images. It also provides various body composition metrics, including\nmuscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle\narea/volume, and skeletal muscle index (SMI), supporting both 2D and 3D\nassessments. To evaluate the model, the segmentation was applied to both\ninternal and external datasets, with body composition metrics analyzed across\ndifferent age, sex, and race groups. The model achieved high dice coefficients\non both internal and external datasets, exceeding 89% for skeletal muscle, SAT,\nand VAT segmentation. The model outperforms the benchmark by 2.40% on skeletal\nmuscle and 10.26% on SAT compared to the manual annotations given by the\npublicly available dataset. Body composition metrics show mean relative\nabsolute errors (MRAEs) under 10% for all measures. Furthermore, the model\nprovided muscular fat segmentation with a Dice coefficient of 56.27%, which can\nbe utilized for additional analyses as needed.\n","authors":["Yaqian Chen","Hanxue Gu","Yuwen Chen","Jicheng Yang","Haoyu Dong","Joseph Y. Cao","Adrian Camarena","Christopher Mantyh","Roy Colglazier","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2502.09779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14256v1","updated":"2025-06-17T07:18:04Z","published":"2025-06-17T07:18:04Z","title":"Comparison of Two Methods for Stationary Incident Detection Based on\n  Background Image","summary":"  In general, background subtraction-based methods are used to detect moving\nobjects in visual tracking applications. In this paper, we employed a\nbackground subtraction-based scheme to detect the temporarily stationary\nobjects. We proposed two schemes for stationary object detection, and we\ncompare those in terms of detection performance and computational complexity.\nIn the first approach, we used a single background, and in the second approach,\nwe used dual backgrounds, generated with different learning rates, in order to\ndetect temporarily stopped objects. Finally, we used normalized cross\ncorrelation (NCC) based image comparison to monitor and track the detected\nstationary object in a video scene. The proposed method is robust with partial\nocclusion, short-time fully occlusion, and illumination changes, and it can\noperate in real time.\n","authors":["Deepak Ghimire","Joonwhoan Lee"],"pdf_url":"https://arxiv.org/pdf/2506.14256v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14255v1","updated":"2025-06-17T07:17:15Z","published":"2025-06-17T07:17:15Z","title":"synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and\n  Robustness for Real-World Bridge Inspections?","summary":"  Adequate bridge inspection is increasingly challenging in many countries due\nto growing ailing stocks, compounded with a lack of staff and financial\nresources. Automating the key task of visual bridge inspection, classification\nof defects and building components on pixel level, improves efficiency,\nincreases accuracy and enhances safety in the inspection process and resulting\nbuilding assessment. Models overtaking this task must cope with an assortment\nof real-world conditions. They must be robust to variations in image quality,\nas well as background texture, as defects often appear on surfaces of diverse\ntexture and degree of weathering. dacl10k is the largest and most diverse\ndataset for real-world concrete bridge inspections. However, the dataset\nexhibits class imbalance, which leads to notably poor model performance\nparticularly when segmenting fine-grained classes such as cracks and cavities.\nThis work introduces \"synth-dacl\", a compilation of three novel dataset\nextensions based on synthetic concrete textures. These extensions are designed\nto balance class distribution in dacl10k and enhance model performance,\nespecially for crack and cavity segmentation. When incorporating the synth-dacl\nextensions, we observe substantial improvements in model robustness across 15\nperturbed test sets. Notably, on the perturbed test set, a model trained on\ndacl10k combined with all synthetic extensions achieves a 2% increase in mean\nIoU, F1 score, Recall, and Precision compared to the same model trained solely\non dacl10k.\n","authors":["Johannes Flotzinger","Fabian Deuser","Achref Jaziri","Heiko Neumann","Norbert Oswald","Visvanathan Ramesh","Thomas Braml"],"pdf_url":"https://arxiv.org/pdf/2506.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14243v1","updated":"2025-06-17T07:04:07Z","published":"2025-06-17T07:04:07Z","title":"Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven\n  Framework for Resilient 3D Place Recognition","summary":"  LiDAR-based place recognition serves as a crucial enabler for long-term\nautonomy in robotics and autonomous driving systems. Yet, prevailing\nmethodologies relying on handcrafted feature extraction face dual challenges:\n(1) Inconsistent point cloud density, induced by ego-motion dynamics and\nenvironmental disturbances during repeated traversals, leads to descriptor\ninstability, and (2) Representation fragility stems from reliance on\nsingle-level geometric abstractions that lack discriminative power in\nstructurally complex scenarios. To address these limitations, we propose a\nnovel framework that redefines 3D place recognition through density-agnostic\ngeometric reasoning. Specifically, we introduce an implicit 3D representation\nbased on elastic points, which is immune to the interference of original scene\npoint cloud density and achieves the characteristic of uniform distribution.\nSubsequently, we derive the occupancy grid and normal vector information of the\nscene from this implicit representation. Finally, with the aid of these two\ntypes of information, we obtain descriptors that fuse geometric information\nfrom both bird's-eye view (capturing macro-level spatial layouts) and 3D\nsegment (encoding micro-scale surface geometries) perspectives. We conducted\nextensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT)\nacross diverse environments. The experimental results demonstrate that our\nmethod achieves state-of-the-art performance. Moreover, our approach strikes an\noptimal balance between accuracy, runtime, and memory optimization for\nhistorical maps, showcasing excellent Resilient and scalability. Our code will\nbe open-sourced in the future.\n","authors":["Xiaohui Jiang","Haijiang Zhu","Chadei Li","Fulin Tang","Ning An"],"pdf_url":"https://arxiv.org/pdf/2506.14243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14238v1","updated":"2025-06-17T06:53:15Z","published":"2025-06-17T06:53:15Z","title":"Unified Representation Space for 3D Visual Grounding","summary":"  3D visual grounding (3DVG) is a critical task in scene understanding that\naims to identify objects in 3D scenes based on text descriptions. However,\nexisting methods rely on separately pre-trained vision and text encoders,\nresulting in a significant gap between the two modalities in terms of spatial\ngeometry and semantic categories. This discrepancy often causes errors in\nobject positioning and classification. The paper proposes UniSpace-3D, which\ninnovatively introduces a unified representation space for 3DVG, effectively\nbridging the gap between visual and textual features. Specifically, UniSpace-3D\nincorporates three innovative designs: i) a unified representation encoder that\nleverages the pre-trained CLIP model to map visual and textual features into a\nunified representation space, effectively bridging the gap between the two\nmodalities; ii) a multi-modal contrastive learning module that further reduces\nthe modality gap; iii) a language-guided query selection module that utilizes\nthe positional and semantic information to identify object candidate points\naligned with textual descriptions. Extensive experiments demonstrate that\nUniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and\nNr3D/Sr3D datasets. The code will be made available upon acceptance of the\npaper.\n","authors":["Yinuo Zheng","Lipeng Gu","Honghua Chen","Liangliang Nan","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2506.14238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13444v2","updated":"2025-06-17T06:43:53Z","published":"2025-06-16T12:57:58Z","title":"Self-Supervised Enhancement for Depth from a Lightweight ToF Sensor with\n  Monocular Images","summary":"  Depth map enhancement using paired high-resolution RGB images offers a\ncost-effective solution for improving low-resolution depth data from\nlightweight ToF sensors. Nevertheless, naively adopting a depth estimation\npipeline to fuse the two modalities requires groundtruth depth maps for\nsupervision. To address this, we propose a self-supervised learning framework,\nSelfToF, which generates detailed and scale-aware depth maps. Starting from an\nimage-based self-supervised depth estimation pipeline, we add low-resolution\ndepth as inputs, design a new depth consistency loss, propose a scale-recovery\nmodule, and finally obtain a large performance boost. Furthermore, since the\nToF signal sparsity varies in real-world applications, we upgrade SelfToF to\nSelfToF* with submanifold convolution and guided feature fusion. Consequently,\nSelfToF* maintain robust performance across varying sparsity levels in ToF\ndata. Overall, our proposed method is both efficient and effective, as verified\nby extensive experiments on the NYU and ScanNet datasets. The code is available\nat\n\\href{https://github.com/denyingmxd/selftof}{https://github.com/denyingmxd/selftof}.\n","authors":["Laiyan Ding","Hualie Jiang","Jiwei Chen","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2506.13444v2.pdf","comment":"accepted by IROS 2025"},{"id":"http://arxiv.org/abs/2506.14229v1","updated":"2025-06-17T06:35:38Z","published":"2025-06-17T06:35:38Z","title":"HRGS: Hierarchical Gaussian Splatting for Memory-Efficient\n  High-Resolution 3D Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D\nscene reconstruction, but faces memory scalability issues in high-resolution\nscenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS),\na memory-efficient framework with hierarchical block-level optimization. First,\nwe generate a global, coarse Gaussian representation from low-resolution data.\nThen, we partition the scene into multiple blocks, refining each block with\nhigh-resolution data. The partitioning involves two steps: Gaussian\npartitioning, where irregular scenes are normalized into a bounded cubic space\nwith a uniform grid for task distribution, and training data partitioning,\nwhere only relevant observations are retained for each block. By guiding block\nrefinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion\nacross adjacent blocks. To reduce computational demands, we introduce\nImportance-Driven Gaussian Pruning (IDGP), which computes importance scores for\neach Gaussian and removes those with minimal contribution, speeding up\nconvergence and reducing memory usage. Additionally, we incorporate normal\npriors from a pretrained model to enhance surface reconstruction quality. Our\nmethod enables high-quality, high-resolution 3D scene reconstruction even under\nmemory constraints. Extensive experiments on three benchmarks show that HRGS\nachieves state-of-the-art performance in high-resolution novel view synthesis\n(NVS) and surface reconstruction tasks.\n","authors":["Changbai Li","Haodong Zhu","Hanlin Chen","Juan Zhang","Tongfei Chen","Shuo Yang","Shuwei Shao","Wenhao Dong","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15521v3","updated":"2025-06-17T06:34:15Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v3.pdf","comment":"arXiv pre-print"},{"id":"http://arxiv.org/abs/2505.13227v2","updated":"2025-06-17T06:33:35Z","published":"2025-05-19T15:09:23Z","title":"Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis","summary":"  Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.\n","authors":["Tianbao Xie","Jiaqi Deng","Xiaochuan Li","Junlin Yang","Haoyuan Wu","Jixuan Chen","Wenjing Hu","Xinyuan Wang","Yuhui Xu","Zekun Wang","Yiheng Xu","Junli Wang","Doyen Sahoo","Tao Yu","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.13227v2.pdf","comment":"49 pages, 13 figures"},{"id":"http://arxiv.org/abs/2303.14608v2","updated":"2025-06-17T06:21:26Z","published":"2023-03-26T03:01:39Z","title":"Analyzing Effects of Mixed Sample Data Augmentation on Model\n  Interpretability","summary":"  Mixed sample data augmentation strategies are actively used when training\ndeep neural networks (DNNs). Recent studies suggest that they are effective at\nvarious tasks. However, the impact of mixed sample data augmentation on model\ninterpretability has not been widely studied. In this paper, we explore the\nrelationship between model interpretability and mixed sample data augmentation,\nspecifically in terms of feature attribution maps. To this end, we introduce a\nnew metric that allows a comparison of model interpretability while minimizing\nthe impact of occlusion robustness of the model. Experimental results show that\nseveral mixed sample data augmentation decreases the interpretability of the\nmodel and label mixing during data augmentation plays a significant role in\nthis effect. This new finding suggests it is important to carefully adopt the\nmixed sample data augmentation method, particularly in applications where\nattribution map-based interpretability is important.\n","authors":["Soyoun Won","Sung-Ho Bae","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2303.14608v2.pdf","comment":"Accepted to Neural Networks"},{"id":"http://arxiv.org/abs/2503.08703v2","updated":"2025-06-17T06:08:36Z","published":"2025-03-09T02:01:40Z","title":"SDTrack: A Baseline for Event-based Tracking via Spiking Neural Networks","summary":"  Event cameras provide superior temporal resolution, dynamic range, power\nefficiency, and pixel bandwidth. Spiking Neural Networks (SNNs) naturally\ncomplement event data through discrete spike signals, making them ideal for\nevent-based tracking. However, current approaches that combine Artificial\nNeural Networks (ANNs) and SNNs, along with suboptimal architectures,\ncompromise energy efficiency and limit tracking performance. To address these\nlimitations, we propose the first Transformer-based spike-driven tracking\npipeline. Our Global Trajectory Prompt (GTP) method effectively captures global\ntrajectory information and aggregates it with event streams into event images\nto enhance spatiotemporal representation. We then introduce SDTrack, a\nTransformer-based spike-driven tracker comprising a Spiking MetaFormer backbone\nand a simple tracking head that directly predicts normalized coordinates using\nspike signals. The framework is end-to-end, does not require data augmentation\nor post-processing. Extensive experiments demonstrate that SDTrack achieves\nstate-of-the-art performance while maintaining the lowest parameter count and\nenergy consumption across multiple event-based tracking benchmarks,\nestablishing a solid baseline for future research in the field of neuromorphic\nvision.\n","authors":["Yimeng Shan","Zhenbang Ren","Haodi Wu","Wenjie Wei","Rui-Jie Zhu","Shuai Wang","Dehao Zhang","Yichen Xiao","Jieyuan Zhang","Kexin Shi","Jingzhinan Wang","Jason K. Eshraghian","Haicheng Qu","Jiqing Zhang","Malu Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2503.08703v2.pdf","comment":"11 pages,7 figures,4 tables"},{"id":"http://arxiv.org/abs/2506.14209v1","updated":"2025-06-17T05:58:04Z","published":"2025-06-17T05:58:04Z","title":"Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in\n  Medical CBCT","summary":"  Advances in treatment technology now allow for the use of customizable\n3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN)\nof the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of\n3D medical images using tools like nnUNet.\n  However, the scarcity of labeled data in ONJ imaging makes supervised\ntraining impractical. This study aims to develop an unsupervised training\napproach for automatically identifying anomalies in imaging scans.\n  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN\nis trained to accurately reconstruct normal subjects. In the second stage,\nrandom cube masking and ONJ-specific masking are applied to train a new encoder\ncapable of recovering the data.\n  The proposed method achieves successful segmentation on both simulated and\nreal patient data.\n  This approach provides a fast initial segmentation solution, reducing the\nburden of manual labeling. Additionally, it has the potential to be directly\nused for 3D printing when combined with hand-tuned post-processing.\n","authors":["Pengwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10069v2","updated":"2025-06-17T05:47:59Z","published":"2025-03-13T05:32:57Z","title":"SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot\n  Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation (VLN) in continuous environments requires\nagents to interpret natural language instructions while navigating\nunconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage\napproach: a waypoint predictor to generate waypoints and a navigator to execute\nmovements. However, current waypoint predictors struggle with spatial\nawareness, while navigators lack historical reasoning and backtracking\ncapabilities, limiting adaptability. We propose a zero-shot VLN-CE framework\nintegrating an enhanced waypoint predictor with a Multi-modal Large Language\nModel (MLLM)-based navigator. Our predictor employs a stronger vision encoder,\nmasked cross-attention fusion, and an occupancy-aware loss for better waypoint\nquality. The navigator incorporates history-aware reasoning and adaptive path\nplanning with backtracking, improving robustness. Experiments on R2R-CE and\nMP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in\nzero-shot settings, demonstrating competitive results compared to fully\nsupervised methods. Real-world validation on Turtlebot 4 further highlights its\nadaptability.\n","authors":["Xiangyu Shi","Zerui Li","Wenqi Lyu","Jiatong Xia","Feras Dayoub","Yanyuan Qiao","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2503.10069v2.pdf","comment":"Accepted by IROS 2025. Project website:\n  https://sxyxs.github.io/smartway/"},{"id":"http://arxiv.org/abs/2503.12553v2","updated":"2025-06-17T05:47:02Z","published":"2025-03-16T15:50:18Z","title":"Niagara: Normal-Integrated Geometric Affine Field for Scene\n  Reconstruction from a Single View","summary":"  Recent advances in single-view 3D scene reconstruction have highlighted the\nchallenges in capturing fine geometric details and ensuring structural\nconsistency, particularly in high-fidelity outdoor scene modeling. This paper\npresents Niagara, a new single-view 3D scene reconstruction framework that can\nfaithfully reconstruct challenging outdoor scenes from a single input image for\nthe first time.\n  Our approach integrates monocular depth and normal estimation as input, which\nsubstantially improves its ability to capture fine details, mitigating common\nissues like geometric detail loss and deformation.\n  Additionally, we introduce a geometric affine field (GAF) and 3D\nself-attention as geometry-constraint, which combines the structural properties\nof explicit geometry with the adaptability of implicit feature fields, striking\na balance between efficient rendering and high-fidelity reconstruction.\n  Our framework finally proposes a specialized encoder-decoder architecture,\nwhere a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian\nparameters, which can be used for novel view synthesis. Extensive results and\nanalyses suggest that our Niagara surpasses prior SoTA approaches such as\nFlash3D in both single-view and dual-view settings, significantly enhancing the\ngeometric accuracy and visual fidelity, especially in outdoor scenes.\n","authors":["Xianzu Wu","Zhenxin Ai","Harry Yang","Ser-Nam Lim","Jun Liu","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01565v2","updated":"2025-06-17T05:37:58Z","published":"2025-06-02T11:43:46Z","title":"Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation","summary":"  Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.\n","authors":["Li Zhou","Lutong Yu","Dongchu Xie","Shaohuan Cheng","Wenyan Li","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2506.01565v2.pdf","comment":"cultural analysis, cultural visual understanding, cultural image\n  transcreation (update dataset license)"},{"id":"http://arxiv.org/abs/2506.14198v1","updated":"2025-06-17T05:31:42Z","published":"2025-06-17T05:31:42Z","title":"AMPLIFY: Actionless Motion Priors for Robot Learning from Videos","summary":"  Action-labeled data for robotics is scarce and expensive, limiting the\ngeneralization of learned policies. In contrast, vast amounts of action-free\nvideo data are readily available, but translating these observations into\neffective policies remains a challenge. We introduce AMPLIFY, a novel framework\nthat leverages large-scale video data by encoding visual dynamics into compact,\ndiscrete motion tokens derived from keypoint trajectories. Our modular approach\nseparates visual motion prediction from action inference, decoupling the\nchallenges of learning what motion defines a task from how robots can perform\nit. We train a forward dynamics model on abundant action-free videos and an\ninverse dynamics model on a limited set of action-labeled examples, allowing\nfor independent scaling. Extensive evaluations demonstrate that the learned\ndynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x\nbetter pixel prediction accuracy compared to prior approaches, and broadly\nuseful. In downstream policy learning, our dynamics predictions enable a\n1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by\nlearning from action-free human videos, and the first generalization to LIBERO\ntasks from zero in-distribution action data. Beyond robotic control, we find\nthe dynamics learned by AMPLIFY to be a versatile latent world model, enhancing\nvideo prediction quality. Our results present a novel paradigm leveraging\nheterogeneous data sources to build efficient, generalizable world models. More\ninformation can be found at https://amplify-robotics.github.io/.\n","authors":["Jeremy A. Collins","Loránd Cheng","Kunal Aneja","Albert Wilcox","Benjamin Joffe","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2506.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04097v2","updated":"2025-06-17T05:15:08Z","published":"2025-05-07T03:32:25Z","title":"3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data\n  Augmentation","summary":"  A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers.\n","authors":["Thien Nhan Vo","Bac Nam Ho"],"pdf_url":"https://arxiv.org/pdf/2505.04097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14189v1","updated":"2025-06-17T05:03:42Z","published":"2025-06-17T05:03:42Z","title":"Egocentric Human-Object Interaction Detection: A New Benchmark and\n  Method","summary":"  Understanding the interaction between humans and objects has gained much\nattention in recent years. Existing human-object interaction (HOI) detection\nmethods mainly focus on the third-person perspectives, overlooking a more\nintuitive way from the egocentric view of HOI, namely Ego-HOI. This paper\nintroduces an Ego-HOIBench, a new dataset to promote the benchmarking and\ndevelopment of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K\negocentric images with high-quality hand-verb-object triplet annotations across\n123 fine-grained interaction categories and locations, covering a rich\ndiversity of scenarios, object types, and hand configurations in daily\nactivities. In addition, we explore and adapt third-person HOI detection\nmethods to Ego-HOIBench and illustrate the challenges of hand-occluded objects\nand the complexity of single- and two-hand interactions. To build a new\nbaseline, we propose a Hand Geometry and Interactivity Refinement (HGIR)\nscheme, which leverages hand pose and geometric information as valuable cues\nfor interpreting interactions. Specifically, the HGIR scheme explicitly\nextracts global hand geometric features from the estimated hand pose proposals\nand refines the interaction-specific features using pose-interaction attention.\nThis scheme enables the model to obtain a robust and powerful interaction\nrepresentation, significantly improving the Ego-HOI detection capability. Our\napproach is lightweight and effective, and it can be easily applied to HOI\nbaselines in a plug-and-play manner to achieve state-of-the-art results on\nEgo-HOIBench. Our project is available at:\nhttps://dengkunyuan.github.io/EgoHOIBench/\n","authors":["Kunyuan Deng","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2506.14189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15077v3","updated":"2025-06-17T05:03:10Z","published":"2025-02-20T22:29:24Z","title":"Hardware-Friendly Static Quantization Method for Video Diffusion\n  Transformers","summary":"  Diffusion Transformers for video generation have gained significant research\ninterest since the impressive performance of SORA. Efficient deployment of such\ngenerative-AI models on GPUs has been demonstrated with dynamic quantization.\nHowever, resource-constrained devices cannot support dynamic quantization, and\nneed static quantization of the models for their efficient deployment on AI\nprocessors. In this paper, we propose a novel method for the post-training\nquantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without\nrelying on dynamic quantization techniques. Our approach employs static\nquantization, achieving video quality comparable to FP16 and dynamically\nquantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,\nwe utilize per-step calibration data to adequately provide a post-training\nstatically quantized model for each time step, incorporating channel-wise\nquantization for weights and tensor-wise quantization for activations. By\nfurther applying the smooth-quantization technique, we can obtain high-quality\nvideo outputs with the statically quantized models. Extensive experimental\nresults demonstrate that static quantization can be a viable alternative to\ndynamic quantization for video diffusion transformers, offering a more\nefficient approach without sacrificing performance.\n","authors":["Sanghyun Yi","Qingfeng Liu","Mostafa El-Khamy"],"pdf_url":"https://arxiv.org/pdf/2502.15077v3.pdf","comment":"Accepted to MIPR 2025"},{"id":"http://arxiv.org/abs/2506.06290v2","updated":"2025-06-17T04:58:45Z","published":"2025-05-16T23:07:51Z","title":"CellCLIP -- Learning Perturbation Effects in Cell Painting via\n  Text-Guided Contrastive Learning","summary":"  High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.\n","authors":["Mingyu Lu","Ethan Weinberger","Chanwoo Kim","Su-In Lee"],"pdf_url":"https://arxiv.org/pdf/2506.06290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12340v2","updated":"2025-06-17T04:52:16Z","published":"2025-06-14T04:22:36Z","title":"Image Corruption-Inspired Membership Inference Attacks against Large\n  Vision-Language Models","summary":"  Large vision-language models (LVLMs) have demonstrated outstanding\nperformance in many downstream tasks. However, LVLMs are trained on large-scale\ndatasets, which can pose privacy risks if training images contain sensitive\ninformation. Therefore, it is important to detect whether an image is used to\ntrain the LVLM. Recent studies have investigated membership inference attacks\n(MIAs) against LVLMs, including detecting image-text pairs and single-modality\ncontent. In this work, we focus on detecting whether a target image is used to\ntrain the target LVLM. We design simple yet effective Image Corruption-Inspired\nMembership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by\nLVLM's different sensitivity to image corruption for member and non-member\nimages. We first perform an MIA method under the white-box setting, where we\ncan obtain the embeddings of the image through the vision part of the target\nLVLM. The attacks are based on the embedding similarity between the image and\nits corrupted version. We further explore a more practical scenario where we\nhave no knowledge about target LVLMs and we can only query the target LVLMs\nwith an image and a question. We then conduct the attack by utilizing the\noutput text embeddings' similarity. Experiments on existing datasets validate\nthe effectiveness of our proposed attack methods under those two different\nsettings.\n","authors":["Zongyu Wu","Minhua Lin","Zhiwei Zhang","Fali Wang","Xianren Zhang","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2506.12340v2.pdf","comment":"Preprint. 15 pages"},{"id":"http://arxiv.org/abs/2506.14181v1","updated":"2025-06-17T04:48:59Z","published":"2025-06-17T04:48:59Z","title":"Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning\n  is Reliable for Online Surgical Phase Recognition","summary":"  Online surgical phase recognition has drawn great attention most recently due\nto its potential downstream applications closely related to human life and\nhealth. Despite deep models have made significant advances in capturing the\ndiscriminative long-term dependency of surgical videos to achieve improved\nrecognition, they rarely account for exploring and modeling the uncertainty in\nsurgical videos, which should be crucial for reliable online surgical phase\nrecognition. We categorize the sources of uncertainty into two types, frame\nambiguity in videos and unbalanced distribution among surgical phases, which\nare inevitable in surgical videos. To address this pivot issue, we introduce a\nmeta-learning-optimized classification diffusion model (Meta-SurDiff), to take\nfull advantage of the deep generative model and meta-learning in achieving\nprecise frame-level distribution estimation for reliable online surgical phase\nrecognition. For coarse recognition caused by ambiguous video frames, we employ\na classification diffusion model to assess the confidence of recognition\nresults at a finer-grained frame-level instance. For coarse recognition caused\nby unbalanced phase distribution, we use a meta-learning based objective to\nlearn the diffusion model, thus enhancing the robustness of classification\nboundaries for different surgical phases.We establish effectiveness of\nMeta-SurDiff in online surgical phase recognition through extensive experiments\non five widely used datasets using more than four practical metrics. The\ndatasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where\nOphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while\nthe others come from laparoscopic surgeries. We will release the code upon\nacceptance.\n","authors":["Yufei Li","Jirui Wu","Long Tian","Liming Wang","Xiaonan Liu","Zijun Liu","Xiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14181v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.07871v2","updated":"2025-06-17T04:46:02Z","published":"2024-06-12T04:55:14Z","title":"Controllable Dance Generation with Style-Guided Motion Diffusion","summary":"  Dance plays an important role as an artistic form and expression in human\nculture, yet the creation of dance remains a challenging task. Most dance\ngeneration methods primarily rely solely on music, seldom taking into\nconsideration intrinsic attributes such as music style or genre. In this work,\nwe introduce Flexible Dance Generation with Style Description Prompts (DGSDP),\na diffusion-based framework suitable for diversified tasks of dance generation\nby fully leveraging the semantics of music style. The core component of this\nframework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a\nTransformer-based network and a music Style Modulation module. The MCSAD seemly\nintegrates music conditions and style description prompts into the dance\ngeneration framework, ensuring that generated dances are consistent with the\nmusic content and style. To facilitate flexible dance generation and\naccommodate different tasks, a spatial-temporal masking strategy is effectively\napplied in the backward diffusion process. The proposed framework successfully\ngenerates realistic dance sequences that are accurately aligned with music for\na variety of tasks such as long-term generation, dance in-betweening, dance\ninpainting, and etc. We hope that this work has the potential to inspire dance\ngeneration and creation, with promising applications in entertainment, art, and\neducation. Code is available on Github: https://github.com/mucunzhuzhu/DGSDP.\n","authors":["Hongsong Wang","Ying Zhu","Yang Zhang","Junbo Wang","Xin Geng","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11099v2","updated":"2025-06-17T04:40:21Z","published":"2025-05-16T10:30:20Z","title":"HyMamba: Mamba with Hybrid Geometry-Feature Coupling for Efficient Point\n  Cloud Classification","summary":"  Point cloud classification is one of the essential technologies for achieving\nintelligent perception of 3D environments by machines, its core challenge is to\nefficiently extract local and global features. Mamba leverages state space\nmodels (SSMs) for global point cloud modeling. Although prior Mamba-based point\ncloud processing methods pay attention to the limitation of its flattened\nsequence modeling mechanism in fusing local and global features, the critical\nissue of weakened local geometric relevance caused by decoupling geometric\nstructures and features in the input patches remains not fully revealed, and\nboth jointly limit local feature extraction. Therefore, we propose HyMamba, a\ngeometry and feature coupled Mamba framework featuring: (1) Geometry-Feature\nCoupled Pooling (GFCP), which achieves physically interpretable geometric\ninformation coupling by dynamically aggregating adjacent geometric information\ninto local features; (2) Collaborative Feature Enhancer (CoFE), which enhances\nsparse signal capture through cross-path feature hybridization while\neffectively integrating global and local contexts. We conducted extensive\nexperiments on ModelNet40 and ScanObjectNN datasets. The results demonstrate\nthat the proposed model achieves superior classification performance,\nparticularly on the ModelNet40, where it elevates accuracy to 95.99% with\nmerely 0.03M additional parameters. Furthermore, it attains 98.9% accuracy on\nthe ModelNetFewShot dataset, validating its robust generalization capabilities\nunder sparse samples. Our code and weights are available at\nhttps://github.com/L1277471578/HyMamba\n","authors":["Bin Liu","Chunyang Wang","Xuelian Liu","Bo Xiao","Guan Xi"],"pdf_url":"https://arxiv.org/pdf/2505.11099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14176v1","updated":"2025-06-17T04:37:02Z","published":"2025-06-17T04:37:02Z","title":"One-Shot Neural Architecture Search with Network Similarity Directed\n  Initialization for Pathological Image Classification","summary":"  Deep learning-based pathological image analysis presents unique challenges\ndue to the practical constraints of network design. Most existing methods apply\ncomputer vision models directly to medical tasks, neglecting the distinct\ncharacteristics of pathological images. This mismatch often leads to\ncomputational inefficiencies, particularly in edge-computing scenarios. To\naddress this, we propose a novel Network Similarity Directed Initialization\n(NSDI) strategy to improve the stability of neural architecture search (NAS).\nFurthermore, we introduce domain adaptation into one-shot NAS to better handle\nvariations in staining and semantic scale across pathology datasets.\nExperiments on the BRACS dataset demonstrate that our method outperforms\nexisting approaches, delivering both superior classification performance and\nclinically relevant feature localization.\n","authors":["Renao Yan"],"pdf_url":"https://arxiv.org/pdf/2506.14176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14170v1","updated":"2025-06-17T04:09:43Z","published":"2025-06-17T04:09:43Z","title":"A multi-stage augmented multimodal interaction network for fish feeding\n  intensity quantification","summary":"  In recirculating aquaculture systems, accurate and effective assessment of\nfish feeding intensity is crucial for reducing feed costs and calculating\noptimal feeding times. However, current studies have limitations in modality\nselection, feature extraction and fusion, and co-inference for decision making,\nwhich restrict further improvement in the accuracy, applicability and\nreliability of multimodal fusion models. To address this problem, this study\nproposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for\nquantifying fish feeding intensity. Firstly, a general feature extraction\nframework is proposed to efficiently extract feature information from input\nimage, audio and water wave datas. Second, an Auxiliary-modality Reinforcement\nPrimary-modality Mechanism (ARPM) is designed for inter-modal interaction and\ngenerate enhanced features, which consists of a Channel Attention Fusion\nNetwork (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an\nEvidence Reasoning (ER) rule is introduced to fuse the output results of each\nmodality and make decisions, thereby completing the quantification of fish\nfeeding intensity. The experimental results show that the constructed MAINet\nreaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and\nF1-Score respectively, and its performance is significantly higher than the\ncomparison models. Compared with models that adopt single-modality,\ndual-modality fusion and different decision-making fusion methods, it also has\nobvious advantages. Meanwhile, the ablation experiments further verified the\nkey role of the proposed improvement strategy in improving the robustness and\nfeature utilization efficiency of model, which can effectively improve the\naccuracy of the quantitative results of fish feeding intensity.\n","authors":["Shulong Zhang","Mingyuan Yao","Jiayin Zhao","Xiao Liu","Haihua Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14168v1","updated":"2025-06-17T04:08:18Z","published":"2025-06-17T04:08:18Z","title":"VideoMAR: Autoregressive Video Generatio with Continuous Tokens","summary":"  Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).\n","authors":["Hu Yu","Biao Gong","Hangjie Yuan","DanDan Zheng","Weilong Chai","Jingdong Chen","Kecheng Zheng","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.14168v1.pdf","comment":"Submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2506.13657v2","updated":"2025-06-17T04:05:44Z","published":"2025-06-16T16:18:21Z","title":"Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual\n  Object Detection in Educational Videos","summary":"  We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain.\n","authors":["Dipayan Biswas","Shishir Shah","Jaspal Subhlok"],"pdf_url":"https://arxiv.org/pdf/2506.13657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00555v2","updated":"2025-06-17T03:59:45Z","published":"2025-05-31T13:22:55Z","title":"MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal\n  Medical Reasoning","summary":"  Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 20.7% over supervised\nfine-tuning baselines.\n","authors":["Peng Xia","Jinglu Wang","Yibo Peng","Kaide Zeng","Xian Wu","Xiangru Tang","Hongtu Zhu","Yun Li","Shujie Liu","Yan Lu","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2506.00555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14704v5","updated":"2025-06-17T03:46:08Z","published":"2023-09-26T06:56:01Z","title":"Tile Classification Based Viewport Prediction with Multi-modal Fusion\n  Transformer","summary":"  Viewport prediction is a crucial aspect of tile-based 360 video streaming\nsystem. However, existing trajectory based methods lack of robustness, also\noversimplify the process of information construction and fusion between\ndifferent modality inputs, leading to the error accumulation problem. In this\npaper, we propose a tile classification based viewport prediction method with\nMulti-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes\ntransformer-based networks to extract the long-range dependencies within each\nmodality, then mine intra- and inter-modality relations to capture the combined\nimpact of user historical inputs and video contents on future viewport\nselection. In addition, MFTR categorizes future tiles into two categories: user\ninterested or not, and selects future viewport as the region that contains most\nuser interested tiles. Comparing with predicting head trajectories, choosing\nfuture viewport based on tile's binary classification results exhibits better\nrobustness and interpretability. To evaluate our proposed MFTR, we conduct\nextensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows\nsuperior performance over state-of-the-art methods in terms of average\nprediction accuracy and overlap ratio, also presents competitive computation\nefficiency.\n","authors":["Zhihao Zhang","Yiwei Chen","Weizhan Zhang","Caixia Yan","Qinghua Zheng","Qi Wang","Wangdu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.14704v5.pdf","comment":"This paper is accepted by ACM-MM 2023"},{"id":"http://arxiv.org/abs/2506.09172v2","updated":"2025-06-17T03:30:48Z","published":"2025-06-10T18:38:19Z","title":"An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and\n  Adaptation of Multimodal Action Models","summary":"  Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.\n","authors":["Pranav Guruprasad","Yangyue Wang","Sudipta Chowdhury","Jaewoo Song","Harshvardhan Sikka"],"pdf_url":"https://arxiv.org/pdf/2506.09172v2.pdf","comment":"ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2505.05540v2","updated":"2025-06-17T03:16:46Z","published":"2025-05-08T16:51:36Z","title":"Benchmarking Vision, Language, & Action Models in Procedurally\n  Generated, Open Ended Action Environments","summary":"  Vision-language-action (VLA) models represent an important step toward\ngeneral-purpose robotic systems by integrating visual perception, language\nunderstanding, and action execution. However, systematic evaluation of these\nmodels, particularly their zero-shot generalization capabilities in\nprocedurally out-of-distribution (OOD) environments, remains limited. In this\npaper, we introduce MultiNet v0.2, a comprehensive benchmark designed to\nevaluate and analyze the generalization performance of state-of-the-art VLMs\nand VLAs - including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST - on\ndiverse procedural tasks from the Procgen benchmark. Our analysis reveals\nseveral critical insights: (1) all evaluated models exhibit significant\nlimitations in zero-shot generalization to OOD tasks, with performance heavily\ninfluenced by factors such as action representation and task complexity; (2)\nVLAs generally outperforms other models due to their robust architectural\ndesign; and (3) VLM variants demonstrate substantial improvements when\nconstrained appropriately, highlighting the sensitivity of model performance to\nprecise prompt engineering. We release our benchmark, evaluation framework, and\nfindings to enable the assessment of future VLA models and identify critical\nareas for improvement in their application to out-of-distribution digital\ntasks.\n","authors":["Pranav Guruprasad","Yangyue Wang","Sudipta Chowdhury","Harshvardhan Sikka","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2505.05540v2.pdf","comment":"16 pages, 26 figures"},{"id":"http://arxiv.org/abs/2506.14144v1","updated":"2025-06-17T03:11:31Z","published":"2025-06-17T03:11:31Z","title":"SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with\n  LLM-Guided Walkability","summary":"  Accurate prediction of pedestrian trajectories is essential for applications\nin robotics and surveillance systems. While existing approaches primarily focus\non social interactions between pedestrians, they often overlook the rich\nenvironmental context that significantly shapes human movement patterns. In\nthis paper, we propose SceneAware, a novel framework that explicitly\nincorporates scene understanding to enhance trajectory prediction accuracy. Our\nmethod leverages a Vision Transformer~(ViT) scene encoder to process\nenvironmental context from static scene images, while Multi-modal Large\nLanguage Models~(MLLMs) generate binary walkability masks that distinguish\nbetween accessible and restricted areas during training. We combine a\nTransformer-based trajectory encoder with the ViT-based scene encoder,\ncapturing both temporal dynamics and spatial constraints. The framework\nintegrates collision penalty mechanisms that discourage predicted trajectories\nfrom violating physical boundaries, ensuring physically plausible predictions.\nSceneAware is implemented in both deterministic and stochastic variants.\nComprehensive experiments on the ETH/UCY benchmark datasets show that our\napproach outperforms state-of-the-art methods, with more than 50\\% improvement\nover previous models. Our analysis based on different trajectory categories\nshows that the model performs consistently well across various types of\npedestrian movement. This highlights the importance of using explicit scene\ninformation and shows that our scene-aware approach is both effective and\nreliable in generating accurate and physically plausible predictions. Code is\navailable at: https://github.com/juho127/SceneAware.\n","authors":["Juho Bai","Inwook Shim"],"pdf_url":"https://arxiv.org/pdf/2506.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14142v1","updated":"2025-06-17T03:10:33Z","published":"2025-06-17T03:10:33Z","title":"RadFabric: Agentic AI System with Reasoning Capability for Radiology","summary":"  Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic\nconditions, but current automated systems face limitations in pathology\ncoverage, diagnostic accuracy, and integration of visual and textual reasoning.\nTo address these gaps, we propose RadFabric, a multi agent, multimodal\nreasoning framework that unifies visual and textual analysis for comprehensive\nCXR interpretation. RadFabric is built on the Model Context Protocol (MCP),\nenabling modularity, interoperability, and scalability for seamless integration\nof new diagnostic agents. The system employs specialized CXR agents for\npathology detection, an Anatomical Interpretation Agent to map visual findings\nto precise anatomical structures, and a Reasoning Agent powered by large\nmultimodal reasoning models to synthesize visual, anatomical, and clinical data\ninto transparent and evidence based diagnoses. RadFabric achieves significant\nperformance improvements, with near-perfect detection of challenging\npathologies like fractures (1.000 accuracy) and superior overall diagnostic\naccuracy (0.799) compared to traditional systems (0.229 to 0.527). By\nintegrating cross modal feature alignment and preference-driven reasoning,\nRadFabric advances AI-driven radiology toward transparent, anatomically\nprecise, and clinically actionable CXR analysis.\n","authors":["Wenting Chen","Yi Dong","Zhaojun Ding","Yucheng Shi","Yifan Zhou","Fang Zeng","Yijun Luo","Tianyu Lin","Yihang Su","Yichen Wu","Kai Zhang","Zhen Xiang","Tianming Liu","Ninghao Liu","Lichao Sun","Yixuan Yuan","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2506.14142v1.pdf","comment":"4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.13340v2","updated":"2025-06-17T03:01:23Z","published":"2024-11-20T14:12:34Z","title":"WHALES: A Multi-agent Scheduling Dataset for Enhanced Cooperation in\n  Autonomous Driving","summary":"  Cooperative perception research is constrained by the scarcity of datasets\nthat capture the complexity of real-world Vehicle-to-Everything (V2X)\ninteractions, particularly under dynamic communication constraints. To address\nthis, we present WHALES (Wireless enhanced Autonomous vehicles with Large\nnumber of Engaged agents), the first large-scale V2X dataset specifically\ndesigned to benchmark communication-aware agent scheduling and scalable\ncooperative perception. WHALES establishes a new state-of-the-art (SOTA)\nstandard with an average of 8.4 cooperative agents per scene and 2.01 million\nannotated 3D objects spanning diverse traffic scenarios. It integrates\ncommunication metadata to simulate real-world communication bottlenecks,\nenabling rigorous evaluation of scheduling strategies. To further advance the\nfield, we propose the Coverage-Aware Historical Scheduler (CAHS), a novel\nscheduling baseline that prioritizes agents based on historical viewpoint\ncoverage, improving perception performance over existing SOTA methods. WHALES\nbridges the gap between simulated and real-world V2X challenges, offering a\nrobust framework to explore perception-scheduling co-design, cross-data\ngeneralization, and scalability limits. The WHALES dataset and code are\navailable at: https://github.com/chensiweiTHU/WHALES.\n","authors":["Richard Wang","Siwei Chen","Ziyi Song","Sheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.13340v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14136v1","updated":"2025-06-17T02:59:42Z","published":"2025-06-17T02:59:42Z","title":"Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An\n  Insight into BiomedCLIP on Radiology","summary":"  In this paper, we construct two research objectives: i) explore the learned\nembedding space of BiomedCLIP, an open-source large vision language model, to\nanalyse meaningful class separations, and ii) quantify the limitations of\nBiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label\nmedical dataset. We experiment on IU-xray dataset, which exhibits the\naforementioned criteria, and evaluate BiomedCLIP in classifying images\n(radiographs) in three contexts: zero-shot inference, full finetuning, and\nlinear probing. The results show that the model under zero-shot settings\nover-predicts all labels, leading to poor precision and inter-class\nseparability. Full fine-tuning improves classification of distinct diseases,\nwhile linear probing detects overlapping features. We demonstrate visual\nunderstanding of the model using Grad-CAM heatmaps and compare with 15\nannotations by a radiologist. We highlight the need for careful adaptations of\nthe models to foster reliability and applicability in a real-world setting. The\ncode for the experiments in this work is available and maintained on GitHub.\n","authors":["Nafiz Sadman","Farhana Zulkernine","Benjamin Kwan"],"pdf_url":"https://arxiv.org/pdf/2506.14136v1.pdf","comment":"GitHub: https://github.com/Nafiz95/BioVLM_Eval_CXR"},{"id":"http://arxiv.org/abs/2506.01391v2","updated":"2025-06-17T02:57:04Z","published":"2025-06-02T07:30:29Z","title":"AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning","summary":"  The recent progress of large language model agents has opened new\npossibilities for automating tasks through graphical user interfaces (GUIs),\nespecially in mobile environments where intelligent interaction can greatly\nenhance usability. However, practical deployment of such agents remains\nconstrained by several key challenges. Existing training data is often noisy\nand lack semantic diversity, which hinders the learning of precise grounding\nand planning. Models trained purely by imitation tend to overfit to seen\ninterface patterns and fail to generalize in unfamiliar scenarios. Moreover,\nmost prior work focuses on English interfaces while overlooks the growing\ndiversity of non-English applications such as those in the Chinese mobile\necosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent\nbuilt for robust and efficient on-device GUI interaction. Our training pipeline\nincludes grounding-aware pre-training to enhance perception, supervised\nfine-tuning on high-quality Chinese and English trajectories to imitate\nhuman-like actions, and reinforcement fine-tuning with GRPO to improve\nreasoning capability. We also introduce a compact action space that reduces\noutput length and supports low-latency execution on mobile devices.\nAgentCPM-GUI achieves state-of-the-art performance on five public benchmarks\nand a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and\n$91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we\npublicly release all code, model checkpoint, and evaluation data.\n","authors":["Zhong Zhang","Yaxi Lu","Yikun Fu","Yupeng Huo","Shenzhi Yang","Yesai Wu","Han Si","Xin Cong","Haotian Chen","Yankai Lin","Jie Xie","Wei Zhou","Wang Xu","Yuanheng Zhang","Zhou Su","Zhongwu Zhai","Xiaoming Liu","Yudong Mei","Jianming Xu","Hongyan Tian","Chongyi Wang","Chi Chen","Yuan Yao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01391v2.pdf","comment":"Updated results in Table 2 and Table 3; The project is available at\n  https://github.com/OpenBMB/AgentCPM-GUI"},{"id":"http://arxiv.org/abs/2506.14135v1","updated":"2025-06-17T02:55:20Z","published":"2025-06-17T02:55:20Z","title":"GAF: Gaussian Action Field as a Dvnamic World Model for Robotic\n  Mlanipulation","summary":"  Accurate action inference is critical for vision-based robotic manipulation.\nExisting approaches typically follow either a Vision-to-Action (V-A) paradigm,\npredicting actions directly from visual inputs, or a Vision-to-3D-to-Action\n(V-3D-A) paradigm, leveraging intermediate 3D representations. However, these\nmethods often struggle with action inaccuracies due to the complexity and\ndynamic nature of manipulation scenes. In this paper, we propose a V-4D-A\nframework that enables direct action reasoning from motion-aware 4D\nrepresentations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian\nSplatting (3DGS) by incorporating learnable motion attributes, allowing\nsimultaneous modeling of dynamic scenes and manipulation actions. To learn\ntime-varying scene geometry and action-aware robot motion, GAF supports three\nkey query types: reconstruction of the current scene, prediction of future\nframes, and estimation of initial action via robot motion. Furthermore, the\nhigh-quality current and future frames generated by GAF facilitate manipulation\naction refinement through a GAF-guided diffusion model. Extensive experiments\ndemonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and\n-0.5574 LPIPS improvements in reconstruction quality, while boosting the\naverage success rate in robotic manipulation tasks by 10.33% over\nstate-of-the-art methods. Project page:\nhttp://chaiying1.github.io/GAF.github.io/project_page/\n","authors":["Ying Chai","Litao Deng","Ruizhi Shao","Jiajun Zhang","Liangjun Xing","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14135v1.pdf","comment":"http://chaiying1.github.io/GAF.github.io/project_page/"},{"id":"http://arxiv.org/abs/2506.12680v2","updated":"2025-06-17T02:48:47Z","published":"2025-06-15T01:30:22Z","title":"3D Hand Mesh-Guided AI-Generated Malformed Hand Refinement with Hand\n  Pose Transformation via Diffusion Model","summary":"  The malformed hands in the AI-generated images seriously affect the\nauthenticity of the images. To refine malformed hands, existing depth-based\napproaches use a hand depth estimator to guide the refinement of malformed\nhands. Due to the performance limitations of the hand depth estimator, many\nhand details cannot be represented, resulting in errors in the generated hands,\nsuch as confusing the palm and the back of the hand. To solve this problem, we\npropose a 3D mesh-guided refinement framework using a diffusion pipeline. We\nuse a state-of-the-art 3D hand mesh estimator, which provides more details of\nthe hands. For training, we collect and reannotate a dataset consisting of RGB\nimages and 3D hand mesh. Then we design a diffusion inpainting model to\ngenerate refined outputs guided by 3D hand meshes. For inference, we propose a\ndouble check algorithm to facilitate the 3D hand mesh estimator to obtain\nrobust hand mesh guidance to obtain our refined results. Beyond malformed hand\nrefinement, we propose a novel hand pose transformation method. It increases\nthe flexibility and diversity of the malformed hand refinement task. We made\nthe restored images mimic the hand poses of the reference images. The pose\ntransformation requires no additional training. Extensive experimental results\ndemonstrate the superior performance of our proposed method.\n","authors":["Chen-Bin Feng","Kangdao Liu","Jian Sun","Jiping Jin","Yiguo Jiang","Chi-Man Vong"],"pdf_url":"https://arxiv.org/pdf/2506.12680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14130v1","updated":"2025-06-17T02:47:49Z","published":"2025-06-17T02:47:49Z","title":"KDMOS:Knowledge Distillation for Motion Segmentation","summary":"  Motion Object Segmentation (MOS) is crucial for autonomous driving, as it\nenhances localization, path planning, map construction, scene flow estimation,\nand future state prediction. While existing methods achieve strong performance,\nbalancing accuracy and real-time inference remains a challenge. To address\nthis, we propose a logits-based knowledge distillation framework for MOS,\naiming to improve accuracy while maintaining real-time efficiency.\nSpecifically, we adopt a Bird's Eye View (BEV) projection-based model as the\nstudent and a non-projection model as the teacher. To handle the severe\nimbalance between moving and non-moving classes, we decouple them and apply\ntailored distillation strategies, allowing the teacher model to better learn\nkey motion-related features. This approach significantly reduces false\npositives and false negatives. Additionally, we introduce dynamic upsampling,\noptimize the network architecture, and achieve a 7.69% reduction in parameter\ncount, mitigating overfitting. Our method achieves a notable IoU of 78.8% on\nthe hidden test set of the SemanticKITTI-MOS dataset and delivers competitive\nresults on the Apollo dataset. The KDMOS implementation is available at\nhttps://github.com/SCNU-RISLAB/KDMOS.\n","authors":["Chunyu Cao","Jintao Cheng","Zeyu Chen","Linfan Zhan","Rui Fan","Zhijian He","Xiaoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2506.14130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14121v1","updated":"2025-06-17T02:33:42Z","published":"2025-06-17T02:33:42Z","title":"FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution","summary":"  Face super-resolution (FSR) under limited computational costs remains an open\nproblem. Existing approaches typically treat all facial pixels equally,\nresulting in suboptimal allocation of computational resources and degraded FSR\nperformance. CNN is relatively sensitive to high-frequency facial features,\nsuch as component contours and facial outlines. Meanwhile, Mamba excels at\ncapturing low-frequency features like facial color and fine-grained texture,\nand does so with lower complexity than Transformers. Motivated by these\nobservations, we propose FADPNet, a Frequency-Aware Dual-Path Network that\ndecomposes facial features into low- and high-frequency components and\nprocesses them via dedicated branches. For low-frequency regions, we introduce\na Mamba-based Low-Frequency Enhancement Block (LFEB), which combines\nstate-space attention with squeeze-and-excitation operations to extract\nlow-frequency global interactions and emphasize informative channels. For\nhigh-frequency regions, we design a CNN-based Deep Position-Aware Attention\n(DPA) module to enhance spatially-dependent structural details, complemented by\na lightweight High-Frequency Refinement (HFR) module that further refines\nfrequency-specific representations. Through the above designs, our method\nachieves an excellent balance between FSR quality and model efficiency,\noutperforming existing approaches.\n","authors":["Siyu Xu","Wenjie Li","Guangwei Gao","Jian Yang","Guo-Jun Qi","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2506.14121v1.pdf","comment":"12 pages, 11 figures, 6 tales"},{"id":"http://arxiv.org/abs/2502.10794v2","updated":"2025-06-17T02:28:34Z","published":"2025-02-15T13:25:12Z","title":"Distraction is All You Need for Multimodal Large Language Model\n  Jailbreaking","summary":"  Multimodal Large Language Models (MLLMs) bridge the gap between visual and\ntextual data, enabling a range of advanced applications. However, complex\ninternal interactions among visual elements and their alignment with text can\nintroduce vulnerabilities, which may be exploited to bypass safety mechanisms.\nTo address this, we analyze the relationship between image content and task and\nfind that the complexity of subimages, rather than their content, is key.\nBuilding on this insight, we propose the Distraction Hypothesis, followed by a\nnovel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ),\nto achieve jailbreaking by disrupting MLLMs alignment through multi-level\ndistraction strategies. CS-DJ consists of two components: structured\ndistraction, achieved through query decomposition that induces a distributional\nshift by fragmenting harmful prompts into sub-queries, and visual-enhanced\ndistraction, realized by constructing contrasting subimages to disrupt the\ninteractions among visual elements within the model. This dual strategy\ndisperses the model's attention, reducing its ability to detect and mitigate\nharmful content. Extensive experiments across five representative scenarios and\nfour popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and\nGemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of\n52.40% for the attack success rate and 74.10% for the ensemble attack success\nrate. These results reveal the potential of distraction-based approaches to\nexploit and bypass MLLMs' defenses, offering new insights for attack\nstrategies.\n","authors":["Zuopeng Yang","Jiluan Fan","Anli Yan","Erdun Gao","Xin Lin","Tao Li","Kanghua Mo","Changyu Dong"],"pdf_url":"https://arxiv.org/pdf/2502.10794v2.pdf","comment":"CVPR 2025 highlight"},{"id":"http://arxiv.org/abs/2506.13754v2","updated":"2025-06-17T02:15:17Z","published":"2025-06-16T17:58:00Z","title":"VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion\n  Models","summary":"  We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines.\n","authors":["Edward Li","Zichen Wang","Jiahe Huang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2506.13754v2.pdf","comment":"Project page: https://videopde.github.io/"},{"id":"http://arxiv.org/abs/2506.12727v2","updated":"2025-06-17T02:12:36Z","published":"2025-06-15T05:40:50Z","title":"Efficient multi-view training for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside\nNeural Radiance Fields (NeRF) in inverse rendering due to its superior\nrendering speed. Currently, the common approach in 3DGS is to utilize\n\"single-view\" mini-batch training, where only one image is processed per\niteration, in contrast to NeRF's \"multi-view\" mini-batch training, which\nleverages multiple images. We observe that such single-view training can lead\nto suboptimal optimization due to increased variance in mini-batch stochastic\ngradients, highlighting the necessity for multi-view training. However,\nimplementing multi-view training in 3DGS poses challenges. Simply rendering\nmultiple images per iteration incurs considerable overhead and may result in\nsuboptimal Gaussian densification due to its reliance on single-view\nassumptions. To address these issues, we modify the rasterization process to\nminimize the overhead associated with multi-view training and propose a 3D\ndistance-aware D-SSIM loss and multi-view adaptive density control that better\nsuits multi-view scenarios. Our experiments demonstrate that the proposed\nmethods significantly enhance the performance of 3DGS and its variants, freeing\n3DGS from the constraints of single-view training.\n","authors":["Minhyuk Choi","Injae Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2506.12727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14107v1","updated":"2025-06-17T01:59:10Z","published":"2025-06-17T01:59:10Z","title":"Déjà Vu: Efficient Video-Language Query Engine with Learning-based\n  Inter-Frame Computation Reuse","summary":"  Recently, Video-Language Models (VideoLMs) have demonstrated remarkable\ncapabilities, offering significant potential for flexible and powerful video\nquery systems. These models typically rely on Vision Transformers (ViTs), which\nprocess video frames individually to extract visual embeddings. However,\ngenerating embeddings for large-scale videos requires ViT inferencing across\nnumerous frames, posing a major hurdle to real-world deployment and\nnecessitating solutions for integration into scalable video data management\nsystems. This paper introduces D\\'ej\\`a Vu, a video-language query engine that\naccelerates ViT-based VideoLMs by reusing computations across consecutive\nframes. At its core is ReuseViT, a modified ViT model specifically designed for\nVideoLM tasks, which learns to detect inter-frame reuse opportunities, striking\nan effective balance between accuracy and reuse. Although ReuseViT\nsignificantly reduces computation, these savings do not directly translate into\nperformance gains on GPUs. To overcome this, D\\'ej\\`a Vu integrates\nmemory-compute joint compaction techniques that convert the FLOP savings into\ntangible performance gains. Evaluations on three VideoLM tasks show that\nD\\'ej\\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error\nbound, dramatically enhancing the practicality of VideoLMs for large-scale\nvideo analytics.\n","authors":["Jinwoo Hwang","Daeun Kim","Sangyeop Lee","Yoonsung Kim","Guseul Heo","Hojoon Kim","Yunseok Jeong","Tadiwos Meaza","Eunhyeok Park","Jeongseob Ahn","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2506.14107v1.pdf","comment":"Accepted to 2025 VLDB"},{"id":"http://arxiv.org/abs/2505.24120v2","updated":"2025-06-17T01:56:01Z","published":"2025-05-30T01:34:25Z","title":"CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs","summary":"  Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremain inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering. Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning. We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6%\naccuracy. This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA\n","authors":["Ai Jian","Weijie Qiu","Xiaokun Wang","Peiyu Wang","Yunzhuo Hao","Jiangbo Pei","Yichen Wei","Yi Peng","Xuchen Song"],"pdf_url":"https://arxiv.org/pdf/2505.24120v2.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2506.14096v1","updated":"2025-06-17T01:20:50Z","published":"2025-06-17T01:20:50Z","title":"Image Segmentation with Large Language Models: A Survey with\n  Perspectives for Intelligent Transportation Systems","summary":"  The integration of Large Language Models (LLMs) with computer vision is\nprofoundly transforming perception tasks like image segmentation. For\nintelligent transportation systems (ITS), where accurate scene understanding is\ncritical for safety and efficiency, this new paradigm offers unprecedented\ncapabilities. This survey systematically reviews the emerging field of\nLLM-augmented image segmentation, focusing on its applications, challenges, and\nfuture directions within ITS. We provide a taxonomy of current approaches based\non their prompting mechanisms and core architectures, and we highlight how\nthese innovations can enhance road scene understanding for autonomous driving,\ntraffic monitoring, and infrastructure maintenance. Finally, we identify key\nchallenges, including real-time performance and safety-critical reliability,\nand outline a perspective centered on explainable, human-centric AI as a\nprerequisite for the successful deployment of this technology in\nnext-generation transportation systems.\n","authors":["Sanjeda Akter","Ibne Farabi Shihab","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2506.14096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11241v2","updated":"2025-06-17T23:39:16Z","published":"2024-08-20T23:39:26Z","title":"CooPre: Cooperative Pretraining for V2X Cooperative Perception","summary":"  Existing Vehicle-to-Everything (V2X) cooperative perception methods rely on\naccurate multi-agent 3D annotations. Nevertheless, it is time-consuming and\nexpensive to collect and annotate real-world data, especially for V2X systems.\nIn this paper, we present a self-supervised learning framwork for V2X\ncooperative perception, which utilizes the vast amount of unlabeled 3D V2X data\nto enhance the perception performance. Specifically, multi-agent sensing\ninformation is aggregated to form a holistic view and a novel proxy task is\nformulated to reconstruct the LiDAR point clouds across multiple connected\nagents to better reason multi-agent spatial correlations. Besides, we develop a\nV2X bird-eye-view (BEV) guided masking strategy which effectively allows the\nmodel to pay attention to 3D features across heterogeneous V2X agents (i.e.,\nvehicles and infrastructure) in the BEV space. Noticeably, such a masking\nstrategy effectively pretrains the 3D encoder with a multi-agent LiDAR point\ncloud reconstruction objective and is compatible with mainstream cooperative\nperception backbones. Our approach, validated through extensive experiments on\nrepresentative datasets (i.e., V2X-Real, V2V4Real, and OPV2V) and multiple\nstate-of-the-art cooperative perception methods (i.e., AttFuse, F-Cooper, and\nV2X-ViT), leads to a performance boost across all V2X settings. Notably, CooPre\nachieves a 4% mAP improvement on V2X-Real dataset and surpasses baseline\nperformance using only 50% of the training data, highlighting its data\nefficiency. Additionally, we demonstrate the framework's powerful performance\nin cross-domain transferability and robustness under challenging scenarios. The\ncode will be made publicly available at\nhttps://github.com/ucla-mobility/CooPre.\n","authors":["Seth Z. Zhao","Hao Xiang","Chenfeng Xu","Xin Xia","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2408.11241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08049v3","updated":"2025-06-17T23:09:37Z","published":"2025-04-10T18:08:16Z","title":"Patch distribution modeling framework adaptive cosine estimator\n  (PaDiM-ACE) for anomaly detection and localization in synthetic aperture\n  radar imagery","summary":"  This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.\n","authors":["Angelina Ibarra","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2504.08049v3.pdf","comment":"Accepted to SPIE, Defense and Commercial Sensing, Algorithms for\n  Synthetic Aperture Radar Imagery XXXII (April 2025)"},{"id":"http://arxiv.org/abs/2405.19569v3","updated":"2025-06-17T22:49:59Z","published":"2024-05-29T23:24:48Z","title":"Improved Convex Decomposition with Ensembling and Boolean Primitives","summary":"  Describing a scene in terms of primitives -- geometrically simple shapes that\noffer a parsimonious but accurate abstraction of structure -- is an established\nand difficult fitting problem. Different scenes require different numbers of\nprimitives, and these primitives interact strongly. Existing methods are\nevaluated by predicting depth, normals and segmentation from the primitives,\nthen evaluating the accuracy of those predictions. The state of the art method\ninvolves a learned regression procedure to predict a start point consisting of\na fixed number of primitives, followed by a descent method to refine the\ngeometry and remove redundant primitives.\n  CSG (Constructive Solid Geometry) representations are significantly enhanced\nby a set-differencing operation. Our representation incorporates negative\nprimitives, which are differenced from the positive primitives. These notably\nenrich the geometry that the model can encode, while complicating the fitting\nproblem. This paper demonstrates a method that can (a) incorporate these\nnegative primitives and (b) choose the overall number of positive and negative\nprimitives by ensembling. Extensive experiments on the standard NYUv2 dataset\nconfirm that (a) this approach results in substantial improvements in depth\nrepresentation and segmentation over SOTA and (b) negative primitives make a\nnotable contribution to accuracy. Our method is robustly applicable across\ndatasets: in a first, we evaluate primitive prediction for LAION images.\n","authors":["Vaibhav Vavilala","Florian Kluger","Seemandhar Jain","Bodo Rosenhahn","Anand Bhattad","David Forsyth"],"pdf_url":"https://arxiv.org/pdf/2405.19569v3.pdf","comment":"25 pages, 16 figures, 9 tables"},{"id":"http://arxiv.org/abs/2506.15010v1","updated":"2025-06-17T22:41:10Z","published":"2025-06-17T22:41:10Z","title":"Hyper-Local Deformable Transformers for Text Spotting on Historical Maps","summary":"  Text on historical maps contains valuable information providing georeferenced\nhistorical, political, and cultural contexts. However, text extraction from\nhistorical maps is challenging due to the lack of (1) effective methods and (2)\ntraining data. Previous approaches use ad-hoc steps tailored to only specific\nmap styles. Recent machine learning-based text spotters (e.g., for scene\nimages) have the potential to solve these challenges because of their\nflexibility in supporting various types of text instances. However, these\nmethods remain challenges in extracting precise image features for predicting\nevery sub-component (boundary points and characters) in a text instance. This\nis critical because map text can be lengthy and highly rotated with complex\nbackgrounds, posing difficulties in detecting relevant image features from a\nrough text region. This paper proposes PALETTE, an end-to-end text spotter for\nscanned historical maps of a wide variety. PALETTE introduces a novel\nhyper-local sampling module to explicitly learn localized image features around\nthe target boundary points and characters of a text instance for detection and\nrecognition. PALETTE also enables hyper-local positional embeddings to learn\nspatial interactions between boundary points and characters within and across\ntext instances. In addition, this paper presents a novel approach to\nautomatically generate synthetic map images, SynthMap+, for training text\nspotters for historical maps. The experiment shows that PALETTE with SynthMap+\noutperforms SOTA text spotters on two new benchmark datasets of historical\nmaps, particularly for long and angled text. We have deployed PALETTE with\nSynthMap+ to process over 60,000 maps in the David Rumsey Historical Map\ncollection and generated over 100 million text labels to support map searching.\nThe project is released at\nhttps://github.com/kartta-foundation/mapkurator-palette-doc.\n","authors":["Yijun Lin","Yao-Yi Chiang"],"pdf_url":"https://arxiv.org/pdf/2506.15010v1.pdf","comment":"Published in KDD2024"},{"id":"http://arxiv.org/abs/2505.18787v2","updated":"2025-06-17T22:31:49Z","published":"2025-05-24T16:58:53Z","title":"Think Twice before Adaptation: Improving Adaptability of DeepFake\n  Detection via Online Test-Time Adaptation","summary":"  Deepfake (DF) detectors face significant challenges when deployed in\nreal-world environments, particularly when encountering test samples deviated\nfrom training data through either postprocessing manipulations or distribution\nshifts. We demonstrate postprocessing techniques can completely obscure\ngeneration artifacts presented in DF samples, leading to performance\ndegradation of DF detectors. To address these challenges, we propose Think\nTwice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation\nmethod that enhances the adaptability of detectors during inference without\nrequiring access to source training data or labels. Our key idea is to enable\nthe model to explore alternative options through an Uncertainty-aware Negative\nLearning objective rather than solely relying on its initial predictions as\ncommonly seen in entropy minimization (EM)-based approaches. We also introduce\nan Uncertain Sample Prioritization strategy and Gradients Masking technique to\nimprove the adaptation by focusing on important samples and model parameters.\nOur theoretical analysis demonstrates that the proposed negative learning\nobjective exhibits complementary behavior to EM, facilitating better adaptation\ncapability. Empirically, our method achieves state-of-the-art results compared\nto existing test-time adaptation (TTA) approaches and significantly enhances\nthe resilience and generalization of DF detectors during inference. Code is\navailable\n\\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.\n","authors":["Hong-Hanh Nguyen-Le","Van-Tuan Tran","Dinh-Thuc Nguyen","Nhien-An Le-Khac"],"pdf_url":"https://arxiv.org/pdf/2505.18787v2.pdf","comment":"Accepted at 34th International Joint Conference on Artificial\n  Intelligence (IJCAI-25)"},{"id":"http://arxiv.org/abs/2506.14980v1","updated":"2025-06-17T21:10:05Z","published":"2025-06-17T21:10:05Z","title":"Advances in Compliance Detection: Novel Models Using Vision-Based\n  Tactile Sensors","summary":"  Compliance is a critical parameter for describing objects in engineering,\nagriculture, and biomedical applications. Traditional compliance detection\nmethods are limited by their lack of portability and scalability, rely on\nspecialized, often expensive equipment, and are unsuitable for robotic\napplications. Moreover, existing neural network-based approaches using\nvision-based tactile sensors still suffer from insufficient prediction\naccuracy. In this paper, we propose two models based on Long-term Recurrent\nConvolutional Networks (LRCNs) and Transformer architectures that leverage RGB\ntactile images and other information captured by the vision-based sensor\nGelSight to predict compliance metrics accurately. We validate the performance\nof these models using multiple metrics and demonstrate their effectiveness in\naccurately estimating compliance. The proposed models exhibit significant\nperformance improvement over the baseline. Additionally, we investigated the\ncorrelation between sensor compliance and object compliance estimation, which\nrevealed that objects that are harder than the sensor are more challenging to\nestimate.\n","authors":["Ziteng Li","Malte Kuhlmann","Ilana Nisky","Nicolás Navarro-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2506.14980v1.pdf","comment":"Accepted in the IEEE International Conference on Development and\n  Learning (ICDL). The paper contains 8 pages and 7 figures"},{"id":"http://arxiv.org/abs/2506.14970v1","updated":"2025-06-17T20:40:06Z","published":"2025-06-17T20:40:06Z","title":"NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for\n  Multi-Modal Neurological Disorder Classification","summary":"  The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.\n","authors":["Wajih Hassan Raza","Aamir Bader Shah","Yu Wen","Yidan Shen","Juan Diego Martinez Lemus","Mya Caryn Schiess","Timothy Michael Ellmore","Renjie Hu","Xin Fu"],"pdf_url":"https://arxiv.org/pdf/2506.14970v1.pdf","comment":"Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society"},{"id":"http://arxiv.org/abs/2506.13496v2","updated":"2025-06-17T19:39:04Z","published":"2025-06-16T13:53:02Z","title":"Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval","summary":"  Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware.\n","authors":["Kshitij Kavimandan","Angelos Nalmpantis","Emma Beauxis-Aussalet","Robert-Jan Sips"],"pdf_url":"https://arxiv.org/pdf/2506.13496v2.pdf","comment":"5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025"},{"id":"http://arxiv.org/abs/2506.14934v1","updated":"2025-06-17T19:32:04Z","published":"2025-06-17T19:32:04Z","title":"Vision Transformers for End-to-End Quark-Gluon Jet Classification from\n  Calorimeter Images","summary":"  Distinguishing between quark- and gluon-initiated jets is a critical and\nchallenging task in high-energy physics, pivotal for improving new physics\nsearches and precision measurements at the Large Hadron Collider. While deep\nlearning, particularly Convolutional Neural Networks (CNNs), has advanced jet\ntagging using image-based representations, the potential of Vision Transformer\n(ViT) architectures, renowned for modeling global contextual information,\nremains largely underexplored for direct calorimeter image analysis, especially\nunder realistic detector and pileup conditions. This paper presents a\nsystematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet\nclassification using simulated 2012 CMS Open Data. We construct multi-channel\njet-view images from detector-level energy deposits (ECAL, HCAL) and\nreconstructed tracks, enabling an end-to-end learning approach. Our\ncomprehensive benchmarking demonstrates that ViT-based models, notably\nViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN\nbaselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of\ncapturing long-range spatial correlations within jet substructure. This work\nestablishes the first systematic framework and robust performance baselines for\napplying ViT architectures to calorimeter image-based jet classification using\npublic collider data, alongside a structured dataset suitable for further deep\nlearning research in this domain.\n","authors":["Md Abrar Jahin","Shahriar Soudeep","Arian Rahman Aditta","M. F. Mridha","Nafiz Fahad","Md. Jakir Hossen"],"pdf_url":"https://arxiv.org/pdf/2506.14934v1.pdf","comment":"Accepted in Third International Workshop on Generalizing from Limited\n  Resources in the Open World Workshop at International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2506.14919v1","updated":"2025-06-17T18:59:43Z","published":"2025-06-17T18:59:43Z","title":"Frequency-Calibrated Membership Inference Attacks on Medical Image\n  Diffusion Models","summary":"  The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods.\n","authors":["Xinkai Zhao","Yuta Tokuoka","Junichiro Iwasawa","Keita Oda"],"pdf_url":"https://arxiv.org/pdf/2506.14919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05750v2","updated":"2025-06-17T18:57:50Z","published":"2025-01-10T06:58:14Z","title":"Semantic Mapping in Indoor Embodied AI -- A Survey on Advances,\n  Challenges, and Future Directions","summary":"  Intelligent embodied agents (e.g. robots) need to perform complex semantic\ntasks in unfamiliar environments. Among many skills that the agents need to\npossess, building and maintaining a semantic map of the environment is most\ncrucial in long-horizon tasks. A semantic map captures information about the\nenvironment in a structured way, allowing the agent to reference it for\nadvanced reasoning throughout the task. While existing surveys in embodied AI\nfocus on general advancements or specific tasks like navigation and\nmanipulation, this paper provides a comprehensive review of semantic\nmap-building approaches in embodied AI, specifically for indoor navigation. We\ncategorize these approaches based on their structural representation (spatial\ngrids, topological graphs, dense point-clouds or hybrid maps) and the type of\ninformation they encode (implicit features or explicit environmental data). We\nalso explore the strengths and limitations of the map building techniques,\nhighlight current challenges, and propose future research directions. We\nidentify that the field is moving towards developing open-vocabulary,\nqueryable, task-agnostic map representations, while high memory demands and\ncomputational inefficiency still remaining to be open challenges. This survey\naims to guide current and future researchers in advancing semantic mapping\ntechniques for embodied AI systems.\n","authors":["Sonia Raychaudhuri","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2501.05750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14914v1","updated":"2025-06-17T18:47:27Z","published":"2025-06-17T18:47:27Z","title":"Recursive Variational Autoencoders for 3D Blood Vessel Generative\n  Modeling","summary":"  Anatomical trees play an important role in clinical diagnosis and treatment\nplanning. Yet, accurately representing these structures poses significant\nchallenges owing to their intricate and varied topology and geometry. Most\nexisting methods to synthesize vasculature are rule based, and despite\nproviding some degree of control and variation in the structures produced, they\nfail to capture the diversity and complexity of actual anatomical data. We\ndeveloped a Recursive variational Neural Network (RvNN) that fully exploits the\nhierarchical organization of the vessel and learns a low-dimensional manifold\nencoding branch connectivity along with geometry features describing the target\nsurface. After training, the RvNN latent space can be sampled to generate new\nvessel geometries. By leveraging the power of generative neural networks, we\ngenerate 3D models of blood vessels that are both accurate and diverse, which\nis crucial for medical and surgical training, hemodynamic simulations, and many\nother purposes. These results closely resemble real data, achieving high\nsimilarity in vessel radii, length, and tortuosity across various datasets,\nincluding those with aneurysms. To the best of our knowledge, this work is the\nfirst to utilize this technique for synthesizing blood vessels.\n","authors":["Paula Feldman","Miguel Fainstein","Viviana Siless","Claudio Delrieux","Emmanuel Iarussi"],"pdf_url":"https://arxiv.org/pdf/2506.14914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14909v1","updated":"2025-06-17T18:28:11Z","published":"2025-06-17T18:28:11Z","title":"Foundation Artificial Intelligence Models for Health Recognition Using\n  Face Photographs (FAHR-Face)","summary":"  Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.\n","authors":["Fridolin Haugg","Grace Lee","John He","Leonard Nürnberg","Dennis Bontempi","Danielle S. Bitterman","Paul Catalano","Vasco Prudente","Dmitrii Glubokov","Andrew Warrington","Suraj Pai","Dirk De Ruysscher","Christian Guthier","Benjamin H. Kann","Vadim N. Gladyshev","Hugo JWL Aerts","Raymond H. Mak"],"pdf_url":"https://arxiv.org/pdf/2506.14909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14907v1","updated":"2025-06-17T18:25:56Z","published":"2025-06-17T18:25:56Z","title":"PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved\n  Vision-Language Reasoning","summary":"  Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.\n","authors":["Yizhen Zhang","Yang Ding","Shuoshuo Zhang","Xinchen Zhang","Haoling Li","Zhong-zhi Li","Peijie Wang","Jie Wu","Lei Ji","Yelong Shen","Yujiu Yang","Yeyun Gong"],"pdf_url":"https://arxiv.org/pdf/2506.14907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14903v1","updated":"2025-06-17T18:17:35Z","published":"2025-06-17T18:17:35Z","title":"DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct\n  Preference Optimization","summary":"  Alignment is crucial for text-to-image (T2I) models to ensure that generated\nimages faithfully capture user intent while maintaining safety and fairness.\nDirect Preference Optimization (DPO), prominent in large language models\n(LLMs), is extending its influence to T2I systems. This paper introduces\nDPO-Kernels for T2I models, a novel extension enhancing alignment across three\ndimensions: (i) Hybrid Loss, integrating embedding-based objectives with\ntraditional probability-based loss for improved optimization; (ii) Kernelized\nRepresentations, employing Radial Basis Function (RBF), Polynomial, and Wavelet\nkernels for richer feature transformations and better separation between safe\nand unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's\ndefault Kullback-Leibler (KL) regularizer by incorporating Wasserstein and\nR'enyi divergences for enhanced stability and robustness. We introduce\nDETONATE, the first large-scale benchmark of its kind, comprising approximately\n100K curated image pairs categorized as chosen and rejected. DETONATE\nencapsulates three axes of social bias and discrimination: Race, Gender, and\nDisability. Prompts are sourced from hate speech datasets, with images\ngenerated by leading T2I models including Stable Diffusion 3.5 Large, Stable\nDiffusion XL, and Midjourney. Additionally, we propose the Alignment Quality\nIndex (AQI), a novel geometric measure quantifying latent-space separability of\nsafe/unsafe image activations, revealing hidden vulnerabilities. Empirically,\nwe demonstrate that DPO-Kernels maintain strong generalization bounds via\nHeavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are\npublicly released.\n","authors":["Renjith Prasad","Abhilekh Borah","Hasnat Md Abdullah","Chathurangi Shyalika","Gurpreet Singh","Ritvik Garimella","Rajarshi Roy","Harshul Surana","Nasrin Imanpour","Suranjana Trivedy","Amit Sheth","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2506.14903v1.pdf","comment":"59 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.10145v2","updated":"2025-06-17T18:03:19Z","published":"2025-06-11T19:50:23Z","title":"RoCA: Robust Cross-Domain End-to-End Autonomous Driving","summary":"  End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance.\n","authors":["Rajeev Yasarla","Shizhong Han","Hsin-Pai Cheng","Litian Liu","Shweta Mahajan","Apratim Bhattacharyya","Yunxiao Shi","Risheek Garrepalli","Hong Cai","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2506.10145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14864v1","updated":"2025-06-17T17:40:21Z","published":"2025-06-17T17:40:21Z","title":"pycnet-audio: A Python package to support bioacoustics data processing","summary":"  Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023).\n","authors":["Zachary J. Ruff","Damon B. Lesmeister"],"pdf_url":"https://arxiv.org/pdf/2506.14864v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2506.11421v2","updated":"2025-06-17T17:08:47Z","published":"2025-06-13T02:39:21Z","title":"Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems","summary":"  With the rapid growth of Internet services, recommendation systems play a\ncentral role in delivering personalized content. Faced with massive user\nrequests and complex model architectures, the key challenge for real-time\nrecommendation systems is how to reduce inference latency and increase system\nthroughput without sacrificing recommendation quality. This paper addresses the\nhigh computational cost and resource bottlenecks of deep learning models in\nreal-time settings by proposing a combined set of modeling- and system-level\nacceleration and optimization strategies. At the model level, we dramatically\nreduce parameter counts and compute requirements through lightweight network\ndesign, structured pruning, and weight quantization. At the system level, we\nintegrate multiple heterogeneous compute platforms and high-performance\ninference libraries, and we design elastic inference scheduling and\nload-balancing mechanisms based on real-time load characteristics. Experiments\nshow that, while maintaining the original recommendation accuracy, our methods\ncut latency to less than 30% of the baseline and more than double system\nthroughput, offering a practical solution for deploying large-scale online\nrecommendation services.\n","authors":["Junli Shao","Jing Dong","Dingzhou Wang","Kowei Shih","Dannier Li","Chengrui Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.11421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14692v1","updated":"2025-06-17T16:29:55Z","published":"2025-06-17T16:29:55Z","title":"A Systematic Replicability and Comparative Study of BSARec and SASRec\n  for Sequential Recommendation","summary":"  This study aims at comparing two sequential recommender systems:\nSelf-Attention based Sequential Recommendation (SASRec), and Beyond\nSelf-Attention based Sequential Recommendation (BSARec) in order to check the\nimprovement frequency enhancement - the added element in BSARec - has on\nrecommendations. The models in the study, have been re-implemented with a\ncommon base-structure from EasyRec, with the aim of obtaining a fair and\nreproducible comparison. The results obtained displayed how BSARec, by\nincluding bias terms for frequency enhancement, does indeed outperform SASRec,\nalthough the increases in performance obtained, are not as high as those\npresented by the authors. This work aims at offering an overview on existing\nmethods, and most importantly at underlying the importance of implementation\ndetails for performance comparison.\n","authors":["Chiara D'Ercoli","Giulia Di Teodoro","Federico Siciliano"],"pdf_url":"https://arxiv.org/pdf/2506.14692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14684v1","updated":"2025-06-17T16:19:21Z","published":"2025-06-17T16:19:21Z","title":"Refining music sample identification with a self-supervised graph neural\n  network","summary":"  Automatic sample identification (ASID), the detection and identification of\nportions of audio recordings that have been reused in new musical works, is an\nessential but challenging task in the field of audio query-based retrieval.\nWhile a related task, audio fingerprinting, has made significant progress in\naccurately retrieving musical content under \"real world\" (noisy, reverberant)\nconditions, ASID systems struggle to identify samples that have undergone\nmusical modifications. Thus, a system robust to common music production\ntransformations such as time-stretching, pitch-shifting, effects processing,\nand underlying or overlaying music is an important open challenge.\n  In this work, we propose a lightweight and scalable encoding architecture\nemploying a Graph Neural Network within a contrastive learning framework. Our\nmodel uses only 9% of the trainable parameters compared to the current\nstate-of-the-art system while achieving comparable performance, reaching a mean\naverage precision (mAP) of 44.2%.\n  To enhance retrieval quality, we introduce a two-stage approach consisting of\nan initial coarse similarity search for candidate selection, followed by a\ncross-attention classifier that rejects irrelevant matches and refines the\nranking of retrieved candidates - an essential capability absent in prior\nmodels. In addition, because queries in real-world applications are often short\nin duration, we benchmark our system for short queries using new fine-grained\nannotations for the Sample100 dataset, which we publish as part of this work.\n","authors":["Aditya Bhattacharjee","Ivan Meresman Higgs","Mark Sandler","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2506.14684v1.pdf","comment":"Accepted at International Conference for Music Information Retrieval\n  (ISMIR) 2025"},{"id":"http://arxiv.org/abs/2505.00039v3","updated":"2025-06-17T15:18:57Z","published":"2025-04-29T18:36:57Z","title":"Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic\n  Approach","summary":"  This article proposes an adaptation of Graph Retrieval-Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms. Legal texts are characterized by a predefined hierarchical structure, an\nextensive network of references and a continuous evolution through multiple\ntemporal versions. This temporal dynamism poses a significant challenge for\nstandard AI systems, demanding a deterministic representation of the law at any\ngiven point in time. To address this, our approach grounds the knowledge graph\nconstruction in a formal, FRBRoo-inspired model that distinguishes abstract\nlegal works from their concrete textual expressions. We introduce a\nmulti-layered representation of Temporal Versions (capturing date-specific\nchanges) and Language Versions (capturing linguistic variations). By modeling\nnormative evolution as a precise sequence of these versioned entities, we\nenable the construction of a knowledge graph that serves as a verifiable\n\"ground truth\". This allows Large Language Models to generate responses based\non accurate, context-aware, and point-in-time correct legal information,\novercoming the risk of temporal inaccuracies. Through a detailed analysis of\nthis formal Graph RAG approach and its application to legal norm datasets, this\narticle aims to advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective and reliable systems in legal\nresearch, legislative analysis, and decision support.\n","authors":["Hudson de Martim"],"pdf_url":"https://arxiv.org/pdf/2505.00039v3.pdf","comment":"This version enhances the theoretical underpinnings of the proposed\n  Graph RAG methodology, including the introduction of a formal, FRBRoo-based\n  model for versioning, and enabling multi-language support for both content\n  and metadata"},{"id":"http://arxiv.org/abs/2308.12420v4","updated":"2025-06-17T14:32:54Z","published":"2023-08-23T20:42:32Z","title":"Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature","summary":"  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,\nparticularly concerning the energy consumption of the Proof of Work (PoW)\nconsensus mechanism and broader Environmental, Social, and Governance (ESG)\nissues. However, existing systematic literature reviews of DLT rely on limited\nanalyses of citations, abstracts, and keywords, failing to fully capture the\nfield's complexity and ESG concerns. We address these challenges by analyzing\nthe full text of 24,539 publications using Natural Language Processing (NLP)\nwith our manually labeled Named Entity Recognition (NER) dataset of 39,427\nentities for DLT. This methodology identified 505 key publications at the\nDLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP\nand temporal graph analysis reveals critical trends in DLT evolution and ESG\nimpacts, including cryptography and peer-to-peer networks research's\nfoundational influence, Bitcoin's persistent impact on research and\nenvironmental concerns (a \"Lindy effect\"), Ethereum's catalytic role on Proof\nof Stake (PoS) and smart contract adoption, and the industry's progressive\nshift toward energy-efficient consensus mechanisms. Our contributions include\nthe first DLT-specific NER dataset addressing the scarcity of high-quality\nlabeled NLP data in blockchain research, a methodology integrating NLP and\ntemporal graph analysis for large-scale interdisciplinary literature reviews,\nand the first NLP-driven literature review focusing on DLT's ESG aspects.\n","authors":["Walter Hernandez Cruz","Kamil Tylinski","Alastair Moore","Niall Roche","Nikhil Vadgama","Horst Treiblmaier","Jiangbo Shangguan","Paolo Tasca","Jiahua Xu"],"pdf_url":"https://arxiv.org/pdf/2308.12420v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14516v1","updated":"2025-06-17T13:41:12Z","published":"2025-06-17T13:41:12Z","title":"RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge","summary":"  This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG\nChallenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies\non generating a hypothetical answer that is used in the retrieval phase,\nalongside the original question. GRAG also incorporates a pointwise large\nlanguage model (LLM)-based re-ranking step prior to final answer generation. We\ndescribe the system architecture and the rationale behind our design choices.\nIn particular, a systematic evaluation using the Grid of Points (GoP) framework\nand N-way ANOVA enabled comparison across multiple configurations, including\nquery variant generation, question decomposition, rank fusion strategies, and\nprompting techniques for answer generation. Our system achieved a Relevance\nscore of 1.199 and a Faithfulness score of 0.477 on the private leaderboard,\nplacing among the top four finalists in the LiveRAG 2025 Challenge.\n","authors":["Kun Ran","Shuoqi Sun","Khoi Nguyen Dinh Anh","Damiano Spina","Oleg Zendel"],"pdf_url":"https://arxiv.org/pdf/2506.14516v1.pdf","comment":"Accepted for oral presentation at SIGIR 2025 LiveRAG"},{"id":"http://arxiv.org/abs/2506.10037v2","updated":"2025-06-17T13:13:31Z","published":"2025-06-10T21:38:26Z","title":"The Cell Ontology in the age of single-cell omics","summary":"  Single-cell omics technologies have transformed our understanding of cellular\ndiversity by enabling high-resolution profiling of individual cells. However,\nthe unprecedented scale and heterogeneity of these datasets demand robust\nframeworks for data integration and annotation. The Cell Ontology (CL) has\nemerged as a pivotal resource for achieving FAIR (Findable, Accessible,\nInteroperable, and Reusable) data principles by providing standardized,\nspecies-agnostic terms for canonical cell types - forming a core component of a\nwide range of platforms and tools. In this paper, we describe the wide variety\nof uses of CL in these platforms and tools and detail ongoing work to improve\nand extend CL content including the addition of transcriptomically defined\ntypes, working closely with major atlasing efforts including the Human Cell\nAtlas and the Brain Initiative Cell Atlas Network to support their needs. We\ncover the challenges and future plans for harmonising classical and\ntranscriptomic cell type definitions, integrating markers and using Large\nLanguage Models (LLMs) to improve content and efficiency of CL workflows.\n","authors":["Shawn Zheng Kai Tan","Aleix Puig-Barbe","Damien Goutte-Gattat","Caroline Eastwood","Brian Aevermann","Alida Avola","James P Balhoff","Ismail Ugur Bayindir","Jasmine Belfiore","Anita Reane Caron","David S Fischer","Nancy George","Benjamin M Gyori","Melissa A Haendel","Charles Tapley Hoyt","Huseyin Kir","Tiago Lubiana","Nicolas Matentzoglu","James A Overton","Beverly Peng","Bjoern Peters","Ellen M Quardokus","Patrick L Ray","Paola Roncaglia","Andrea D Rivera","Ray Stefancsik","Wei Kheng Teh","Sabrina Toro","Nicole Vasilevsky","Chuan Xu","Yun Zhang","Richard H Scheuermann","Chirstopher J Mungall","Alexander D Diehl","David Osumi-Sutherland"],"pdf_url":"https://arxiv.org/pdf/2506.10037v2.pdf","comment":"41 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2506.14445v1","updated":"2025-06-17T12:10:19Z","published":"2025-06-17T12:10:19Z","title":"Vela: Scalable Embeddings with Voice Large Language Models for\n  Multimodal Retrieval","summary":"  Multimodal large language models (MLLMs) have seen substantial progress in\nrecent years. However, their ability to represent multimodal information in the\nacoustic domain remains underexplored. In this work, we introduce Vela, a novel\nframework designed to adapt MLLMs for the generation of universal multimodal\nembeddings. By leveraging MLLMs with specially crafted prompts and selected\nin-context learning examples, Vela effectively bridges the modality gap across\nvarious modalities. We then propose a single-modality training approach, where\nthe model is trained exclusively on text pairs. Our experiments show that Vela\noutperforms traditional CLAP models in standard text-audio retrieval tasks.\nFurthermore, we introduce new benchmarks that expose CLAP models' limitations\nin handling long texts and complex retrieval tasks. In contrast, Vela, by\nharnessing the capabilities of MLLMs, demonstrates robust performance in these\nscenarios. Our code will soon be available.\n","authors":["Ruofan Hu","Yan Xia","Minjie Hong","Jieming Zhu","Bo Chen","Xiaoda Yang","Minghui Fang","Tao Jin"],"pdf_url":"https://arxiv.org/pdf/2506.14445v1.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.14437v1","updated":"2025-06-17T11:56:11Z","published":"2025-06-17T11:56:11Z","title":"Similarity = Value? Consultation Value Assessment and Alignment for\n  Personalized Search","summary":"  Personalized search systems in e-commerce platforms increasingly involve user\ninteractions with AI assistants, where users consult about products, usage\nscenarios, and more. Leveraging consultation to personalize search services is\ntrending. Existing methods typically rely on semantic similarity to align\nhistorical consultations with current queries due to the absence of 'value'\nlabels, but we observe that semantic similarity alone often fails to capture\nthe true value of consultation for personalization. To address this, we propose\na consultation value assessment framework that evaluates historical\nconsultations from three novel perspectives: (1) Scenario Scope Value, (2)\nPosterior Action Value, and (3) Time Decay Value. Based on this, we introduce\nVAPS, a value-aware personalized search model that selectively incorporates\nhigh-value consultations through a consultation-user action interaction module\nand an explicit objective that aligns consultations with user actions.\nExperiments on both public and commercial datasets show that VAPS consistently\noutperforms baselines in both retrieval and ranking tasks.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Teng Shi","Chenglei Shen","Ming He","Jianping Fan","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14412v1","updated":"2025-06-17T11:14:22Z","published":"2025-06-17T11:14:22Z","title":"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition","summary":"  Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR\n2025 LiveRAG Challenge.\n","authors":["Tim Cofala","Oleh Astappiev","William Xion","Hailay Teklehaymanot"],"pdf_url":"https://arxiv.org/pdf/2506.14412v1.pdf","comment":"4 pages, 5 figures. Report for SIGIR 2025 LiveRAG Challenge"},{"id":"http://arxiv.org/abs/2506.14349v1","updated":"2025-06-17T09:45:08Z","published":"2025-06-17T09:45:08Z","title":"hyperFA*IR: A hypergeometric approach to fair rankings with finite\n  candidate pool","summary":"  Ranking algorithms play a pivotal role in decision-making processes across\ndiverse domains, from search engines to job applications. When rankings\ndirectly impact individuals, ensuring fairness becomes essential, particularly\nfor groups that are marginalised or misrepresented in the data. Most of the\nexisting group fairness frameworks often rely on ensuring proportional\nrepresentation of protected groups. However, these approaches face limitations\nin accounting for the stochastic nature of ranking processes or the finite size\nof candidate pools. To this end, we present hyperFA*IR, a framework for\nassessing and enforcing fairness in rankings drawn from a finite set of\ncandidates. It relies on a generative process based on the hypergeometric\ndistribution, which models real-world scenarios by sampling without replacement\nfrom fixed group sizes. This approach improves fairness assessment when top-$k$\nselections are large relative to the pool or when protected groups are small.\nWe compare our approach to the widely used binomial model, which treats each\ndraw as independent with fixed probability, and demonstrate$-$both analytically\nand empirically$-$that our method more accurately reproduces the statistical\nproperties of sampling from a finite population. To operationalise this\nframework, we propose a Monte Carlo-based algorithm that efficiently detects\nunfair rankings by avoiding computationally expensive parameter tuning.\nFinally, we adapt our generative approach to define affirmative action policies\nby introducing weights into the sampling process.\n","authors":["Mauritz N. Cartier van Dissel","Samuel Martin-Gutierrez","Lisette Espín-Noboa","Ana María Jaramillo","Fariba Karimi"],"pdf_url":"https://arxiv.org/pdf/2506.14349v1.pdf","comment":"In Proceedings of the 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT'25)"},{"id":"http://arxiv.org/abs/2506.14345v1","updated":"2025-06-17T09:38:45Z","published":"2025-06-17T09:38:45Z","title":"A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive,\n  Transparent, and Reproducible Geo-Temporal Information Synthesis","summary":"  The emergence of Large Language Models (LLMs) has transformed information\naccess, with current LLMs also powering deep research systems that can generate\ncomprehensive report-style answers, through planned iterative search,\nretrieval, and reasoning. Still, current deep research systems lack the\ngeo-temporal capabilities that are essential for answering context-rich\nquestions involving geographic and/or temporal constraints, frequently\noccurring in domains like public health, environmental science, or\nsocio-economic analysis. This paper reports our vision towards next generation\nsystems, identifying important technical, infrastructural, and evaluative\nchallenges in integrating geo-temporal reasoning into deep research pipelines.\nWe argue for augmenting retrieval and synthesis processes with the ability to\nhandle geo-temporal constraints, supported by open and reproducible\ninfrastructures and rigorous evaluation protocols. Our vision outlines a path\ntowards more advanced and geo-temporally aware deep research systems, of\npotential impact to the future of AI-driven information access.\n","authors":["Bruno Martins","Piotr Szymański","Piotr Gramacki"],"pdf_url":"https://arxiv.org/pdf/2506.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14231v1","updated":"2025-06-17T06:38:46Z","published":"2025-06-17T06:38:46Z","title":"ImpReSS: Implicit Recommender System for Support Conversations","summary":"  Following recent advancements in large language models (LLMs), LLM-based\nchatbots have transformed customer support by automating interactions and\nproviding consistent, scalable service. While LLM-based conversational\nrecommender systems (CRSs) have attracted attention for their ability to\nenhance the quality of recommendations, limited research has addressed the\nimplicit integration of recommendations within customer support interactions.\nIn this work, we introduce ImpReSS, an implicit recommender system designed for\ncustomer support conversations. ImpReSS operates alongside existing support\nchatbots, where users report issues and chatbots provide solutions. Based on a\ncustomer support conversation, ImpReSS identifies opportunities to recommend\nrelevant solution product categories (SPCs) that help resolve the issue or\nprevent its recurrence -- thereby also supporting business growth. Unlike\ntraditional CRSs, ImpReSS functions entirely implicitly and does not rely on\nany assumption of a user's purchasing intent. Our empirical evaluation of\nImpReSS's ability to recommend relevant SPCs that can help address issues\nraised in support conversations shows promising results, including an MRR@1\n(and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for\ninformation security support, and 0.85 (0.67) for cybersecurity\ntroubleshooting. To support future research, our data and code will be shared\nupon request.\n","authors":["Omri Haller","Yair Meidan","Dudu Mimran","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2506.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14086v1","updated":"2025-06-17T01:04:45Z","published":"2025-06-17T01:04:45Z","title":"InsertRank: LLMs can reason over BM25 scores to Improve Listwise\n  Reranking","summary":"  Large Language Models (LLMs) have demonstrated significant strides across\nvarious information retrieval tasks, particularly as rerankers, owing to their\nstrong generalization and knowledge-transfer capabilities acquired from\nextensive pretraining. In parallel, the rise of LLM-based chat interfaces has\nraised user expectations, encouraging users to pose more complex queries that\nnecessitate retrieval by ``reasoning'' over documents rather than through\nsimple keyword matching or semantic similarity. While some recent efforts have\nexploited reasoning abilities of LLMs for reranking such queries, considerable\npotential for improvement remains. In that regards, we introduce InsertRank, an\nLLM-based reranker that leverages lexical signals like BM25 scores during\nreranking to further improve retrieval performance. InsertRank demonstrates\nimproved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning\n12 diverse domains, and R2MED, a specialized medical reasoning retrieval\nbenchmark spanning 8 different tasks. We conduct an exhaustive evaluation and\nseveral ablation studies and demonstrate that InsertRank consistently improves\nretrieval effectiveness across multiple families of LLMs, including GPT,\nGemini, and Deepseek models. %In addition, we also conduct ablation studies on\nnormalization by varying the scale of the BM25 scores, and positional bias by\nshuffling the order of the documents. With Deepseek-R1, InsertRank achieves a\nscore of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark,\nsurpassing previous methods.\n","authors":["Rahul Seetharaman","Kaustubh D. Dhole","Aman Bansal"],"pdf_url":"https://arxiv.org/pdf/2506.14086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13496v2","updated":"2025-06-17T19:39:04Z","published":"2025-06-16T13:53:02Z","title":"Hierarchical Multi-Positive Contrastive Learning for Patent Image\n  Retrieval","summary":"  Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware.\n","authors":["Kshitij Kavimandan","Angelos Nalmpantis","Emma Beauxis-Aussalet","Robert-Jan Sips"],"pdf_url":"https://arxiv.org/pdf/2506.13496v2.pdf","comment":"5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.08679v4","updated":"2025-06-17T17:59:57Z","published":"2025-03-11T17:56:30Z","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful when models face an explicit bias in their prompts, i.e., the\nCoT can give an incorrect picture of how models arrive at conclusions. We go\nfurther and show that unfaithful CoT can also occur on realistic prompts with\nno artificial bias. We find that when separately presented with the questions\n\"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce\nsuperficially coherent arguments to justify systematically answering Yes to\nboth questions or No to both questions, despite such responses being logically\ncontradictory. We show preliminary evidence that this is due to models'\nimplicit biases towards Yes or No, thus labeling this unfaithfulness as\nImplicit Post-Hoc Rationalization. Our results reveal that several production\nmodels exhibit surprisingly high rates of post-hoc rationalization in our\nsettings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more\nfaithful, especially thinking ones, none are entirely faithful: Gemini 2.5\nFlash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%),\nand Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical\nShortcuts, where models use subtly illogical reasoning to try to make a\nspeculative answer to hard maths problems seem rigorously proven. Our findings\nraise challenges for strategies for detecting undesired behavior in LLMs via\nthe chain of thought.\n","authors":["Iván Arcuschin","Jett Janiak","Robert Krzyzanowski","Senthooran Rajamanoharan","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2503.08679v4.pdf","comment":"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages"},{"id":"http://arxiv.org/abs/2506.14767v1","updated":"2025-06-17T17:58:17Z","published":"2025-06-17T17:58:17Z","title":"A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models","summary":"  The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.\n","authors":["Li-Wei Chen","Takuya Higuchi","Zakaria Aldeneh","Ahmed Hussen Abdelaziz","Alexander Rudnicky"],"pdf_url":"https://arxiv.org/pdf/2506.14767v1.pdf","comment":"International Conference on Machine Learning (ICML) 2025"},{"id":"http://arxiv.org/abs/2506.14762v1","updated":"2025-06-17T17:55:42Z","published":"2025-06-17T17:55:42Z","title":"Markov Regime-Switching Intelligent Driver Model for Interpretable\n  Car-Following Behavior","summary":"  Accurate and interpretable car-following models are essential for traffic\nsimulation and autonomous vehicle development. However, classical models like\nthe Intelligent Driver Model (IDM) are fundamentally limited by their\nparsimonious and single-regime structure. They fail to capture the multi-modal\nnature of human driving, where a single driving state (e.g., speed, relative\nspeed, and gap) can elicit many different driver actions. This forces the model\nto average across distinct behaviors, reducing its fidelity and making its\nparameters difficult to interpret. To overcome this, we introduce a\nregime-switching framework that allows driving behavior to be governed by\ndifferent IDM parameter sets, each corresponding to an interpretable behavioral\nmode. This design enables the model to dynamically switch between interpretable\nbehavioral modes, rather than averaging across diverse driving contexts. We\ninstantiate the framework using a Factorial Hidden Markov Model with IDM\ndynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes\n(e.g., aggressive acceleration, steady-state following) from external traffic\nscenarios (e.g., free-flow, congestion, stop-and-go) through two independent\nlatent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC)\nis used to jointly estimate the regime-specific parameters, transition\ndynamics, and latent state trajectories. Experiments on the HighD dataset\ndemonstrate that FHMM-IDM uncovers interpretable structure in human driving,\neffectively disentangling internal driver actions from contextual traffic\nconditions and revealing dynamic regime-switching patterns. This framework\nprovides a tractable and principled solution to modeling context-dependent\ndriving behavior under uncertainty, offering improvements in the fidelity of\ntraffic simulations, the efficacy of safety analyses, and the development of\nmore human-centric ADAS.\n","authors":["Chengyuan Zhang","Cathy Wu","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2506.14762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01876v2","updated":"2025-06-17T17:54:41Z","published":"2025-02-03T23:08:42Z","title":"Reinforcement Learning with Segment Feedback","summary":"  Standard reinforcement learning (RL) assumes that an agent can observe a\nreward for each state-action pair. However, in practical applications, it is\noften difficult and costly to collect a reward for each state-action pair.\nWhile there have been several works considering RL with trajectory feedback, it\nis unclear if trajectory feedback is inefficient for learning when trajectories\nare long. In this work, we consider a model named RL with segment feedback,\nwhich offers a general paradigm filling the gap between per-state-action\nfeedback and trajectory feedback. In this model, we consider an episodic Markov\ndecision process (MDP), where each episode is divided into $m$ segments, and\nthe agent observes reward feedback only at the end of each segment. Under this\nmodel, we study two popular feedback settings: binary feedback and sum\nfeedback, where the agent observes a binary outcome and a reward sum according\nto the underlying reward function, respectively. To investigate the impact of\nthe number of segments $m$ on learning performance, we design efficient\nalgorithms and establish regret upper and lower bounds for both feedback\nsettings. Our theoretical and experimental results show that: under binary\nfeedback, increasing the number of segments $m$ decreases the regret at an\nexponential rate; in contrast, surprisingly, under sum feedback, increasing $m$\ndoes not reduce the regret significantly.\n","authors":["Yihan Du","Anna Winnicki","Gal Dalal","Shie Mannor","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2502.01876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14753v1","updated":"2025-06-17T17:48:50Z","published":"2025-06-17T17:48:50Z","title":"Cost-Aware Routing for Efficient Text-To-Image Generation","summary":"  Diffusion models are well known for their ability to generate a high-fidelity\nimage for an input prompt through an iterative denoising process.\nUnfortunately, the high fidelity also comes at a high computational cost due\nthe inherently sequential generative process. In this work, we seek to\noptimally balance quality and computational cost, and propose a framework to\nallow the amount of computation to vary for each prompt, depending on its\ncomplexity. Each prompt is automatically routed to the most appropriate\ntext-to-image generation function, which may correspond to a distinct number of\ndenoising steps of a diffusion model, or a disparate, independent text-to-image\nmodel. Unlike uniform cost reduction techniques (e.g., distillation, model\nquantization), our approach achieves the optimal trade-off by learning to\nreserve expensive choices (e.g., 100+ denoising steps) only for a few complex\nprompts, and employ more economical choices (e.g., small distilled model) for\nless sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB\nthat by learning to route to nine already-trained text-to-image models, our\napproach is able to deliver an average quality that is higher than that\nachievable by any of these models alone.\n","authors":[" Qinchan"," Li","Kenneth Chen"," Changyue"," Su","Wittawat Jitkrittum","Qi Sun","Patsorn Sangkloy"],"pdf_url":"https://arxiv.org/pdf/2506.14753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18653v3","updated":"2025-06-17T17:41:07Z","published":"2024-10-24T11:32:01Z","title":"Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework","summary":"  Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging due to trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. This paper addresses the specific problem\nof multicriteria evaluation for open-ended text generation, proposing novel\nmethods for both relative and absolute rankings of decoding methods.\nSpecifically, we employ benchmarking approaches based on partial orderings and\npresent a new summary metric to balance existing automatic indicators,\nproviding a more holistic evaluation of text generation quality. Our\nexperiments demonstrate that the proposed approaches offer a robust way to\ncompare decoding strategies and serve as valuable tools to guide model\nselection for open-ended text generation tasks. We suggest future directions\nfor improving evaluation methodologies in text generation and make our code,\ndatasets, and models publicly available.\n","authors":["Esteban Garces Arias","Hannah Blocher","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2410.18653v3.pdf","comment":"Accepted at the $GEM^2$ Workshop (co-located with ACL 2025)"},{"id":"http://arxiv.org/abs/2506.14746v1","updated":"2025-06-17T17:35:25Z","published":"2025-06-17T17:35:25Z","title":"On the Hardness of Bandit Learning","summary":"  We study the task of bandit learning, also known as best-arm identification,\nunder the assumption that the true reward function f belongs to a known, but\narbitrary, function class F. We seek a general theory of bandit learnability,\nakin to the PAC framework for classification. Our investigation is guided by\nthe following two questions: (1) which classes F are learnable, and (2) how\nthey are learnable. For example, in the case of binary PAC classification,\nlearnability is fully determined by a combinatorial dimension - the VC\ndimension- and can be attained via a simple algorithmic principle, namely,\nempirical risk minimization (ERM). In contrast to classical learning-theoretic\nresults, our findings reveal limitations of learning in structured bandits,\noffering insights into the boundaries of bandit learnability. First, for the\nquestion of \"which\", we show that the paradigm of identifying the learnable\nclasses via a dimension-like quantity fails for bandit learning. We give a\nsimple proof demonstrating that no combinatorial dimension can characterize\nbandit learnability, even in finite classes, following a standard definition of\ndimension introduced by Ben-David et al. (2019). For the question of \"how\", we\nprove a computational hardness result: we construct a reward function class for\nwhich at most two queries are needed to find the optimal action, yet no\nalgorithm can do so in polynomial time unless RP=NP. We also prove that this\nclass admits efficient algorithms for standard algorithmic operations often\nconsidered in learning theory, such as an ERM. This implies that computational\nhardness is in this case inherent to the task of bandit learning. Beyond these\nresults, we investigate additional themes such as learning under noise,\ntrade-offs between noise models, and the relationship between query complexity\nand regret minimization.\n","authors":["Nataly Brukhim","Aldo Pacchiano","Miroslav Dudik","Robert Schapire"],"pdf_url":"https://arxiv.org/pdf/2506.14746v1.pdf","comment":"13 main pages"},{"id":"http://arxiv.org/abs/2506.11421v2","updated":"2025-06-17T17:08:47Z","published":"2025-06-13T02:39:21Z","title":"Deep Learning Model Acceleration and Optimization Strategies for\n  Real-Time Recommendation Systems","summary":"  With the rapid growth of Internet services, recommendation systems play a\ncentral role in delivering personalized content. Faced with massive user\nrequests and complex model architectures, the key challenge for real-time\nrecommendation systems is how to reduce inference latency and increase system\nthroughput without sacrificing recommendation quality. This paper addresses the\nhigh computational cost and resource bottlenecks of deep learning models in\nreal-time settings by proposing a combined set of modeling- and system-level\nacceleration and optimization strategies. At the model level, we dramatically\nreduce parameter counts and compute requirements through lightweight network\ndesign, structured pruning, and weight quantization. At the system level, we\nintegrate multiple heterogeneous compute platforms and high-performance\ninference libraries, and we design elastic inference scheduling and\nload-balancing mechanisms based on real-time load characteristics. Experiments\nshow that, while maintaining the original recommendation accuracy, our methods\ncut latency to less than 30% of the baseline and more than double system\nthroughput, offering a practical solution for deploying large-scale online\nrecommendation services.\n","authors":["Junli Shao","Jing Dong","Dingzhou Wang","Kowei Shih","Dannier Li","Chengrui Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.11421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11347v2","updated":"2025-06-17T16:55:20Z","published":"2025-06-12T22:47:21Z","title":"Improving Group Robustness on Spurious Correlation via Evidential\n  Alignment","summary":"  Deep neural networks often learn and rely on spurious correlations, i.e.,\nsuperficial associations between non-causal features and the targets. For\ninstance, an image classifier may identify camels based on the desert\nbackgrounds. While it can yield high overall accuracy during training, it\ndegrades generalization on more diverse scenarios where such correlations do\nnot hold. This problem poses significant challenges for out-of-distribution\nrobustness and trustworthiness. Existing methods typically mitigate this issue\nby using external group annotations or auxiliary deterministic models to learn\nunbiased representations. However, such information is costly to obtain, and\ndeterministic models may fail to capture the full spectrum of biases learned by\nthe models. To address these limitations, we propose Evidential Alignment, a\nnovel framework that leverages uncertainty quantification to understand the\nbehavior of the biased models without requiring group annotations. By\nquantifying the evidence of model prediction with second-order risk\nminimization and calibrating the biased models with the proposed evidential\ncalibration technique, Evidential Alignment identifies and suppresses spurious\ncorrelations while preserving core features. We theoretically justify the\neffectiveness of our method as capable of learning the patterns of biased\nmodels and debiasing the model without requiring any spurious correlation\nannotations. Empirical results demonstrate that our method significantly\nimproves group robustness across diverse architectures and data modalities,\nproviding a scalable and principled solution to spurious correlations.\n","authors":["Wenqian Ye","Guangtao Zheng","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.11347v2.pdf","comment":"Accepted at KDD 2025 (Research Track)"},{"id":"http://arxiv.org/abs/2411.06311v2","updated":"2025-06-17T16:54:13Z","published":"2024-11-09T23:44:17Z","title":"When are dynamical systems learned from time series data statistically\n  accurate?","summary":"  Conventional notions of generalization often fail to describe the ability of\nlearned models to capture meaningful information from dynamical data. A neural\nnetwork that learns complex dynamics with a small test error may still fail to\nreproduce its \\emph{physical} behavior, including associated statistical\nmoments and Lyapunov exponents. To address this gap, we propose an ergodic\ntheoretic approach to generalization of complex dynamical models learned from\ntime series data. Our main contribution is to define and analyze generalization\nof a broad suite of neural representations of classes of ergodic systems,\nincluding chaotic systems, in a way that captures emulating underlying\ninvariant, physical measures. Our results provide theoretical justification for\nwhy regression methods for generators of dynamical systems (Neural ODEs) fail\nto generalize, and why their statistical accuracy improves upon adding Jacobian\ninformation during training. We verify our results on a number of ergodic\nchaotic systems and neural network parameterizations, including MLPs, ResNets,\nFourier Neural layers, and RNNs.\n","authors":["Jeongjin Park","Nicole Yang","Nisha Chandramoorthy"],"pdf_url":"https://arxiv.org/pdf/2411.06311v2.pdf","comment":"in NeuRIPS 2024"},{"id":"http://arxiv.org/abs/2506.08001v3","updated":"2025-06-17T16:44:36Z","published":"2025-06-09T17:59:34Z","title":"Reparameterized LLM Training via Orthogonal Equivalence Transformation","summary":"  While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.\n","authors":["Zeju Qiu","Simon Buchholz","Tim Z. Xiao","Maximilian Dax","Bernhard Schölkopf","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.08001v3.pdf","comment":"Technical report v3 (38 pages, 26 figures, project page:\n  https://spherelab.ai/poet/, v3: added singular spectrum and energy analyses\n  in Section 4)"},{"id":"http://arxiv.org/abs/2505.11076v2","updated":"2025-06-17T16:42:33Z","published":"2025-05-16T10:07:36Z","title":"Addition is almost all you need: Compressing neural networks with double\n  binary factorization","summary":"  Binary quantization approaches, which replace weight matrices with binary\nmatrices and substitute costly multiplications with cheaper additions, offer a\ncomputationally efficient approach to address the increasing computational and\nstorage requirements of Large Language Models (LLMs). However, the severe\nquantization constraint ($\\pm1$) can lead to significant accuracy degradation.\nIn this paper, we propose Double Binary Factorization (DBF), a novel method\nthat factorizes dense weight matrices into products of two binary (sign)\nmatrices, each accompanied by scaling vectors. DBF preserves the efficiency\nadvantages of binary representations while achieving compression rates that are\ncompetitive with or superior to state-of-the-art methods. Specifically, in a\n1-bit per weight range, DBF is better than existing binarization approaches. In\na 2-bit per weight range, DBF is competitive with the best quantization methods\nlike QuIP\\# and QTIP. Unlike most existing compression techniques, which offer\nlimited compression level choices, DBF allows fine-grained control over\ncompression ratios by adjusting the factorization's intermediate dimension.\nBased on this advantage, we further introduce an algorithm for estimating\nnon-uniform layer-wise compression ratios for DBF, based on previously\ndeveloped channel pruning criteria.\n  Code available at: https://github.com/usamec/double_binary\n","authors":["Vladimír Boža","Vladimír Macko"],"pdf_url":"https://arxiv.org/pdf/2505.11076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14702v1","updated":"2025-06-17T16:40:42Z","published":"2025-06-17T16:40:42Z","title":"Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers","summary":"  One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.\n","authors":["Daniel D'souza","Julia Kreutzer","Adrien Morisot","Ahmet Üstün","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2506.14702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11423v4","updated":"2025-06-17T16:39:08Z","published":"2024-06-17T11:22:04Z","title":"Bridging Social Media and Search Engines: Dredge Words and the Detection\n  of Unreliable Domains","summary":"  Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.\n","authors":["Evan M. Williams","Peter Carragher","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2406.11423v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14698v1","updated":"2025-06-17T16:38:15Z","published":"2025-06-17T16:38:15Z","title":"Towards Desiderata-Driven Design of Visual Counterfactual Explainers","summary":"  Visual counterfactual explainers (VCEs) are a straightforward and promising\napproach to enhancing the transparency of image classifiers. VCEs complement\nother types of explanations, such as feature attribution, by revealing the\nspecific data transformations to which a machine learning model responds most\nstrongly. In this paper, we argue that existing VCEs focus too narrowly on\noptimizing sample quality or change minimality; they fail to consider the more\nholistic desiderata for an explanation, such as fidelity, understandability,\nand sufficiency. To address this shortcoming, we explore new mechanisms for\ncounterfactual generation and investigate how they can help fulfill these\ndesiderata. We combine these mechanisms into a novel 'smooth counterfactual\nexplorer' (SCE) algorithm and demonstrate its effectiveness through systematic\nevaluations on synthetic and real data.\n","authors":["Sidney Bender","Jan Herrmann","Klaus-Robert Müller","Grégoire Montavon"],"pdf_url":"https://arxiv.org/pdf/2506.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05478v2","updated":"2025-06-17T16:28:39Z","published":"2025-01-07T16:01:25Z","title":"Language and Planning in Robotic Navigation: A Multilingual Evaluation\n  of State-of-the-Art Models","summary":"  Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.\n","authors":["Malak Mansour","Ahmed Aly","Bahey Tharwat","Sarim Hashmi","Dong An","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2501.05478v2.pdf","comment":"This work has been accepted for presentation at LM4Plan@AAAI'25. For\n  more details, please check: https://llmforplanning.github.io/"},{"id":"http://arxiv.org/abs/2501.04227v2","updated":"2025-06-17T16:19:14Z","published":"2025-01-08T01:58:42Z","title":"Agent Laboratory: Using LLM Agents as Research Assistants","summary":"  Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.\n","authors":["Samuel Schmidgall","Yusheng Su","Ze Wang","Ximeng Sun","Jialian Wu","Xiaodong Yu","Jiang Liu","Michael Moor","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2501.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14673v1","updated":"2025-06-17T16:07:36Z","published":"2025-06-17T16:07:36Z","title":"Uniform Mean Estimation for Heavy-Tailed Distributions via\n  Median-of-Means","summary":"  The Median of Means (MoM) is a mean estimator that has gained popularity in\nthe context of heavy-tailed data. In this work, we analyze its performance in\nthe task of simultaneously estimating the mean of each function in a class\n$\\mathcal{F}$ when the data distribution possesses only the first $p$ moments\nfor $p \\in (1,2]$. We prove a new sample complexity bound using a novel\nsymmetrization technique that may be of independent interest. Additionally, we\npresent applications of our result to $k$-means clustering with unbounded\ninputs and linear regression with general losses, improving upon existing\nworks.\n","authors":["Mikael Møller Høgsgaard","Andrea Paudice"],"pdf_url":"https://arxiv.org/pdf/2506.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10867v2","updated":"2025-06-17T16:07:14Z","published":"2023-07-20T13:40:22Z","title":"FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with\n  Human Feedback","summary":"  Captions are crucial for understanding scientific visualizations and\ndocuments. Existing captioning methods for scientific figures rely on\nfigure-caption pairs extracted from documents for training, many of which fall\nshort with respect to metrics like helpfulness, explainability, and\nvisual-descriptiveness [15] leading to generated captions being misaligned with\nreader preferences. To enable the generation of high-quality figure captions,\nwe introduce FigCaps-HF a new framework for figure-caption generation that can\nincorporate domain expert feedback in generating captions optimized for reader\npreferences. Our framework comprises of 1) an automatic method for evaluating\nquality of figure-caption pairs, 2) a novel reinforcement learning with human\nfeedback (RLHF) method to optimize a generative figure-to-caption model for\nreader preferences. We demonstrate the effectiveness of our simple learning\nframework by improving performance over standard fine-tuning across different\ntypes of models. In particular, when using BLIP as the base model, our RLHF\nframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and\nMeteor, respectively. Finally, we release a large-scale benchmark dataset with\nhuman feedback on figure-caption pairs to enable further evaluation and\ndevelopment of RLHF techniques for this problem.\n","authors":["Ashish Singh","Ashutosh Singh","Prateek Agarwal","Zixuan Huang","Arpita Singh","Tong Yu","Sungchul Kim","Victor Bursztyn","Nesreen K. Ahmed","Puneet Mathur","Erik Learned-Miller","Franck Dernoncourt","Ryan A. Rossi"],"pdf_url":"https://arxiv.org/pdf/2307.10867v2.pdf","comment":"16 pages, 4 figures. Benchmark Documentation:\n  https://figcapshf.github.io/"},{"id":"http://arxiv.org/abs/2503.11808v2","updated":"2025-06-17T16:07:13Z","published":"2025-03-14T18:55:48Z","title":"Understanding the Trade-offs in Accuracy and Uncertainty Quantification:\n  Architecture and Inference Choices in Bayesian Neural Networks","summary":"  As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models, coupled with the lack of identifiability,\nmake Markov chain Monte Carlo (MCMC) tremendously expensive and unable to fully\nexplore the multimodal posterior. On the other hand, variational inference\nbenefits from improved computational complexity but lacks the asymptotical\nguarantees of sampling-based inference and tends to concentrate around a single\nmode. The performance of both approaches heavily depends on architectural\nchoices; this paper aims to shed some light on this by considering the\ncomputational costs, accuracy and uncertainty quantification in different\nscenarios including large width and out-of-sample data. To improve posterior\nexploration, different model averaging and ensembling techniques are studied,\nalong with their benefits on predictive performance. In our experiments,\nvariational inference overall provided better uncertainty quantification than\nMCMC; further, stacking and ensembles of variational approximations provided\ncomparable accuracy to MCMC at a much-reduced cost.\n","authors":["Alisa Sheinkman","Sara Wade"],"pdf_url":"https://arxiv.org/pdf/2503.11808v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2501.03905v2","updated":"2025-06-17T16:02:34Z","published":"2025-01-07T16:19:40Z","title":"mFabric: An Efficient and Scalable Fabric for Mixture-of-Experts\n  Training","summary":"  Mixture-of-Expert (MoE) models outperform conventional models by selectively\nactivating different subnets, named \\emph{experts}, on a per-token basis. This\ngated computation generates dynamic communications that cannot be determined\nbeforehand, challenging the existing GPU interconnects that remain\n\\emph{static} during the distributed training process. In this paper, we\nadvocate for a first-of-its-kind system, called mFabric, that unlocks topology\nreconfiguration \\emph{during} distributed MoE training. Towards this vision, we\nfirst perform a production measurement study and show that the MoE dynamic\ncommunication pattern has \\emph{strong locality}, alleviating the requirement\nof global reconfiguration. Based on this, we design and implement a\n\\emph{regionally reconfigurable high-bandwidth domain} on top of existing\nelectrical interconnects using optical circuit switching (OCS), achieving\nscalability while maintaining rapid adaptability. We have built a fully\nfunctional mFabric prototype with commodity hardware and a customized\ncollective communication runtime that trains state-of-the-art MoE models with\n\\emph{in-training} topology reconfiguration across 32 A100 GPUs. Large-scale\npacket-level simulations show that mFabric delivers comparable performance as\nthe non-blocking fat-tree fabric while boosting the training cost efficiency\n(e.g., performance per dollar) of four representative MoE models by\n1.2$\\times$--1.5$\\times$ and 1.9$\\times$--2.3$\\times$ at 100 Gbps and 400 Gbps\nlink bandwidths, respectively.\n","authors":["Xudong Liao","Yijun Sun","Han Tian","Xinchen Wan","Yilun Jin","Zilong Wang","Zhenghang Ren","Xinyang Huang","Wenxue Li","Kin Fai Tse","Zhizhen Zhong","Guyue Liu","Ying Zhang","Xiaofeng Ye","Yiming Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2501.03905v2.pdf","comment":"Corresponding authors: zhizhenz@mit.edu (Z. Zhong),\n  kaichen@cse.ust.hk (K. Chen)"},{"id":"http://arxiv.org/abs/2412.06745v2","updated":"2025-06-17T15:57:52Z","published":"2024-12-09T18:37:14Z","title":"ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended\n  Capabilities","summary":"  Traditional fixed test sets fall short in evaluating open-ended capabilities\nof foundation models. To address this, we propose ONEBench(OpeN-Ended\nBenchmarking), a new testing paradigm that consolidates individual evaluation\ndatasets into a unified, ever-expanding sample pool. ONEBench allows users to\ngenerate custom, open-ended evaluation benchmarks from this pool, corresponding\nto specific capabilities of interest. By aggregating samples across test sets,\nONEBench enables the assessment of diverse capabilities beyond those covered by\nthe original test sets, while mitigating overfitting and dataset bias. Most\nimportantly, it frames model evaluation as a collective process of selecting\nand aggregating sample-level tests.\n  The shift from task-specific benchmarks to ONEBench introduces two\nchallenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the\naggregation over diverse metrics, while incompleteness describes comparing\nmodels evaluated on different data subsets. To address these challenges, we\nexplore algorithms to aggregate sparse measurements into reliable model scores.\nOur aggregation algorithm ensures identifiability(asymptotically recovering\nground-truth scores) and rapid convergence, enabling accurate model ranking\nwith less data. On homogenous datasets, we show our aggregation algorithm\nprovides rankings that highly correlate with those produced by average scores.\nWe also demonstrate robustness to ~95% of measurements missing, reducing\nevaluation cost by up to 20x with little-to-no change in model rankings. We\nintroduce ONEBench-LLM for language models and ONEBench-LMM for vision-language\nmodels, unifying evaluations across these domains. Overall, we present a\ntechnique for open-ended evaluation, which can aggregate over incomplete,\nheterogeneous sample-level measurements to continually grow a benchmark\nalongside the rapidly developing foundation models.\n","authors":["Adhiraj Ghosh","Sebastian Dziadzio","Ameya Prabhu","Vishaal Udandarao","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2412.06745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14665v1","updated":"2025-06-17T15:56:56Z","published":"2025-06-17T15:56:56Z","title":"Accurate and scalable exchange-correlation with deep learning","summary":"  Density Functional Theory (DFT) is the most widely used electronic structure\nmethod for predicting the properties of molecules and materials. Although DFT\nis, in principle, an exact reformulation of the Schr\\\"odinger equation,\npractical applications rely on approximations to the unknown\nexchange-correlation (XC) functional. Most existing XC functionals are\nconstructed using a limited set of increasingly complex, hand-crafted features\nthat improve accuracy at the expense of computational efficiency. Yet, no\ncurrent approximation achieves the accuracy and generality for predictive\nmodeling of laboratory experiments at chemical accuracy -- typically defined as\nerrors below 1 kcal/mol. In this work, we present Skala, a modern deep\nlearning-based XC functional that bypasses expensive hand-designed features by\nlearning representations directly from data. Skala achieves chemical accuracy\nfor atomization energies of small molecules while retaining the computational\nefficiency typical of semi-local DFT. This performance is enabled by training\non an unprecedented volume of high-accuracy reference data generated using\ncomputationally intensive wavefunction-based methods. Notably, Skala\nsystematically improves with additional training data covering diverse\nchemistry. By incorporating a modest amount of additional high-accuracy data\ntailored to chemistry beyond atomization energies, Skala achieves accuracy\ncompetitive with the best-performing hybrid functionals across general main\ngroup chemistry, at the cost of semi-local DFT. As the training dataset\ncontinues to expand, Skala is poised to further enhance the predictive power of\nfirst-principles simulations.\n","authors":["Giulia Luise","Chin-Wei Huang","Thijs Vogels","Derk P. Kooi","Sebastian Ehlert","Stephanie Lanius","Klaas J. H. Giesbertz","Amir Karton","Deniz Gunceler","Megan Stanley","Wessel P. Bruinsma","Lin Huang","Xinran Wei","José Garrido Torres","Abylay Katbashev","Bálint Máté","Sékou-Oumar Kaba","Roberto Sordillo","Yingrong Chen","David B. Williams-Young","Christopher M. Bishop","Jan Hermann","Rianne van den Berg","Paola Gori-Giorgi"],"pdf_url":"https://arxiv.org/pdf/2506.14665v1.pdf","comment":"Main: 13 pages plus references, 11 figures and tables. Supplementary\n  information: 19 pages, 12 figures and tables"},{"id":"http://arxiv.org/abs/2502.13174v2","updated":"2025-06-17T15:47:28Z","published":"2025-02-17T21:24:18Z","title":"Diverse Topology Optimization using Modulated Neural Fields","summary":"  Topology optimization (TO) is a family of computational methods that derive\nnear-optimal geometries from formal problem descriptions. Despite their\nsuccess, established TO methods are limited to generating single solutions,\nrestricting the exploration of alternative designs. To address this limitation,\nwe introduce Topology Optimization using Modulated Neural Fields (TOM) - a\ndata-free method that trains a neural network to generate structurally\ncompliant shapes and explores diverse solutions through an explicit diversity\nconstraint. The network is trained with a solver-in-the-loop, optimizing the\nmaterial distribution in each iteration. The trained model produces diverse\nshapes that closely adhere to the design requirements. We validate TOM on 2D\nand 3D TO problems. Our results show that TOM generates more diverse solutions\nthan any previous method, all while maintaining near-optimality and without\nrelying on a dataset. These findings open new avenues for engineering and\ndesign, offering enhanced flexibility and innovation in structural\noptimization.\n","authors":["Andreas Radler","Eric Volkmann","Johannes Brandstetter","Arturs Berzins"],"pdf_url":"https://arxiv.org/pdf/2502.13174v2.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.17060v3","updated":"2025-06-17T15:45:05Z","published":"2025-02-24T11:21:08Z","title":"Analytics Modelling over Multiple Datasets using Vector Embeddings","summary":"  The massive increase in the data volume and dataset availability for analysts\ncompels researchers to focus on data content and select high-quality datasets\nto enhance the performance of analytics operators. While selecting high-quality\ndata significantly boosts analytical accuracy and efficiency, the exact process\nis very challenging given large-scale dataset availability. To address this\nissue, we propose a novel methodology that infers the outcome of analytics\noperators by creating a model from the available datasets. Each dataset is\ntransformed to a vector embedding representation generated by our proposed deep\nlearning model NumTabData2Vec, where similarity search are employed. Through\nexperimental evaluation, we compare the prediction performance and the\nexecution time of our framework to another state-of-the-art modelling operator\nframework, illustrating that our approach predicts analytics outcomes\naccurately, and increases speedup. Furthermore, our vectorization model can\nproject different real-world scenarios to a lower vector embedding\nrepresentation accurately and distinguish them.\n","authors":["Andreas Loizou","Dimitrios Tsoumakos"],"pdf_url":"https://arxiv.org/pdf/2502.17060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14652v1","updated":"2025-06-17T15:44:41Z","published":"2025-06-17T15:44:41Z","title":"Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible\n  AI-Informed Conception of Rigor","summary":"  In AI research and practice, rigor remains largely understood in terms of\nmethodological rigor -- such as whether mathematical, statistical, or\ncomputational methods are correctly applied. We argue that this narrow\nconception of rigor has contributed to the concerns raised by the responsible\nAI community, including overblown claims about AI capabilities. Our position is\nthat a broader conception of what rigorous AI research and practice should\nentail is needed. We believe such a conception -- in addition to a more\nexpansive understanding of (1) methodological rigor -- should include aspects\nrelated to (2) what background knowledge informs what to work on (epistemic\nrigor); (3) how disciplinary, community, or personal norms, standards, or\nbeliefs influence the work (normative rigor); (4) how clearly articulated the\ntheoretical constructs under use are (conceptual rigor); (5) what is reported\nand how (reporting rigor); and (6) how well-supported the inferences from\nexisting evidence are (interpretative rigor). In doing so, we also aim to\nprovide useful language and a framework for much-needed dialogue about the AI\ncommunity's work by researchers, policymakers, journalists, and other\nstakeholders.\n","authors":["Alexandra Olteanu","Su Lin Blodgett","Agathe Balayn","Angelina Wang","Fernando Diaz","Flavio du Pin Calmon","Margaret Mitchell","Michael Ekstrand","Reuben Binns","Solon Barocas"],"pdf_url":"https://arxiv.org/pdf/2506.14652v1.pdf","comment":"20 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2505.22807v3","updated":"2025-06-17T15:41:41Z","published":"2025-05-28T19:33:12Z","title":"Distribution free M-estimation","summary":"  The basic question of delineating those statistical problems that are\nsolvable without making any assumptions on the underlying data distribution has\nlong animated statistics and learning theory. This paper characterizes when a\nconvex M-estimation or stochastic optimization problem is solvable in such an\nassumption-free setting, providing a precise dividing line between solvable and\nunsolvable problems. The conditions we identify show, perhaps surprisingly,\nthat Lipschitz continuity of the loss being minimized is not necessary for\ndistribution free minimization, and they are also distinct from classical\ncharacterizations of learnability in machine learning.\n","authors":["Felipe Areces","John C. Duchi"],"pdf_url":"https://arxiv.org/pdf/2505.22807v3.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2506.14641v1","updated":"2025-06-17T15:39:33Z","published":"2025-06-17T15:39:33Z","title":"Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot","summary":"  In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.\n","authors":["Xiang Cheng","Chengyan Pan","Minjun Zhao","Deyang Li","Fangchao Liu","Xinyu Zhang","Xiao Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2506.14641v1.pdf","comment":"19 pages,22 figures"},{"id":"http://arxiv.org/abs/2506.13566v2","updated":"2025-06-17T15:27:49Z","published":"2025-06-16T14:50:26Z","title":"A Production Scheduling Framework for Reinforcement Learning Under\n  Real-World Constraints","summary":"  The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing\nmakespan under deterministic constraints. Real-world production environments\nintroduce additional complexities that cause traditional scheduling approaches\nto be less effective. Reinforcement learning (RL) holds potential in addressing\nthese challenges, as it allows agents to learn adaptive scheduling strategies.\nHowever, there is a lack of a comprehensive, general-purpose frameworks for\neffectively training and evaluating RL agents under real-world constraints. To\naddress this gap, we propose a modular framework that extends classical JSSP\nformulations by incorporating key real-world constraints inherent to the\nshopfloor, including transport logistics, buffer management, machine\nbreakdowns, setup times, and stochastic processing conditions, while also\nsupporting multi-objective optimization. The framework is a customizable\nsolution that offers flexibility in defining problem instances and configuring\nsimulation parameters, enabling adaptation to diverse production scenarios. A\nstandardized interface ensures compatibility with various RL approaches,\nproviding a robust environment for training RL agents and facilitating the\nstandardized comparison of different scheduling methods under dynamic and\nuncertain conditions. We release JobShopLab as an open-source tool for both\nresearch and industrial applications, accessible at:\nhttps://github.com/proto-lab-ro/jobshoplab\n","authors":["Jonathan Hoss","Felix Schelling","Noah Klarmann"],"pdf_url":"https://arxiv.org/pdf/2506.13566v2.pdf","comment":"This paper has been accepted for presentation at the IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)"},{"id":"http://arxiv.org/abs/2506.14619v1","updated":"2025-06-17T15:16:22Z","published":"2025-06-17T15:16:22Z","title":"Feasibility-Driven Trust Region Bayesian Optimization","summary":"  Bayesian optimization is a powerful tool for solving real-world optimization\ntasks under tight evaluation budgets, making it well-suited for applications\ninvolving costly simulations or experiments. However, many of these tasks are\nalso characterized by the presence of expensive constraints whose analytical\nformulation is unknown and often defined in high-dimensional spaces where\nfeasible regions are small, irregular, and difficult to identify. In such\ncases, a substantial portion of the optimization budget may be spent just\ntrying to locate the first feasible solution, limiting the effectiveness of\nexisting methods. In this work, we present a Feasibility-Driven Trust Region\nBayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust\nregion from which the next candidate solution is selected, using information\nfrom both the objective and constraint surrogate models. Our adaptive strategy\nallows the trust region to shift and resize significantly between iterations,\nenabling the optimizer to rapidly refocus its search and consistently\naccelerate the discovery of feasible and good-quality solutions. We empirically\ndemonstrate the effectiveness of FuRBO through extensive testing on the full\nBBOB-constrained COCO benchmark suite and other physics-inspired benchmarks,\ncomparing it against state-of-the-art baselines for constrained black-box\noptimization across varying levels of constraint severity and problem\ndimensionalities ranging from 2 to 60.\n","authors":["Paolo Ascia","Elena Raponi","Thomas Bäck","Fabian Duddeck"],"pdf_url":"https://arxiv.org/pdf/2506.14619v1.pdf","comment":"Accepted for publication at AutoML2025"},{"id":"http://arxiv.org/abs/2506.14607v1","updated":"2025-06-17T15:08:16Z","published":"2025-06-17T15:08:16Z","title":"Expressive Score-Based Priors for Distribution Matching with\n  Geometry-Preserving Regularization","summary":"  Distribution matching (DM) is a versatile domain-invariant representation\nlearning technique that has been applied to tasks such as fair classification,\ndomain adaptation, and domain translation. Non-parametric DM methods struggle\nwith scalability and adversarial DM approaches suffer from instability and mode\ncollapse. While likelihood-based methods are a promising alternative, they\noften impose unnecessary biases through fixed priors or require explicit\ndensity models (e.g., flows) that can be challenging to train. We address this\nlimitation by introducing a novel approach to training likelihood-based DM\nusing expressive score-based prior distributions. Our key insight is that\ngradient-based DM training only requires the prior's score function -- not its\ndensity -- allowing us to train the prior via denoising score matching. This\napproach eliminates biases from fixed priors (e.g., in VAEs), enabling more\neffective use of geometry-preserving regularization, while avoiding the\nchallenge of learning an explicit prior density model (e.g., a flow-based\nprior). Our method also demonstrates better stability and computational\nefficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore,\nexperiments demonstrate superior performance across multiple tasks,\nestablishing our score-based method as a stable and effective approach to\ndistribution matching. Source code available at\nhttps://github.com/inouye-lab/SAUB.\n","authors":["Ziyu Gong","Jim Lim","David I. Inouye"],"pdf_url":"https://arxiv.org/pdf/2506.14607v1.pdf","comment":"32 pages, 20 figures. Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2506.14606v1","updated":"2025-06-17T15:06:54Z","published":"2025-06-17T15:06:54Z","title":"Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees","summary":"  The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.\n","authors":["Ahmed Heakl","Sarim Hashmi","Chaimaa Abi","Celine Lee","Abdulrahman Mahmoud"],"pdf_url":"https://arxiv.org/pdf/2506.14606v1.pdf","comment":"Project page: https://ahmedheakl.github.io/Guaranteed-Guess/"},{"id":"http://arxiv.org/abs/2506.14605v1","updated":"2025-06-17T15:06:43Z","published":"2025-06-17T15:06:43Z","title":"Unsupervised Imaging Inverse Problems with Diffusion Distribution\n  Matching","summary":"  This work addresses image restoration tasks through the lens of inverse\nproblems using unpaired datasets. In contrast to traditional approaches --\nwhich typically assume full knowledge of the forward model or access to paired\ndegraded and ground-truth images -- the proposed method operates under minimal\nassumptions and relies only on small, unpaired datasets. This makes it\nparticularly well-suited for real-world scenarios, where the forward model is\noften unknown or misspecified, and collecting paired data is costly or\ninfeasible. The method leverages conditional flow matching to model the\ndistribution of degraded observations, while simultaneously learning the\nforward model via a distribution-matching loss that arises naturally from the\nframework. Empirically, it outperforms both single-image blind and unsupervised\napproaches on deblurring and non-uniform point spread function (PSF)\ncalibration tasks. It also matches state-of-the-art performance on blind\nsuper-resolution. We also showcase the effectiveness of our method with a proof\nof concept for lens calibration: a real-world application traditionally\nrequiring time-consuming experiments and specialized equipment. In contrast,\nour approach achieves this with minimal data acquisition effort.\n","authors":["Giacomo Meanti","Thomas Ryckeboer","Michael Arbel","Julien Mairal"],"pdf_url":"https://arxiv.org/pdf/2506.14605v1.pdf","comment":"Code available at https://github.com/inria-thoth/ddm4ip"},{"id":"http://arxiv.org/abs/2506.14603v1","updated":"2025-06-17T15:06:07Z","published":"2025-06-17T15:06:07Z","title":"Align Your Flow: Scaling Continuous-Time Flow Map Distillation","summary":"  Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.\n","authors":["Amirmojtaba Sabour","Sanja Fidler","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2506.14603v1.pdf","comment":"Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/"},{"id":"http://arxiv.org/abs/2401.16852v3","updated":"2025-06-17T15:05:12Z","published":"2024-01-30T09:55:14Z","title":"Checkmating One, by Using Many: Combining Mixture of Experts with MCTS\n  to Improve in Chess","summary":"  In games like chess, strategy evolves dramatically across distinct phases -\nthe opening, middlegame, and endgame each demand different forms of reasoning\nand decision-making. Yet, many modern chess engines rely on a single neural\nnetwork to play the entire game uniformly, often missing opportunities to\nspecialize. In this work, we introduce M2CTS, a modular framework that combines\nMixture of Experts with Monte Carlo Tree Search to adapt strategy dynamically\nbased on game phase. We explore three different methods for training the neural\nnetworks: Separated Learning, Staged Learning, and Weighted Learning. By\nrouting decisions through specialized neural networks trained for each phase,\nM2CTS improves both computational efficiency and playing strength. In\nexperiments on chess, M2CTS achieves up to +122 Elo over standard single-model\nbaselines and shows promising generalization to multi-agent domains such as\nPommerman. These results highlight how modular, phase-aware systems can better\nalign with the structured nature of games and move us closer to human-like\nbehavior in dividing a problem into many smaller units.\n","authors":["Felix Helfenstein","Johannes Czech","Jannis Blüml","Max Eisel","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2401.16852v3.pdf","comment":"31 pages, 33 figures, 15 tables. Code available under\n  https://github.com/QueensGambit/CrazyAra"},{"id":"http://arxiv.org/abs/2401.05308v3","updated":"2025-06-17T15:04:58Z","published":"2024-01-10T18:22:00Z","title":"Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks","summary":"  The deployment of federated learning (FL) in non-terrestrial networks (NTN)\nthat are supported by high-altitude platform stations (HAPS) offers numerous\nadvantages. Due to its large footprint, it facilitates interaction with a large\nnumber of line-of-sight (LoS) ground clients, each possessing diverse datasets\nalong with distinct communication and computational capabilities. The presence\nof many clients enhances the accuracy of the FL model and speeds up\nconvergence. However, the variety of datasets among these clients poses a\nsignificant challenge, as it leads to pervasive non-independent and identically\ndistributed (non-IID) data. The data non-IIDness results in markedly reduced\ntraining accuracy and slower convergence rates. To address this issue, we\npropose a novel weighted attribute-based client selection strategy that\nleverages multiple user-specific attributes, including historical traffic\npatterns, instantaneous channel conditions, computational capabilities, and\nprevious-round learning performance. By combining these attributes into a\ncomposite score for each user at every FL round and selecting users with higher\nscores as FL clients, the framework ensures more uniform and representative\ndata distributions, effectively mitigating the adverse effects of non-IID data.\nSimulation results corroborate the effectiveness of the proposed client\nselection strategy in enhancing FL model accuracy and convergence rate, as well\nas reducing training loss, by effectively addressing the critical challenge of\ndata non-IIDness in large-scale FL system implementations.\n","authors":["Amin Farajzadeh","Animesh Yadav","Halim Yanikomeroglu"],"pdf_url":"https://arxiv.org/pdf/2401.05308v3.pdf","comment":"Submitted to IEEE for possible publication"},{"id":"http://arxiv.org/abs/2506.14597v1","updated":"2025-06-17T15:03:21Z","published":"2025-06-17T15:03:21Z","title":"Deep Learning Surrogates for Real-Time Gas Emission Inversion","summary":"  Real-time identification and quantification of greenhouse-gas emissions under\ntransient atmospheric conditions is a critical challenge in environmental\nmonitoring. We introduce a spatio-temporal inversion framework that embeds a\ndeep-learning surrogate of computational fluid dynamics (CFD) within a\nsequential Monte Carlo algorithm to perform Bayesian inference of both emission\nrate and source location in dynamic flow fields. By substituting costly\nnumerical solvers with a multilayer perceptron trained on high-fidelity CFD\noutputs, our surrogate captures spatial heterogeneity and temporal evolution of\ngas dispersion, while delivering near-real-time predictions. Validation on the\nChilbolton methane release dataset demonstrates comparable accuracy to full CFD\nsolvers and Gaussian plume models, yet achieves orders-of-magnitude faster\nruntimes. Further experiments under simulated obstructed-flow scenarios confirm\nrobustness in complex environments. This work reconciles physical fidelity with\ncomputational feasibility, offering a scalable solution for industrial\nemissions monitoring and other time-sensitive spatio-temporal inversion tasks\nin environmental and scientific modeling.\n","authors":["Thomas Newman","Christopher Nemeth","Matthew Jones","Philip Jonathan"],"pdf_url":"https://arxiv.org/pdf/2506.14597v1.pdf","comment":"3 figures, 11 pages"},{"id":"http://arxiv.org/abs/2506.14587v1","updated":"2025-06-17T14:49:29Z","published":"2025-06-17T14:49:29Z","title":"SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks\n  for Robust Classification","summary":"  Shortcut learning undermines model generalization to out-of-distribution\ndata. While the literature attributes shortcuts to biases in superficial\nfeatures, we show that imbalances in the semantic distribution of sample\nembeddings induce spurious semantic correlations, compromising model\nrobustness. To address this issue, we propose SCISSOR (Semantic Cluster\nIntervention for Suppressing ShORtcut), a Siamese network-based debiasing\napproach that remaps the semantic space by discouraging latent clusters\nexploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR\neliminates the need for data augmentation and rewriting. We evaluate SCISSOR on\n6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and\nGYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports\n+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,\nand +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models\nwith ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for\nBERT on NLP. Our study redefines the landscape of model generalization by\naddressing overlooked semantic biases, establishing SCISSOR as a foundational\nframework for mitigating shortcut learning and fostering more robust,\nbias-resistant AI systems.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2506.14587v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.11132v3","updated":"2025-06-17T14:49:16Z","published":"2024-11-17T17:36:30Z","title":"Variational Bayesian Bow tie Neural Networks with Shrinkage","summary":"  Despite the dominant role of deep models in machine learning, limitations\npersist, including overconfident predictions, susceptibility to adversarial\nattacks, and underestimation of variability in predictions. The Bayesian\nparadigm provides a natural framework to overcome such issues and has become\nthe gold standard for uncertainty estimation with deep models, also providing\nimproved accuracy and a framework for tuning critical hyperparameters. However,\nexact Bayesian inference is challenging, typically involving variational\nalgorithms that impose strong independence and distributional assumptions.\nMoreover, existing methods are sensitive to the architectural choice of the\nnetwork. We address these issues by focusing on a stochastic relaxation of the\nstandard feed-forward rectified neural network and using sparsity-promoting\npriors on the weights of the neural network for increased robustness to\narchitectural design. Thanks to Polya-Gamma data augmentation tricks, which\nrender a conditionally linear and Gaussian model, we derive a fast, approximate\nvariational inference algorithm that avoids distributional assumptions and\nindependence across layers. Suitable strategies to further improve scalability\nand account for multimodality are considered.\n","authors":["Alisa Sheinkman","Sara Wade"],"pdf_url":"https://arxiv.org/pdf/2411.11132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13244v2","updated":"2025-06-17T14:48:28Z","published":"2025-06-16T08:42:31Z","title":"No-Regret Learning Under Adversarial Resource Constraints: A Spending\n  Plan Is All You Need!","summary":"  We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan.\n","authors":["Francesco Emanuele Stradi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti","Christian Kroer"],"pdf_url":"https://arxiv.org/pdf/2506.13244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12063v6","updated":"2025-06-17T14:45:22Z","published":"2025-02-17T17:30:14Z","title":"Low-Rank Thinning","summary":"  The goal in thinning is to summarize a dataset using a small set of\nrepresentative points. Remarkably, sub-Gaussian thinning algorithms like Kernel\nHalving and Compress can match the quality of uniform subsampling while\nsubstantially reducing the number of summary points. However, existing\nguarantees cover only a restricted range of distributions and kernel-based\nquality measures and suffer from pessimistic dimension dependence. To address\nthese deficiencies, we introduce a new low-rank analysis of sub-Gaussian\nthinning that applies to any distribution and any kernel, guaranteeing\nhigh-quality compression whenever the kernel or data matrix is approximately\nlow-rank. To demonstrate the broad applicability of the techniques, we design\npractical sub-Gaussian thinning approaches that improve upon the best known\nguarantees for approximating attention in transformers, accelerating stochastic\ngradient training through reordering, and distinguishing distributions in\nnear-linear time.\n","authors":["Annabelle Michael Carrell","Albert Gong","Abhishek Shetty","Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2502.12063v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14582v1","updated":"2025-06-17T14:38:08Z","published":"2025-06-17T14:38:08Z","title":"Busting the Paper Ballot: Voting Meets Adversarial Machine Learning","summary":"  We show the security risk associated with using machine learning classifiers\nin United States election tabulators. The central classification task in\nelection tabulation is deciding whether a mark does or does not appear on a\nbubble associated to an alternative in a contest on the ballot. Barretto et al.\n(E-Vote-ID 2021) reported that convolutional neural networks are a viable\noption in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To\ndemonstrate and analyze the hypothetical vulnerability of machine learning\nmodels on election tabulators, we first introduce four new ballot datasets.\nSecond, we train and test a variety of different models on our new datasets.\nThese models include support vector machines, convolutional neural networks (a\nbasic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,\nusing our new datasets and trained models, we demonstrate that traditional\nwhite box attacks are ineffective in the voting domain due to gradient masking.\nOur analyses further reveal that gradient masking is a product of numerical\ninstability. We use a modified difference of logits ratio loss to overcome this\nissue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct\nattacks with the adversarial examples generated using our new methods. In\ntraditional adversarial machine learning, a high (50% or greater) attack\nsuccess rate is ideal. However, for certain elections, even a 5% attack success\nrate can flip the outcome of a race. We show such an impact is possible in the\nphysical domain. We thoroughly discuss attack realism, and the challenges and\npracticality associated with printing and scanning ballot adversarial examples.\n","authors":["Kaleel Mahmood","Caleb Manicke","Ethan Rathbun","Aayushi Verma","Sohaib Ahmad","Nicholas Stamatakis","Laurent Michel","Benjamin Fuller"],"pdf_url":"https://arxiv.org/pdf/2506.14582v1.pdf","comment":"18 Pages. Author version of article to appear at CCS 2025"},{"id":"http://arxiv.org/abs/2506.14577v1","updated":"2025-06-17T14:35:01Z","published":"2025-06-17T14:35:01Z","title":"Object-Centric Neuro-Argumentative Learning","summary":"  Over the last decade, as we rely more on deep learning technologies to make\ncritical decisions, concerns regarding their safety, reliability and\ninterpretability have emerged. We introduce a novel Neural Argumentative\nLearning (NAL) architecture that integrates Assumption-Based Argumentation\n(ABA) with deep learning for image analysis. Our architecture consists of\nneural and symbolic components. The former segments and encodes images into\nfacts using object-centric learning, while the latter applies ABA learning to\ndevelop ABA frameworks enabling predictions with images. Experiments on\nsynthetic data show that the NAL architecture can be competitive with a\nstate-of-the-art alternative.\n","authors":["Abdul Rahman Jacob","Avinash Kori","Emanuele De Angelis","Ben Glocker","Maurizio Proietti","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2506.14577v1.pdf","comment":"Proceedings of Machine Learning Research, 2025 19th Conference on\n  Neurosymbolic Learning and Reasoning"},{"id":"http://arxiv.org/abs/2308.12420v4","updated":"2025-06-17T14:32:54Z","published":"2023-08-23T20:42:32Z","title":"Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature","summary":"  Distributed Ledger Technology (DLT) faces increasing environmental scrutiny,\nparticularly concerning the energy consumption of the Proof of Work (PoW)\nconsensus mechanism and broader Environmental, Social, and Governance (ESG)\nissues. However, existing systematic literature reviews of DLT rely on limited\nanalyses of citations, abstracts, and keywords, failing to fully capture the\nfield's complexity and ESG concerns. We address these challenges by analyzing\nthe full text of 24,539 publications using Natural Language Processing (NLP)\nwith our manually labeled Named Entity Recognition (NER) dataset of 39,427\nentities for DLT. This methodology identified 505 key publications at the\nDLT/ESG intersection, enabling comprehensive domain analysis. Our combined NLP\nand temporal graph analysis reveals critical trends in DLT evolution and ESG\nimpacts, including cryptography and peer-to-peer networks research's\nfoundational influence, Bitcoin's persistent impact on research and\nenvironmental concerns (a \"Lindy effect\"), Ethereum's catalytic role on Proof\nof Stake (PoS) and smart contract adoption, and the industry's progressive\nshift toward energy-efficient consensus mechanisms. Our contributions include\nthe first DLT-specific NER dataset addressing the scarcity of high-quality\nlabeled NLP data in blockchain research, a methodology integrating NLP and\ntemporal graph analysis for large-scale interdisciplinary literature reviews,\nand the first NLP-driven literature review focusing on DLT's ESG aspects.\n","authors":["Walter Hernandez Cruz","Kamil Tylinski","Alastair Moore","Niall Roche","Nikhil Vadgama","Horst Treiblmaier","Jiangbo Shangguan","Paolo Tasca","Jiahua Xu"],"pdf_url":"https://arxiv.org/pdf/2308.12420v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14574v1","updated":"2025-06-17T14:30:06Z","published":"2025-06-17T14:30:06Z","title":"TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct\n  Preference Optimization","summary":"  Recent advancements in reinforcement learning from human feedback have shown\nthat utilizing fine-grained token-level reward models can substantially enhance\nthe performance of Proximal Policy Optimization (PPO) in aligning large\nlanguage models. However, it is challenging to leverage such token-level reward\nas guidance for Direct Preference Optimization (DPO), since DPO is formulated\nas a sequence-level bandit problem. To address this challenge, this work\ndecomposes the sequence-level PPO into a sequence of token-level proximal\npolicy optimization problems and then frames the problem of token-level PPO\nwith token-level reward guidance, from which closed-form optimal token-level\npolicy and the corresponding token-level reward can be derived. Using the\nobtained reward and Bradley-Terry model, this work establishes a framework of\ncomputable loss functions with token-level reward guidance for DPO, and\nproposes a practical reward guidance based on the induced DPO reward. This\nformulation enables different tokens to exhibit varying degrees of deviation\nfrom reference policy based on their respective rewards. Experiment results\ndemonstrate that our method achieves substantial performance improvements over\nDPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on\nAlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at\nhttps://github.com/dvlab-research/TGDPO.\n","authors":["Mingkang Zhu","Xi Chen","Zhongdao Wang","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2506.14574v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.14571v1","updated":"2025-06-17T14:28:14Z","published":"2025-06-17T14:28:14Z","title":"The Perception of Phase Intercept Distortion and its Application in Data\n  Augmentation","summary":"  Phase distortion refers to the alteration of the phase relationships between\nfrequencies in a signal, which can be perceptible. In this paper, we discuss a\nspecial case of phase distortion known as phase-intercept distortion, which is\ncreated by a frequency-independent phase shift. We hypothesize that, though\nthis form of distortion changes a signal's waveform significantly, the\ndistortion is imperceptible. Human-subject experiment results are reported\nwhich are consistent with this hypothesis. Furthermore, we discuss how the\nimperceptibility of phase-intercept distortion can be useful for machine\nlearning, specifically for data augmentation. We conducted multiple experiments\nusing phase-intercept distortion as a novel approach to data augmentation, and\nobtained improved results for audio machine learning tasks.\n","authors":["Venkatakrishnan Vaidyanathapuram Krishnan","Nathaniel Condit-Schultz"],"pdf_url":"https://arxiv.org/pdf/2506.14571v1.pdf","comment":"Submitted to the IEEE Workshop on Applications of Signal Processing\n  to Audio and Acoustics (WASPAA) 2025"},{"id":"http://arxiv.org/abs/2506.14563v1","updated":"2025-06-17T14:22:07Z","published":"2025-06-17T14:22:07Z","title":"Single-Example Learning in a Mixture of GPDMs with Latent Geometries","summary":"  We present the Gaussian process dynamical mixture model (GPDMM) and show its\nutility in single-example learning of human motion data. The Gaussian process\ndynamical model (GPDM) is a form of the Gaussian process latent variable model\n(GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM\ncombines multiple GPDMs in a probabilistic mixture-of-experts framework,\nutilizing embedded geometric features to allow for diverse sequences to be\nencoded in a single latent space, enabling the categorization and generation of\neach sequence class. GPDMs and our mixture model are particularly advantageous\nin addressing the challenges of modeling human movement in scenarios where data\nis limited and model interpretability is vital, such as in patient-specific\nmedical applications like prosthesis control. We score the GPDMM on\nclassification accuracy and generative ability in single-example learning,\nshowcase model variations, and benchmark it against LSTMs, VAEs, and\ntransformers.\n","authors":["Jesse St. Amand","Leonardo Gizzi","Martin A. Giese"],"pdf_url":"https://arxiv.org/pdf/2506.14563v1.pdf","comment":"13 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.02390v2","updated":"2025-06-17T14:21:52Z","published":"2025-06-03T03:10:20Z","title":"GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on\n  Granular-Ball Structure","summary":"  Adaptive Boosting (AdaBoost) faces significant challenges posed by label\nnoise, especially in multiclass classification tasks. Existing methods either\nlack mechanisms to handle label noise effectively or suffer from high\ncomputational costs due to redundant data usage. Inspired by granular\ncomputing, this paper proposes granular adaptive boosting (GAdaBoost), a novel\ntwo-stage framework comprising a data granulation stage and an adaptive\nboosting stage, to enhance efficiency and robustness under noisy conditions. To\nvalidate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is\nproposed. Specifically, first, a granular-ball generation method is designed to\ncompress data while preserving diversity and mitigating label noise. Second,\nthe granular ball-based SAMME algorithm focuses on granular balls rather than\nindividual samples, improving efficiency and reducing sensitivity to noise.\nExperimental results on some noisy datasets show that the proposed approach\nachieves superior robustness and efficiency compared with existing methods,\ndemonstrating that this work effectively extends AdaBoost and SAMME.\n","authors":["Qin Xie","Qinghua Zhang","Shuyin Xia","Xinran Zhou","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14562v1","updated":"2025-06-17T14:21:10Z","published":"2025-06-17T14:21:10Z","title":"AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs","summary":"  Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines.\n","authors":["Di He","Ajay Jaiswal","Songjun Tu","Li Shen","Ganzhao Yuan","Shiwei Liu","Lu Yin"],"pdf_url":"https://arxiv.org/pdf/2506.14562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14560v1","updated":"2025-06-17T14:15:39Z","published":"2025-06-17T14:15:39Z","title":"Risk Estimation of Knee Osteoarthritis Progression via Predictive\n  Multi-task Modelling from Efficient Diffusion Model using X-ray Images","summary":"  Medical imaging plays a crucial role in assessing knee osteoarthritis (OA)\nrisk by enabling early detection and disease monitoring. Recent machine\nlearning methods have improved risk estimation (i.e., predicting the likelihood\nof disease progression) and predictive modelling (i.e., the forecasting of\nfuture outcomes based on current data) using medical images, but clinical\nadoption remains limited due to their lack of interpretability. Existing\napproaches that generate future images for risk estimation are complex and\nimpractical. Additionally, previous methods fail to localize anatomical knee\nlandmarks, limiting interpretability. We address these gaps with a new\ninterpretable machine learning method to estimate the risk of knee OA\nprogression via multi-task predictive modelling that classifies future knee OA\nseverity and predicts anatomical knee landmarks from efficiently generated\nhigh-quality future images. Such image generation is achieved by leveraging a\ndiffusion model in a class-conditioned latent space to forecast disease\nprogression, offering a visual representation of how particular health\nconditions may evolve. Applied to the Osteoarthritis Initiative dataset, our\napproach improves the state-of-the-art (SOTA) by 2\\%, achieving an AUC of 0.71\nin predicting knee OA progression while offering ~9% faster inference time.\n","authors":["David Butler","Adrian Hilton","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2506.14560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14540v1","updated":"2025-06-17T14:01:39Z","published":"2025-06-17T14:01:39Z","title":"Aligning Evaluation with Clinical Priorities: Calibration, Label Shift,\n  and Error Costs","summary":"  Machine learning-based decision support systems are increasingly deployed in\nclinical settings, where probabilistic scoring functions are used to inform and\nprioritize patient management decisions. However, widely used scoring rules,\nsuch as accuracy and AUC-ROC, fail to adequately reflect key clinical\npriorities, including calibration, robustness to distributional shifts, and\nsensitivity to asymmetric error costs. In this work, we propose a principled\nyet practical evaluation framework for selecting calibrated thresholded\nclassifiers that explicitly accounts for the uncertainty in class prevalences\nand domain-specific cost asymmetries often found in clinical settings. Building\non the theory of proper scoring rules, particularly the Schervish\nrepresentation, we derive an adjusted variant of cross-entropy (log score) that\naverages cost-weighted performance over clinically relevant ranges of class\nbalance. The resulting evaluation is simple to apply, sensitive to clinical\ndeployment conditions, and designed to prioritize models that are both\ncalibrated and robust to real-world variations.\n","authors":["Gerardo A. Flores","Alyssa H. Smith","Julia A. Fukuyama","Ashia C. Wilson"],"pdf_url":"https://arxiv.org/pdf/2506.14540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14530v1","updated":"2025-06-17T13:55:13Z","published":"2025-06-17T13:55:13Z","title":"Sharp Generalization Bounds for Foundation Models with Asymmetric\n  Randomized Low-Rank Adapters","summary":"  Low-Rank Adaptation (LoRA) has emerged as a widely adopted\nparameter-efficient fine-tuning (PEFT) technique for foundation models. Recent\nwork has highlighted an inherent asymmetry in the initialization of LoRA's\nlow-rank factors, which has been present since its inception and was presumably\nderived experimentally. This paper focuses on providing a comprehensive\ntheoretical characterization of asymmetric LoRA with frozen random factors.\nFirst, while existing research provides upper-bound generalization guarantees\nbased on averages over multiple experiments, the behaviour of a single\nfine-tuning run with specific random factors remains an open question. We\naddress this by investigating the concentration of the typical LoRA\ngeneralization gap around its mean. Our main upper bound reveals a sample\ncomplexity of $\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{r}}{\\sqrt{N}}\\right)$ with\nhigh probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we\nalso determine the fundamental limits in terms of sample efficiency,\nestablishing a matching lower bound of\n$\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)$. By more closely reflecting the\npractical scenario of a single fine-tuning run, our findings offer crucial\ninsights into the reliability and practicality of asymmetric LoRA.\n","authors":["Anastasis Kratsios","Tin Sum Cheng","Aurelien Lucchi","Haitz Sáez de Ocáriz Borde"],"pdf_url":"https://arxiv.org/pdf/2506.14530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14529v1","updated":"2025-06-17T13:53:48Z","published":"2025-06-17T13:53:48Z","title":"Automated Decision-Making on Networks with LLMs through Knowledge-Guided\n  Evolution","summary":"  Effective decision-making on networks often relies on learning from\ngraph-structured data, where Graph Neural Networks (GNNs) play a central role,\nbut they take efforts to configure and tune. In this demo, we propose LLMNet,\nshowing how to design GNN automated through Large Language Models. Our system\ndevelops a set of agents that construct graph-related knowlege bases and then\nleverages Retrieval-Augmented Generation (RAG) to support automated\nconfiguration and refinement of GNN models through a knowledge-guided evolution\nprocess. These agents, equipped with specialized knowledge bases, extract\ninsights into tasks and graph structures by interacting with the knowledge\nbases. Empirical results show LLMNet excels in twelve datasets across three\ngraph learning tasks, validating its effectiveness of GNN model designing.\n","authors":["Xiaohan Zheng","Lanning Wei","Yong Li","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2506.14529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14521v1","updated":"2025-06-17T13:48:38Z","published":"2025-06-17T13:48:38Z","title":"Towards Improved Research Methodologies for Industrial AI: A case study\n  of false call reduction","summary":"  Are current artificial intelligence (AI) research methodologies ready to\ncreate successful, productive, and profitable AI applications? This work\npresents a case study on an industrial AI use case called false call reduction\nfor automated optical inspection to demonstrate the shortcomings of current\nbest practices. We identify seven weaknesses prevalent in related peer-reviewed\nwork and experimentally show their consequences. We show that the best-practice\nmethodology would fail for this use case. We argue amongst others for the\nnecessity of requirement-aware metrics to ensure achieving business objectives,\nclear definitions of success criteria, and a thorough analysis of temporal\ndynamics in experimental datasets. Our work encourages researchers to\ncritically assess their methodologies for more successful applied AI research.\n","authors":["Korbinian Pfab","Marcel Rothering"],"pdf_url":"https://arxiv.org/pdf/2506.14521v1.pdf","comment":"Submitted and accepted to IEEE COMPSAC 2025"},{"id":"http://arxiv.org/abs/2506.14518v1","updated":"2025-06-17T13:46:32Z","published":"2025-06-17T13:46:32Z","title":"Two-Player Zero-Sum Games with Bandit Feedback","summary":"  We study a two-player zero-sum game (TPZSG) in which the row player aims to\nmaximize their payoff against an adversarial column player, under an unknown\npayoff matrix estimated through bandit feedback. We propose and analyze two\nalgorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and\nETC-TPZSG-AE, which improves upon it by incorporating an action pair\nelimination (AE) strategy that leverages the $\\varepsilon$-Nash Equilibrium\nproperty to efficiently select the optimal action pair. Our objective is to\ndemonstrate the applicability of ETC in a TPZSG setting by focusing on learning\npure strategy Nash Equilibrium. A key contribution of our work is a derivation\nof instance-dependent upper bounds on the expected regret for both algorithms,\nhas received limited attention in the literature on zero-sum games.\nParticularly, after $T$ rounds, we achieve an instance-dependent regret upper\nbounds of $O(\\Delta + \\sqrt{T})$ for ETC-TPZSG and $O(\\frac{\\log (T\n\\Delta^2)}{\\Delta})$ for ETC-TPZSG-AE, where $\\Delta$ denotes the suboptimality\ngap. Therefore, our results indicate that ETC-based algorithms perform\neffectively in adversarial game settings, achieving regret bounds comparable to\nexisting methods while providing insights through instance-dependent analysis.\n","authors":["Elif Yılmaz","Christos Dimitrakakis"],"pdf_url":"https://arxiv.org/pdf/2506.14518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14515v1","updated":"2025-06-17T13:40:48Z","published":"2025-06-17T13:40:48Z","title":"Train Once, Forget Precisely: Anchored Optimization for Efficient\n  Post-Hoc Unlearning","summary":"  As machine learning systems increasingly rely on data subject to privacy\nregulation, selectively unlearning specific information from trained models has\nbecome essential. In image classification, this involves removing the influence\nof particular training samples, semantic classes, or visual styles without full\nretraining. We introduce \\textbf{Forget-Aligned Model Reconstruction (FAMR)}, a\ntheoretically grounded and computationally efficient framework for post-hoc\nunlearning in deep image classifiers. FAMR frames forgetting as a constrained\noptimization problem that minimizes a uniform-prediction loss on the forget set\nwhile anchoring model parameters to their original values via an $\\ell_2$\npenalty. A theoretical analysis links FAMR's solution to\ninfluence-function-based retraining approximations, with bounds on parameter\nand output deviation. Empirical results on class forgetting tasks using\nCIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong\nperformance retention and minimal computational overhead. The framework\ngeneralizes naturally to concept and style erasure, offering a scalable and\ncertifiable route to efficient post-hoc forgetting in vision models.\n","authors":["Prabhav Sanga","Jaskaran Singh","Arun K. Dubey"],"pdf_url":"https://arxiv.org/pdf/2506.14515v1.pdf","comment":"Accepted at ICML MUGen'25"},{"id":"http://arxiv.org/abs/2302.09049v2","updated":"2025-06-17T13:31:48Z","published":"2023-02-17T18:27:27Z","title":"Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy","summary":"  We construct multiperiodic processes -- a simple example of stationary\nergodic stochastic processes over natural numbers that enjoy the vanishing\nentropy rate under a mild condition. Multiperiodic processes are supported on\nrandomly shifted deterministic sequences called multiperiodic sequences, which\ncan be efficiently generated using an algorithm called the Infinite Clock.\nUnder a suitable parameterization, multiperiodic sequences exhibit relative\nfrequencies of particular numbers given by Zipf's law. Exactly in the same\nsetting, the respective multiperiodic processes satisfy an asymptotic power-law\ngrowth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold\nfor statistical language models, in particular.\n","authors":["Łukasz Dębowski"],"pdf_url":"https://arxiv.org/pdf/2302.09049v2.pdf","comment":"22 pages; 1 figure"},{"id":"http://arxiv.org/abs/2503.22313v2","updated":"2025-06-17T13:15:03Z","published":"2025-03-28T10:42:52Z","title":"Hybrid Time-Domain Behavior Model Based on Neural Differential Equations\n  and RNNs","summary":"  Nonlinear dynamics system identification is crucial for circuit emulation.\nTraditional continuous-time domain modeling approaches have limitations in\nfitting capability and computational efficiency when used for modeling circuit\nIPs and device behaviors.This paper presents a novel continuous-time domain\nhybrid modeling paradigm. It integrates neural network differential models with\nrecurrent neural networks (RNNs), creating NODE-RNN and NCDE-RNN models based\non neural ordinary differential equations (NODE) and neural controlled\ndifferential equations (NCDE), respectively.Theoretical analysis shows that\nthis hybrid model has mathematical advantages in event-driven dynamic mutation\nresponse and gradient propagation stability. Validation using real data from\nPIN diodes in high-power microwave environments shows NCDE-RNN improves fitting\naccuracy by 33\\% over traditional NCDE, and NODE-RNN by 24\\% over CTRNN,\nespecially in capturing nonlinear memory effects.The model has been\nsuccessfully deployed in Verilog-A and validated through circuit emulation,\nconfirming its compatibility with existing platforms and practical value.This\nhybrid dynamics paradigm, by restructuring the neural differential equation\nsolution path, offers new ideas for high-precision circuit time-domain modeling\nand is significant for complex nonlinear circuit system modeling.\n","authors":["Zenghui Chang","Yang Zhang","Hu Tan","Hong Cai Chen"],"pdf_url":"https://arxiv.org/pdf/2503.22313v2.pdf","comment":"7 pages,5 figures"},{"id":"http://arxiv.org/abs/2506.08673v3","updated":"2025-06-17T13:14:47Z","published":"2025-06-10T10:33:21Z","title":"Towards Fair Representation: Clustering and Consensus","summary":"  Consensus clustering, a fundamental task in machine learning and data\nanalysis, aims to aggregate multiple input clusterings of a dataset,\npotentially based on different non-sensitive attributes, into a single\nclustering that best represents the collective structure of the data. In this\nwork, we study this fundamental problem through the lens of fair clustering, as\nintroduced by Chierichetti et al. [NeurIPS'17], which incorporates the\ndisparate impact doctrine to ensure proportional representation of each\nprotected group in the dataset within every cluster. Our objective is to find a\nconsensus clustering that is not only representative but also fair with respect\nto specific protected attributes. To the best of our knowledge, we are the\nfirst to address this problem and provide a constant-factor approximation.\n  As part of our investigation, we examine how to minimally modify an existing\nclustering to enforce fairness -- an essential postprocessing step in many\nclustering applications that require fair representation. We develop an optimal\nalgorithm for datasets with equal group representation and near-linear time\nconstant factor approximation algorithms for more general scenarios with\ndifferent proportions of two group sizes. We complement our approximation\nresult by showing that the problem is NP-hard for two unequal-sized groups.\nGiven the fundamental nature of this problem, we believe our results on Closest\nFair Clustering could have broader implications for other clustering problems,\nparticularly those for which no prior approximation guarantees exist for their\nfair variants.\n","authors":["Diptarka Chakraborty","Kushagra Chatterjee","Debarati Das","Tien Long Nguyen","Romina Nobahari"],"pdf_url":"https://arxiv.org/pdf/2506.08673v3.pdf","comment":"The paper has been accepted at the Conference on Learning Theory\n  (COLT) 2025. We have fixed some typos in the theorem statements"},{"id":"http://arxiv.org/abs/2506.14488v1","updated":"2025-06-17T13:09:11Z","published":"2025-06-17T13:09:11Z","title":"Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced\n  Aligned Diffusion","summary":"  Breakthroughs in high-accuracy protein structure prediction, such as\nAlphaFold, have established receptor-based molecule design as a critical driver\nfor rapid early-phase drug discovery. However, most approaches still struggle\nto balance pocket-specific geometric fit with strict valence and synthetic\nconstraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion\ntermed READ is introduced, which is the first to merge molecular\nRetrieval-Augmented Generation with an SE(3)-equivariant diffusion model.\nSpecifically, a contrastively pre-trained encoder aligns atom-level\nrepresentations during training, then retrieves graph embeddings of\npocket-matched scaffolds to guide each reverse-diffusion step at inference.\nThis single mechanism can inject real-world chemical priors exactly where\nneeded, producing valid, diverse, and shape-complementary ligands. Experimental\nresults demonstrate that READ can achieve very competitive performance in\nCBGBench, surpassing state-of-the-art generative models and even native\nligands. That suggests retrieval and diffusion can be co-optimized for faster,\nmore reliable structure-based drug design.\n","authors":["Dong Xu","Zhangfan Yang","Ka-chun Wong","Zexuan Zhu","Jiangqiang Li","Junkai Ji"],"pdf_url":"https://arxiv.org/pdf/2506.14488v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.15664v5","updated":"2025-06-17T12:57:49Z","published":"2024-06-21T21:44:27Z","title":"Flat Posterior Does Matter For Bayesian Model Averaging","summary":"  Bayesian neural networks (BNNs) estimate the posterior distribution of model\nparameters and utilize posterior samples for Bayesian Model Averaging (BMA) in\nprediction. However, despite the crucial role of flatness in the loss landscape\nin improving the generalization of neural networks, its impact on BMA has been\nlargely overlooked. In this work, we explore how posterior flatness influences\nBMA generalization and empirically demonstrate that (1) most approximate\nBayesian inference methods fail to yield a flat posterior and (2) BMA\npredictions, without considering posterior flatness, are less effective at\nimproving generalization. To address this, we propose Flat Posterior-aware\nBayesian Model Averaging (FP-BMA), a novel training objective that explicitly\nencourages flat posteriors in a principled Bayesian manner. We also introduce a\nFlat Posterior-aware Bayesian Transfer Learning scheme that enhances\ngeneralization in downstream tasks. Empirically, we show that FP-BMA\nsuccessfully captures flat posteriors, improving generalization performance.\n","authors":["Sungjun Lim","Jeyoon Yeom","Sooyon Kim","Hoyoon Byun","Jinho Kang","Yohan Jung","Jiyoung Jung","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2406.15664v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14479v1","updated":"2025-06-17T12:57:33Z","published":"2025-06-17T12:57:33Z","title":"Adaptive Data Augmentation for Thompson Sampling","summary":"  In linear contextual bandits, the objective is to select actions that\nmaximize cumulative rewards, modeled as a linear function with unknown\nparameters. Although Thompson Sampling performs well empirically, it does not\nachieve optimal regret bounds. This paper proposes a nearly minimax optimal\nThompson Sampling for linear contextual bandits by developing a novel estimator\nwith the adaptive augmentation and coupling of the hypothetical samples that\nare designed for efficient parameter learning. The proposed estimator\naccurately predicts rewards for all arms without relying on assumptions for the\ncontext distribution. Empirical results show robust performance and significant\nimprovement over existing methods.\n","authors":["Wonyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2506.14479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06499v2","updated":"2025-06-17T12:55:30Z","published":"2025-06-06T19:49:42Z","title":"SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity\n  Algorithms","summary":"  Large language model (LLM) driven synthetic data generation has emerged as a\npowerful method for improving model reasoning capabilities. However, most\nmethods either distill large state-of-the-art models into small students or use\nnatural ground-truth problem statements to guarantee problem statement quality.\nThis limits the scalability of these approaches to more complex and diverse\nproblem domains. To address this, we present SPARQ: Synthetic Problem\nGeneration for Reasoning via Quality-Diversity Algorithms, a novel approach for\ngenerating high-quality and diverse synthetic math problem and solution pairs\nusing only a single model by measuring a problem's solve-rate: a proxy for\nproblem difficulty. Starting from a seed dataset of 7.5K samples, we generate\nover 20 million new problem-solution pairs. We show that filtering the\ngenerated data by difficulty and then fine-tuning the same model on the\nresulting data improves relative model performance by up to 24\\%. Additionally,\nwe conduct ablations studying the impact of synthetic data quantity, quality\nand diversity on model generalization. We find that higher quality, as measured\nby problem difficulty, facilitates better in-distribution performance. Further,\nwhile generating diverse synthetic data does not as strongly benefit\nin-distribution performance, filtering for more diverse data facilitates more\nrobust OOD generalization. We also confirm the existence of model and data\nscaling laws for synthetically generated problems, which positively benefit\ndownstream model generalization.\n","authors":["Alex Havrilla","Edward Hughes","Mikayel Samvelyan","Jacob Abernethy"],"pdf_url":"https://arxiv.org/pdf/2506.06499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23145v2","updated":"2025-06-17T12:51:50Z","published":"2025-05-29T06:33:16Z","title":"FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image\n  Editing","summary":"  Recent inversion-free, flow-based image editing methods such as FlowEdit\nleverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,\nenabling text-driven manipulation by solving an ordinary differential equation\n(ODE). While the lack of exact latent inversion is a core advantage of these\nmethods, it often results in unstable editing trajectories and poor source\nconsistency. To address this limitation, we propose FlowAlign, a novel\ninversion-free flow-based framework for consistent image editing with\nprincipled trajectory control. FlowAlign introduces a flow-matching loss as a\nregularization mechanism to promote smoother and more stable trajectories\nduring the editing process. Notably, the flow-matching loss is shown to\nexplicitly balance semantic alignment with the edit prompt and structural\nconsistency with the source image along the trajectory. Furthermore, FlowAlign\nnaturally supports reverse editing by simply reversing the ODE trajectory,\nhighlighting the reversible and consistent nature of the transformation.\nExtensive experiments demonstrate that FlowAlign outperforms existing methods\nin both source preservation and editing controllability.\n","authors":["Jeongsol Kim","Yeobin Hong","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2505.23145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04907v3","updated":"2025-06-17T12:42:05Z","published":"2025-02-07T13:23:40Z","title":"Scalable and consistent embedding of probability measures into Hilbert\n  spaces via measure quantization","summary":"  This paper is focused on statistical learning from data that come as\nprobability measures. In this setting, popular approaches consist in embedding\nsuch data into a Hilbert space with either Linearized Optimal Transport or\nKernel Mean Embedding. However, the cost of computing such embeddings prohibits\ntheir direct use in large-scale settings. We study two methods based on measure\nquantization for approximating input probability measures with discrete\nmeasures of small-support size. The first one is based on optimal quantization\nof each input measure, while the second one relies on mean-measure\nquantization. We study the consistency of such approximations, and its\nimplication for scalable embeddings of probability measures into a Hilbert\nspace at a low computational cost. We finally illustrate our findings with\nvarious numerical experiments.\n","authors":["Erell Gachon","Elsa Cazelles","Jérémie Bigot"],"pdf_url":"https://arxiv.org/pdf/2502.04907v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v6","updated":"2025-06-17T12:41:10Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimple manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, object relocation, and hammer striking demonstrate\nDexHandDiff's effectiveness on goals outside training distributions, achieving\nover twice the average success rate (59.2% vs. 29.5%) compared to existing\nmethods. Our framework achieves an average of 70.7% success rate on goal\nadaptive dexterous tasks, highlighting its robustness and flexibility in\ncontact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v6.pdf","comment":"Accepted by CVPR 2025. Camera ready version. Previous DexDiffuser.\n  Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2506.14473v1","updated":"2025-06-17T12:37:24Z","published":"2025-06-17T12:37:24Z","title":"Foundation Model Insights and a Multi-Model Approach for Superior\n  Fine-Grained One-shot Subset Selection","summary":"  One-shot subset selection serves as an effective tool to reduce deep learning\ntraining costs by identifying an informative data subset based on the\ninformation extracted by an information extractor (IE). Traditional IEs,\ntypically pre-trained on the target dataset, are inherently dataset-dependent.\nFoundation models (FMs) offer a promising alternative, potentially mitigating\nthis limitation. This work investigates two key questions: (1) Can FM-based\nsubset selection outperform traditional IE-based methods across diverse\ndatasets? (2) Do all FMs perform equally well as IEs for subset selection?\nExtensive experiments uncovered surprising insights: FMs consistently\noutperform traditional IEs on fine-grained datasets, whereas their advantage\ndiminishes on coarse-grained datasets with noisy labels. Motivated by these\nfinding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a\nmethod tailored for fine-grained image datasets. RAM-APL leverages multiple FMs\nto enhance subset selection by exploiting their complementary strengths. Our\napproach achieves state-of-the-art performance on fine-grained datasets,\nincluding Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.\n","authors":["Zhijing Wan","Zhixiang Wang","Zheng Wang","Xin Xu","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2506.14473v1.pdf","comment":"18 pages, 10 figures, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.14472v1","updated":"2025-06-17T12:35:24Z","published":"2025-06-17T12:35:24Z","title":"Leveraging External Factors in Household-Level Electrical Consumption\n  Forecasting using Hypernetworks","summary":"  Accurate electrical consumption forecasting is crucial for efficient energy\nmanagement and resource allocation. While traditional time series forecasting\nrelies on historical patterns and temporal dependencies, incorporating external\nfactors -- such as weather indicators -- has shown significant potential for\nimproving prediction accuracy in complex real-world applications. However, the\ninclusion of these additional features often degrades the performance of global\npredictive models trained on entire populations, despite improving individual\nhousehold-level models. To address this challenge, we found that a hypernetwork\narchitecture can effectively leverage external factors to enhance the accuracy\nof global electrical consumption forecasting models, by specifically adjusting\nthe model weights to each consumer.\n  We collected a comprehensive dataset spanning two years, comprising\nconsumption data from over 6000 luxembourgish households and corresponding\nexternal factors such as weather indicators, holidays, and major local events.\nBy comparing various forecasting models, we demonstrate that a hypernetwork\napproach outperforms existing methods when associated to external factors,\nreducing forecasting errors and achieving the best accuracy while maintaining\nthe benefits of a global model.\n","authors":["Fabien Bernier","Maxime Cordy","Yves Le Traon"],"pdf_url":"https://arxiv.org/pdf/2506.14472v1.pdf","comment":"ECML PKDD 2025"},{"id":"http://arxiv.org/abs/2506.14464v1","updated":"2025-06-17T12:27:25Z","published":"2025-06-17T12:27:25Z","title":"A Scalable Hybrid Training Approach for Recurrent Spiking Neural\n  Networks","summary":"  Recurrent spiking neural networks (RSNNs) can be implemented very efficiently\nin neuromorphic systems. Nevertheless, training of these models with powerful\ngradient-based learning algorithms is mostly performed on standard digital\nhardware using Backpropagation through time (BPTT). However, BPTT has\nsubstantial limitations. It does not permit online training and its memory\nconsumption scales linearly with the number of computation steps. In contrast,\nlearning methods using forward propagation of gradients operate in an online\nmanner with a memory consumption independent of the number of time steps. These\nmethods enable SNNs to learn from continuous, infinite-length input sequences.\nYet, slow execution speed on conventional hardware as well as inferior\nperformance has hindered their widespread application. In this work, we\nintroduce HYbrid PRopagation (HYPR) that combines the efficiency of\nparallelization with approximate online forward learning. Our algorithm yields\nhigh-throughput online learning through parallelization, paired with constant,\ni.e., sequence length independent, memory demands. HYPR enables parallelization\nof parameter update computation over the sub sequences for RSNNs consisting of\nalmost arbitrary non-linear spiking neuron models. We apply HYPR to networks of\nspiking neurons with oscillatory subthreshold dynamics. We find that this type\nof neuron model is particularly well trainable by HYPR, resulting in an\nunprecedentedly low task performance gap between approximate forward gradient\nlearning and BPTT.\n","authors":["Maximilian Baronig","Yeganeh Bahariasl","Ozan Özdenizci","Robert Legenstein"],"pdf_url":"https://arxiv.org/pdf/2506.14464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11384v2","updated":"2025-06-17T12:23:08Z","published":"2025-01-20T10:24:33Z","title":"Transductive Conformal Inference for Full Ranking","summary":"  We introduce a method based on Conformal Prediction (CP) to quantify the\nuncertainty of full ranking algorithms. We focus on a specific scenario where\n$n+m$ items are to be ranked by some ``black box'' algorithm. It is assumed\nthat the relative (ground truth) ranking of $n$ of them is known. The objective\nis then to quantify the error made by the algorithm on the ranks of the $m$ new\nitems among the total $(n+m)$. In such a setting, the true ranks of the $n$\noriginal items in the total $(n+m)$ depend on the (unknown) true ranks of the\n$m$ new ones. Consequently, we have no direct access to a calibration set to\napply a classical CP method. To address this challenge, we propose to construct\ndistribution-free bounds of the unknown conformity scores using recent results\non the distribution of conformal p-values. Using these scores upper bounds, we\nprovide valid prediction sets for the rank of any item. We also control the\nfalse coverage proportion, a crucial quantity when dealing with multiple\nprediction sets. Finally, we empirically show on both synthetic and real data\nthe efficiency of our CP method for state-of-the-art algorithms such as RankNet\nor LambdaMart.\n","authors":["Jean-Baptiste Fermanian","Pierre Humbert","Gilles Blanchard"],"pdf_url":"https://arxiv.org/pdf/2501.11384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14460v1","updated":"2025-06-17T12:20:04Z","published":"2025-06-17T12:20:04Z","title":"Zeroth-Order Optimization is Secretly Single-Step Policy Optimization","summary":"  Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing\nfunctions where explicit gradients are unavailable or expensive to compute.\nHowever, the underlying mechanisms of popular ZOO methods, particularly those\nemploying randomized finite differences, and their connection to other\noptimization paradigms like Reinforcement Learning (RL) are not fully\nelucidated. This paper establishes a fundamental and previously unrecognized\nconnection: ZOO with finite differences is equivalent to a specific instance of\nsingle-step Policy Optimization (PO). We formally unveil that the implicitly\nsmoothed objective function optimized by common ZOO algorithms is identical to\na single-step PO objective. Furthermore, we show that widely used ZOO gradient\nestimators, are mathematically equivalent to the REINFORCE gradient estimator\nwith a specific baseline function, revealing the variance-reducing mechanism in\nZOO from a PO perspective.Built on this unified framework, we propose ZoAR\n(Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO\nalgorithm incorporating PO-inspired variance reduction techniques: an averaged\nbaseline from recent evaluations and query reuse analogous to experience\nreplay. Our theoretical analysis further substantiates these techniques reduce\nvariance and enhance convergence. Extensive empirical studies validate our\ntheory and demonstrate that ZoAR significantly outperforms other methods in\nterms of convergence speed and final performance. Overall, our work provides a\nnew theoretical lens for understanding ZOO and offers practical algorithmic\nimprovements derived from its connection to PO.\n","authors":["Junbin Qiu","Zhengpeng Xie","Xiangda Yan","Yongjie Yang","Yao Shu"],"pdf_url":"https://arxiv.org/pdf/2506.14460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14459v1","updated":"2025-06-17T12:19:40Z","published":"2025-06-17T12:19:40Z","title":"A Model-Mediated Stacked Ensemble Approach for Depression Prediction\n  Among Professionals","summary":"  Depression is a significant mental health concern, particularly in\nprofessional environments where work-related stress, financial pressure, and\nlifestyle imbalances contribute to deteriorating well-being. Despite increasing\nawareness, researchers and practitioners face critical challenges in developing\naccurate and generalizable predictive models for mental health disorders.\nTraditional classification approaches often struggle with the complexity of\ndepression, as it is influenced by multifaceted, interdependent factors,\nincluding occupational stress, sleep patterns, and job satisfaction. This study\naddresses these challenges by proposing a stacking-based ensemble learning\napproach to improve the predictive accuracy of depression classification among\nprofessionals. The Depression Professional Dataset has been collected from\nKaggle. The dataset comprises demographic, occupational, and lifestyle\nattributes that influence mental well-being. Our stacking model integrates\nmultiple base learners with a logistic regression-mediated model, effectively\ncapturing diverse learning patterns. The experimental results demonstrate that\nthe proposed model achieves high predictive performance, with an accuracy of\n99.64% on training data and 98.75% on testing data, with precision, recall, and\nF1-score all exceeding 98%. These findings highlight the effectiveness of\nensemble learning in mental health analytics and underscore its potential for\nearly detection and intervention strategies.\n","authors":["Md. Mortuza Ahmmed","Abdullah Al Noman","Mahin Montasir Afif","K. M. Tahsin Kabir","Md. Mostafizur Rahman","Mufti Mahmud"],"pdf_url":"https://arxiv.org/pdf/2506.14459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14457v1","updated":"2025-06-17T12:18:37Z","published":"2025-06-17T12:18:37Z","title":"Dataset distillation for memorized data: Soft labels can leak held-out\n  teacher knowledge","summary":"  Dataset distillation aims to compress training data into fewer examples via a\nteacher, from which a student can learn effectively. While its success is often\nattributed to structure in the data, modern neural networks also memorize\nspecific facts, but if and how such memorized information is can transferred in\ndistillation settings remains less understood. In this work, we show that\nstudents trained on soft labels from teachers can achieve non-trivial accuracy\non held-out memorized data they never directly observed. This effect persists\non structured data when the teacher has not generalized.To analyze it in\nisolation, we consider finite random i.i.d. datasets where generalization is a\npriori impossible and a successful teacher fit implies pure memorization.\nStill, students can learn non-trivial information about the held-out data, in\nsome cases up to perfect accuracy. In those settings, enough soft labels are\navailable to recover the teacher functionally - the student matches the\nteacher's predictions on all possible inputs, including the held-out memorized\ndata. We show that these phenomena strongly depend on the temperature with\nwhich the logits are smoothed, but persist across varying network capacities,\narchitectures and dataset compositions.\n","authors":["Freya Behrens","Lenka Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2506.14457v1.pdf","comment":"9 pages, 21 figures"},{"id":"http://arxiv.org/abs/2506.14449v1","updated":"2025-06-17T12:14:02Z","published":"2025-06-17T12:14:02Z","title":"Detecting immune cells with label-free two-photon autofluorescence and\n  deep learning","summary":"  Label-free imaging has gained broad interest because of its potential to omit\nelaborate staining procedures which is especially relevant for in vivo use.\nLabel-free multiphoton microscopy (MPM), for instance, exploits two-photon\nexcitation of natural autofluorescence (AF) from native, metabolic proteins,\nmaking it ideal for in vivo endomicroscopy. Deep learning (DL) models have been\nwidely used in other optical imaging technologies to predict specific target\nannotations and thereby digitally augment the specificity of these label-free\nimages. However, this computational specificity has only rarely been\nimplemented for MPM. In this work, we used a data set of label-free MPM images\nfrom a series of different immune cell types (5,075 individual cells for binary\nclassification in mixed samples and 3,424 cells for a multi-class\nclassification task) and trained a convolutional neural network (CNN) to\nclassify cell types based on this label-free AF as input. A low-complexity\nsqueezeNet architecture was able to achieve reliable immune cell classification\nresults (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples;\n0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class\nclassification in isolated samples). Perturbation tests confirmed that the\nmodel is not confused by extracellular environment and that both input AF\nchannels (NADH and FAD) are about equally important to the classification. In\nthe future, such predictive DL models could directly detect specific immune\ncells in unstained images and thus, computationally improve the specificity of\nlabel-free MPM which would have great potential for in vivo endomicroscopy.\n","authors":["Lucas Kreiss","Amey Chaware","Maryam Roohian","Sarah Lemire","Oana-Maria Thoma","Birgitta Carlé","Maximilian Waldner","Sebastian Schürmann","Oliver Friedrich","Roarke Horstmeyer"],"pdf_url":"https://arxiv.org/pdf/2506.14449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13662v3","updated":"2025-06-17T12:04:49Z","published":"2025-02-19T12:14:52Z","title":"Generalization error bound for denoising score matching under relaxed\n  manifold assumption","summary":"  We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.\n","authors":["Konstantin Yakovlev","Nikita Puchkin"],"pdf_url":"https://arxiv.org/pdf/2502.13662v3.pdf","comment":"Accepted for presentation at the 38th Annual Conference on Learning\n  Theory (COLT 2025)"},{"id":"http://arxiv.org/abs/2506.14440v1","updated":"2025-06-17T12:00:23Z","published":"2025-06-17T12:00:23Z","title":"Model compression using knowledge distillation with integrated gradients","summary":"  Model compression is critical for deploying deep learning models on\nresource-constrained devices. We introduce a novel method enhancing knowledge\ndistillation with integrated gradients (IG) as a data augmentation strategy.\nOur approach overlays IG maps onto input images during training, providing\nstudent models with deeper insights into teacher models' decision-making\nprocesses. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented\nknowledge distillation achieves 92.6% testing accuracy with a 4.1x compression\nfactor-a significant 1.1 percentage point improvement ($p<0.001$) over\nnon-distilled models (91.5%). This compression reduces inference time from 140\nms to 13 ms. Our method precomputes IG maps before training, transforming\nsubstantial runtime costs into a one-time preprocessing step. Our comprehensive\nexperiments include: (1) comparisons with attention transfer, revealing\ncomplementary benefits when combined with our approach; (2) Monte Carlo\nsimulations confirming statistical robustness; (3) systematic evaluation of\ncompression factor versus accuracy trade-offs across a wide range (2.2x-1122x);\nand (4) validation on an ImageNet subset aligned with CIFAR-10 classes,\ndemonstrating generalisability beyond the initial dataset. These extensive\nablation studies confirm that IG-based knowledge distillation consistently\noutperforms conventional approaches across varied architectures and compression\nratios. Our results establish this framework as a viable compression technique\nfor real-world deployment on edge devices while maintaining competitive\naccuracy.\n","authors":["David E. Hernandez","Jose Chang","Torbjörn E. M. Nordling"],"pdf_url":"https://arxiv.org/pdf/2506.14440v1.pdf","comment":"49 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.14439v1","updated":"2025-06-17T11:58:11Z","published":"2025-06-17T11:58:11Z","title":"A General Framework for Off-Policy Learning with Partially-Observed\n  Reward","summary":"  Off-policy learning (OPL) in contextual bandits aims to learn a\ndecision-making policy that maximizes the target rewards by using only\nhistorical interaction data collected under previously developed policies.\nUnfortunately, when rewards are only partially observed, the effectiveness of\nOPL degrades severely. Well-known examples of such partial rewards include\nexplicit ratings in content recommendations, conversion signals on e-commerce\nplatforms that are partial due to delay, and the issue of censoring in medical\nproblems. One possible solution to deal with such partial rewards is to use\nsecondary rewards, such as dwelling time, clicks, and medical indicators, which\nare more densely observed. However, relying solely on such secondary rewards\ncan also lead to poor policy learning since they may not align with the target\nreward. Thus, this work studies a new and general problem of OPL where the goal\nis to learn a policy that maximizes the expected target reward by leveraging\ndensely observed secondary rewards as supplemental data. We then propose a new\nmethod called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR),\nwhich effectively uses the secondary rewards in addition to the\npartially-observed target reward to achieve effective OPL despite the\nchallenging scenario. We also discuss a case where we aim to optimize not only\nthe expected target reward but also the expected secondary rewards to some\nextent; counter-intuitively, we will show that leveraging the two objectives is\nin fact advantageous also for the optimization of only the target reward. Along\nwith statistical analysis of our proposed methods, empirical evaluations on\nboth synthetic and real-world data show that HyPeR outperforms existing methods\nin various scenarios.\n","authors":["Rikiya Takehi","Masahiro Asami","Kosuke Kawakami","Yuta Saito"],"pdf_url":"https://arxiv.org/pdf/2506.14439v1.pdf","comment":"10 pages, 5 figures. Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2506.14438v1","updated":"2025-06-17T11:58:07Z","published":"2025-06-17T11:58:07Z","title":"sHGCN: Simplified hyperbolic graph convolutional neural networks","summary":"  Hyperbolic geometry has emerged as a powerful tool for modeling complex,\nstructured data, particularly where hierarchical or tree-like relationships are\npresent. By enabling embeddings with lower distortion, hyperbolic neural\nnetworks offer promising alternatives to Euclidean-based models for capturing\nintricate data structures. Despite these advantages, they often face\nperformance challenges, particularly in computational efficiency and tasks\nrequiring high precision. In this work, we address these limitations by\nsimplifying key operations within hyperbolic neural networks, achieving notable\nimprovements in both runtime and performance. Our findings demonstrate that\nstreamlined hyperbolic operations can lead to substantial gains in\ncomputational speed and predictive accuracy, making hyperbolic neural networks\na more viable choice for a broader range of applications.\n","authors":["Pol Arévalo","Alexis Molina","Álvaro Ciudad"],"pdf_url":"https://arxiv.org/pdf/2506.14438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19834v5","updated":"2025-06-17T11:56:41Z","published":"2025-02-27T07:14:11Z","title":"Knowledge Bridger: Towards Training-free Missing Modality Completion","summary":"  Previous successful approaches to missing modality completion rely on\ncarefully designed fusion techniques and extensive pre-training on complete\ndata, which can limit their generalizability in out-of-domain (OOD) scenarios.\nIn this study, we pose a new challenge: can we develop a missing modality\ncompletion model that is both resource-efficient and robust to OOD\ngeneralization? To address this, we present a training-free framework for\nmissing modality completion that leverages large multimodal models (LMMs). Our\napproach, termed the \"Knowledge Bridger\", is modality-agnostic and integrates\ngeneration and ranking of missing modalities. By defining domain-specific\npriors, our method automatically extracts structured information from available\nmodalities to construct knowledge graphs. These extracted graphs connect the\nmissing modality generation and ranking modules through the LMM, resulting in\nhigh-quality imputations of missing modalities. Experimental results across\nboth general and medical domains show that our approach consistently\noutperforms competing methods, including in OOD generalization. Additionally,\nour knowledge-driven generation and ranking techniques demonstrate superiority\nover variants that directly employ LMMs for generation and ranking, offering\ninsights that may be valuable for applications in other domains.\n","authors":["Guanzhou Ke","Shengfeng He","Xiao Li Wang","Bo Wang","Guoqing Chao","Yuanyang Zhang","Yi Xie","HeXing Su"],"pdf_url":"https://arxiv.org/pdf/2502.19834v5.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2506.14436v1","updated":"2025-06-17T11:55:08Z","published":"2025-06-17T11:55:08Z","title":"MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant\n  Multi-Task Adaptation","summary":"  Adapting large-scale foundation models in multi-task scenarios often suffers\nfrom task conflict and oblivion. To mitigate such issues, we propose a novel\n''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant\nmulti-task adaptation method. Given a weight matrix of a pre-trained model, our\nmethod applies SVD to it and introduces a learnable router to adjust its\nsingular values based on tasks and samples. Accordingly, the weight matrix\nbecomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert\ncorresponds to the outer product of a left singular vector and the\ncorresponding right one. We can improve the model capacity by imposing a\nlearnable orthogonal transform on the right singular vectors. Unlike low-rank\nadaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'\northogonality and maintains the column space of the original weight matrix.\nThese two properties make the adapted model resistant to the conflicts among\nthe new tasks and the oblivion of its original tasks, respectively. Experiments\non various datasets demonstrate that MoORE outperforms existing multi-task\nadaptation methods consistently, showing its superiority in terms of conflict-\nand oblivion-resistance. The code of the experiments is available at\nhttps://github.com/DaShenZi721/MoORE.\n","authors":["Shen Yuan","Yin Zheng","Taifeng Wang","Binbin Liu","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14436v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14435v1","updated":"2025-06-17T11:53:49Z","published":"2025-06-17T11:53:49Z","title":"MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models","summary":"  Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.\n","authors":["Hongyu Wang","Jiayu Xu","Ruiping Wang","Yan Feng","Yitao Zhai","Peng Pei","Xunliang Cai","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2506.14435v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.14420v1","updated":"2025-06-17T11:30:04Z","published":"2025-06-17T11:30:04Z","title":"Unsupervised Skill Discovery through Skill Regions Differentiation","summary":"  Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors\nthat can accelerate the learning of downstream tasks. Previous methods\ntypically focus on entropy-based exploration or empowerment-driven skill\nlearning. However, entropy-based exploration struggles in large-scale state\nspaces (e.g., images), and empowerment-based methods with Mutual Information\n(MI) estimations have limitations in state exploration. To address these\nchallenges, we propose a novel skill discovery objective that maximizes the\ndeviation of the state density of one skill from the explored regions of other\nskills, encouraging inter-skill state diversity similar to the initial MI\nobjective. For state-density estimation, we construct a novel conditional\nautoencoder with soft modularization for different skill policies in\nhigh-dimensional space. Meanwhile, to incentivize intra-skill exploration, we\nformulate an intrinsic reward based on the learned autoencoder that resembles\ncount-based exploration in a compact latent space. Through extensive\nexperiments in challenging state and image-based tasks, we find our method\nlearns meaningful skills and achieves superior performance in various\ndownstream tasks.\n","authors":["Ting Xiao","Jiakun Zheng","Rushuai Yang","Kang Xu","Qiaosheng Zhang","Peng Liu","Chenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2506.14420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11054v2","updated":"2025-06-17T11:24:33Z","published":"2025-05-22T11:31:00Z","title":"Adaptive Composition of Machine Learning as a Service (MLaaS) for IoT\n  Environments","summary":"  The dynamic nature of Internet of Things (IoT) environments challenges the\nlong-term effectiveness of Machine Learning as a Service (MLaaS) compositions.\nThe uncertainty and variability of IoT environments lead to fluctuations in\ndata distribution, e.g., concept drift and data heterogeneity, and evolving\nsystem requirements, e.g., scalability demands and resource limitations. This\npaper proposes an adaptive MLaaS composition framework to ensure a seamless,\nefficient, and scalable MLaaS composition. The framework integrates a service\nassessment model to identify underperforming MLaaS services and a candidate\nselection model to filter optimal replacements. An adaptive composition\nmechanism is developed that incrementally updates MLaaS compositions using a\ncontextual multi-armed bandit optimization strategy. By continuously adapting\nto evolving IoT constraints, the approach maintains Quality of Service (QoS)\nwhile reducing the computational cost associated with recomposition from\nscratch. Experimental results on a real-world dataset demonstrate the\nefficiency of our proposed approach.\n","authors":["Deepak Kanneganti","Sajib Mistry","Sheik Mohammad Mostakim Fattah","Aneesh Krishna","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2506.11054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14412v1","updated":"2025-06-17T11:14:22Z","published":"2025-06-17T11:14:22Z","title":"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG\n  Systems for the SIGIR LiveRAG Competition","summary":"  Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by\ncombining their internal, parametric knowledge with external, non-parametric\nsources, with the goal of improving factual correctness and minimizing\nhallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize\naccuracy on DataMorgana's QA pairs, which are composed of single-hop and\nmulti-hop questions. The challenge provides access to sparse OpenSearch and\ndense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to\nLLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A\njudge-LLM assesses the submitted answers along with human evaluators. By\nexploring distinct retriever combinations and RAG solutions under the challenge\nconditions, our final solution emerged using InstructRAG in combination with a\nPinecone retriever and a BGE reranker. Our solution achieved a correctness\nscore of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR\n2025 LiveRAG Challenge.\n","authors":["Tim Cofala","Oleh Astappiev","William Xion","Hailay Teklehaymanot"],"pdf_url":"https://arxiv.org/pdf/2506.14412v1.pdf","comment":"4 pages, 5 figures. Report for SIGIR 2025 LiveRAG Challenge"},{"id":"http://arxiv.org/abs/2506.14411v1","updated":"2025-06-17T11:11:37Z","published":"2025-06-17T11:11:37Z","title":"Adaptive Reinforcement Learning for Unobservable Random Delays","summary":"  In standard Reinforcement Learning (RL) settings, the interaction between the\nagent and the environment is typically modeled as a Markov Decision Process\n(MDP), which assumes that the agent observes the system state instantaneously,\nselects an action without delay, and executes it immediately. In real-world\ndynamic environments, such as cyber-physical systems, this assumption often\nbreaks down due to delays in the interaction between the agent and the system.\nThese delays can vary stochastically over time and are typically unobservable,\nmeaning they are unknown when deciding on an action. Existing methods deal with\nthis uncertainty conservatively by assuming a known fixed upper bound on the\ndelay, even if the delay is often much lower. In this work, we introduce the\ninteraction layer, a general framework that enables agents to adaptively and\nseamlessly handle unobservable and time-varying delays. Specifically, the agent\ngenerates a matrix of possible future actions to handle both unpredictable\ndelays and lost action packets sent over networks. Building on this framework,\nwe develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA),\nwhich dynamically adjusts to delay patterns. Our method significantly\noutperforms state-of-the-art approaches across a wide range of locomotion\nbenchmark environments.\n","authors":["John Wikman","Alexandre Proutiere","David Broman"],"pdf_url":"https://arxiv.org/pdf/2506.14411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07774v3","updated":"2025-06-17T11:11:09Z","published":"2024-04-11T14:09:41Z","title":"Sketch-Plan-Generalize: Learning and Planning with Neuro-Symbolic\n  Programmatic Representations for Inductive Spatial Concepts","summary":"  Effective human-robot collaboration requires the ability to learn\npersonalized concepts from a limited number of demonstrations, while exhibiting\ninductive generalization, hierarchical composition, and adaptability to novel\nconstraints. Existing approaches that use code generation capabilities of\npre-trained large (vision) language models as well as purely neural models show\npoor generalization to \\emph{a-priori} unseen complex concepts. Neuro-symbolic\nmethods (Grand et al., 2023) offer a promising alternative by searching in\nprogram space, but face challenges in large program spaces due to the inability\nto effectively guide the search using demonstrations. Our key insight is to\nfactor inductive concept learning as: (i) {\\it Sketch:} detecting and inferring\na coarse signature of a new concept (ii) {\\it Plan:} performing an MCTS search\nover grounded action sequences guided by human demonstrations (iii) {\\it\nGeneralize:} abstracting out grounded plans as inductive programs. Our pipeline\nfacilitates generalization and modular re-use, enabling continual concept\nlearning. Our approach combines the benefits of code generation ability of\nlarge language models (LLMs) along with grounded neural representations,\nresulting in neuro-symbolic programs that show stronger inductive\ngeneralization on the task of constructing complex structures vis-\\'a-vis\nLLM-only and purely neural approaches. Further, we demonstrate reasoning and\nplanning capabilities with learned concepts for embodied instruction following.\n","authors":["Namasivayam Kalithasan","Sachit Sachdeva","Himanshu Gaurav Singh","Vishal Bindal","Arnav Tuli","Gurarmaan Singh Panjeta","Harsh Himanshu Vora","Divyanshu Aggarwal","Rohan Paul","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2404.07774v3.pdf","comment":"Programmatic Representations for Agent Learning Worskop, ICML 2025"},{"id":"http://arxiv.org/abs/2305.02093v2","updated":"2025-06-17T11:11:05Z","published":"2023-05-03T12:56:43Z","title":"Efficient Online Decision Tree Learning with Active Feature Acquisition","summary":"  Constructing decision trees online is a classical machine learning problem.\nExisting works often assume that features are readily available for each\nincoming data point. However, in many real world applications, both feature\nvalues and the labels are unknown a priori and can only be obtained at a cost.\nFor example, in medical diagnosis, doctors have to choose which tests to\nperform (i.e., making costly feature queries) on a patient in order to make a\ndiagnosis decision (i.e., predicting labels). We provide a fresh perspective to\ntackle this practical challenge. Our framework consists of an active planning\noracle embedded in an online learning scheme for which we investigate several\ninformation acquisition functions. Specifically, we employ a surrogate\ninformation acquisition function based on adaptive submodularity to actively\nquery feature values with a minimal cost, while using a posterior sampling\nscheme to maintain a low regret for online prediction. We demonstrate the\nefficiency and effectiveness of our framework via extensive experiments on\nvarious real-world datasets. Our framework also naturally adapts to the\nchallenging setting of online learning with concept drift and is shown to be\ncompetitive with baseline models while being more flexible.\n","authors":["Arman Rahbar","Ziyu Ye","Yuxin Chen","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2305.02093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14400v1","updated":"2025-06-17T10:59:02Z","published":"2025-06-17T10:59:02Z","title":"One Size Fits None: Rethinking Fairness in Medical AI","summary":"  Machine learning (ML) models are increasingly used to support clinical\ndecision-making. However, real-world medical datasets are often noisy,\nincomplete, and imbalanced, leading to performance disparities across patient\nsubgroups. These differences raise fairness concerns, particularly when they\nreinforce existing disadvantages for marginalized groups. In this work, we\nanalyze several medical prediction tasks and demonstrate how model performance\nvaries with patient characteristics. While ML models may demonstrate good\noverall performance, we argue that subgroup-level evaluation is essential\nbefore integrating them into clinical workflows. By conducting a performance\nanalysis at the subgroup level, differences can be clearly identified-allowing,\non the one hand, for performance disparities to be considered in clinical\npractice, and on the other hand, for these insights to inform the responsible\ndevelopment of more effective models. Thereby, our work contributes to a\npractical discussion around the subgroup-sensitive development and deployment\nof medical ML models and the interconnectedness of fairness and transparency.\n","authors":["Roland Roller","Michael Hahn","Ajay Madhavan Ravichandran","Bilgin Osmanodja","Florian Oetke","Zeineb Sassi","Aljoscha Burchardt","Klaus Netter","Klemens Budde","Anne Herrmann","Tobias Strapatsas","Peter Dabrock","Sebastian Möller"],"pdf_url":"https://arxiv.org/pdf/2506.14400v1.pdf","comment":"Accepted at the 6th Workshop on Gender Bias in Natural Language\n  Processing at ACL 2025"},{"id":"http://arxiv.org/abs/2505.19802v2","updated":"2025-06-17T10:43:24Z","published":"2025-05-26T10:35:42Z","title":"GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity\n  Estimation","summary":"  Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation.\n","authors":["Zhiyu Wang","Yang Liu","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2505.19802v2.pdf","comment":"MiGA@IJCAI25"},{"id":"http://arxiv.org/abs/2506.14391v1","updated":"2025-06-17T10:39:42Z","published":"2025-06-17T10:39:42Z","title":"HiLight: A Hierarchical Reinforcement Learning Framework with Global\n  Adversarial Guidance for Large-Scale Traffic Signal Control","summary":"  Efficient traffic signal control (TSC) is essential for mitigating urban\ncongestion, yet existing reinforcement learning (RL) methods face challenges in\nscaling to large networks while maintaining global coordination. Centralized RL\nsuffers from scalability issues, while decentralized approaches often lack\nunified objectives, resulting in limited network-level efficiency. In this\npaper, we propose HiLight, a hierarchical reinforcement learning framework with\nglobal adversarial guidance for large-scale TSC. HiLight consists of a\nhigh-level Meta-Policy, which partitions the traffic network into subregions\nand generates sub-goals using a Transformer-LSTM architecture, and a low-level\nSub-Policy, which controls individual intersections with global awareness. To\nimprove the alignment between global planning and local execution, we introduce\nan adversarial training mechanism, where the Meta-Policy generates challenging\nyet informative sub-goals, and the Sub-Policy learns to surpass these targets,\nleading to more effective coordination. We evaluate HiLight across both\nsynthetic and real-world benchmarks, and additionally construct a large-scale\nManhattan network with diverse traffic conditions, including peak transitions,\nadverse weather, and holiday surges. Experimental results show that HiLight\nexhibits significant advantages in large-scale scenarios and remains\ncompetitive across standard benchmarks of varying sizes.\n","authors":["Yaqiao Zhu","Hongkai Wen","Geyong Min","Man Luo"],"pdf_url":"https://arxiv.org/pdf/2506.14391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14390v1","updated":"2025-06-17T10:38:29Z","published":"2025-06-17T10:38:29Z","title":"Enclosing Prototypical Variational Autoencoder for Explainable\n  Out-of-Distribution Detection","summary":"  Understanding the decision-making and trusting the reliability of Deep\nMachine Learning Models is crucial for adopting such methods to safety-relevant\napplications. We extend self-explainable Prototypical Variational models with\nautoencoder-based out-of-distribution (OOD) detection: A Variational\nAutoencoder is applied to learn a meaningful latent space which can be used for\ndistance-based classification, likelihood estimation for OOD detection, and\nreconstruction. The In-Distribution (ID) region is defined by a Gaussian\nmixture distribution with learned prototypes representing the center of each\nmode. Furthermore, a novel restriction loss is introduced that promotes a\ncompact ID region in the latent space without collapsing it into single points.\nThe reconstructive capabilities of the Autoencoder ensure the explainability of\nthe prototypes and the ID region of the classifier, further aiding the\ndiscrimination of OOD samples. Extensive evaluations on common OOD detection\nbenchmarks as well as a large-scale dataset from a real-world railway\napplication demonstrate the usefulness of the approach, outperforming previous\nmethods.\n","authors":["Conrad Orglmeister","Erik Bochinski","Volker Eiselein","Elvira Fleig"],"pdf_url":"https://arxiv.org/pdf/2506.14390v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Computer Safety, Reliability and Security - SAFECOMP 2024\n  Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online at\n  https://doi.org/10.1007/978-3-031-68738-9_29"},{"id":"http://arxiv.org/abs/2506.14386v1","updated":"2025-06-17T10:33:22Z","published":"2025-06-17T10:33:22Z","title":"ResNets Are Deeper Than You Think","summary":"  Residual connections remain ubiquitous in modern neural network architectures\nnearly a decade after their introduction. Their widespread adoption is often\ncredited to their dramatically improved trainability: residual networks train\nfaster, more stably, and achieve higher accuracy than their feedforward\ncounterparts. While numerous techniques, ranging from improved initialization\nto advanced learning rate schedules, have been proposed to close the\nperformance gap between residual and feedforward networks, this gap has\npersisted. In this work, we propose an alternative explanation: residual\nnetworks do not merely reparameterize feedforward networks, but instead inhabit\na different function space. We design a controlled post-training comparison to\nisolate generalization performance from trainability; we find that\nvariable-depth architectures, similar to ResNets, consistently outperform\nfixed-depth networks, even when optimization is unlikely to make a difference.\nThese results suggest that residual connections confer performance advantages\nbeyond optimization, pointing instead to a deeper inductive bias aligned with\nthe structure of natural data.\n","authors":["Christian H. X. Ali Mehmeti-Göpel","Michael Wand"],"pdf_url":"https://arxiv.org/pdf/2506.14386v1.pdf","comment":"NeurIPS 2025 Submission"},{"id":"http://arxiv.org/abs/2307.16189v8","updated":"2025-06-17T10:25:04Z","published":"2023-07-30T10:03:36Z","title":"Stabilizing Backpropagation in 16-bit Neural Training with Modified Adam\n  Optimizer","summary":"  In this research, we address critical concerns related to the numerical\ninstability observed in 16-bit computations of machine learning models. Such\ninstability, particularly when employing popular optimization algorithms like\nAdam, often leads to unstable training of deep neural networks. This not only\ndisrupts the learning process but also poses significant challenges in\ndeploying dependable models in real-world applications. Our investigation\nidentifies the epsilon hyperparameter as the primary source of this\ninstability. A nuanced exploration reveals that subtle adjustments to epsilon\nwithin 16-bit computations can enhance the numerical stability of Adam,\nenabling more stable training of 16-bit neural networks. We propose a novel,\ndependable approach that leverages updates from the Adam optimizer to bolster\nthe stability of the learning process. Our contributions provide deeper\ninsights into optimization challenges in low-precision computations and offer\nsolutions to ensure the stability of deep neural network training, paving the\nway for their dependable use in various applications.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2307.16189v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14375v1","updated":"2025-06-17T10:17:26Z","published":"2025-06-17T10:17:26Z","title":"IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with\n  Hybrid Actions and Clinically Aligned Rewards","summary":"  Invasive mechanical ventilation (MV) is a life-sustaining therapy for\ncritically ill patients in the intensive care unit (ICU). However, optimizing\nits settings remains a complex and error-prone process due to patient-specific\nvariability. While Offline Reinforcement Learning (RL) shows promise for MV\ncontrol, current stateof-the-art (SOTA) methods struggle with the hybrid\n(continuous and discrete) nature of MV actions. Discretizing the action space\nlimits available actions due to exponential growth in combinations and\nintroduces distribution shifts that can compromise safety. In this paper, we\npropose optimizations that build upon prior work in action space reduction to\naddress the challenges of discrete action spaces. We also adapt SOTA offline RL\nalgorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby\navoiding the pitfalls of discretization. Additionally, we introduce a\nclinically grounded reward function based on ventilator-free days and\nphysiological targets, which provides a more meaningful optimization objective\ncompared to traditional sparse mortality-based rewards. Our findings\ndemonstrate that AI-assisted MV optimization may enhance patient safety and\nenable individualized lung support, representing a significant advancement\ntoward intelligent, data-driven critical care solutions.\n","authors":["Muhammad Hamza Yousuf","Jason Li","Sahar Vahdati","Raphael Theilen","Jakob Wittenstein","Jens Lehmann"],"pdf_url":"https://arxiv.org/pdf/2506.14375v1.pdf","comment":"under review, PAIS track @ ECAI 2025"},{"id":"http://arxiv.org/abs/2506.14374v1","updated":"2025-06-17T10:16:52Z","published":"2025-06-17T10:16:52Z","title":"Excessive Reasoning Attack on Reasoning LLMs","summary":"  Recent reasoning large language models (LLMs), such as OpenAI o1 and\nDeepSeek-R1, exhibit strong performance on complex tasks through test-time\ninference scaling. However, prior studies have shown that these models often\nincur significant computational costs due to excessive reasoning, such as\nfrequent switching between reasoning trajectories (e.g., underthinking) or\nredundant reasoning on simple questions (e.g., overthinking). In this work, we\nexpose a novel threat: adversarial inputs can be crafted to exploit excessive\nreasoning behaviors and substantially increase computational overhead without\ncompromising model utility. Therefore, we propose a novel loss framework\nconsisting of three components: (1) Priority Cross-Entropy Loss, a modification\nof the standard cross-entropy objective that emphasizes key tokens by\nleveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss,\nwhich encourages the model to initiate additional reasoning paths during\ninference; and (3) Delayed Termination Loss, which is designed to extend the\nreasoning process and defer the generation of final outputs. We optimize and\nevaluate our attack for the GSM8K and ORCA datasets on\nDeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results\ndemonstrate a 3x to 9x increase in reasoning length with comparable utility\nperformance. Furthermore, our crafted adversarial inputs exhibit\ntransferability, inducing computational overhead in o3-mini, o1-mini,\nDeepSeek-R1, and QWQ models.\n","authors":["Wai Man Si","Mingjie Li","Michael Backes","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13277v2","updated":"2025-06-17T10:12:39Z","published":"2025-06-16T09:16:40Z","title":"SeqPE: Transformer with Sequential Position Encoding","summary":"  Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.\n","authors":["Huayang Li","Yahui Liu","Hongyu Sun","Deng Cai","Leyang Cui","Wei Bi","Peilin Zhao","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.13277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11037v2","updated":"2025-06-17T09:42:40Z","published":"2025-05-21T02:28:26Z","title":"Mini-Game Lifetime Value Prediction in WeChat","summary":"  The LifeTime Value (LTV) prediction, which endeavors to forecast the\ncumulative purchase contribution of a user to a particular item, remains a\nvital challenge that advertisers are keen to resolve. A precise LTV prediction\nsystem enhances the alignment of user interests with meticulously designed\nadvertisements, thereby generating substantial profits for advertisers.\nNonetheless, this issue is complicated by the paucity of data typically\nobserved in real-world advertising scenarios. The purchase rate among\nregistered users is often as critically low as 0.1%, resulting in a dataset\nwhere the majority of users make only several purchases. Consequently, there is\ninsufficient supervisory signal for effectively training the LTV prediction\nmodel. An additional challenge emerges from the interdependencies among tasks\nwith high correlation. It is a common practice to estimate a user's\ncontribution to a game over a specified temporal interval. Varying the lengths\nof these intervals corresponds to distinct predictive tasks, which are highly\ncorrelated. For instance, predictions over a 7-day period are heavily reliant\non forecasts made over a 3-day period, where exceptional cases can adversely\naffect the accuracy of both tasks. In order to comprehensively address the\naforementioned challenges, we introduce an innovative framework denoted as\nGraph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph\nrepresentation learning is initially employed to address the issue of data\nscarcity. Subsequently, Pareto-Optimization is utilized to manage the\ninterdependence of prediction tasks.\n","authors":["Aochuan Chen","Yifan Niu","Ziqi Gao","Yujie Sun","Shoujun Liu","Gong Chen","Yang Liu","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2506.11037v2.pdf","comment":"KDD ADS Track 2025"},{"id":"http://arxiv.org/abs/2409.05144v3","updated":"2025-06-17T09:32:41Z","published":"2024-09-08T15:57:58Z","title":"QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with\n  Variance-bounded REINFORCE","summary":"  Alpha factor mining aims to discover investment signals from the historical\nfinancial market data, which can be used to predict asset returns and gain\nexcess profits. Powerful deep learning methods for alpha factor mining lack\ninterpretability, making them unacceptable in the risk-sensitive real markets.\nFormulaic alpha factors are preferred for their interpretability, while the\nsearch space is complex and powerful explorative methods are urged. Recently, a\npromising framework is proposed for generating formulaic alpha factors using\ndeep reinforcement learning, and quickly gained research focuses from both\nacademia and industries. This paper first argues that the originally employed\npolicy training method, i.e., Proximal Policy Optimization (PPO), faces several\nimportant issues in the context of alpha factors mining. Herein, a novel\nreinforcement learning algorithm based on the well-known REINFORCE algorithm is\nproposed. REINFORCE employs Monte Carlo sampling to estimate the policy\ngradient-yielding unbiased but high variance estimates. The minimal\nenvironmental variability inherent in the underlying state transition function,\nwhich adheres to the Dirac distribution, can help alleviate this high variance\nissue, making REINFORCE algorithm more appropriate than PPO. A new dedicated\nbaseline is designed to theoretically reduce the commonly suffered high\nvariance of REINFORCE. Moreover, the information ratio is introduced as a\nreward shaping mechanism to encourage the generation of steady alpha factors\nthat can better adapt to changes in market volatility. Evaluations on real\nassets data indicate the proposed algorithm boosts correlation with returns by\n3.83\\%, and a stronger ability to obtain excess returns compared to the latest\nalpha factors mining methods, which meets the theoretical results well.\n","authors":["Junjie Zhao","Chengxi Zhang","Min Qin","Peng Yang"],"pdf_url":"https://arxiv.org/pdf/2409.05144v3.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.01129v6","updated":"2025-06-17T09:27:55Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Hui Liu","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18609v4","updated":"2025-06-17T09:21:55Z","published":"2024-07-26T09:00:18Z","title":"Heavy-Tailed Diffusion with Denoising Lévy Probabilistic Models","summary":"  Exploring noise distributions beyond Gaussian in diffusion models remains an\nopen challenge. While Gaussian-based models succeed within a unified SDE\nframework, recent studies suggest that heavy-tailed noise distributions, like\n$\\alpha$-stable distributions, may better handle mode collapse and effectively\nmanage datasets exhibiting class imbalance, heavy tails, or prominent outliers.\nRecently, Yoon et al.\\ (NeurIPS 2023), presented the L\\'evy-It\\^o model (LIM),\ndirectly extending the SDE-based framework to a class of heavy-tailed SDEs,\nwhere the injected noise followed an $\\alpha$-stable distribution, a rich class\nof heavy-tailed distributions. However, the LIM framework relies on highly\ninvolved mathematical techniques with limited flexibility, potentially\nhindering broader adoption and further development. In this study, instead of\nstarting from the SDE formulation, we extend the denoising diffusion\nprobabilistic model (DDPM) by replacing the Gaussian noise with $\\alpha$-stable\nnoise. By using only elementary proof techniques, the proposed approach,\nDenoising L\\'evy Probabilistic Models (DLPM), boils down to vanilla DDPM with\nminor modifications. As opposed to the Gaussian case, DLPM and LIM yield\ndifferent training algorithms and different backward processes, leading to\ndistinct sampling algorithms. These fundamental differences translate favorably\nfor DLPM as compared to LIM: our experiments show improvements in coverage of\ndata distribution tails, better robustness to unbalanced datasets, and improved\ncomputation times requiring smaller number of backward steps.\n","authors":["Dario Shariatian","Umut Simsekli","Alain Durmus"],"pdf_url":"https://arxiv.org/pdf/2407.18609v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14329v1","updated":"2025-06-17T09:11:17Z","published":"2025-06-17T09:11:17Z","title":"Adjustment for Confounding using Pre-Trained Representations","summary":"  There is growing interest in extending average treatment effect (ATE)\nestimation to incorporate non-tabular data, such as images and text, which may\nact as sources of confounding. Neglecting these effects risks biased results\nand flawed scientific conclusions. However, incorporating non-tabular data\nnecessitates sophisticated feature extractors, often in combination with ideas\nof transfer learning. In this work, we investigate how latent features from\npre-trained neural networks can be leveraged to adjust for sources of\nconfounding. We formalize conditions under which these latent features enable\nvalid adjustment and statistical inference in ATE estimation, demonstrating\nresults along the example of double machine learning. We discuss critical\nchallenges inherent to latent feature learning and downstream parameter\nestimation arising from the high dimensionality and non-identifiability of\nrepresentations. Common structural assumptions for obtaining fast convergence\nrates with additive or sparse linear models are shown to be unrealistic for\nlatent features. We argue, however, that neural networks are largely\ninsensitive to these issues. In particular, we show that neural networks can\nachieve fast convergence rates by adapting to intrinsic notions of sparsity and\ndimension of the learning problem.\n","authors":["Rickmer Schulte","David Rügamer","Thomas Nagler"],"pdf_url":"https://arxiv.org/pdf/2506.14329v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.14322v1","updated":"2025-06-17T09:02:46Z","published":"2025-06-17T09:02:46Z","title":"FRIDU: Functional Map Refinement with Guided Image Diffusion","summary":"  We propose a novel approach for refining a given correspondence map between\ntwo shapes. A correspondence map represented as a functional map, namely a\nchange of basis matrix, can be additionally treated as a 2D image. With this\nperspective, we train an image diffusion model directly in the space of\nfunctional maps, enabling it to generate accurate maps conditioned on an\ninaccurate initial map. The training is done purely in the functional space,\nand thus is highly efficient. At inference time, we use the pointwise map\ncorresponding to the current functional map as guidance during the diffusion\nprocess. The guidance can additionally encourage different functional map\nobjectives, such as orthogonality and commutativity with the Laplace-Beltrami\noperator. We show that our approach is competitive with state-of-the-art\nmethods of map refinement and that guided diffusion models provide a promising\npathway to functional map processing.\n","authors":["Avigail Cohen Rimon","Mirela Ben-Chen","Or Litany"],"pdf_url":"https://arxiv.org/pdf/2506.14322v1.pdf","comment":"Accepted to SGP 2025 (Symposium on Geometry Processing)"},{"id":"http://arxiv.org/abs/2506.01413v4","updated":"2025-06-17T08:52:06Z","published":"2025-06-02T08:11:44Z","title":"Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models","summary":"  Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nwill be available later (under review).\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions\n","authors":["Yulei Qin","Gang Li","Zongyi Li","Zihan Xu","Yuchen Shi","Zhekai Lin","Xiao Cui","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2506.01413v4.pdf","comment":"13 pages of main body, 3 tables, 5 figures, 45 pages of appendix"},{"id":"http://arxiv.org/abs/2503.09483v3","updated":"2025-06-17T08:50:23Z","published":"2025-03-12T15:38:11Z","title":"Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization","summary":"  We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.\n","authors":["Andreas Kofler","Luca Calatroni","Christoph Kolbitsch","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2503.09483v3.pdf","comment":"Accepted for publication in the proceedings of the EUSIPCO 2025\n  conference"},{"id":"http://arxiv.org/abs/2506.13087v2","updated":"2025-06-17T08:43:30Z","published":"2025-06-16T04:12:04Z","title":"IKDiffuser: Fast and Diverse Inverse Kinematics Solution Generation for\n  Multi-arm Robotic Systems","summary":"  Solving Inverse Kinematics (IK) problems is fundamental to robotics, but has\nprimarily been successful with single serial manipulators. For multi-arm\nrobotic systems, IK remains challenging due to complex self-collisions, coupled\njoints, and high-dimensional redundancy. These complexities make traditional IK\nsolvers slow, prone to failure, and lacking in solution diversity. In this\npaper, we present IKDiffuser, a diffusion-based model designed for fast and\ndiverse IK solution generation for multi-arm robotic systems. IKDiffuser learns\nthe joint distribution over the configuration space, capturing complex\ndependencies and enabling seamless generalization to multi-arm robotic systems\nof different structures. In addition, IKDiffuser can incorporate additional\nobjectives during inference without retraining, offering versatility and\nadaptability for task-specific requirements. In experiments on 6 different\nmulti-arm systems, the proposed IKDiffuser achieves superior solution accuracy,\nprecision, diversity, and computational efficiency compared to existing\nsolvers. The proposed IKDiffuser framework offers a scalable, unified approach\nto solving multi-arm IK problems, facilitating the potential of multi-arm\nrobotic systems in real-time manipulation tasks.\n","authors":["Zeyu Zhang","Ziyuan Jiao"],"pdf_url":"https://arxiv.org/pdf/2506.13087v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2411.16298v2","updated":"2025-06-17T08:42:46Z","published":"2024-11-25T11:31:53Z","title":"Evaluating Rank-N-Contrast: Continuous and Robust Representations for\n  Regression","summary":"  This document is a replication of the original \"Rank-N-Contrast\"\n(arXiv:2210.01189v2) paper published in 2023. This evaluation is done for\nacademic purposes. Deep regression models often fail to capture the continuous\nnature of sample orders, creating fragmented representations and suboptimal\nperformance. To address this, we reproduced the Rank-N-Contrast (RNC)\nframework, which learns continuous representations by contrasting samples by\ntheir rankings in the target space. Our study validates RNC's theoretical and\nempirical benefits, including improved performance and robustness. We extended\nthe evaluation to an additional regression dataset and conducted robustness\ntests using a holdout method, where a specific range of continuous data was\nexcluded from the training set. This approach assessed the model's ability to\ngeneralise to unseen data and achieve state-of-the-art performance. This\nreplication study validates the original findings and broadens the\nunderstanding of RNC's applicability and robustness.\n","authors":["Valentin Six","Alexandre Chidiac","Arkin Worlikar"],"pdf_url":"https://arxiv.org/pdf/2411.16298v2.pdf","comment":"Academic project ; not made for publication"},{"id":"http://arxiv.org/abs/2412.08194v2","updated":"2025-06-17T08:36:16Z","published":"2024-12-11T08:35:56Z","title":"Magneto: Combining Small and Large Language Models for Schema Matching","summary":"  Recent advances in language models opened new opportunities to address\ncomplex schema matching tasks. Schema matching approaches have been proposed\nthat demonstrate the usefulness of language models, but they have also\nuncovered important limitations: Small language models (SLMs) require training\ndata (which can be both expensive and challenging to obtain), and large\nlanguage models (LLMs) often incur high computational costs and must deal with\nconstraints imposed by context windows. We present Magneto, a cost-effective\nand accurate solution for schema matching that combines the advantages of SLMs\nand LLMs to address their limitations. By structuring the schema matching\npipeline in two phases, retrieval and reranking, Magneto can use\ncomputationally efficient SLM-based strategies to derive candidate matches\nwhich can then be reranked by LLMs, thus making it possible to reduce runtime\nwithout compromising matching accuracy. We propose a self-supervised approach\nto fine-tune SLMs which uses LLMs to generate syntactically diverse training\ndata, and prompting strategies that are effective for reranking. We also\nintroduce a new benchmark, developed in collaboration with domain experts,\nwhich includes real biomedical datasets and presents new challenges to schema\nmatching methods. Through a detailed experimental evaluation, using both our\nnew and existing benchmarks, we show that Magneto is scalable and attains high\naccuracy for datasets from different domains.\n","authors":["Yurong Liu","Eduardo Pena","Aecio Santos","Eden Wu","Juliana Freire"],"pdf_url":"https://arxiv.org/pdf/2412.08194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14306v1","updated":"2025-06-17T08:34:56Z","published":"2025-06-17T08:34:56Z","title":"Fair for a few: Improving Fairness in Doubly Imbalanced Datasets","summary":"  Fairness has been identified as an important aspect of Machine Learning and\nArtificial Intelligence solutions for decision making. Recent literature offers\na variety of approaches for debiasing, however many of them fall short when the\ndata collection is imbalanced. In this paper, we focus on a particular case,\nfairness in doubly imbalanced datasets, such that the data collection is\nimbalanced both for the label and the groups in the sensitive attribute.\nFirstly, we present an exploratory analysis to illustrate limitations in\ndebiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution\nis proposed for finding the most suitable sampling and distribution for label\nand sensitive attribute, in terms of fairness and classification accuracy\n","authors":["Ata Yalcin","Asli Umay Ozturk","Yigit Sever","Viktoria Pauw","Stephan Hachinger","Ismail Hakki Toroslu","Pinar Karagoz"],"pdf_url":"https://arxiv.org/pdf/2506.14306v1.pdf","comment":"33 pages, 3 figures, submitted to AI Review"},{"id":"http://arxiv.org/abs/2506.13390v2","updated":"2025-06-17T08:20:19Z","published":"2025-06-16T11:53:00Z","title":"Experimental Design for Semiparametric Bandits","summary":"  We study finite-armed semiparametric bandits, where each arm's reward\ncombines a linear component with an unknown, potentially adversarial shift.\nThis model strictly generalizes classical linear bandits and reflects\ncomplexities common in practice. We propose the first experimental-design\napproach that simultaneously offers a sharp regret bound, a PAC bound, and a\nbest-arm identification guarantee. Our method attains the minimax regret\n$\\tilde{O}(\\sqrt{dT})$, matching the known lower bound for finite-armed linear\nbandits, and further achieves logarithmic regret under a positive suboptimality\ngap condition. These guarantees follow from our refined non-asymptotic analysis\nof orthogonalized regression that attains the optimal $\\sqrt{d}$ rate, paving\nthe way for robust and efficient learning across a broad class of\nsemiparametric bandit problems.\n","authors":["Seok-Jin Kim","Gi-Soo Kim","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2506.13390v2.pdf","comment":"Accepted at COLT 2025"},{"id":"http://arxiv.org/abs/2501.19077v2","updated":"2025-06-17T08:15:54Z","published":"2025-01-31T12:09:40Z","title":"Temperature-Annealed Boltzmann Generators","summary":"  Efficient sampling of unnormalized probability densities such as the\nBoltzmann distribution of molecular systems is a longstanding challenge. Next\nto conventional approaches like molecular dynamics or Markov chain Monte Carlo,\nvariational approaches, such as training normalizing flows with the reverse\nKullback-Leibler divergence, have been introduced. However, such methods are\nprone to mode collapse and often do not learn to sample the full\nconfigurational space. Here, we present temperature-annealed Boltzmann\ngenerators (TA-BG) to address this challenge. First, we demonstrate that\ntraining a normalizing flow with the reverse Kullback-Leibler divergence at\nhigh temperatures is possible without mode collapse. Furthermore, we introduce\na reweighting-based training objective to anneal the distribution to lower\ntarget temperatures. We apply this methodology to three molecular systems of\nincreasing complexity and, compared to the baseline, achieve better results in\nalmost all metrics while requiring up to three times fewer target energy\nevaluations. For the largest system, our approach is the only method that\naccurately resolves the metastable states of the system.\n","authors":["Henrik Schopmans","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2501.19077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14293v1","updated":"2025-06-17T08:08:08Z","published":"2025-06-17T08:08:08Z","title":"SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative\n  music modeling","summary":"  We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music\nand song. To the best of our knowledge, there are no open-source high-quality\ndataset representing popular and well-known songs for generative music modeling\ntasks such as text-music, music-captioning, singing-voice synthesis, melody\nreconstruction and cross-model retrieval. Past contributions focused on\nisolated and constrained factors whose core perspective was to create synthetic\nor re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily\nlarge-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another\nfocus for the community. Unfortunately, adoption of these datasets has been\nbelow substantial in the generative music community as these datasets fail to\nreflect real-world music and its flavour. Our dataset changes this narrative\nand provides a dataset that is constructed using actual popular music and\nworld-renowned artists.\n","authors":["Tawsif Ahmed","Andrej Radonjic","Gollam Rabby"],"pdf_url":"https://arxiv.org/pdf/2506.14293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14291v1","updated":"2025-06-17T08:05:08Z","published":"2025-06-17T08:05:08Z","title":"Equivariance Everywhere All At Once: A Recipe for Graph Foundation\n  Models","summary":"  Graph machine learning architectures are typically tailored to specific tasks\non specific datasets, which hinders their broader applicability. This has led\nto a new quest in graph machine learning: how to build graph foundation models\ncapable of generalizing across arbitrary graphs and features? In this work, we\npresent a recipe for designing graph foundation models for node-level tasks\nfrom first principles. The key ingredient underpinning our study is a\nsystematic investigation of the symmetries that a graph foundation model must\nrespect. In a nutshell, we argue that label permutation-equivariance alongside\nfeature permutation-invariance are necessary in addition to the common node\npermutation-equivariance on each local neighborhood of the graph. To this end,\nwe first characterize the space of linear transformations that are equivariant\nto permutations of nodes and labels, and invariant to permutations of features.\nWe then prove that the resulting network is a universal approximator on\nmultisets that respect the aforementioned symmetries. Our recipe uses such\nlayers on the multiset of features induced by the local neighborhood of the\ngraph to obtain a class of graph foundation models for node property\nprediction. We validate our approach through extensive experiments on 29\nreal-world node classification datasets, demonstrating both strong zero-shot\nempirical performance and consistent improvement as the number of training\ngraphs increases.\n","authors":["Ben Finkelshtein","İsmail İlkan Ceylan","Michael Bronstein","Ron Levie"],"pdf_url":"https://arxiv.org/pdf/2506.14291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14287v1","updated":"2025-06-17T07:59:07Z","published":"2025-06-17T07:59:07Z","title":"Steering Robots with Inference-Time Interactions","summary":"  Imitation learning has driven the development of generalist policies capable\nof autonomously solving multiple tasks. However, when a pretrained policy makes\nerrors during deployment, there are limited mechanisms for users to correct its\nbehavior. While collecting additional data for finetuning can address such\nissues, doing so for each downstream use case is inefficient at deployment. My\nresearch proposes an alternative: keeping pretrained policies frozen as a fixed\nskill repertoire while allowing user interactions to guide behavior generation\ntoward user preferences at inference time. By making pretrained policies\nsteerable, users can help correct policy errors when the model struggles to\ngeneralize-without needing to finetune the policy. Specifically, I propose (1)\ninference-time steering, which leverages user interactions to switch between\ndiscrete skills, and (2) task and motion imitation, which enables user\ninteractions to edit continuous motions while satisfying task constraints\ndefined by discrete symbolic plans. These frameworks correct misaligned policy\npredictions without requiring additional training, maximizing the utility of\npretrained models while achieving inference-time user objectives.\n","authors":["Yanwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14287v1.pdf","comment":"MIT Robotics PhD Thesis"},{"id":"http://arxiv.org/abs/2411.13993v2","updated":"2025-06-17T07:56:29Z","published":"2024-11-21T10:13:55Z","title":"Market Making without Regret","summary":"  We consider a sequential decision-making setting where, at every round $t$, a\nmarket maker posts a bid price $B_t$ and an ask price $A_t$ to an incoming\ntrader (the taker) with a private valuation for one unit of some asset. If the\ntrader's valuation is lower than the bid price, or higher than the ask price,\nthen a trade (sell or buy) occurs. If a trade happens at round $t$, then\nletting $M_t$ be the market price (observed only at the end of round $t$), the\nmaker's utility is $M_t - B_t$ if the maker bought the asset, and $A_t - M_t$\nif they sold it. We characterize the maker's regret with respect to the best\nfixed choice of bid and ask pairs under a variety of assumptions (adversarial,\ni.i.d., and their variants) on the sequence of market prices and valuations.\nOur upper bound analysis unveils an intriguing connection relating market\nmaking to first-price auctions and dynamic pricing. Our main technical\ncontribution is a lower bound for the i.i.d. case with Lipschitz distributions\nand independence between prices and valuations. The difficulty in the analysis\nstems from the unique structure of the reward and feedback functions, allowing\nan algorithm to acquire information by graduating the \"cost of exploration\" in\nan arbitrary way.\n","authors":["Nicolò Cesa-Bianchi","Tommaso Cesari","Roberto Colomboni","Luigi Foscari","Vinayak Pathak"],"pdf_url":"https://arxiv.org/pdf/2411.13993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14280v1","updated":"2025-06-17T07:49:43Z","published":"2025-06-17T07:49:43Z","title":"Improving LoRA with Variational Learning","summary":"  Bayesian methods have recently been used to improve LoRA finetuning and,\nalthough they improve calibration, their effect on other metrics (such as\naccuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian\nmethods also increase computational overheads and require additional tricks for\nthem to work well. Here, we fix these issues by using a recently proposed\nvariational algorithm called IVON. We show that IVON is easy to implement and\nhas similar costs to AdamW, and yet it can also drastically improve many\nmetrics by using a simple posterior pruning technique. We present extensive\nresults on billion-scale LLMs (Llama and Qwen series) going way beyond the\nscale of existing applications of IVON. For example, we finetune a Llama-3.2-3B\nmodel on a set of commonsense reasoning tasks and improve accuracy over AdamW\nby 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian\nmethods like Laplace-LoRA and BLoB. Overall, our results show that variational\nlearning with IVON can effectively improve LoRA finetuning.\n","authors":["Bai Cong","Nico Daheim","Yuesong Shen","Rio Yokota","Mohammad Emtiyaz Khan","Thomas Möllenhoff"],"pdf_url":"https://arxiv.org/pdf/2506.14280v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.14276v1","updated":"2025-06-17T07:40:39Z","published":"2025-06-17T07:40:39Z","title":"Don't throw the baby out with the bathwater: How and why deep learning\n  for ARC","summary":"  The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable\nchallenge for AI systems. Despite the typically low performance on ARC, the\ndeep learning paradigm remains the most effective known strategy for generating\nskillful (state-of-the-art) neural networks (NN) across varied modalities and\ntasks in vision, language etc. The deep learning paradigm has proven to be able\nto train these skillful neural networks and learn the abstractions needed in\nthese diverse domains. Our work doubles down on that and continues to leverage\nthis paradigm by incorporating on-the-fly NN training at test time. We\ndemonstrate that fully committing to deep learning's capacity to acquire novel\nabstractions yields state-of-the-art performance on ARC. Specifically, we treat\nboth the neural network and the optimizer (rather than just a pre-trained\nnetwork) as integral components of the inference process, fostering\ngeneralization to unseen tasks. Concretely, we propose a methodology for\ntraining on ARC, starting from pretrained LLMs, and enhancing their ARC\nreasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment\nInference Reverse-Augmentation and Vote (AIRV) as effective test-time\ntechniques. We are the first to propose and show deep learning can be used\neffectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a\nfurther 300% boost with TTFT. An early version of this approach secured first\nplace in the 2023 ARCathon competition, while the final version achieved the\ncurrent best score on the ARC private test-set (58%). Our findings highlight\nthe key ingredients of a robust reasoning system in unfamiliar domains,\nunderscoring the central mechanisms that improve broad perceptual reasoning.\n","authors":["Jack Cole","Mohamed Osman"],"pdf_url":"https://arxiv.org/pdf/2506.14276v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14270v1","updated":"2025-06-17T07:35:02Z","published":"2025-06-17T07:35:02Z","title":"NeuralPDR: Neural Differential Equations as surrogate models for\n  Photodissociation Regions","summary":"  Computational astrochemical models are essential for helping us interpret and\nunderstand the observations of different astrophysical environments. In the age\nof high-resolution telescopes such as JWST and ALMA, the substructure of many\nobjects can be resolved, raising the need for astrochemical modeling at these\nsmaller scales, meaning that the simulations of these objects need to include\nboth the physics and chemistry to accurately model the observations. The\ncomputational cost of the simulations coupling both the three-dimensional\nhydrodynamics and chemistry is enormous, creating an opportunity for surrogate\nmodels that can effectively substitute the chemical solver. In this work we\npresent surrogate models that can replace the original chemical code, namely\nLatent Augmented Neural Ordinary Differential Equations. We train these\nsurrogate architectures on three datasets of increasing physical complexity,\nwith the last dataset derived directly from a three-dimensional simulation of a\nmolecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show\nthat these surrogate models can provide speedup and reproduce the original\nobservable column density maps of the dataset. This enables the rapid inference\nof the chemistry (on the GPU), allowing for the faster statistical inference of\nobservations or increasing the resolution in hydrodynamical simulations of\nastrophysical environments.\n","authors":["Gijs Vermariën","Thomas G. Bisbas","Serena Viti","Yue Zhao","Xuefei Tang","Rahul Ravichandran"],"pdf_url":"https://arxiv.org/pdf/2506.14270v1.pdf","comment":"Accepted for publication in Machine Learning: Science and Technology.\n  Focus on ML and the Physical Sciences, Mach. Learn.: Sci. Technol (2025)"},{"id":"http://arxiv.org/abs/2410.10652v2","updated":"2025-06-17T07:33:44Z","published":"2024-10-14T16:01:27Z","title":"Querying functional and structural niches on spatial transcriptomics\n  data","summary":"  Cells in multicellular organisms coordinate to form functional and structural\nniches. With spatial transcriptomics enabling gene expression profiling in\nspatial contexts, it has been revealed that spatial niches serve as cohesive\nand recurrent units in physiological and pathological processes. These\nobservations suggest universal tissue organization principles encoded by\nconserved niche patterns, and call for a query-based niche analytical paradigm\nbeyond current computational tools. In this work, we defined the Niche Query\nTask, which is to identify similar niches across ST samples given a niche of\ninterest (NOI). We further developed QueST, a specialized method for solving\nthis task. QueST models each niche as a subgraph, uses contrastive learning to\nlearn discriminative niche embeddings, and incorporates adversarial training to\nmitigate batch effects. In simulations and benchmark datasets, QueST\noutperformed existing methods repurposed for niche querying, accurately\ncapturing niche structures in heterogeneous environments and demonstrating\nstrong generalizability across diverse sequencing platforms. Applied to\ntertiary lymphoid structures in renal and lung cancers, QueST revealed\nfunctionally distinct niches associated with patient prognosis and uncovered\nconserved and divergent spatial architectures across cancer types. These\nresults demonstrate that QueST enables systematic, quantitative profiling of\nspatial niches across samples, providing a powerful tool to dissect spatial\ntissue architecture in health and disease.\n","authors":["Mo Chen","Minsheng Hao","Xinquan Liu","Lin Deng","Chen Li","Dongfang Wang","Kui Hua","Xuegong Zhang","Lei Wei"],"pdf_url":"https://arxiv.org/pdf/2410.10652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14263v1","updated":"2025-06-17T07:25:07Z","published":"2025-06-17T07:25:07Z","title":"Towards Robust Learning to Optimize with Theoretical Guarantees","summary":"  Learning to optimize (L2O) is an emerging technique to solve mathematical\noptimization problems with learning-based methods. Although with great success\nin many real-world scenarios such as wireless communications, computer\nnetworks, and electronic design, existing L2O works lack theoretical\ndemonstration of their performance and robustness in out-of-distribution (OOD)\nscenarios. We address this gap by providing comprehensive proofs. First, we\nprove a sufficient condition for a robust L2O model with homogeneous\nconvergence rates over all In-Distribution (InD) instances. We assume an L2O\nmodel achieves robustness for an InD scenario. Based on our proposed\nmethodology of aligning OOD problems to InD problems, we also demonstrate that\nthe L2O model's convergence rate in OOD scenarios will deteriorate by an\nequation of the L2O model's input features. Moreover, we propose an L2O model\nwith a concise gradient-only feature construction and a novel gradient-based\nhistory modeling method. Numerical simulation demonstrates that our proposed\nmodel outperforms the state-of-the-art baseline in both InD and OOD scenarios\nand achieves up to 10 $\\times$ convergence speedup. The code of our method can\nbe found from https://github.com/NetX-lab/GoMathL2O-Official.\n","authors":["Qingyu Song","Wei Lin","Juncheng Wang","Hong Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14263v1.pdf","comment":"Published in CVPR 2024, 55 pages, 17 figures, this version fixed some\n  typo"},{"id":"http://arxiv.org/abs/2506.14262v1","updated":"2025-06-17T07:22:32Z","published":"2025-06-17T07:22:32Z","title":"Knowledge Adaptation as Posterior Correction","summary":"  Adaptation is the holy grail of intelligence, but even the best AI models\n(like GPT) lack the adaptivity of toddlers. So the question remains: how can\nmachines adapt quickly? Despite a lot of progress on model adaptation to\nfacilitate continual and federated learning, as well as model merging, editing,\nunlearning, etc., little is known about the mechanisms by which machines can\nnaturally learn to adapt in a similar way as humans and animals. Here, we show\nthat all such adaptation methods can be seen as different ways of `correcting'\nthe approximate posteriors. More accurate posteriors lead to smaller\ncorrections, which in turn imply quicker adaptation. The result is obtained by\nusing a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023)\nwhere interference created during adaptation is characterized by the\nnatural-gradient mismatch over the past data. We present many examples to\ndemonstrate the use of posterior-correction as a natural mechanism for the\nmachines to learn to adapt quickly.\n","authors":["Mohammad Emtiyaz Khan"],"pdf_url":"https://arxiv.org/pdf/2506.14262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14261v1","updated":"2025-06-17T07:22:20Z","published":"2025-06-17T07:22:20Z","title":"RL-Obfuscation: Can Language Models Learn to Evade Latent-Space\n  Monitors?","summary":"  Latent-space monitors aim to detect undesirable behaviours in large language\nmodels by leveraging internal model representations rather than relying solely\non black-box outputs. These methods have shown promise in identifying\nbehaviours such as deception and unsafe completions, but a critical open\nquestion remains: can LLMs learn to evade such monitors? To study this, we\nintroduce RL-Obfuscation, in which LLMs are finetuned via reinforcement\nlearning to bypass latent-space monitors while maintaining coherent\ngenerations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters\nand evaluate evasion success against a suite of monitors. We find that\ntoken-level latent-space monitors are highly vulnerable to this attack. More\nholistic monitors, such as max-pooling or attention-based probes, remain\nrobust. Moreover, we show that adversarial policies trained to evade a single\nstatic monitor generalise to unseen monitors of the same type. Finally, we\nstudy how the policy learned by RL bypasses these monitors and find that the\nmodel can also learn to repurpose tokens to mean something different\ninternally.\n","authors":["Rohan Gupta","Erik Jenner"],"pdf_url":"https://arxiv.org/pdf/2506.14261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14251v1","updated":"2025-06-17T07:15:28Z","published":"2025-06-17T07:15:28Z","title":"Convergence-Privacy-Fairness Trade-Off in Personalized Federated\n  Learning","summary":"  Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a\nbalance between personalization and generalization by conducting federated\nlearning (FL) to guide personalized learning (PL). While FL is unaffected by\npersonalized model training, in Ditto, PL depends on the outcome of the FL.\nHowever, the clients' concern about their privacy and consequent perturbation\nof their local models can affect the convergence and (performance) fairness of\nPL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension\nof Ditto under the protection of differential privacy (DP), and analyzes the\ntrade-off among its privacy guarantee, model convergence, and performance\ndistribution fairness. We also analyze the convergence upper bound of the\npersonalized models under DP-Ditto and derive the optimal number of global\naggregations given a privacy budget. Further, we analyze the performance\nfairness of the personalized models, and reveal the feasibility of optimizing\nDP-Ditto jointly for convergence and fairness. Experiments validate our\nanalysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of\nthe state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by\nover 32.71% in fairness and 9.66% in accuracy.\n","authors":["Xiyu Zhao","Qimei Cui","Weicai Li","Wei Ni","Ekram Hossain","Quan Z. Sheng","Xiaofeng Tao","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14239v1","updated":"2025-06-17T06:55:54Z","published":"2025-06-17T06:55:54Z","title":"Causes in neuron diagrams, and testing causal reasoning in Large\n  Language Models. A glimpse of the future of philosophy?","summary":"  We propose a test for abstract causal reasoning in AI, based on scholarship\nin the philosophy of causation, in particular on the neuron diagrams\npopularized by D. Lewis. We illustrate the test on advanced Large Language\nModels (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already\ncapable of correctly identifying causes in cases that are hotly debated in the\nliterature. In order to assess the results of these LLMs and future dedicated\nAI, we propose a definition of cause in neuron diagrams with a wider validity\nthan published hitherto, which challenges the widespread view that such a\ndefinition is elusive. We submit that these results are an illustration of how\nfuture philosophical research might evolve: as an interplay between human and\nartificial expertise.\n","authors":["Louis Vervoort","Vitaly Nikolaev"],"pdf_url":"https://arxiv.org/pdf/2506.14239v1.pdf","comment":"Accepted by Journal for General Philosophy of Science"},{"id":"http://arxiv.org/abs/2505.23247v2","updated":"2025-06-17T06:41:40Z","published":"2025-05-29T08:54:06Z","title":"Accelerating RLHF Training with Reward Variance Increase","summary":"  Reinforcement learning from human feedback (RLHF) is an essential technique\nfor ensuring that large language models (LLMs) are aligned with human values\nand preferences during the post-training phase. As an effective RLHF approach,\ngroup relative policy optimization (GRPO) has demonstrated success in many\nLLM-based applications. However, efficient GRPO-based RLHF training remains a\nchallenge. Recent studies reveal that a higher reward variance of the initial\npolicy model leads to faster RLHF training. Inspired by this finding, we\npropose a practical reward adjustment model to accelerate RLHF training by\nprovably increasing the reward variance and preserving the relative preferences\nand reward expectation. Our reward adjustment method inherently poses a\nnonconvex optimization problem, which is NP-hard to solve in general. To\novercome the computational challenges, we design a novel $O(n \\log n)$\nalgorithm to find a global solution of the nonconvex reward adjustment model by\nexplicitly characterizing the extreme points of the feasible set. As an\nimportant application, we naturally integrate this reward adjustment model into\nthe GRPO algorithm, leading to a more efficient GRPO with reward variance\nincrease (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we\nprovide an indirect explanation for the empirical effectiveness of GRPO with\nrule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment\nresults demonstrate that the GRPOVI algorithm can significantly improve the\nRLHF training efficiency compared to the original GRPO algorithm.\n","authors":["Zonglin Yang","Zhexuan Gu","Houduo Qi","Yancheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2505.23247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06926v2","updated":"2025-06-17T06:41:25Z","published":"2025-03-10T05:11:58Z","title":"Effect of Selection Format on LLM Performance","summary":"  This paper investigates a critical aspect of large language model (LLM)\nperformance: the optimal formatting of classification task options in prompts.\nThrough an extensive experimental study, we compared two selection formats --\nbullet points and plain English -- to determine their impact on model\nperformance. Our findings suggest that presenting options via bullet points\ngenerally yields better results, although there are some exceptions.\nFurthermore, our research highlights the need for continued exploration of\noption formatting to drive further improvements in model performance.\n","authors":["Yuchen Han","Yucheng Wu","Jeffrey Willard"],"pdf_url":"https://arxiv.org/pdf/2503.06926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14608v2","updated":"2025-06-17T06:21:26Z","published":"2023-03-26T03:01:39Z","title":"Analyzing Effects of Mixed Sample Data Augmentation on Model\n  Interpretability","summary":"  Mixed sample data augmentation strategies are actively used when training\ndeep neural networks (DNNs). Recent studies suggest that they are effective at\nvarious tasks. However, the impact of mixed sample data augmentation on model\ninterpretability has not been widely studied. In this paper, we explore the\nrelationship between model interpretability and mixed sample data augmentation,\nspecifically in terms of feature attribution maps. To this end, we introduce a\nnew metric that allows a comparison of model interpretability while minimizing\nthe impact of occlusion robustness of the model. Experimental results show that\nseveral mixed sample data augmentation decreases the interpretability of the\nmodel and label mixing during data augmentation plays a significant role in\nthis effect. This new finding suggests it is important to carefully adopt the\nmixed sample data augmentation method, particularly in applications where\nattribution map-based interpretability is important.\n","authors":["Soyoun Won","Sung-Ho Bae","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2303.14608v2.pdf","comment":"Accepted to Neural Networks"},{"id":"http://arxiv.org/abs/2506.14220v1","updated":"2025-06-17T06:17:19Z","published":"2025-06-17T06:17:19Z","title":"Can Large Language Models Improve Spectral Graph Neural Networks?","summary":"  Spectral Graph Neural Networks (SGNNs) have attracted significant attention\ndue to their ability to approximate arbitrary filters. They typically rely on\nsupervision from downstream tasks to adaptively learn appropriate filters.\nHowever, under label-scarce conditions, SGNNs may learn suboptimal filters,\nleading to degraded performance. Meanwhile, the remarkable success of Large\nLanguage Models (LLMs) has inspired growing interest in exploring their\npotential within the GNN domain. This naturally raises an important question:\n\\textit{Can LLMs help overcome the limitations of SGNNs and enhance their\nperformance?} In this paper, we propose a novel approach that leverages LLMs to\nestimate the homophily of a given graph. The estimated homophily is then used\nto adaptively guide the design of polynomial spectral filters, thereby\nimproving the expressiveness and adaptability of SGNNs across diverse graph\nstructures. Specifically, we introduce a lightweight pipeline in which the LLM\ngenerates homophily-aware priors, which are injected into the filter\ncoefficients to better align with the underlying graph topology. Extensive\nexperiments on benchmark datasets demonstrate that our LLM-driven SGNN\nframework consistently outperforms existing baselines under both homophilic and\nheterophilic settings, with minimal computational and monetary overhead.\n","authors":["Kangkang Lu","Yanhua Yu","Zhiyong Huang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2506.14220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14217v1","updated":"2025-06-17T06:12:36Z","published":"2025-06-17T06:12:36Z","title":"TriGuard: Testing Model Safety with Attribution Entropy, Verification,\n  and Drift","summary":"  Deep neural networks often achieve high accuracy, but ensuring their\nreliability under adversarial and distributional shifts remains a pressing\nchallenge. We propose TriGuard, a unified safety evaluation framework that\ncombines (1) formal robustness verification, (2) attribution entropy to\nquantify saliency concentration, and (3) a novel Attribution Drift Score\nmeasuring explanation stability. TriGuard reveals critical mismatches between\nmodel accuracy and interpretability: verified models can still exhibit unstable\nreasoning, and attribution-based signals provide complementary safety insights\nbeyond adversarial accuracy. Extensive experiments across three datasets and\nfive architectures show how TriGuard uncovers subtle fragilities in neural\nreasoning. We further demonstrate that entropy-regularized training reduces\nexplanation drift without sacrificing performance. TriGuard advances the\nfrontier in robust, interpretable model evaluation.\n","authors":["Dipesh Tharu Mahato","Rohan Poudel","Pramod Dhungana"],"pdf_url":"https://arxiv.org/pdf/2506.14217v1.pdf","comment":"12 pages, 6 tables, 6 figures"},{"id":"http://arxiv.org/abs/2201.01965v2","updated":"2025-06-17T06:09:41Z","published":"2022-01-06T08:24:11Z","title":"Efficient Global Optimization of Two-Layer ReLU Networks: Quadratic-Time\n  Algorithms and Adversarial Training","summary":"  The non-convexity of the artificial neural network (ANN) training landscape\nbrings inherent optimization difficulties. While the traditional\nback-propagation stochastic gradient descent (SGD) algorithm and its variants\nare effective in certain cases, they can become stuck at spurious local minima\nand are sensitive to initializations and hyperparameters. Recent work has shown\nthat the training of an ANN with ReLU activations can be reformulated as a\nconvex program, bringing hope to globally optimizing interpretable ANNs.\nHowever, naively solving the convex training formulation has an exponential\ncomplexity, and even an approximation heuristic requires cubic time. In this\nwork, we characterize the quality of this approximation and develop two\nefficient algorithms that train ANNs with global convergence guarantees. The\nfirst algorithm is based on the alternating direction method of multiplier\n(ADMM). It solves both the exact convex formulation and the approximate\ncounterpart. Linear global convergence is achieved, and the initial several\niterations often yield a solution with high prediction accuracy. When solving\nthe approximate formulation, the per-iteration time complexity is quadratic.\nThe second algorithm, based on the \"sampled convex programs\" theory, solves\nunconstrained convex formulations and converges to an approximately globally\noptimal classifier. The non-convexity of the ANN training landscape exacerbates\nwhen adversarial training is considered. We apply the robust convex\noptimization theory to convex training and develop convex formulations that\ntrain ANNs robust to adversarial inputs. Our analysis explicitly focuses on\none-hidden-layer fully connected ANNs, but can extend to more sophisticated\narchitectures.\n","authors":["Yatong Bai","Tanmay Gautam","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2201.01965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13617v5","updated":"2025-06-17T06:02:59Z","published":"2022-05-26T20:46:01Z","title":"Does DQN Learn?","summary":"  A primary requirement for any reinforcement learning method is that it should\nproduce policies that improve upon the initial guess. In this work, we show\nthat the widely used Deep Q-Network (DQN) fails to satisfy this minimal\ncriterion -- even when it gets to see all possible states and actions\ninfinitely often (a condition under which tabular Q-learning is guaranteed to\nconverge to the optimal Q-value function). Our specific contributions are\ntwofold. First, we numerically show that DQN often returns a policy that\nperforms worse than the initial one. Second, we offer a theoretical explanation\nfor this phenomenon in linear DQN, a simplified version of DQN that uses linear\nfunction approximation in place of neural networks while retaining the other\nkey components such as $\\epsilon$-greedy exploration, experience replay, and\ntarget network. Using tools from differential inclusion theory, we prove that\nthe limit points of linear DQN correspond to fixed points of projected Bellman\noperators. Crucially, we show that these fixed points need not relate to\noptimal -- or even near-optimal -- policies, thus explaining linear DQN's\nsub-optimal behaviors. We also give a scenario where linear DQN always\nidentifies the worst policy. Our work fills a longstanding gap in understanding\nthe convergence behaviors of Q-learning with function approximation and\n$\\epsilon$-greedy exploration.\n","authors":["Aditya Gopalan","Gugan Thoppe"],"pdf_url":"https://arxiv.org/pdf/2205.13617v5.pdf","comment":"24 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.17421v2","updated":"2025-06-17T05:58:01Z","published":"2025-02-24T18:53:31Z","title":"LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification","summary":"  As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.\n","authors":["Penghui Yang","Cunxiao Du","Fengzhuo Zhang","Haonan Wang","Tianyu Pang","Chao Du","Bo An"],"pdf_url":"https://arxiv.org/pdf/2502.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14202v1","updated":"2025-06-17T05:44:18Z","published":"2025-06-17T05:44:18Z","title":"DiffusionBlocks: Blockwise Training for Generative Models via\n  Score-Based Diffusion","summary":"  Training large neural networks with end-to-end backpropagation creates\nsignificant memory bottlenecks, limiting accessibility to state-of-the-art AI\nresearch. We propose $\\textit{DiffusionBlocks}$, a novel training framework\nthat interprets neural network blocks as performing denoising operations in a\ncontinuous-time diffusion process. By partitioning the network into\nindependently trainable blocks and optimizing noise level assignments based on\nequal cumulative probability mass, our approach achieves significant memory\nefficiency while maintaining competitive performance compared to traditional\nbackpropagation in generative tasks. Experiments on image generation and\nlanguage modeling tasks demonstrate memory reduction proportional to the number\nof blocks while achieving superior performance. DiffusionBlocks provides a\npromising pathway for democratizing access to large-scale neural network\ntraining with limited computational resources.\n","authors":["Makoto Shing","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2506.14202v1.pdf","comment":"To appear at TTODLer-FM Workshop of the 42nd International Conference\n  on Machine Learning"},{"id":"http://arxiv.org/abs/2410.01444v5","updated":"2025-06-17T05:34:44Z","published":"2024-10-02T11:54:06Z","title":"Geometric Signatures of Compositionality Across a Language Model's\n  Lifetime","summary":"  By virtue of linguistic compositionality, few syntactic rules and a finite\nlexicon can generate an unbounded number of sentences. That is, language,\nthough seemingly high-dimensional, can be explained using relatively few\ndegrees of freedom. An open question is whether contemporary language models\n(LMs) reflect the intrinsic simplicity of language that is enabled by\ncompositionality. We take a geometric view of this problem by relating the\ndegree of compositionality in a dataset to the intrinsic dimension (ID) of its\nrepresentations under an LM, a measure of feature complexity. We find not only\nthat the degree of dataset compositionality is reflected in representations'\nID, but that the relationship between compositionality and geometric complexity\narises due to learned linguistic features over training. Finally, our analyses\nreveal a striking contrast between nonlinear and linear dimensionality, showing\nthey respectively encode semantic and superficial aspects of linguistic\ncomposition.\n","authors":["Jin Hwa Lee","Thomas Jiralerspong","Lei Yu","Yoshua Bengio","Emily Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01444v5.pdf","comment":"Published at ACL 2025"},{"id":"http://arxiv.org/abs/2506.14198v1","updated":"2025-06-17T05:31:42Z","published":"2025-06-17T05:31:42Z","title":"AMPLIFY: Actionless Motion Priors for Robot Learning from Videos","summary":"  Action-labeled data for robotics is scarce and expensive, limiting the\ngeneralization of learned policies. In contrast, vast amounts of action-free\nvideo data are readily available, but translating these observations into\neffective policies remains a challenge. We introduce AMPLIFY, a novel framework\nthat leverages large-scale video data by encoding visual dynamics into compact,\ndiscrete motion tokens derived from keypoint trajectories. Our modular approach\nseparates visual motion prediction from action inference, decoupling the\nchallenges of learning what motion defines a task from how robots can perform\nit. We train a forward dynamics model on abundant action-free videos and an\ninverse dynamics model on a limited set of action-labeled examples, allowing\nfor independent scaling. Extensive evaluations demonstrate that the learned\ndynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x\nbetter pixel prediction accuracy compared to prior approaches, and broadly\nuseful. In downstream policy learning, our dynamics predictions enable a\n1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by\nlearning from action-free human videos, and the first generalization to LIBERO\ntasks from zero in-distribution action data. Beyond robotic control, we find\nthe dynamics learned by AMPLIFY to be a versatile latent world model, enhancing\nvideo prediction quality. Our results present a novel paradigm leveraging\nheterogeneous data sources to build efficient, generalizable world models. More\ninformation can be found at https://amplify-robotics.github.io/.\n","authors":["Jeremy A. Collins","Loránd Cheng","Kunal Aneja","Albert Wilcox","Benjamin Joffe","Animesh Garg"],"pdf_url":"https://arxiv.org/pdf/2506.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22743v2","updated":"2025-06-17T05:24:14Z","published":"2025-05-28T18:04:10Z","title":"Information-Computation Gaps in Quantum Learning via Low-Degree\n  Likelihood","summary":"  In a variety of physically relevant settings for learning from quantum data,\ndesigning protocols that can computationally efficiently extract information\nremains largely an art, and there are important cases where we believe this to\nbe impossible, that is, where there is an information-computation gap. While\nthere is a large array of tools in the classical literature for giving evidence\nfor average-case hardness of statistical inference problems, the corresponding\ntools in the quantum literature are far more limited. One such framework in the\nclassical literature, the low-degree method, makes predictions about hardness\nof inference problems based on the failure of estimators given by low-degree\npolynomials. In this work, we extend this framework to the quantum setting.\n  We establish a general connection between state designs and low-degree\nhardness. We use this to obtain the first information-computation gaps for\nlearning Gibbs states of random, sparse, non-local Hamiltonians. We also use it\nto prove hardness for learning random shallow quantum circuit states in a\nchallenging model where states can be measured in adaptively chosen bases. To\nour knowledge, the ability to model adaptivity within the low-degree framework\nwas open even in classical settings. In addition, we also obtain a low-degree\nhardness result for quantum error mitigation against strategies with\nsingle-qubit measurements.\n  We define a new quantum generalization of the planted biclique problem and\nidentify the threshold at which this problem becomes computationally hard for\nprotocols that perform local measurements. Interestingly, the complexity\nlandscape for this problem shifts when going from local measurements to more\nentangled single-copy measurements.\n  We show average-case hardness for the \"standard\" variant of Learning\nStabilizers with Noise and for agnostically learning product states.\n","authors":["Sitan Chen","Weiyuan Gong","Jonas Haferkamp","Yihui Quek"],"pdf_url":"https://arxiv.org/pdf/2505.22743v2.pdf","comment":"88 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.14194v1","updated":"2025-06-17T05:17:36Z","published":"2025-06-17T05:17:36Z","title":"A Variational Information Theoretic Approach to Out-of-Distribution\n  Detection","summary":"  We present a theory for the construction of out-of-distribution (OOD)\ndetection features for neural networks. We introduce random features for OOD\nthrough a novel information-theoretic loss functional consisting of two terms,\nthe first based on the KL divergence separates resulting in-distribution (ID)\nand OOD feature distributions and the second term is the Information\nBottleneck, which favors compressed features that retain the OOD information.\nWe formulate a variational procedure to optimize the loss and obtain OOD\nfeatures. Based on assumptions on OOD distributions, one can recover properties\nof existing OOD features, i.e., shaping functions. Furthermore, we show that\nour theory can predict a new shaping function that out-performs existing ones\non OOD benchmarks. Our theory provides a general framework for constructing a\nvariety of new features with clear explainability.\n","authors":["Sudeepta Mondal","Zhuolin Jiang","Ganesh Sundaramoorthi"],"pdf_url":"https://arxiv.org/pdf/2506.14194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03617v2","updated":"2025-06-17T05:17:16Z","published":"2025-05-06T15:16:38Z","title":"Understand the Effect of Importance Weighting in Deep Learning on\n  Dataset Shift","summary":"  We evaluate the effectiveness of importance weighting in deep neural networks\nunder label shift and covariate shift. On synthetic 2D data (linearly separable\nand moon-shaped) using logistic regression and MLPs, we observe that weighting\nstrongly affects decision boundaries early in training but fades with prolonged\noptimization. On CIFAR-10 with various class imbalances, only L2 regularization\n(not dropout) helps preserve weighting effects. In a covariate-shift\nexperiment, importance weighting yields no significant performance gain,\nhighlighting challenges on complex data. Our results call into question the\npractical utility of importance weighting for real-world distribution shifts.\n","authors":["Thien Nhan Vo"],"pdf_url":"https://arxiv.org/pdf/2505.03617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04097v2","updated":"2025-06-17T05:15:08Z","published":"2025-05-07T03:32:25Z","title":"3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data\n  Augmentation","summary":"  A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers.\n","authors":["Thien Nhan Vo","Bac Nam Ho"],"pdf_url":"https://arxiv.org/pdf/2505.04097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18770v3","updated":"2025-06-17T05:05:12Z","published":"2025-02-26T02:57:59Z","title":"Reward Shaping to Mitigate Reward Hacking in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\n\\emph{reward hacking}, where the agent exploits flaws in the reward function\nrather than learning the intended behavior, thus degrading alignment. Although\nreward shaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests two key\ndesign principles: (1) the RL reward should be bounded, and (2) the RL reward\nbenefits from rapid initial growth followed by gradual convergence. Guided by\nthese insights, we propose Preference As Reward (PAR), a novel approach that\nleverages the latent preferences embedded within the reward model as the signal\nfor reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. The code is available at\nhttps://github.com/PorUna-byte/PAR, and the Work done during the internship at\nStepFun by Jiayi Fu.\n","authors":["Jiayi Fu","Xuandong Zhao","Chengyuan Yao","Heng Wang","Qi Han","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.18770v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2506.06290v2","updated":"2025-06-17T04:58:45Z","published":"2025-05-16T23:07:51Z","title":"CellCLIP -- Learning Perturbation Effects in Cell Painting via\n  Text-Guided Contrastive Learning","summary":"  High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.\n","authors":["Mingyu Lu","Ethan Weinberger","Chanwoo Kim","Su-In Lee"],"pdf_url":"https://arxiv.org/pdf/2506.06290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00854v2","updated":"2025-06-17T04:58:31Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14186v1","updated":"2025-06-17T04:58:08Z","published":"2025-06-17T04:58:08Z","title":"Hard Contacts with Soft Gradients: Refining Differentiable Simulators\n  for Learning and Control","summary":"  Contact forces pose a major challenge for gradient-based optimization of\nrobot dynamics as they introduce jumps in the system's velocities.\nPenalty-based simulators, such as MuJoCo, simplify gradient computation by\nsoftening the contact forces. However, realistically simulating hard contacts\nrequires very stiff contact settings, which leads to incorrect gradients when\nusing automatic differentiation. On the other hand, using non-stiff settings\nstrongly increases the sim-to-real gap. We analyze the contact computation of\npenalty-based simulators to identify the causes of gradient errors. Then, we\npropose DiffMJX, which combines adaptive integration with MuJoCo XLA, to\nnotably improve gradient quality in the presence of hard contacts. Finally, we\naddress a key limitation of contact gradients: they vanish when objects do not\ntouch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism\nthat enables the simulator to generate informative contact gradients even\nbefore objects are in contact. To preserve physical realism, we apply CFD only\nin the backward pass using a straight-through trick, allowing us to compute\nuseful gradients without modifying the forward simulation.\n","authors":["Anselm Paulus","A. René Geist","Pierre Schumacher","Vít Musil","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2506.14186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09316v3","updated":"2025-06-17T04:56:46Z","published":"2025-06-11T01:25:06Z","title":"On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear\n  Attention","summary":"  Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose dual-state linear attention (DSLA), a novel design that\nmaintains two specialized hidden states-one for preserving historical context\nand one for tracking recency-thereby mitigating the short-range bias typical of\nlinear-attention architectures. To further balance efficiency and accuracy\nunder dynamic workload conditions, we introduce DSLA-Serve, an online adaptive\ndistillation framework that progressively replaces Transformer layers with DSLA\nlayers at inference time, guided by a sensitivity-based layer ordering.\nDSLA-Serve uses a chained fine-tuning strategy to ensure that each newly\nconverted DSLA layer remains consistent with previously replaced layers,\npreserving the overall quality. Extensive evaluations on commonsense reasoning,\nlong-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x\nfaster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.\n","authors":["Yeonju Ro","Zhenyu Zhang","Souvik Kundu","Zhangyang Wang","Aditya Akella"],"pdf_url":"https://arxiv.org/pdf/2506.09316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13628v2","updated":"2025-06-17T04:36:55Z","published":"2025-06-16T15:55:56Z","title":"Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm\n  Generation","summary":"  Synthetic data generation plays a crucial role in medical research by\nmitigating privacy concerns and enabling large-scale patient data analysis.\nThis study presents a beta-Variational Autoencoder Graph Convolutional Neural\nNetwork framework for generating synthetic Abdominal Aorta Aneurysms (AAA).\nUsing a small real-world dataset, our approach extracts key anatomical features\nand captures complex statistical relationships within a compact disentangled\nlatent space. To address data limitations, low-impact data augmentation based\non Procrustes analysis was employed, preserving anatomical integrity. The\ngeneration strategies, both deterministic and stochastic, manage to enhance\ndata diversity while ensuring realism. Compared to PCA-based approaches, our\nmodel performs more robustly on unseen data by capturing complex, nonlinear\nanatomical variations. This enables more comprehensive clinical and statistical\nanalyses than the original dataset alone. The resulting synthetic AAA dataset\npreserves patient privacy while providing a scalable foundation for medical\nresearch, device testing, and computational modeling.\n","authors":["Francesco Fabbri","Martino Andrea Scarpolini","Angelo Iollo","Francesco Viola","Francesco Tudisco"],"pdf_url":"https://arxiv.org/pdf/2506.13628v2.pdf","comment":"Typo in the title"},{"id":"http://arxiv.org/abs/2503.13562v2","updated":"2025-06-17T04:34:01Z","published":"2025-03-17T07:42:27Z","title":"Achieving Unbiased Multi-Instance Learning via Balanced Fine-Grained\n  Positive-Unlabeled Learning","summary":"  In real-world applications, it is often challenging to detect anomalous\nsamples when the anomalous information they contain is extremely limited. In\nsuch cases, both macro-level and micro-level detection using multi-instance\nlearning (MIL) encounter significant difficulties. The former struggles because\nnormal and anomalous samples are highly similar and hard to distinguish at the\nmacro level, while the latter is limited by the lack of labels at the micro\nlevel. In MIL, micro-level labels are inferred from macro-level labels, which\ncan lead to severe bias. Moreover, the more imbalanced the distribution between\nnormal and anomalous samples, the more pronounced these limitations become. In\nthis study, we observe that the MIL problem can be elegantly transformed into a\nfine-grained Positive-Unlabeled (PU) learning problem. This transformation\nallows us to address the imbalance issue in an unbiased manner using a\nmicro-level balancing mechanism. To this end, we propose a novel\nframework-Balanced Fine-Grained Positive-Unlabeled (BFGPU)-based on rigorous\ntheoretical foundations to address the challenges above. Extensive experiments\non both public and real-world datasets demonstrate the effectiveness of BFGPU,\nwhich outperforms existing methods, even in extreme scenarios where both macro\nand micro-level distributions are highly imbalanced. The code is open-sourced\nat https://github.com/BFGPU/BFGPU.\n","authors":["Lin-Han Jia","Lan-Zhe Guo","Zhi Zhou","Si-Ye Han","Zi-Wen Li","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2503.13562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05818v2","updated":"2025-06-17T04:24:53Z","published":"2024-05-09T14:56:49Z","title":"Fine-grained Analysis and Faster Algorithms for Iteratively Solving\n  Linear Systems","summary":"  Despite being a key bottleneck in many machine learning tasks, the cost of\nsolving large linear systems has proven challenging to quantify due to\nproblem-dependent quantities such as condition numbers. To tackle this, we\nconsider a fine-grained notion of complexity for solving linear systems, which\nis motivated by applications where the data exhibits low-dimensional structure,\nincluding spiked covariance models and kernel machines, and when the linear\nsystem is explicitly regularized, such as ridge regression. Concretely, let\n$\\kappa_\\ell$ be the ratio between the $\\ell$th largest and the smallest\nsingular value of $n\\times n$ matrix $A$. We give a stochastic algorithm based\non the Sketch-and-Project paradigm, that solves the linear system $Ax = b$,\nthat is, finds $\\bar{x}$ such that $\\|A\\bar{x} - b\\| \\le \\epsilon \\|b\\|$, in\ntime $\\bar O(\\kappa_\\ell\\cdot n^2\\log 1/\\epsilon)$, for any $\\ell =\nO(n^{0.729})$. This is a direct improvement over preconditioned conjugate\ngradient, and it provides a stronger separation between stochastic linear\nsolvers and algorithms accessing $A$ only through matrix-vector products. Our\nmain technical contribution is the new analysis of the first and second moments\nof the random projection matrix that arises in Sketch-and-Project.\n","authors":["Michał Dereziński","Daniel LeJeune","Deanna Needell","Elizaveta Rebrova"],"pdf_url":"https://arxiv.org/pdf/2405.05818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14167v1","updated":"2025-06-17T04:07:32Z","published":"2025-06-17T04:07:32Z","title":"Structured and Informed Probabilistic Modeling with the Thermodynamic\n  Kolmogorov-Arnold Model","summary":"  We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling\nby reinterpreting its inner functions as a Markov Kernel between probability\nspaces via inverse transform sampling. We present a generative model that is\ninterpretable, easy to design, and efficient. Our approach couples a\nKolmogorov-Arnold Network generator with independent energy-based priors,\ntrained via Maximum Likelihood. Inverse sampling enables fast inference, while\nprior knowledge can be incorporated before training to better align priors with\nposteriors, thereby improving learning efficiency and sample quality. The\nlearned prior is also recoverable and visualizable post-training, offering an\nempirical Bayes perspective. To address inflexibility and mitigate\nprior-posterior mismatch, we introduce scalable extensions based on mixture\ndistributions and Langevin Monte Carlo methods, admitting a trade-off between\nflexibility and training efficiency. Our contributions connect classical\nrepresentation theorems with modern probabilistic modeling, while balancing\ntraining stability, inference speed, and the quality and diversity of\ngenerations.\n","authors":["Prithvi Raj"],"pdf_url":"https://arxiv.org/pdf/2506.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13657v2","updated":"2025-06-17T04:05:44Z","published":"2025-06-16T16:18:21Z","title":"Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual\n  Object Detection in Educational Videos","summary":"  We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain.\n","authors":["Dipayan Biswas","Shishir Shah","Jaspal Subhlok"],"pdf_url":"https://arxiv.org/pdf/2506.13657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05657v3","updated":"2025-06-17T04:01:39Z","published":"2025-05-08T21:34:35Z","title":"ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior","summary":"  Blind Speech Separation (BSS) aims to separate multiple speech sources from\naudio mixtures recorded by a microphone array. The problem is challenging\nbecause it is a blind inverse problem, i.e., the microphone array geometry, the\nroom impulse response (RIR), and the speech sources, are all unknown. We\npropose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,\nand generative manner. The core idea builds on diffusion posterior sampling\n(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must\napproximate the likelihood by formulating a separate optimization problem. The\nsolution to the optimization approximates room acoustics and the relative\ntransfer functions between microphones. These approximations, along with the\ndiffusion priors, iterate through the ArrayDPS sampling process and ultimately\nyield separated voice sources. We only need a simple single-speaker speech\ndiffusion model as a prior along with the mixtures recorded at the microphones;\nno microphone array information is necessary. Evaluation results show that\nArrayDPS outperforms all baseline unsupervised methods while being comparable\nto supervised methods in terms of SDR. Audio demos are provided at:\nhttps://arraydps.github.io/ArrayDPSDemo/.\n","authors":["Zhongweiyang Xu","Xulin Fan","Zhong-Qiu Wang","Xilin Jiang","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2505.05657v3.pdf","comment":"Paper Accepted at ICML2025 Demo:\n  https://arraydps.github.io/ArrayDPSDemo/ Code:\n  https://github.com/ArrayDPS/ArrayDPS"},{"id":"http://arxiv.org/abs/2506.00555v2","updated":"2025-06-17T03:59:45Z","published":"2025-05-31T13:22:55Z","title":"MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal\n  Medical Reasoning","summary":"  Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential\nin multimodal diagnostic tasks. However, existing single-agent models struggle\nto generalize across diverse medical specialties, limiting their performance.\nRecent efforts introduce multi-agent collaboration frameworks inspired by\nclinical workflows, where general practitioners (GPs) and specialists interact\nin a fixed sequence. Despite improvements, these static pipelines lack\nflexibility and adaptability in reasoning. To address this, we propose\nMMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that\nenables dynamic, optimized collaboration among medical agents. Specifically, we\ntrain two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to\nassign patients to appropriate specialties, while the attending physician\nintegrates the judgments from multi-specialists and its own knowledge to make\nfinal decisions. To address the inconsistency in specialist outputs, we\nintroduce a curriculum learning (CL)-guided RL strategy that progressively\nteaches the attending physician to balance between imitating specialists and\ncorrecting their mistakes. Experiments on five medical VQA benchmarks\ndemonstrate that MMedAgent-RL not only outperforms both open-source and\nproprietary Med-LVLMs, but also exhibits human-like reasoning patterns.\nNotably, it achieves an average performance gain of 20.7% over supervised\nfine-tuning baselines.\n","authors":["Peng Xia","Jinglu Wang","Yibo Peng","Kaide Zeng","Xian Wu","Xiangru Tang","Hongtu Zhu","Yun Li","Shujie Liu","Yan Lu","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2506.00555v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.13038v2","updated":"2025-06-17T14:31:50Z","published":"2025-06-16T02:03:41Z","title":"HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for\n  Robust Multimodal Hallucination and Factuality Detection in VLMs","summary":"  Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains.\n","authors":["Zijian Zhang","Xuecheng Wu","Danlei Huang","Siyu Yan","Chong Peng","Xuezhi Cao"],"pdf_url":"https://arxiv.org/pdf/2506.13038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19834v5","updated":"2025-06-17T11:56:41Z","published":"2025-02-27T07:14:11Z","title":"Knowledge Bridger: Towards Training-free Missing Modality Completion","summary":"  Previous successful approaches to missing modality completion rely on\ncarefully designed fusion techniques and extensive pre-training on complete\ndata, which can limit their generalizability in out-of-domain (OOD) scenarios.\nIn this study, we pose a new challenge: can we develop a missing modality\ncompletion model that is both resource-efficient and robust to OOD\ngeneralization? To address this, we present a training-free framework for\nmissing modality completion that leverages large multimodal models (LMMs). Our\napproach, termed the \"Knowledge Bridger\", is modality-agnostic and integrates\ngeneration and ranking of missing modalities. By defining domain-specific\npriors, our method automatically extracts structured information from available\nmodalities to construct knowledge graphs. These extracted graphs connect the\nmissing modality generation and ranking modules through the LMM, resulting in\nhigh-quality imputations of missing modalities. Experimental results across\nboth general and medical domains show that our approach consistently\noutperforms competing methods, including in OOD generalization. Additionally,\nour knowledge-driven generation and ranking techniques demonstrate superiority\nover variants that directly employ LMMs for generation and ranking, offering\ninsights that may be valuable for applications in other domains.\n","authors":["Guanzhou Ke","Shengfeng He","Xiao Li Wang","Bo Wang","Guoqing Chao","Yuanyang Zhang","Yi Xie","HeXing Su"],"pdf_url":"https://arxiv.org/pdf/2502.19834v5.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2506.14427v1","updated":"2025-06-17T11:44:20Z","published":"2025-06-17T11:44:20Z","title":"M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization\n  Dataset","summary":"  In the field of speaker diarization, the development of technology is\nconstrained by two problems: insufficient data resources and poor\ngeneralization ability of deep learning models. To address these two problems,\nfirstly, we propose an automated method for constructing speaker diarization\ndatasets, which generates more accurate pseudo-labels for massive data through\nthe combination of audio and video. Relying on this method, we have released\nMulti-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD)\ndatasets. This dataset is derived from real network videos and is highly\ndiverse. In addition, we further propose a scenario-related model fine-tuning\nstrategy. Based on the general model pre-trained using the above dataset, we\ncombine the specific data of the target scenario (e.g., meetings) and achieve\ntargeted optimization by using Adapter and LoRA joint fine-tuning, thus\nachieving the model's domain adaptation. Our dataset and code have been\nopen-sourced at https://huggingface.co/spaces/OldDragon/m3sd.\n","authors":["Shilong Wu","Hang Chen","Jun Du"],"pdf_url":"https://arxiv.org/pdf/2506.14427v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.14396v1","updated":"2025-06-17T10:51:34Z","published":"2025-06-17T10:51:34Z","title":"Manipulated Regions Localization For Partially Deepfake Audio: A Survey","summary":"  With the development of audio deepfake techniques, attacks with partially\ndeepfake audio are beginning to rise. Compared to fully deepfake, it is much\nharder to be identified by the detector due to the partially cryptic\nmanipulation, resulting in higher security risks. Although some studies have\nbeen launched, there is no comprehensive review to systematically introduce the\ncurrent situations and development trends for addressing this issue. Thus, in\nthis survey, we are the first to outline a systematic introduction for\npartially deepfake audio manipulated region localization tasks, including the\nfundamentals, branches of existing methods, current limitations and potential\ntrends, providing a revealing insight into this scope.\n","authors":["Jiayi He","Jiangyan Yi","Jianhua Tao","Siding Zeng","Hao Gu"],"pdf_url":"https://arxiv.org/pdf/2506.14396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10013v2","updated":"2025-06-17T10:10:50Z","published":"2025-05-28T01:44:03Z","title":"Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives\n  for the Korean Millennials and Gen Z","summary":"  This study introduces the media artwork Dear Passenger, Please Wear a Mask,\ndesigned to offer a layered exploration of single-use mask waste, which\nescalated during the COVID-19 pandemic. The piece reframes underappreciated\necological concerns by interweaving digital nostalgia and airline travel\nrecollections of Millennials and Gen Z with a unique fantasy narrative. Via a\npoint-and-click game and an immersive exhibition, participants traverse both\nvirtual and real domains, facing ethical and environmental dilemmas. While it\nfosters empathy and potential action, resource use and post-experience\nengagement challenges persist.\n","authors":["Yerin Doh","Joonhyung Bae"],"pdf_url":"https://arxiv.org/pdf/2506.10013v2.pdf","comment":"Accepted at ISEA 2025 (International Symposium on Electronic Art)"},{"id":"http://arxiv.org/abs/2408.15521v3","updated":"2025-06-17T06:34:15Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v3.pdf","comment":"arXiv pre-print"},{"id":"http://arxiv.org/abs/2506.14223v1","updated":"2025-06-17T06:25:35Z","published":"2025-06-17T06:25:35Z","title":"Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature\n  Transcription","summary":"  Music transcription plays a pivotal role in Music Information Retrieval\n(MIR), particularly for stringed instruments like the guitar, where symbolic\nmusic notations such as MIDI lack crucial playability information. This\ncontribution introduces the Fretting-Transformer, an encoderdecoder model that\nutilizes a T5 transformer architecture to automate the transcription of MIDI\nsequences into guitar tablature. By framing the task as a symbolic translation\nproblem, the model addresses key challenges, including string-fret ambiguity\nand physical playability. The proposed system leverages diverse datasets,\nincluding DadaGP, GuitarToday, and Leduc, with novel data pre-processing and\ntokenization strategies. We have developed metrics for tablature accuracy and\nplayability to quantitatively evaluate the performance. The experimental\nresults demonstrate that the Fretting-Transformer surpasses baseline methods\nlike A* and commercial applications like Guitar Pro. The integration of\ncontext-sensitive processing and tuning/capo conditioning further enhances the\nmodel's performance, laying a robust foundation for future developments in\nautomated guitar transcription.\n","authors":["Anna Hamberger","Sebastian Murgul","Jochen Schmidt","Michael Heizmann"],"pdf_url":"https://arxiv.org/pdf/2506.14223v1.pdf","comment":"Accepted to the 50th International Computer Music Conference (ICMC),\n  2025"},{"id":"http://arxiv.org/abs/2506.00854v2","updated":"2025-06-17T04:58:31Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07871v2","updated":"2025-06-17T04:46:02Z","published":"2024-06-12T04:55:14Z","title":"Controllable Dance Generation with Style-Guided Motion Diffusion","summary":"  Dance plays an important role as an artistic form and expression in human\nculture, yet the creation of dance remains a challenging task. Most dance\ngeneration methods primarily rely solely on music, seldom taking into\nconsideration intrinsic attributes such as music style or genre. In this work,\nwe introduce Flexible Dance Generation with Style Description Prompts (DGSDP),\na diffusion-based framework suitable for diversified tasks of dance generation\nby fully leveraging the semantics of music style. The core component of this\nframework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a\nTransformer-based network and a music Style Modulation module. The MCSAD seemly\nintegrates music conditions and style description prompts into the dance\ngeneration framework, ensuring that generated dances are consistent with the\nmusic content and style. To facilitate flexible dance generation and\naccommodate different tasks, a spatial-temporal masking strategy is effectively\napplied in the backward diffusion process. The proposed framework successfully\ngenerates realistic dance sequences that are accurately aligned with music for\na variety of tasks such as long-term generation, dance in-betweening, dance\ninpainting, and etc. We hope that this work has the potential to inspire dance\ngeneration and creation, with promising applications in entertainment, art, and\neducation. Code is available on Github: https://github.com/mucunzhuzhu/DGSDP.\n","authors":["Hongsong Wang","Ying Zhu","Yang Zhang","Junbo Wang","Xin Geng","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05657v3","updated":"2025-06-17T04:01:39Z","published":"2025-05-08T21:34:35Z","title":"ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior","summary":"  Blind Speech Separation (BSS) aims to separate multiple speech sources from\naudio mixtures recorded by a microphone array. The problem is challenging\nbecause it is a blind inverse problem, i.e., the microphone array geometry, the\nroom impulse response (RIR), and the speech sources, are all unknown. We\npropose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,\nand generative manner. The core idea builds on diffusion posterior sampling\n(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must\napproximate the likelihood by formulating a separate optimization problem. The\nsolution to the optimization approximates room acoustics and the relative\ntransfer functions between microphones. These approximations, along with the\ndiffusion priors, iterate through the ArrayDPS sampling process and ultimately\nyield separated voice sources. We only need a simple single-speaker speech\ndiffusion model as a prior along with the mixtures recorded at the microphones;\nno microphone array information is necessary. Evaluation results show that\nArrayDPS outperforms all baseline unsupervised methods while being comparable\nto supervised methods in terms of SDR. Audio demos are provided at:\nhttps://arraydps.github.io/ArrayDPSDemo/.\n","authors":["Zhongweiyang Xu","Xulin Fan","Zhong-Qiu Wang","Xilin Jiang","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2505.05657v3.pdf","comment":"Paper Accepted at ICML2025 Demo:\n  https://arraydps.github.io/ArrayDPSDemo/ Code:\n  https://github.com/ArrayDPS/ArrayDPS"},{"id":"http://arxiv.org/abs/2309.14704v5","updated":"2025-06-17T03:46:08Z","published":"2023-09-26T06:56:01Z","title":"Tile Classification Based Viewport Prediction with Multi-modal Fusion\n  Transformer","summary":"  Viewport prediction is a crucial aspect of tile-based 360 video streaming\nsystem. However, existing trajectory based methods lack of robustness, also\noversimplify the process of information construction and fusion between\ndifferent modality inputs, leading to the error accumulation problem. In this\npaper, we propose a tile classification based viewport prediction method with\nMulti-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes\ntransformer-based networks to extract the long-range dependencies within each\nmodality, then mine intra- and inter-modality relations to capture the combined\nimpact of user historical inputs and video contents on future viewport\nselection. In addition, MFTR categorizes future tiles into two categories: user\ninterested or not, and selects future viewport as the region that contains most\nuser interested tiles. Comparing with predicting head trajectories, choosing\nfuture viewport based on tile's binary classification results exhibits better\nrobustness and interpretability. To evaluate our proposed MFTR, we conduct\nextensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows\nsuperior performance over state-of-the-art methods in terms of average\nprediction accuracy and overlap ratio, also presents competitive computation\nefficiency.\n","authors":["Zhihao Zhang","Yiwei Chen","Weizhan Zhang","Caixia Yan","Qinghua Zheng","Qi Wang","Wangdu Chen"],"pdf_url":"https://arxiv.org/pdf/2309.14704v5.pdf","comment":"This paper is accepted by ACM-MM 2023"}]},"2025-06-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2506.15683v1","updated":"2025-06-18T17:59:58Z","published":"2025-06-18T17:59:58Z","title":"PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via\n  Family-Aware Learning","summary":"  With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.\n","authors":["Yuhui Shi","Yehan Yang","Qiang Sheng","Hao Mi","Beizhe Hu","Chaoxi Xu","Juan Cao"],"pdf_url":"https://arxiv.org/pdf/2506.15683v1.pdf","comment":"17 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2506.15681v1","updated":"2025-06-18T17:59:49Z","published":"2025-06-18T17:59:49Z","title":"GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models","summary":"  Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.\n","authors":["Byung-Kwan Lee","Ryo Hachiuma","Yong Man Ro","Yu-Chiang Frank Wang","Yueh-Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15681v1.pdf","comment":"Project page: https://byungkwanlee.github.io/GenRecal-page/"},{"id":"http://arxiv.org/abs/2506.15679v1","updated":"2025-06-18T17:59:35Z","published":"2025-06-18T17:59:35Z","title":"Dense SAE Latents Are Features, Not Bugs","summary":"  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n","authors":["Xiaoqing Sun","Alessandro Stolfo","Joshua Engels","Ben Wu","Senthooran Rajamanoharan","Mrinmaya Sachan","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2506.15679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15677v1","updated":"2025-06-18T17:58:17Z","published":"2025-06-18T17:58:17Z","title":"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence","summary":"  AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.\n","authors":["Yining Hong","Rui Sun","Bingxuan Li","Xingcheng Yao","Maxine Wu","Alexander Chien","Da Yin","Ying Nian Wu","Zhecan James Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2506.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15676v1","updated":"2025-06-18T17:57:39Z","published":"2025-06-18T17:57:39Z","title":"Gender-Neutral Machine Translation Strategies in Practice","summary":"  Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Chi-kiu Lo"],"pdf_url":"https://arxiv.org/pdf/2506.15676v1.pdf","comment":"to appear at GITT 2025"},{"id":"http://arxiv.org/abs/2506.15674v1","updated":"2025-06-18T17:57:01Z","published":"2025-06-18T17:57:01Z","title":"Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers","summary":"  We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.\n","authors":["Tommaso Green","Martin Gubri","Haritz Puerto","Sangdoo Yun","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2506.15674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15662v1","updated":"2025-06-18T17:41:28Z","published":"2025-06-18T17:41:28Z","title":"CC-LEARN: Cohort-based Consistency Learning","summary":"  Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.\n","authors":["Xiao Ye","Shaswat Shrivastava","Zhaonan Li","Jacob Dineen","Shijie Lu","Avneet Ahuja","Ming Shen","Zhikun Xu","Ben Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.15662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15651v1","updated":"2025-06-18T17:29:19Z","published":"2025-06-18T17:29:19Z","title":"AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning","summary":"  Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.\n","authors":["Tevin Wang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.15651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15650v1","updated":"2025-06-18T17:28:37Z","published":"2025-06-18T17:28:37Z","title":"Oldies but Goldies: The Potential of Character N-grams for Romanian\n  Texts","summary":"  This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.\n","authors":["Dana Lupsa","Sanda-Maria Avram"],"pdf_url":"https://arxiv.org/pdf/2506.15650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.16065v2","updated":"2025-06-18T17:04:04Z","published":"2025-05-21T22:33:40Z","title":"Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated\n  Synthetic Data Augmentation","summary":"  Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.\n","authors":["Ruijie Xi","He Ba","Hao Yuan","Rishu Agrawal","Yuxin Tian","Ruoyan Long","Arul Prakash"],"pdf_url":"https://arxiv.org/pdf/2505.16065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15629v1","updated":"2025-06-18T17:00:54Z","published":"2025-06-18T17:00:54Z","title":"Revisiting Compositional Generalization Capability of Large Language\n  Models Considering Instruction Following Ability","summary":"  In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.\n","authors":["Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2506.15629v1.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.13346v3","updated":"2025-06-18T16:58:25Z","published":"2025-05-19T16:50:35Z","title":"J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Policy Optimization","summary":"  To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.\n","authors":["Austin Xu","Yilun Zhou","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2505.13346v3.pdf","comment":"25 pages, 4 figures, 6 tables. Updated with code and benchmark"},{"id":"http://arxiv.org/abs/2411.05060v4","updated":"2025-06-18T16:56:37Z","published":"2024-11-07T18:47:39Z","title":"A Guide to Misinformation Detection Data and Evaluation","summary":"  Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\npropose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the\nfield toward systemic solutions rather than inadvertently propagating issues in\nevaluation. Overall, this guide aims to provide a roadmap for higher quality\ndata and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nmisinfo-datasets.complexdatalab.com.\n","authors":["Camille Thibault","Jacob-Junqi Tian","Gabrielle Peloquin-Skulski","Taylor Lynn Curtis","James Zhou","Florence Laflamme","Yuxiang Guan","Reihaneh Rabbany","Jean-François Godbout","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2411.05060v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15623v1","updated":"2025-06-18T16:52:20Z","published":"2025-06-18T16:52:20Z","title":"Minding the Politeness Gap in Cross-cultural Communication","summary":"  Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.\n","authors":["Yuka Machino","Matthias Hofer","Max Siegel","Joshua B. Tenenbaum","Robert D. Hawkins"],"pdf_url":"https://arxiv.org/pdf/2506.15623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15617v1","updated":"2025-06-18T16:50:34Z","published":"2025-06-18T16:50:34Z","title":"The Compositional Architecture of Regret in Large Language Models","summary":"  Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.\n","authors":["Xiangxiang Cui","Shu Yang","Tianjin Huang","Wanyu Lin","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15617v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2506.09033v2","updated":"2025-06-18T16:49:26Z","published":"2025-06-10T17:56:45Z","title":"Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning","summary":"  The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To facilitate learning, we employ a lightweight\nrule-based reward comprising format rewards, final outcome rewards, and a novel\ncost reward for optimizing the balance between performance and cost, opening a\npathway toward enhancing performance-cost trade-offs via RL. Router-R1 also\nconditions only on simple model descriptors such as pricing, latency, and\nexample performance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.\n","authors":["Haozhen Zhang","Tao Feng","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.09033v2.pdf","comment":"Code is available at https://github.com/ulab-uiuc/Router-R1. Models\n  and Datasets are available at\n  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03"},{"id":"http://arxiv.org/abs/2506.15606v1","updated":"2025-06-18T16:30:02Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabrel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15598v1","updated":"2025-06-18T16:19:46Z","published":"2025-06-18T16:19:46Z","title":"From Model to Classroom: Evaluating Generated MCQs for Portuguese with\n  Narrative and Difficulty Concerns","summary":"  While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.\n","authors":["Bernardo Leite","Henrique Lopes Cardoso","Pedro Pinto","Abel Ferreira","Luís Abreu","Isabel Rangel","Sandra Monteiro"],"pdf_url":"https://arxiv.org/pdf/2506.15598v1.pdf","comment":"This is a preprint version of the manuscript currently under review\n  at an international journal"},{"id":"http://arxiv.org/abs/2506.15594v1","updated":"2025-06-18T16:09:18Z","published":"2025-06-18T16:09:18Z","title":"WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts","summary":"  Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.\n","authors":["Negar Foroutan","Angelika Romanou","Matin Ansaripour","Julian Martin Eisenschlos","Karl Aberer","Rémi Lebret"],"pdf_url":"https://arxiv.org/pdf/2506.15594v1.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2406.03847v3","updated":"2025-06-18T16:07:47Z","published":"2024-06-06T08:25:43Z","title":"Lean Workbook: A large-scale Lean problem set formalized from natural\n  language math problems","summary":"  Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.\n","authors":["Huaiyuan Ying","Zijian Wu","Yihan Geng","Zheng Yuan","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15583v1","updated":"2025-06-18T16:00:19Z","published":"2025-06-18T16:00:19Z","title":"DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through\n  Iterative Graph Refinement","summary":"  Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG\n","authors":["Shaoqing Lin","Chong Teng","Fei Li","Donghong Ji","Lizhen Qu","Zhuang Li"],"pdf_url":"https://arxiv.org/pdf/2506.15583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15569v1","updated":"2025-06-18T15:43:26Z","published":"2025-06-18T15:43:26Z","title":"SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification","summary":"  We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.\n","authors":["Chengye Wang","Yifei Shen","Zexi Kuang","Arman Cohan","Yilun Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.15569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15568v1","updated":"2025-06-18T15:43:16Z","published":"2025-06-18T15:43:16Z","title":"Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for\n  Evaluating Gender Diversity in Large Language Models","summary":"  We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.\n","authors":["Zhengyang Shan","Emily Ruth Diana","Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.15568v1.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2505.12992v3","updated":"2025-06-18T15:41:14Z","published":"2025-05-19T11:30:41Z","title":"Fractured Chain-of-Thought Reasoning","summary":"  Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning. Code is available at\nhttps://github.com/BaohaoLiao/frac-cot.\n","authors":["Baohao Liao","Hanze Dong","Yuhui Xu","Doyen Sahoo","Christof Monz","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.12992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15556v1","updated":"2025-06-18T15:29:02Z","published":"2025-06-18T15:29:02Z","title":"PredGen: Accelerated Inference of Large Language Models through\n  Input-Time Speculation for Real-Time Speech Interaction","summary":"  Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.\n","authors":["Shufan Li","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2506.15556v1.pdf","comment":"16 pages,4 figures"},{"id":"http://arxiv.org/abs/2505.24832v3","updated":"2025-06-18T15:27:03Z","published":"2025-05-30T17:34:03Z","title":"How much do language models memorize?","summary":"  We propose a new method for estimating how much a model knows about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: unintended memorization, the information a model contains about a\nspecific dataset, and generalization, the information a model contains about\nthe true data-generation process. When we completely eliminate generalization,\nwe can compute the total memorization, which provides an estimate of model\ncapacity: our measurements estimate that GPT-style models have a capacity of\napproximately 3.6 bits per parameter. We train language models on datasets of\nincreasing size and observe that models memorize until their capacity fills, at\nwhich point \"grokking\" begins, and unintended memorization decreases as models\nbegin to generalize. We train hundreds of transformer language models ranging\nfrom $500K$ to $1.5B$ parameters and produce a series of scaling laws relating\nmodel capacity and data size to membership inference.\n","authors":["John X. Morris","Chawin Sitawarin","Chuan Guo","Narine Kokhlikyan","G. Edward Suh","Alexander M. Rush","Kamalika Chaudhuri","Saeed Mahloujifar"],"pdf_url":"https://arxiv.org/pdf/2505.24832v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15553v1","updated":"2025-06-18T15:26:43Z","published":"2025-06-18T15:26:43Z","title":"Approximating Language Model Training Data from Weights","summary":"  Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.\n","authors":["John X. Morris","Junjie Oscar Yin","Woojeong Kim","Vitaly Shmatikov","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2506.15553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15545v1","updated":"2025-06-18T15:18:07Z","published":"2025-06-18T15:18:07Z","title":"RATTENTION: Towards the Minimal Sliding Window Size in Local-Global\n  Attention Models","summary":"  Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.\n","authors":["Bailin Wang","Chang Lan","Chong Wang","Ruoming Pang"],"pdf_url":"https://arxiv.org/pdf/2506.15545v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2506.15538v1","updated":"2025-06-18T15:13:07Z","published":"2025-06-18T15:13:07Z","title":"Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework","summary":"  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).\n","authors":["Laura Kopf","Nils Feldhus","Kirill Bykov","Philine Lou Bommer","Anna Hedström","Marina M. -C. Höhne","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2506.15538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07009v3","updated":"2025-06-18T15:08:29Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs","summary":"  Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Hätty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v3.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.15522v1","updated":"2025-06-18T14:58:13Z","published":"2025-06-18T14:58:13Z","title":"Lessons from Training Grounded LLMs with Verifiable Rewards","summary":"  Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.\n","authors":["Shang Hong Sim","Tej Deep Pala","Vernon Toh","Hai Leong Chieu","Amir Zadeh","Chuan Li","Navonil Majumder","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2506.15522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15621v3","updated":"2025-06-18T14:52:47Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Online Retrieval-augmented Generation for Radiology Question\n  Answering","summary":"  Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval-augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from Radiopaedia in real-time. Accuracy\nwas investigated. Statistical analyses were performed using bootstrapping. The\nresults were further compared with human performance. RadioRAG improved\ndiagnostic accuracy across most LLMs, with relative accuracy increases ranging\nup to 54% for different LLMs. It matched or exceeded non-RAG models and the\nhuman radiologist in question answering across radiologic subspecialties,\nparticularly in breast imaging and emergency radiology. However, the degree of\nimprovement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1\nsaw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,\nhighlighting variability in RadioRAG's effectiveness. LLMs benefit when\nprovided access to domain-specific data beyond their training data. RadioRAG\nshows potential to improve LLM accuracy and factuality in radiology question\nanswering by integrating real-time domain-specific data.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Lisa Adams","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v3.pdf","comment":"Published in Radiology: Artificial Intelligence"},{"id":"http://arxiv.org/abs/2506.08343v2","updated":"2025-06-18T14:43:36Z","published":"2025-06-10T01:54:04Z","title":"Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency","summary":"  Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.\n","authors":["Chenlong Wang","Yuanning Feng","Dongping Chen","Zhaoyang Chu","Ranjay Krishna","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.08343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15504v1","updated":"2025-06-18T14:42:34Z","published":"2025-06-18T14:42:34Z","title":"Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge","summary":"  Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.\n","authors":["Li Zheng","Sihang Wang","Hao Fei","Zuquan Peng","Fei Li","Jianming Fu","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2506.15504v1.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2410.17161v3","updated":"2025-06-18T14:42:07Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v3.pdf","comment":"ICML 2025 Poster Paper, Camera Ready Version"},{"id":"http://arxiv.org/abs/2506.15498v1","updated":"2025-06-18T14:37:59Z","published":"2025-06-18T14:37:59Z","title":"SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling","summary":"  Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.\n","authors":["Md Imbesat Hassan Rizvi","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2506.15498v1.pdf","comment":"8 pages main content, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.08327v2","updated":"2025-06-18T14:23:15Z","published":"2025-03-11T11:40:10Z","title":"Adding Chocolate to Mint: Mitigating Metric Interference in Machine\n  Translation","summary":"  As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (MINT), i.e., the use of the same\nor related metrics for both model tuning and evaluation. MINT can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of MINT in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMINT strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for-questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MINTADJUST, a method for more reliable\nevaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks\ntranslations and systems more accurately than state-of-the-art metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.\n","authors":["José Pombal","Nuno M. Guerreiro","Ricardo Rei","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2503.08327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15480v1","updated":"2025-06-18T14:13:56Z","published":"2025-06-18T14:13:56Z","title":"Context-Informed Grounding Supervision","summary":"  Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.\n","authors":["Hyunji Lee","Seunghyun Yoon","Yunjae Won","Hanseok Oh","Geewook Kim","Trung Bui","Franck Dernoncourt","Elias Stengel-Eskin","Mohit Bansal","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2506.15480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10912v2","updated":"2025-06-18T14:00:37Z","published":"2025-06-12T17:25:53Z","title":"Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular\n  Detoxification?","summary":"  Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.\n","authors":["Fei Lin","Ziyang Gong","Cong Wang","Yonglin Tian","Tengchao Zhang","Xue Yang","Gen Luo","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2506.10912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20302v3","updated":"2025-06-18T13:36:39Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"  Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose yet another approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurement for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in the OV tested originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some false mappings detected\nby OV systems are not actually untrue.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2409.20302v3.pdf","comment":"15 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.15456v1","updated":"2025-06-18T13:36:34Z","published":"2025-06-18T13:36:34Z","title":"Factorized RVQ-GAN For Disentangled Speech Tokenization","summary":"  We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks.\n","authors":["Sameer Khurana","Dominik Klement","Antoine Laurent","Dominik Bobos","Juraj Novosad","Peter Gazdik","Ellen Zhang","Zili Huang","Amir Hussein","Ricard Marxer","Yoshiki Masuyama","Ryo Aihara","Chiori Hori","Francois G. Germain","Gordon Wichern","Jonathan Le Roux"],"pdf_url":"https://arxiv.org/pdf/2506.15456v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2506.15455v1","updated":"2025-06-18T13:35:47Z","published":"2025-06-18T13:35:47Z","title":"RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation","summary":"  Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.\n","authors":["Xinnuo Xu","Rachel Lawrence","Kshitij Dubey","Atharva Pandey","Risa Ueno","Fabian Falck","Aditya V. Nori","Rahul Sharma","Amit Sharma","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2506.15455v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.15451v1","updated":"2025-06-18T13:24:04Z","published":"2025-06-18T13:24:04Z","title":"AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent\n  System Need","summary":"  Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.\n","authors":["Zhouhong Gu","Xiaoxuan Zhu","Yin Cai","Hao Shen","Xingzhou Chen","Qingyi Wang","Jialin Li","Xiaoran Shi","Haoran Guo","Wenxuan Huang","Hongwei Feng","Yanghua Xiao","Zheyu Ye","Yao Hu","Shaosheng Cao"],"pdf_url":"https://arxiv.org/pdf/2506.15451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14625v2","updated":"2025-06-18T13:21:13Z","published":"2025-06-17T15:22:21Z","title":"Probabilistic Aggregation and Targeted Embedding Optimization for\n  Collective Moral Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.\n","authors":["Chenchen Yuan","Zheyu Zhang","Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2506.14625v2.pdf","comment":"Accepted to ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2506.15425v1","updated":"2025-06-18T12:55:35Z","published":"2025-06-18T12:55:35Z","title":"Understanding GUI Agent Localization Biases through Logit Sharpness","summary":"  Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.\n","authors":["Xingjian Tao","Yiwei Wang","Yujun Cai","Zhicheng Yang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2506.15425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15415v1","updated":"2025-06-18T12:35:53Z","published":"2025-06-18T12:35:53Z","title":"Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in\n  Lugha-Llama via Early-Layer LoRA Fine-Tuning","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.\n","authors":["Stanley Ngugi"],"pdf_url":"https://arxiv.org/pdf/2506.15415v1.pdf","comment":"11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning"},{"id":"http://arxiv.org/abs/2503.01903v2","updated":"2025-06-18T12:24:25Z","published":"2025-02-28T12:17:41Z","title":"PsychBench: A comprehensive and professional benchmark for evaluating\n  the performance of LLM-assisted psychiatric clinical practice","summary":"  The advent of Large Language Models (LLMs) offers potential solutions to\naddress problems such as shortage of medical resources and low diagnostic\nconsistency in psychiatric clinical practice. Despite this potential, a robust\nand comprehensive benchmarking framework to assess the efficacy of LLMs in\nauthentic psychiatric clinical environments is absent. This has impeded the\nadvancement of specialized LLMs tailored to psychiatric applications. In\nresponse to this gap, by incorporating clinical demands in psychiatry and\nclinical data, we proposed a benchmarking system, PsychBench, to evaluate the\npractical performance of LLMs in psychiatric clinical settings. We conducted a\ncomprehensive quantitative evaluation of 16 LLMs using PsychBench, and\ninvestigated the impact of prompt design, chain-of-thought reasoning, input\ntext length, and domain-specific knowledge fine-tuning on model performance.\nThrough detailed error analysis, we identified strengths and potential\nlimitations of the existing models and suggested directions for improvement.\nSubsequently, a clinical reader study involving 60 psychiatrists of varying\nseniority was conducted to further explore the practical benefits of existing\nLLMs as supportive tools for psychiatrists of varying seniority. Through the\nquantitative and reader evaluation, we show that while existing models\ndemonstrate significant potential, they are not yet adequate as decision-making\ntools in psychiatric clinical practice. The reader study further indicates\nthat, as an auxiliary tool, LLM could provide particularly notable support for\njunior psychiatrists, effectively enhancing their work efficiency and overall\nclinical quality. To promote research in this area, we will make the dataset\nand evaluation framework publicly available, with the hope of advancing the\napplication of LLMs in psychiatric clinical settings.\n","authors":["Shuyu Liu","Ruoxi Wang","Ling Zhang","Xuequan Zhu","Rui Yang","Xinzhu Zhou","Fei Wu","Zhi Yang","Cheng Jin","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21342v3","updated":"2025-06-18T12:03:10Z","published":"2025-05-27T15:34:39Z","title":"PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims","summary":"  Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date. We introduce PEDANTIC (Patent Definiteness Examination\nCorpus), a novel dataset of 14k US patent claims from patent applications\nrelating to Natural Language Processing (NLP), annotated with reasons for\nindefiniteness. We construct PEDANTIC using a fully automatic pipeline that\nretrieves office action documents from the USPTO and uses Large Language Models\n(LLMs) to extract the reasons for indefiniteness. A human validation study\nconfirms the pipeline's accuracy in generating high-quality annotations. To\ngain insight beyond binary classification metrics, we implement an LLM-as-Judge\nevaluation that compares the free-form reasoning of every model-cited reason\nwith every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B\nand 72B struggle to outperform logistic regression baselines on definiteness\nprediction, even though they often correctly identify the underlying reasons.\nPEDANTIC provides a valuable resource for patent AI researchers, enabling the\ndevelopment of advanced examination models. We will publicly release the\ndataset and code.\n","authors":["Valentin Knappich","Annemarie Friedrich","Anna Hätty","Simon Razniewski"],"pdf_url":"https://arxiv.org/pdf/2505.21342v3.pdf","comment":"PatentSemTech@SIGIR2025"},{"id":"http://arxiv.org/abs/2506.15372v1","updated":"2025-06-18T11:38:23Z","published":"2025-06-18T11:38:23Z","title":"COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for\n  Summarization and Headline Generation","summary":"  Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.\n","authors":["Raghvendra Kumar","S. A. Mohammed Salman","Aryan Sahu","Tridib Nandi","Pragathi Y. P.","Sriparna Saha","Jose G. Moreno"],"pdf_url":"https://arxiv.org/pdf/2506.15372v1.pdf","comment":"ACL 2025 MAINs"},{"id":"http://arxiv.org/abs/2506.15355v1","updated":"2025-06-18T11:19:25Z","published":"2025-06-18T11:19:25Z","title":"SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models'\n  Knowledge of Indian Culture","summary":"  Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.\n","authors":["Arijit Maji","Raghvendra Kumar","Akash Ghosh"," Anushka","Sriparna Saha"],"pdf_url":"https://arxiv.org/pdf/2506.15355v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.15339v1","updated":"2025-06-18T10:42:22Z","published":"2025-06-18T10:42:22Z","title":"DeVisE: Behavioral Testing of Medical Large Language Models","summary":"  Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.\n","authors":["Camila Zurdo Tagliabue","Heloisa Oss Boll","Aykut Erdem","Erkut Erdem","Iacer Calixto"],"pdf_url":"https://arxiv.org/pdf/2506.15339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17267v2","updated":"2025-06-18T10:12:11Z","published":"2025-05-22T20:24:17Z","title":"GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and\n  Citations","summary":"  We introduce GreekBarBench, a benchmark that evaluates LLMs on legal\nquestions across five different legal areas from the Greek Bar exams, requiring\ncitations to statutory articles and case facts. To tackle the challenges of\nfree-text evaluation, we propose a three-dimensional scoring system combined\nwith an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to\nassess the correlation between LLM-judges and human expert evaluations,\nrevealing that simple, span-based rubrics improve their alignment. Our\nsystematic evaluation of 13 proprietary and open-weight LLMs shows that even\nthough the best models outperform average expert scores, they fall short of the\n95th percentile of experts.\n","authors":["Odysseas S. Chlapanis","Dimitrios Galanis","Nikolaos Aletras","Ion Androutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.17267v2.pdf","comment":"19 pages, 17 figures, submitted to May ARR"},{"id":"http://arxiv.org/abs/2506.15329v1","updated":"2025-06-18T10:01:17Z","published":"2025-06-18T10:01:17Z","title":"When and How Unlabeled Data Provably Improve In-Context Learning","summary":"  Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.\n","authors":["Yingcong Li","Xiangyu Chang","Muti Kara","Xiaofeng Liu","Amit Roy-Chowdhury","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2506.15329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14634v2","updated":"2025-06-18T09:56:49Z","published":"2025-06-17T15:28:53Z","title":"AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation","summary":"  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n","authors":["Leah von der Heyde","Anna-Carolina Haensch","Bernd Weiß","Jessica Daikeler"],"pdf_url":"https://arxiv.org/pdf/2506.14634v2.pdf","comment":"to appear in Survey Research Methods"},{"id":"http://arxiv.org/abs/2205.02225v4","updated":"2025-06-18T09:49:21Z","published":"2022-05-04T17:56:48Z","title":"HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised\n  Relation Extraction","summary":"  Unsupervised relation extraction aims to extract the relationship between\nentities from natural language sentences without prior information on\nrelational scope or distribution. Existing works either utilize self-supervised\nschemes to refine relational feature signals by iteratively leveraging adaptive\nclustering and classification that provoke gradual drift problems, or adopt\ninstance-wise contrastive learning which unreasonably pushes apart those\nsentence pairs that are semantically similar. To overcome these defects, we\npropose a novel contrastive learning framework named HiURE, which has the\ncapability to derive hierarchical signals from relational feature space using\ncross hierarchy attention and effectively optimize relation representation of\nsentences under exemplar-wise contrastive learning. Experimental results on two\npublic datasets demonstrate the advanced effectiveness and robustness of HiURE\non unsupervised relation extraction when compared with state-of-the-art models.\n","authors":["Shuliang Liu","Xuming Hu","Chenwei Zhang","Shu`ang Li","Lijie Wen","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2205.02225v4.pdf","comment":"In NAACL 2022 as a long paper. Code and data available at\n  https://github.com/THU-BPM/HiURE"},{"id":"http://arxiv.org/abs/2505.19797v3","updated":"2025-06-18T09:47:20Z","published":"2025-05-26T10:29:42Z","title":"The Avengers: A Simple Recipe for Uniting Smaller Language Models to\n  Challenge Proprietary Giants","summary":"  Proprietary giants are increasingly dominating the race for ever-larger\nlanguage models. Can open-source, smaller models remain competitive across a\nbroad range of tasks? In this paper, we present the Avengers -- a simple recipe\nthat leverages the collective intelligence of these smaller models. The\nAvengers builds upon four lightweight operations: (i) embedding: encode queries\nusing a text embedding model; (ii) clustering: group queries based on their\nsemantic similarity; (iii) scoring: scores each model's performance within each\ncluster; and (iv) voting: improve outputs via repeated sampling and voting. At\ninference time, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse with repeated sampling. Remarkably, with 10 open-source models (~7B\nparameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average\nperformance across 15 diverse datasets spanning mathematics, coding, logical\nreasoning, general knowledge, and affective tasks. In particular, it surpasses\nGPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,\nthe Avengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter -- the number of clusters.\n","authors":["Yiqun Zhang","Hao Li","Chenxu Wang","Linyao Chen","Qiaosheng Zhang","Peng Ye","Shi Feng","Daling Wang","Zhen Wang","Xinrun Wang","Jia Xu","Lei Bai","Wanli Ouyang","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2505.19797v3.pdf","comment":"9 pages, 4 figures, 6 tables, supplementary material (appendix)\n  included separately"},{"id":"http://arxiv.org/abs/2506.15304v1","updated":"2025-06-18T09:35:33Z","published":"2025-06-18T09:35:33Z","title":"ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification","summary":"  Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.\n","authors":["Negar Foroutan","Jakhongir Saydaliev","Ye Eun Kim","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2506.15304v1.pdf","comment":"Submitted to EMNLP"},{"id":"http://arxiv.org/abs/2506.15301v1","updated":"2025-06-18T09:32:16Z","published":"2025-06-18T09:32:16Z","title":"Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment","summary":"  Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.\n","authors":["Shrestha Ghosh","Moritz Schneider","Carina Reinicke","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2506.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13534v2","updated":"2025-06-18T08:59:03Z","published":"2024-02-21T05:04:29Z","title":"An Effective Incorporating Heterogeneous Knowledge Curriculum Learning\n  for Sequence Labeling","summary":"  Sequence labeling models often benefit from incorporating external knowledge.\nHowever, this practice introduces data heterogeneity and complicates the model\nwith additional modules, leading to increased expenses for training a\nhigh-performing model. To address this challenge, we propose a two-stage\ncurriculum learning (TCL) framework specifically designed for sequence labeling\ntasks. The TCL framework enhances training by gradually introducing data\ninstances from easy to hard, aiming to improve both performance and training\nspeed. Furthermore, we explore different metrics for assessing the difficulty\nlevels of sequence labeling tasks. Through extensive experimentation on six\nChinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we\ndemonstrate the effectiveness of our model in enhancing the performance of\nsequence labeling models. Additionally, our analysis indicates that TCL\naccelerates training and alleviates the slow training problem associated with\ncomplex models.\n","authors":["Xuemei Tang","Jun Wang","Qi Su","Chu-ren Huang","Jinghang Gu"],"pdf_url":"https://arxiv.org/pdf/2402.13534v2.pdf","comment":"10 pages, 9 tables, 3 figures, Accepted by ACL 2025 (short paper)"},{"id":"http://arxiv.org/abs/2506.15266v1","updated":"2025-06-18T08:41:28Z","published":"2025-06-18T08:41:28Z","title":"Thunder-DeID: Accurate and Efficient De-identification Framework for\n  Korean Court Judgments","summary":"  To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.\n","authors":["Sungen Hahm","Heejin Kim","Gyuseong Lee","Hyunji Park","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.15266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13612v4","updated":"2025-06-18T08:37:18Z","published":"2024-12-18T08:42:25Z","title":"Large Language Models for Automated Literature Review: An Evaluation of\n  Reference Generation, Abstract Writing, and Review Composition","summary":"  Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews.\n","authors":["Xuemei Tang","Xufeng Duan","Zhenguang G. Cai"],"pdf_url":"https://arxiv.org/pdf/2412.13612v4.pdf","comment":"12 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.15246v1","updated":"2025-06-18T08:24:27Z","published":"2025-06-18T08:24:27Z","title":"TopClustRAG at SIGIR 2025 LiveRAG Challenge","summary":"  We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.\n","authors":["Juli Bakagianni","John Pavlopoulos","Aristidis Likas"],"pdf_url":"https://arxiv.org/pdf/2506.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18043v2","updated":"2025-06-18T08:22:47Z","published":"2024-12-23T23:39:05Z","title":"Aligning AI Research with the Needs of Clinical Coding Workflows: Eight\n  Recommendations Based on US Data Analysis and Critical Review","summary":"  Clinical coding is crucial for healthcare billing and data analysis. Manual\nclinical coding is labour-intensive and error-prone, which has motivated\nresearch towards full automation of the process. However, our analysis, based\non US English electronic health records and automated coding research using\nthese records, shows that widely used evaluation methods are not aligned with\nreal clinical contexts. For example, evaluations that focus on the top 50 most\ncommon codes are an oversimplification, as there are thousands of codes used in\npractice. This position paper aims to align AI coding research more closely\nwith practical challenges of clinical coding. Based on our analysis, we offer\neight specific recommendations, suggesting ways to improve current evaluation\nmethods. Additionally, we propose new AI-based methods beyond automated coding,\nsuggesting alternative approaches to assist clinical coders in their workflows.\n","authors":["Yidong Gan","Maciej Rybinski","Ben Hachey","Jonathan K. Kummerfeld"],"pdf_url":"https://arxiv.org/pdf/2412.18043v2.pdf","comment":"Accepted to the ACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.15241v1","updated":"2025-06-18T08:20:29Z","published":"2025-06-18T08:20:29Z","title":"Research on Graph-Retrieval Augmented Generation Based on Historical\n  Text Knowledge Graphs","summary":"  This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.\n","authors":["Yang Fan","Zhang Qi","Xing Wenqian","Liu Chang","Liu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15239v1","updated":"2025-06-18T08:20:19Z","published":"2025-06-18T08:20:19Z","title":"Lost in Variation? Evaluating NLI Performance in Basque and Spanish\n  Geographical Variants","summary":"  In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.\n","authors":["Jaione Bengoetxea","Itziar Gonzalez-Dios","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2506.15239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13180v2","updated":"2025-06-18T08:02:29Z","published":"2025-06-16T07:47:34Z","title":"Dynamic Acoustic Model Architecture Optimization in Training for ASR","summary":"  Architecture design is inherently complex. Existing approaches rely on either\nhandcrafted rules, which demand extensive empirical expertise, or automated\nmethods like neural architecture search, which are computationally intensive.\nIn this paper, we introduce DMAO, an architecture optimization framework that\nemploys a grow-and-drop strategy to automatically reallocate parameters during\ntraining. This reallocation shifts resources from less-utilized areas to those\nparts of the model where they are most beneficial. Notably, DMAO only\nintroduces negligible training overhead at a given model complexity. We\nevaluate DMAO through experiments with CTC on LibriSpeech, TED-LIUM-v2 and\nSwitchboard datasets. The results show that, using the same amount of training\nresources, our proposed DMAO consistently improves WER by up to 6% relatively\nacross various architectures, model sizes, and datasets. Furthermore, we\nanalyze the pattern of parameter redistribution and uncover insightful\nfindings.\n","authors":["Jingjing Xu","Zijian Yang","Albert Zeyer","Eugen Beck","Ralf Schlueter","Hermann Ney"],"pdf_url":"https://arxiv.org/pdf/2506.13180v2.pdf","comment":"Accepted by Interspeech 2025"},{"id":"http://arxiv.org/abs/2407.11770v2","updated":"2025-06-18T08:01:35Z","published":"2024-07-16T14:28:56Z","title":"Robust Utility-Preserving Text Anonymization Based on Large Language\n  Models","summary":"  Anonymizing text that contains sensitive information is crucial for a wide\nrange of applications. Existing techniques face the emerging challenges of the\nre-identification ability of large language models (LLMs), which have shown\nadvanced capability in memorizing detailed information and reasoning over\ndispersed pieces of patterns to draw conclusions. When defending against\nLLM-based re-identification, anonymization could jeopardize the utility of the\nresulting anonymized data in downstream tasks. In general, the interaction\nbetween anonymization and data utility requires a deeper understanding within\nthe context of LLMs. In this paper, we propose a framework composed of three\nkey LLM-based components: a privacy evaluator, a utility evaluator, and an\noptimization component, which work collaboratively to perform anonymization.\nExtensive experiments demonstrate that the proposed model outperforms existing\nbaselines, showing robustness in reducing the risk of re-identification while\npreserving greater data utility in downstream tasks. We provide detailed\nstudies on these core modules. To consider large-scale and real-time\napplications, we investigate the distillation of the anonymization capabilities\ninto lightweight models. All of our code and datasets will be made publicly\navailable at https://github.com/UKPLab/acl2025-rupta.\n","authors":["Tianyu Yang","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.11770v2.pdf","comment":"Accepted by ACL'2025 Main Conference"},{"id":"http://arxiv.org/abs/2506.15220v1","updated":"2025-06-18T07:58:41Z","published":"2025-06-18T07:58:41Z","title":"video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07890v4","updated":"2025-06-18T07:55:43Z","published":"2025-05-11T14:30:56Z","title":"TSLFormer: A Lightweight Transformer Model for Turkish Sign Language\n  Recognition Using Skeletal Landmarks","summary":"  This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information. Our approach revisits sign language recognition\nas sequence-to-sequence translation, inspired by the linguistic nature of sign\nlanguages and the success of transformers in natural language processing. Since\nTSLFormer uses the self-attention mechanism, it effectively captures temporal\nco-occurrence within gesture sequences and highlights meaningful motion\npatterns as words unfold. Evaluated on the AUTSL dataset with over 36,000\nsamples and 227 different words, TSLFormer achieves competitive performance\nwith minimal computational cost. These results show that joint-based input is\nsufficient for enabling real-time, mobile, and assistive communication systems\nfor hearing-impaired individuals.\n","authors":["Kutay Ertürk","Furkan Altınışık","İrem Sarıaltın","Ömer Nezih Gerek"],"pdf_url":"https://arxiv.org/pdf/2505.07890v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15215v1","updated":"2025-06-18T07:49:13Z","published":"2025-06-18T07:49:13Z","title":"MinosEval: Distinguishing Factoid and Non-Factoid for Tailored\n  Open-Ended QA Evaluation with LLMs","summary":"  Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.\n","authors":["Yongqi Fan","Yating Wang","Guandong Wang","Jie Zhai","Jingping Liu","Qi Ye","Tong Ruan"],"pdf_url":"https://arxiv.org/pdf/2506.15215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15211v1","updated":"2025-06-18T07:44:09Z","published":"2025-06-18T07:44:09Z","title":"ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs","summary":"  Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.\n","authors":["Feng He","Zijun Chen","Xinnian Liang","Tingting Ma","Yunqi Qiu","Shuangzhi Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2506.15211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15208v1","updated":"2025-06-18T07:42:32Z","published":"2025-06-18T07:42:32Z","title":"A Comparative Study of Task Adaptation Techniques of Large Language\n  Models for Identifying Sustainable Development Goals","summary":"  In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).\n","authors":["Andrea Cadeddu","Alessandro Chessa","Vincenzo De Leo","Gianni Fenu","Enrico Motta","Francesco Osborne","Diego Reforgiato Recupero","Angelo Salatino","Luca Secchi"],"pdf_url":"https://arxiv.org/pdf/2506.15208v1.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2506.06955v2","updated":"2025-06-18T07:39:43Z","published":"2025-06-08T00:38:18Z","title":"BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning","summary":"  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.\n","authors":["Ha-Thanh Nguyen","Chaoran Liu","Koichi Takeda","Yusuke Miyao","Pontus Stenetorp","Qianying Liu","Su Myat Noe","Hideyuki Tachibana","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2506.06955v2.pdf","comment":"This version includes an updated literature review, added\n  acknowledgements, and a revised author list"},{"id":"http://arxiv.org/abs/2407.09861v4","updated":"2025-06-18T07:16:37Z","published":"2024-07-13T12:01:52Z","title":"A Systematic Survey of Natural Language Processing for the Greek\n  Language","summary":"  Comprehensive monolingual Natural Language Processing (NLP) surveys are\nessential for assessing language-specific challenges, resource availability,\nand research gaps. However, existing surveys often lack standardized\nmethodologies, leading to selection bias and fragmented coverage of NLP tasks\nand resources. This study introduces a generalizable framework for systematic\nmonolingual NLP surveys. Our approach integrates a structured search protocol\nto minimize bias, an NLP task taxonomy for classification, and language\nresource taxonomies to identify potential benchmarks and highlight\nopportunities for improving resource availability. We apply this framework to\nGreek NLP (2012-2023), providing an in-depth analysis of its current state,\ntask-specific progress, and resource gaps. The survey results are publicly\navailable (https://doi.org/10.5281/zenodo.15314882) and are regularly updated\nto provide an evergreen resource. This systematic survey of Greek NLP serves as\na case study, demonstrating the effectiveness of our framework and its\npotential for broader application to other not so well-resourced languages as\nregards NLP.\n","authors":["Juli Bakagianni","Kanella Pouli","Maria Gavriilidou","John Pavlopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.09861v4.pdf","comment":"This version matches the paper published in Patterns (Cell Press).\n  The title has been updated to reflect the published version"},{"id":"http://arxiv.org/abs/2506.13300v3","updated":"2025-06-18T06:57:58Z","published":"2025-06-16T09:42:05Z","title":"Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning\n  Language Models","summary":"  This paper presents Seewo's systems for both tracks of the Multilingual\nConversational Speech Language Model Challenge (MLC-SLM), addressing automatic\nspeech recognition (ASR) and speaker diarization with ASR (SD-ASR). We\nintroduce a multi-stage training pipeline that explicitly enhances reasoning\nand self-correction in speech language models for ASR. Our approach combines\ncurriculum learning for progressive capability acquisition, Chain-of-Thought\ndata augmentation to foster intermediate reflection, and Reinforcement Learning\nwith Verifiable Rewards (RLVR) to further refine self-correction through\nreward-driven optimization. This approach achieves substantial improvements\nover the official challenge baselines. On the evaluation set, our best system\nattains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track\n2. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent under challenge constraints.\n","authors":["Bo Li","Chengben Xu","Wufeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.13300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11171v5","updated":"2025-06-18T06:29:57Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Transparent, Compact and Competitive German-Only Language\n  Models from Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v5.pdf","comment":"camera ready @ACL25;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2506.13366v3","updated":"2025-06-18T06:17:35Z","published":"2025-06-16T11:15:21Z","title":"Enhancing Goal-oriented Proactive Dialogue Systems via Consistency\n  Reflection and Correction","summary":"  Goal-oriented proactive dialogue systems are designed to guide user\nconversations seamlessly towards specific objectives by planning a\ngoal-oriented path. However, previous research has focused predominantly on\noptimizing these paths while neglecting the inconsistencies that may arise\nbetween generated responses and dialogue contexts, including user profiles,\ndialogue history, domain knowledge, and subgoals. To address this issue, we\nintroduce a model-agnostic two-stage Consistency Reflection and Correction\n(CRC) framework. Specifically, in the consistency reflection stage, the model\nis prompted to reflect on the discrepancies between generated responses and\ndialogue contexts, identifying inconsistencies and suggesting possible\ncorrections. In the consistency correction stage, the model generates responses\nthat are more consistent with the dialogue context based on these reflection\nresults. We conducted experiments on various model architectures with different\nparameter sizes, including encoder-decoder models (BART, T5) and decoder-only\nmodels (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental\nresults on three datasets demonstrate that our CRC framework significantly\nimproves the consistency between generated responses and dialogue contexts.\n","authors":["Didi Zhang","Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.13366v3.pdf","comment":"Accepted by ACL'25 (main conference)"},{"id":"http://arxiv.org/abs/2505.18440v2","updated":"2025-06-18T06:11:08Z","published":"2025-05-24T00:22:52Z","title":"Efficient Long CoT Reasoning in Small Language Models","summary":"  Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.\n","authors":["Zhaoyang Wang","Jinqi Jiang","Tian Qiu","Hui Liu","Xianfeng Tang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2505.18440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15156v1","updated":"2025-06-18T06:02:02Z","published":"2025-06-18T06:02:02Z","title":"Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of\n  View","summary":"  We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.\n","authors":["Muhammad Cendekia Airlangga","Hilal AlQuabeh","Munachiso S Nwadike","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2506.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18799v4","updated":"2025-06-18T05:56:01Z","published":"2025-05-24T17:19:34Z","title":"ALPS: Attention Localization and Pruning Strategy for Efficient\n  Alignment of Large Language Models","summary":"  Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the Attention Localization and Pruning\nStrategy (ALPS), an efficient algorithm that localizes the most task-sensitive\nattention heads and prunes by restricting attention training updates to these\nheads, thereby reducing alignment costs. Experimental results demonstrate that\nour method activates only 10% of attention parameters during fine-tuning while\nachieving a 2% performance improvement over baselines on three tasks. Moreover,\nthe identified task-specific heads are transferable across datasets and\nmitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS.\n","authors":["Hao Chen","Haoze Li","Zhiqing Xiao","Lirong Gao","Qi Zhang","Xiaomeng Hu","Ningtao Wang","Xing Fu","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.18799v4.pdf","comment":"Accepted@ACL25-findings, 17 pages, 8 figures, 14 tables"},{"id":"http://arxiv.org/abs/2506.15154v1","updated":"2025-06-18T05:51:36Z","published":"2025-06-18T05:51:36Z","title":"SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning","summary":"  Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.\n","authors":["Anuradha Chopra","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2506.15154v1.pdf","comment":"14 pages, 2 figures, Accepted to AIMC 2025"},{"id":"http://arxiv.org/abs/2506.09507v3","updated":"2025-06-18T05:38:44Z","published":"2025-06-11T08:26:51Z","title":"TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary\n  Position Embedding","summary":"  Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongr inuity their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance.To address this\nimpediment, we propose a unified rotary position embedding (Unified RoPE)\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this Unified RoPE, we\nintroduce TransXSSM, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4 sequenceK length, TransXSSM exhibits training and inference speeds that are\n42.3% and 29.5% faster, respectively, relative to standard Transformer models.\nIt also delivers higher accuracy: under comparable settings, it surpasses a\nTransformer baseline by over 4% on language modeling benchmarks.TransXSSM\nfurthermore scales more effectively: TransXSSM-1.3B gains 7.22% in average\naccuracy over its 320M version (versus about 6% gains for equivalent\nTransformers or SSMs). Our results show that unified positional encoding\nresolves positional incompatibility in hybrid models, enabling efficient,\nhigh-performance long-context modeling.\n","authors":["Bingheng Wu","Jingze Shi","Yifan Wu","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2506.09507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06619v2","updated":"2025-06-18T04:49:11Z","published":"2025-06-07T01:33:44Z","title":"BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs","summary":"  A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work.\n","authors":["Jesse Woo","Fateme Hashemi Chaleshtori","Ana Marasović","Kenneth Marino"],"pdf_url":"https://arxiv.org/pdf/2506.06619v2.pdf","comment":"ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix"},{"id":"http://arxiv.org/abs/2506.15138v1","updated":"2025-06-18T04:40:44Z","published":"2025-06-18T04:40:44Z","title":"Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for\n  Generative Language Models","summary":"  This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.\n","authors":["Gyeongje Cho","Yeonkyoun So","Chanwoo Park","Sangmin Lee","Sungmok Jung","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.15138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14175v2","updated":"2025-06-18T04:31:51Z","published":"2025-06-17T04:34:27Z","title":"GRAM: A Generative Foundation Reward Model for Reward Generalization","summary":"  In aligning large language models (LLMs), reward models have played an\nimportant role, but are standardly trained as discriminative models and rely\nonly on labeled human preference data. In this paper, we explore methods that\ntrain reward models using both unlabeled and labeled data. Building on the\ngenerative models in LLMs, we develop a generative reward model that is first\ntrained via large-scale unsupervised learning and then fine-tuned via\nsupervised learning. We also show that by using label smoothing, we are in fact\noptimizing a regularized pairwise ranking loss. This result, in turn, provides\na new view of training reward models, which links generative models and\ndiscriminative models under the same class of training objectives. The outcome\nof these techniques is a foundation reward model, which can be applied to a\nwide range of tasks with little or no further fine-tuning effort. Extensive\nexperiments show that this model generalizes well across several tasks,\nincluding response ranking, reinforcement learning from human feedback, and\ntask adaptation with fine-tuning, achieving significant performance\nimprovements over several strong baseline models.\n","authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Qiaozhi He","Murun Yang","Bei Li","Tong Xiao","Chunliang Zhang","Tongran Liu","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.14175v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.15131v1","updated":"2025-06-18T04:19:33Z","published":"2025-06-18T04:19:33Z","title":"Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs","summary":"  Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.\n","authors":["Jing Yang Lee","Kong-Aik Lee","Woon-Seng Gan"],"pdf_url":"https://arxiv.org/pdf/2506.15131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03092v2","updated":"2025-06-18T04:06:03Z","published":"2024-12-04T07:44:35Z","title":"REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual\n  Optimization","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced the ability of LLM-based systems to perform complex tasks through\nnatural language processing and tool interaction. However, optimizing these\nLLM-based systems for specific tasks remains challenging, often requiring\nmanual interventions like prompt engineering and hyperparameter tuning.\nExisting automatic optimization methods, such as textual feedback-based\ntechniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to\nusing immediate derivatives in traditional numerical gradient descent. However,\nrelying solely on such feedback can be limited when the adjustments made in\nresponse to this feedback are either too small or fluctuate irregularly,\npotentially slowing down or even stalling the optimization process. To overcome\nthese challenges, more adaptive methods are needed, especially in situations\nwhere the system's response is evolving slowly or unpredictably. In this paper,\nwe introduce REVOLVE, an optimization method that tracks how \"R\"esponses\n\"EVOLVE\" across iterations in LLM systems. By focusing on the evolution of\nresponses over time, REVOLVE enables more stable and effective optimization by\nmaking thoughtful, progressive adjustments at each step. Experimental results\ndemonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%\nimprovement in prompt optimization, a 20.72% gain in solution refinement, and a\n29.17% increase in code optimization. Additionally, REVOLVE converges in fewer\niterations, resulting in significant computational savings. Beyond its\npractical contributions, REVOLVE highlights a promising direction, where the\nrich knowledge from established optimization principles can be leveraged to\nenhance LLM systems, which paves the way for further advancements in this\nhybrid domain.\n","authors":["Peiyan Zhang","Haibo Jin","Leyang Hu","Xinnuo Li","Liying Kang","Man Luo","Yangqiu Song","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.03092v2.pdf","comment":"20 pages, 2 figures, accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2502.19941v3","updated":"2025-06-18T04:05:18Z","published":"2025-02-27T10:11:53Z","title":"Alleviating Distribution Shift in Synthetic Data for Machine Translation\n  Quality Estimation","summary":"  Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce DCSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. DCSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\ntoken-level labels. DCSQE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both\nsupervised and unsupervised settings. Further analysis offers insights into\nsynthetic data generation that could benefit reward models for other tasks. The\ncode is available at https://github.com/NJUNLP/njuqe.\n","authors":["Xiang Geng","Zhejian Lai","Jiajun Chen","Hao Yang","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19941v3.pdf","comment":"ACL2025 Main"},{"id":"http://arxiv.org/abs/2505.11810v3","updated":"2025-06-18T03:57:51Z","published":"2025-05-17T03:43:16Z","title":"Efficiently Building a Domain-Specific Large Language Model from\n  Scratch: A Case Study of a Classical Chinese Large Language Model","summary":"  General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many natural language processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to language processing of Classical Chinese\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies.\n","authors":["Shen Li","Renfen Hu","Lijun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.11810v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16645v2","updated":"2025-06-18T03:54:24Z","published":"2025-02-23T16:46:18Z","title":"CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale","summary":"  Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.\n","authors":["Chenlong Wang","Zhaoyang Chu","Zhengxiang Cheng","Xuyi Yang","Kaiyue Qiu","Yao Wan","Zhou Zhao","Xuanhua Shi","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2502.16645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13040v6","updated":"2025-06-18T03:38:12Z","published":"2023-05-22T13:47:51Z","title":"SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents","summary":"  Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.\n","authors":["Shuzheng Si","Wentao Ma","Haoyu Gao","Yuchuan Wu","Ting-En Lin","Yinpei Dai","Hangyu Li","Rui Yan","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2305.13040v6.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2506.15118v1","updated":"2025-06-18T03:35:24Z","published":"2025-06-18T03:35:24Z","title":"CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records","summary":"  Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.\n","authors":["Junke Wang","Hongshun Ling","Li Zhang","Longqian Zhang","Fang Wang","Yuan Gao","Zhi Li"],"pdf_url":"https://arxiv.org/pdf/2506.15118v1.pdf","comment":"20 pages,5 figures"},{"id":"http://arxiv.org/abs/2501.09265v2","updated":"2025-06-18T03:29:11Z","published":"2025-01-16T03:30:47Z","title":"Perspective Transition of Large Language Models for Solving Subjective\n  Tasks","summary":"  Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, enabling remarkable progress in various tasks. Different\nfrom objective tasks such as commonsense reasoning and arithmetic\nquestion-answering, the performance of LLMs on subjective tasks is still\nlimited, where the perspective on the specific problem plays crucial roles for\nbetter interpreting the context and giving proper response. For example, in\ncertain scenarios, LLMs may perform better when answering from an expert role\nperspective, potentially eliciting their relevant domain knowledge. In\ncontrast, in some scenarios, LLMs may provide more accurate responses when\nanswering from a third-person standpoint, enabling a more comprehensive\nunderstanding of the problem and potentially mitigating inherent biases. In\nthis paper, we propose Reasoning through Perspective Transition (RPT), a method\nbased on in-context learning that enables LLMs to dynamically select among\ndirect, role, and third-person perspectives for the best way to solve\ncorresponding subjective problem. Through extensive experiments on totally 12\nsubjective tasks by using both closed-source and open-source LLMs including\nGPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single\nfixed perspective based methods such as chain-of-thought prompting and expert\nprompting, highlights the intricate ways that LLMs can adapt their perspectives\nto provide nuanced and contextually appropriate responses for different\nproblems.\n","authors":["Xiaolong Wang","Yuanchi Zhang","Ziyue Wang","Yuzhuang Xu","Fuwen Luo","Yile Wang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2501.09265v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2206.13155v2","updated":"2025-06-18T03:26:43Z","published":"2022-06-27T09:58:34Z","title":"Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich\n  Document Understanding","summary":"  Multi-modal document pre-trained models have proven to be very effective in a\nvariety of visually-rich document understanding (VrDU) tasks. Though existing\ndocument pre-trained models have achieved excellent performance on standard\nbenchmarks for VrDU, the way they model and exploit the interactions between\nvision and language on documents has hindered them from better generalization\nability and higher accuracy. In this work, we investigate the problem of\nvision-language joint representation learning for VrDU mainly from the\nperspective of supervisory signals. Specifically, a pre-training paradigm\ncalled Bi-VLDoc is proposed, in which a bidirectional vision-language\nsupervision strategy and a vision-language hybrid-attention mechanism are\ndevised to fully explore and utilize the interactions between these two\nmodalities, to learn stronger cross-modal document representations with richer\nsemantics. Benefiting from the learned informative cross-modal document\nrepresentations, Bi-VLDoc significantly advances the state-of-the-art\nperformance on three widely-used document understanding benchmarks, including\nForm Understanding (from 85.14% to 93.44%), Receipt Information Extraction\n(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).\nOn Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance\ncompared to previous single model methods.\n","authors":["Chuwei Luo","Guozhi Tang","Qi Zheng","Cong Yao","Lianwen Jin","Chenliang Li","Yang Xue","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2206.13155v2.pdf","comment":"IJDAR 2025"},{"id":"http://arxiv.org/abs/2502.14693v3","updated":"2025-06-18T03:15:07Z","published":"2025-02-20T16:19:09Z","title":"I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search","summary":"  Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.\nFurthermore, we integrate a Large Language Model (LLM)-based value model to\nfacilitate direct evaluation of each node's solution prior to conducting\ncomprehensive computational rollouts. A hybrid rewarding mechanism is\nimplemented to seamlessly transition the Q-value from LLM-estimated scores to\nactual performance scores. This allows higher-quality nodes to be traversed\nearlier. Applied to the various ML tasks, our approach demonstrates a 6%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.\nResource available at https://github.com/jokieleung/I-MCTS\n","authors":["Zujie Liang","Feng Wei","Wujiang Xu","Lin Chen","Yuxi Qian","Xinhui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.14693v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.21569v2","updated":"2025-06-18T03:05:54Z","published":"2025-05-27T06:22:57Z","title":"ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools","summary":"  Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.\n","authors":["Zhucong Li","Bowei Zhang","Jin Xiao","Zhijian Zhou","Fenglei Cao","Jiaqing Liang","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2505.21569v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2506.14731v2","updated":"2025-06-18T02:53:14Z","published":"2025-06-17T17:12:34Z","title":"Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs","summary":"  We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.\n","authors":[" Ling Team","Bin Hu","Cai Chen","Deng Zhao","Ding Liu","Dingnan Jin","Feng Zhu","Hao Dai","Hongzhi Luan","Jia Guo","Jiaming Liu","Jiewei Wu","Jun Mei","Jun Zhou","Junbo Zhao","Junwu Xiong","Kaihong Zhang","Kuan Xu","Lei Liang","Liang Jiang","Liangcheng Fu","Longfei Zheng","Qiang Gao","Qing Cui","Quan Wan","Shaomian Zheng","Shuaicheng Li","Tongkai Yang","Wang Ren","Xiaodong Yan","Xiaopei Wan","Xiaoyun Feng","Xin Zhao","Xinxing Yang","Xinyu Kong","Xuemin Yang","Yang Li","Yingting Wu","Yongkang Liu","Zhankai Xu","Zhenduo Zhang","Zhenglei Zhou","Zhenyu Huang","Zhiqiang Zhang","Zihao Wang","Zujie Wen"],"pdf_url":"https://arxiv.org/pdf/2506.14731v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.06809v3","updated":"2025-06-18T02:50:22Z","published":"2024-10-09T12:09:30Z","title":"Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level","summary":"  Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.\n","authors":["Xinyi Zeng","Yuying Shang","Jiawei Chen","Jingyuan Zhang","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2410.06809v3.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2506.15081v1","updated":"2025-06-18T02:47:14Z","published":"2025-06-18T02:47:14Z","title":"Improving Dialogue Discourse Parsing through Discourse-aware Utterance\n  Clarification","summary":"  Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.\n","authors":["Yaxin Fan","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.15081v1.pdf","comment":"Accepted by ACL2025(main conference)"},{"id":"http://arxiv.org/abs/2506.15076v1","updated":"2025-06-18T02:42:02Z","published":"2025-06-18T02:42:02Z","title":"Learning-Time Encoding Shapes Unlearning in LLMs","summary":"  As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.\n","authors":["Ruihan Wu","Konstantin Garov","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2506.15076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16205v6","updated":"2025-06-18T02:41:56Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought impressive\nadvancements across various tasks. However, despite these achievements, LLMs\nstill pose inherent safety risks, especially in the context of jailbreak\nattacks. Most existing jailbreak methods follow an input-level manipulation\nparadigm to bypass safety mechanisms. Yet, as alignment techniques improve,\nsuch attacks are becoming increasingly detectable. In this work, we identify an\nunderexplored threat vector: the model's internal reasoning process, which can\nbe manipulated to elicit harmful outputs in a more stealthy way. To explore\nthis overlooked attack surface, we propose a novel black-box jailbreak attack\nmethod, Analyzing-based Jailbreak (ABJ). ABJ comprises two independent attack\npaths: textual and visual reasoning attacks, which exploit the model's\nmultimodal reasoning capabilities to bypass safety mechanisms, comprehensively\nexposing vulnerabilities in its reasoning chain. We conduct extensive\nexperiments on ABJ across various open-source and closed-source LLMs, VLMs, and\nRLMs. In particular, ABJ achieves high attack success rate (ASR) (82.1% on\nGPT-4o-2024-11-20) with exceptional attack efficiency (AE) among all target\nmodels, showcasing its remarkable attack effectiveness, transferability, and\nefficiency. Our work reveals a new type of safety risk and highlights the\nurgent need to mitigate implicit vulnerabilities in the model's reasoning\nprocess.\n","authors":["Shi Lin","Hongming Yang","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15068v1","updated":"2025-06-18T02:16:53Z","published":"2025-06-18T02:16:53Z","title":"Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation","summary":"  Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.\n","authors":["Zongxia Li","Yapei Chang","Yuhang Zhou","Xiyang Wu","Zichao Liang","Yoo Yeon Sung","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2506.15068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16930v4","updated":"2025-06-18T02:05:18Z","published":"2024-10-22T12:00:58Z","title":"Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities\n  Using Only Forward Passes","summary":"  Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.\n","authors":["Bryan R. Christ","Zack Gottesman","Jonathan Kropko","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2410.16930v4.pdf","comment":"38 pages, 54 figures, Accepted to ACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2506.02803v2","updated":"2025-06-18T01:50:39Z","published":"2025-06-03T12:33:47Z","title":"SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via\n  Visual Global Thinking","summary":"  Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.\n","authors":["Sifan Li","Yujun Cai","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14397v2","updated":"2025-06-18T01:18:11Z","published":"2025-06-17T10:51:39Z","title":"Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation\n  Understanding","summary":"  Negation is a fundamental linguistic phenomenon that poses persistent\nchallenges for Large Language Models (LLMs), particularly in tasks requiring\ndeep semantic understanding. Existing benchmarks often treat negation as a side\ncase within broader tasks like natural language inference, resulting in a lack\nof benchmarks that exclusively target negation understanding. In this work, we\nintroduce Thunder-NUBench, a novel benchmark explicitly designed to assess\nsentence-level negation understanding in LLMs. Thunder-NUBench goes beyond\nsurface-level cue detection by contrasting standard negation with structurally\ndiverse alternatives such as local negation, contradiction, and paraphrase. The\nbenchmark consists of manually curated sentence-negation pairs and a\nmultiple-choice dataset that enables in-depth evaluation of models' negation\nunderstanding.\n","authors":["Yeonkyoung So","Gyuseong Lee","Sungmok Jung","Joonhak Lee","JiA Kang","Sangho Kim","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2506.14397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15041v1","updated":"2025-06-18T01:00:59Z","published":"2025-06-18T01:00:59Z","title":"Identifying economic narratives in large text corpora -- An integrated\n  approach using Large Language Models","summary":"  As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.\n","authors":["Tobias Schmidt","Kai-Robin Lange","Matthias Reccius","Henrik Müller","Michael Roos","Carsten Jentsch"],"pdf_url":"https://arxiv.org/pdf/2506.15041v1.pdf","comment":"53 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15030v1","updated":"2025-06-18T00:13:42Z","published":"2025-06-18T00:13:42Z","title":"Identifying social isolation themes in NVDRS text narratives using topic\n  modeling and text-classification methods","summary":"  Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.\n","authors":["Drew Walker","Swati Rajwal","Sudeshna Das","Snigdha Peddireddy","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2506.15030v1.pdf","comment":"22 pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2506.15029v1","updated":"2025-06-18T00:11:06Z","published":"2025-06-18T00:11:06Z","title":"An accurate and revised version of optical character recognition-based\n  speech synthesis using LabVIEW","summary":"  Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).\n","authors":["Prateek Mehta","Anasuya Patil"],"pdf_url":"https://arxiv.org/pdf/2506.15029v1.pdf","comment":"9 pages, 9 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2506.15684v1","updated":"2025-06-18T17:59:59Z","published":"2025-06-18T17:59:59Z","title":"Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards","summary":"  Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.\n","authors":["Qingming Liu","Zhen Liu","Dinghuai Zhang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2506.15684v1.pdf","comment":"Technical Report (21 pages, 21 figures)"},{"id":"http://arxiv.org/abs/2506.15682v1","updated":"2025-06-18T17:59:50Z","published":"2025-06-18T17:59:50Z","title":"Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model","summary":"  Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.\n","authors":["Anirud Aggarwal","Abhinav Shrivastava","Matthew Gwilliam"],"pdf_url":"https://arxiv.org/pdf/2506.15682v1.pdf","comment":"29 pages, 22 figures, 9 tables"},{"id":"http://arxiv.org/abs/2506.15680v1","updated":"2025-06-18T17:59:38Z","published":"2025-06-18T17:59:38Z","title":"Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos","summary":"  Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .\n","authors":["Kaifeng Zhang","Baoyu Li","Kris Hauser","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2506.15680v1.pdf","comment":"Project page: https://kywind.github.io/pgnd"},{"id":"http://arxiv.org/abs/2506.15677v1","updated":"2025-06-18T17:58:17Z","published":"2025-06-18T17:58:17Z","title":"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence","summary":"  AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.\n","authors":["Yining Hong","Rui Sun","Bingxuan Li","Xingcheng Yao","Maxine Wu","Alexander Chien","Da Yin","Ying Nian Wu","Zhecan James Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2506.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15675v1","updated":"2025-06-18T17:57:06Z","published":"2025-06-18T17:57:06Z","title":"Sekai: A Video Dataset towards World Exploration","summary":"  Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.\n","authors":["Zhen Li","Chuanhao Li","Xiaofeng Mao","Shaoheng Lin","Ming Li","Shitian Zhao","Zhaopan Xu","Xinyue Li","Yukang Feng","Jianwen Sun","Zizhen Li","Fanrui Zhang","Jiaxin Ai","Zhixiang Wang","Yuwei Wu","Tong He","Jiangmiao Pang","Yu Qiao","Yunde Jia","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15675v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.15673v1","updated":"2025-06-18T17:56:45Z","published":"2025-06-18T17:56:45Z","title":"UniRelight: Learning Joint Decomposition and Synthesis for Video\n  Relighting","summary":"  We address the challenge of relighting a single image or video, a task that\ndemands precise scene intrinsic understanding and high-quality light transport\nsynthesis. Existing end-to-end relighting models are often limited by the\nscarcity of paired multi-illumination data, restricting their ability to\ngeneralize across diverse scenes. Conversely, two-stage pipelines that combine\ninverse and forward rendering can mitigate data requirements but are\nsusceptible to error accumulation and often fail to produce realistic outputs\nunder complex lighting conditions or with sophisticated materials. In this\nwork, we introduce a general-purpose approach that jointly estimates albedo and\nsynthesizes relit outputs in a single pass, harnessing the generative\ncapabilities of video diffusion models. This joint formulation enhances\nimplicit scene comprehension and facilitates the creation of realistic lighting\neffects and intricate material interactions, such as shadows, reflections, and\ntransparency. Trained on synthetic multi-illumination data and extensive\nautomatically labeled real-world videos, our model demonstrates strong\ngeneralization across diverse domains and surpasses previous methods in both\nvisual fidelity and temporal consistency.\n","authors":["Kai He","Ruofan Liang","Jacob Munkberg","Jon Hasselgren","Nandita Vijaykumar","Alexander Keller","Sanja Fidler","Igor Gilitschenski","Zan Gojcic","Zian Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15673v1.pdf","comment":"Project page: https://research.nvidia.com/labs/toronto-ai/UniRelight/"},{"id":"http://arxiv.org/abs/2506.09042v3","updated":"2025-06-18T17:37:28Z","published":"2025-06-10T17:58:17Z","title":"Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with\n  World Foundation Models","summary":"  Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams\n","authors":["Xuanchi Ren","Yifan Lu","Tianshi Cao","Ruiyuan Gao","Shengyu Huang","Amirmojtaba Sabour","Tianchang Shen","Tobias Pfaff","Jay Zhangjie Wu","Runjian Chen","Seung Wook Kim","Jun Gao","Laura Leal-Taixe","Mike Chen","Sanja Fidler","Huan Ling"],"pdf_url":"https://arxiv.org/pdf/2506.09042v3.pdf","comment":"Only the core contributors are listed. The full list of contributors\n  can be found in Appendix A of this paper"},{"id":"http://arxiv.org/abs/2506.15649v1","updated":"2025-06-18T17:23:36Z","published":"2025-06-18T17:23:36Z","title":"Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning","summary":"  Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.\n","authors":["Ankan Deria","Adinath Madhavrao Dukre","Feilong Tang","Sara Atito","Sudipta Roy","Muhammad Awais","Muhammad Haris Khan","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2506.15649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15645v1","updated":"2025-06-18T17:14:07Z","published":"2025-06-18T17:14:07Z","title":"Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models","summary":"  Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.\n","authors":["Shuo Xing","Lanqing Guo","Hongyuan Hua","Seoyoung Lee","Peiran Li","Yufei Wang","Zhangyang Wang","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2506.15645v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2506.15635v1","updated":"2025-06-18T17:06:28Z","published":"2025-06-18T17:06:28Z","title":"FindingDory: A Benchmark to Evaluate Memory in Embodied Agents","summary":"  Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.\n","authors":["Karmesh Yadav","Yusuf Ali","Gunshi Gupta","Yarin Gal","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2506.15635v1.pdf","comment":"Our dataset and code will be made available at:\n  https://findingdory-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2407.21794v2","updated":"2025-06-18T17:03:35Z","published":"2024-07-31T17:59:58Z","title":"Generalized Out-of-Distribution Detection and Beyond in Vision Language\n  Model Era: A Survey","summary":"  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of these fields in\nthe VLM era. Our framework reveals that, with some field inactivity and\nintegration, the demanding challenges have become OOD detection and AD. Then,\nwe highlight the significant shift in the definition, problem settings, and\nbenchmarks; we thus feature a comprehensive review of the methodology for OOD\ndetection and related tasks to clarify their relationship to OOD detection.\nFinally, we explore the advancements in the emerging Large Vision Language\nModel (LVLM) era, such as GPT-4V. We conclude with open challenges and future\ndirections. The resource is available at\nhttps://github.com/AtsuMiyai/Awesome-OOD-VLM.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Yueqian Lin","Qing Yu","Go Irie","Shafiq Joty","Yixuan Li","Hai Li","Ziwei Liu","Toshihiko Yamasaki","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2407.21794v2.pdf","comment":"Accepted at TMLR2025. Survey paper. We welcome questions, issues, and\n  paper requests via https://github.com/AtsuMiyai/Awesome-OOD-VLM"},{"id":"http://arxiv.org/abs/2506.15625v1","updated":"2025-06-18T16:54:56Z","published":"2025-06-18T16:54:56Z","title":"HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization","summary":"  We present HOIDiNi, a text-driven diffusion framework for synthesizing\nrealistic and plausible human-object interaction (HOI). HOI generation is\nextremely challenging since it induces strict contact accuracies alongside a\ndiverse motion manifold. While current literature trades off between realism\nand physical correctness, HOIDiNi optimizes directly in the noise space of a\npretrained diffusion model using Diffusion Noise Optimization (DNO), achieving\nboth. This is made feasible thanks to our observation that the problem can be\nseparated into two phases: an object-centric phase, primarily making discrete\nchoices of hand-object contact locations, and a human-centric phase that\nrefines the full-body motion to realize this blueprint. This structured\napproach allows for precise hand-object contact without compromising motion\nnaturalness. Quantitative, qualitative, and subjective evaluations on the GRAB\ndataset alone clearly indicate HOIDiNi outperforms prior works and baselines in\ncontact accuracy, physical validity, and overall quality. Our results\ndemonstrate the ability to generate complex, controllable interactions,\nincluding grasping, placing, and full-body coordination, driven solely by\ntextual prompts. https://hoidini.github.io.\n","authors":["Roey Ron","Guy Tevet","Haim Sawdayee","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2506.15625v1.pdf","comment":"Project page: https://hoidini.github.io"},{"id":"http://arxiv.org/abs/2506.15610v1","updated":"2025-06-18T16:40:05Z","published":"2025-06-18T16:40:05Z","title":"BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via\n  Real-Time Multi-View Box Fusion","summary":"  Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.\n","authors":["Yuqing Lan","Chenyang Zhu","Zhirui Gao","Jiazhao Zhang","Yihan Cao","Renjiao Yi","Yijie Wang","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15610v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.08010v3","updated":"2025-06-18T16:30:46Z","published":"2025-06-09T17:59:57Z","title":"Vision Transformers Don't Need Trained Registers","summary":"  We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.\n","authors":["Nick Jiang","Amil Dravid","Alexei Efros","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2506.08010v3.pdf","comment":"Project page and code: https://avdravid.github.io/test-time-registers"},{"id":"http://arxiv.org/abs/2405.14022v5","updated":"2025-06-18T16:30:05Z","published":"2024-05-22T21:55:58Z","title":"I2I-Mamba: Multi-modal medical image synthesis via selective state space\n  modeling","summary":"  Multi-modal medical image synthesis involves nonlinear transformation of\ntissue signals between source and target modalities, where tissues exhibit\ncontextual interactions across diverse spatial distances. As such, the utility\nof a network architecture in synthesis depends on its ability to express these\ncontextual features. Convolutional neural networks (CNNs) offer high local\nprecision at the expense of poor sensitivity to long-range context. While\ntransformers promise to alleviate this issue, they suffer from an unfavorable\ntrade-off between sensitivity to long- versus short-range context due to the\nintrinsic complexity of attention filters. To effectively capture contextual\nfeatures while avoiding the complexity-driven trade-offs, here we introduce a\nnovel multi-modal synthesis method, I2I-Mamba, based on the state space\nmodeling (SSM) framework. Focusing on semantic representations across a hybrid\nresidual architecture, I2I-Mamba leverages novel dual-domain Mamba (ddMamba)\nblocks for complementary contextual modeling in image and Fourier domains,\nwhile maintaining spatial precision with convolutional layers. Diverting from\nconventional raster-scan trajectories, ddMamba leverages novel SSM operators\nbased on a spiral-scan trajectory to learn context with enhanced radial\ncoverage and angular isotropy, and a channel-mixing layer to aggregate context\nacross the channel dimension. Comprehensive demonstrations on multi-contrast\nMRI and MRI-CT protocols indicate that I2I-Mamba offers superior performance\nagainst state-of-the-art CNNs, transformers and SSMs.\n","authors":["Omer F. Atli","Bilal Kabas","Fuat Arslan","Arda C. Demirtas","Mahmut Yurt","Onat Dalmaz","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2405.14022v5.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.17244v4","updated":"2025-06-18T16:28:29Z","published":"2025-02-24T15:21:02Z","title":"A dataset of high-resolution plantar pressures for gait analysis across\n  varying footwear and walking speeds","summary":"  Gait refers to the patterns of limb movement generated during walking, which\nare unique to each individual due to both physical and behavioral traits.\nWalking patterns have been widely studied in biometrics, biomechanics, sports,\nand rehabilitation. While traditional methods rely on video and motion capture,\nadvances in plantar pressure sensing technology now offer deeper insights into\ngait. However, underfoot pressures during walking remain underexplored due to\nthe lack of large, publicly accessible datasets. To address this, we introduce\nthe UNB StepUP-P150 dataset: a footStep database for gait analysis and\nrecognition using Underfoot Pressure, including data from 150 individuals. This\ndataset comprises high-resolution plantar pressure data (4 sensors per\ncm-squared) collected using a 1.2m by 3.6m pressure-sensing walkway. It\ncontains over 200,000 footsteps from participants walking with various speeds\n(preferred, slow-to-stop, fast, and slow) and footwear conditions (barefoot,\nstandard shoes, and two personal shoes), supporting advancements in biometric\ngait recognition and presenting new research opportunities in biomechanics and\ndeep learning. UNB StepUP-P150 establishes a new benchmark for plantar\npressure-based gait analysis and recognition.\n","authors":["Robyn Larracy","Angkoon Phinyomark","Ala Salehi","Eve MacDonald","Saeed Kazemi","Shikder Shafiul Bashar","Aaron Tabor","Erik Scheme"],"pdf_url":"https://arxiv.org/pdf/2502.17244v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01481v3","updated":"2025-06-18T16:21:42Z","published":"2025-05-02T15:58:38Z","title":"VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding","summary":"  Synthetic video generation has gained significant attention for its realism\nand broad applications, but remains prone to violations of common sense and\nphysical laws. This highlights the need for reliable abnormality detectors that\nunderstand such principles and are robust to hallucinations. To address this,\nwe introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from\nsynthetic videos generated by models like Veo2, Sora, and Kling, paired with\nexpert-crafted counterintuitive QA to evaluate the critical thinking abilities\nof Multi-modal Large Language Models (MLLMs) on abnormalities that are\nperceptually obvious to humans but often hallucinated due to language priors.\nVideoHallu evaluates MLLMs' abnormality detection abilities with examples\nacross alignment, consistency, commonsense, and physics. We benchmark SOTA\nMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and\nVideoChat-R1. We observe that these models perform well on many real-world\nbenchmarks like MVBench and MovieChat, but still struggle with basic\nphysics-based and commonsense reasoning in synthetic videos. We further show\nthat post-training with Group Relative Policy Optimization (GRPO), using\ncurriculum learning on datasets combining video QA with counterintuitive\ncommonsense and physics reasoning over real and synthetic videos, improves\nMLLMs' abnormality detection and critical thinking, demonstrating the value of\ntargeted training for improving their understanding of commonsense and physical\nlaws. Our code is available at https://github.com/zli12321/VideoHallu.git.\n","authors":["Zongxia Li","Xiyang Wu","Guangyao Shi","Yubin Qin","Hongyang Du","Tianyi Zhou","Dinesh Manocha","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2505.01481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15596v1","updated":"2025-06-18T16:12:46Z","published":"2025-06-18T16:12:46Z","title":"Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image\n  Registration","summary":"  In clinical practice, imaging modalities with functional characteristics,\nsuch as positron emission tomography (PET) and fractional anisotropy (FA), are\noften aligned with a structural reference (e.g., MRI, CT) for accurate\ninterpretation or group analysis, necessitating multi-modal deformable image\nregistration (DIR). However, due to the extreme heterogeneity of these\nmodalities compared to standard structural scans, conventional unsupervised DIR\nmethods struggle to learn reliable spatial mappings and often distort images.\nWe find that the similarity metrics guiding these models fail to capture\nalignment between highly disparate modalities. To address this, we propose\nM2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal\nDIR models using only mono-modal similarity while preserving the established\narchitectural paradigm for seamless integration into existing models. We also\nintroduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training\nscheme to promote diffeomorphism. Furthermore, our framework naturally extends\nto a semi-supervised setting, integrating pre-aligned and unaligned pairs only,\nwithout requiring ground-truth transformations or segmentation masks.\nExperiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for\nPET-MRI and FA-MRI registration, highlighting its effectiveness in handling\nhighly heterogeneous multi-modal DIR. Our code is available at\nhttps://github.com/MICV-yonsei/M2M-Reg.\n","authors":["Kyobin Choo","Hyunkyung Han","Jinyeong Kim","Chanyong Yoon","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2506.15596v1.pdf","comment":"11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025"},{"id":"http://arxiv.org/abs/2506.15591v1","updated":"2025-06-18T16:06:30Z","published":"2025-06-18T16:06:30Z","title":"One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution","summary":"  It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.\n","authors":["Yujing Sun","Lingchen Sun","Shuaizheng Liu","Rongyuan Wu","Zhengqiang Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.08013v2","updated":"2025-06-18T16:02:35Z","published":"2025-05-12T19:24:45Z","title":"RDD: Robust Feature Detector and Descriptor using Deformable Transformer","summary":"  As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes.\n","authors":["Gonglin Chen","Tianwen Fu","Haiwei Chen","Wenbin Teng","Hanyuan Xiao","Yajie Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.08013v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11302v2","updated":"2025-06-18T15:59:47Z","published":"2025-06-12T21:08:11Z","title":"TARDIS STRIDE: A Spatio-Temporal Road Image Dataset and World Model for\n  Autonomy","summary":"  World models aim to simulate environments and enable effective agent\nbehavior. However, modeling real-world environments presents unique challenges\nas they dynamically change across both space and, crucially, time. To capture\nthese composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for\nExploration (STRIDE) permuting 360-degree panoramic imagery into rich\ninterconnected observation, state and action nodes. Leveraging this structure,\nwe can simultaneously model the relationship between egocentric views,\npositional coordinates, and movement commands across both space and time. We\nbenchmark this dataset via TARDIS, a transformer-based generative world model\nthat integrates spatial and temporal dynamics through a unified autoregressive\nframework trained on STRIDE. We demonstrate robust performance across a range\nof agentic tasks such as controllable photorealistic image synthesis,\ninstruction following, autonomous self-control, and state-of-the-art\ngeoreferencing. These results suggest a promising direction towards\nsophisticated generalist agents--capable of understanding and manipulating the\nspatial and temporal aspects of their material environments--with enhanced\nembodied reasoning capabilities. Training code, datasets, and model checkpoints\nare made available at https://huggingface.co/datasets/Tera-AI/STRIDE.\n","authors":["Héctor Carrión","Yutong Bai","Víctor A. Hernández Castro","Kishan Panaganti","Ayush Zenith","Matthew Trang","Tony Zhang","Pietro Perona","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2506.11302v2.pdf","comment":"Computer Vision, Pattern Recognition, Early-Fusion, Dataset, Data\n  Augmentation"},{"id":"http://arxiv.org/abs/2506.15577v1","updated":"2025-06-18T15:55:47Z","published":"2025-06-18T15:55:47Z","title":"A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and\n  Non-Destructive Biomass Estimation from Point Clouds","summary":"  Estimating forest above-ground biomass (AGB) is crucial for assessing carbon\nstorage and supporting sustainable forest management. Quantitative Structural\nModel (QSM) offers a non-destructive approach to AGB estimation through 3D tree\nstructural reconstruction. However, current QSM methods face significant\nlimitations, as they are primarily designed for individual trees,depend on\nhigh-quality point cloud data from terrestrial laser scanning (TLS), and also\nrequire multiple pre-processing steps that hinder scalability and practical\ndeployment. This study presents a novel unified framework that enables\nend-to-end processing of large-scale point clouds using an innovative\ngraph-based pipeline. The proposed approach seamlessly integrates tree\nsegmentation,leaf-wood separation and 3D skeletal reconstruction through\ndedicated graph operations including pathing and abstracting for tree topology\nreasoning. Comprehensive validation was conducted on datasets with varying leaf\nconditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and\ndata sources (TLS and UAV-based laser scanning, ULS). Experimental results\ndemonstrate strong performance under challenging conditions, particularly in\nleaf-on scenarios (~20% relative error) and low-density ULS datasets with\npartial coverage (~30% relative error). These findings indicate that the\nproposed framework provides a robust and scalable solution for large-scale,\nnon-destructive AGB estimation. It significantly reduces dependency on\nspecialized pre-processing tools and establishes ULS as a viable alternative to\nTLS. To our knowledge, this is the first method capable of enabling seamless,\nend-to-end 3D tree reconstruction at operational scales. This advancement\nsubstantially improves the feasibility of QSM-based AGB estimation, paving the\nway for broader applications in forest inventory and climate change research.\n","authors":["Di Wang","Shi Li"],"pdf_url":"https://arxiv.org/pdf/2506.15577v1.pdf","comment":"17 pages,19 figures"},{"id":"http://arxiv.org/abs/2506.15565v1","updated":"2025-06-18T15:41:29Z","published":"2025-06-18T15:41:29Z","title":"Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high\n  Spatial Resolution Land Cover Classification","summary":"  Ultra-high Spatial Resolution Land Cover Classification is essential for\nfine-grained land cover analysis, yet it remains challenging due to the high\ncost of pixel-level annotations, significant scale variation, and the limited\nadaptability of large-scale vision models. Existing methods typically focus on\n1-meter spatial resolution imagery and rely heavily on annotated data, whereas\npractical applications often require processing higher-resolution imagery under\nweak supervision. To address this, we propose a parameter-efficient\nsemi-supervised segmentation framework for 0.3 m spatial resolution imagery,\nwhich leverages the knowledge of SAM2 and introduces a remote sensing-specific\nFreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a\nlightweight design at only 5.96% of the total model parameters. By effectively\nleveraging unlabeled data and maintaining minimal parameter overhead, the\nproposed method delivers robust segmentation results with superior structural\nconsistency, achieving a 1.78% improvement over existing parameter-efficient\ntuning strategies and a 3.44% gain compared to state-of-the-art high-resolution\nremote sensing segmentation approaches.\n","authors":["Junhao Wu","Aboagye-Ntow Stephen","Chuyuan Wang","Gang Chen","Xin Huang"],"pdf_url":"https://arxiv.org/pdf/2506.15565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15564v1","updated":"2025-06-18T15:39:15Z","published":"2025-06-18T15:39:15Z","title":"Show-o2: Improved Native Unified Multimodal Models","summary":"  This paper presents improved native unified multimodal models, \\emph{i.e.,}\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.\n","authors":["Jinheng Xie","Zhenheng Yang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2506.15564v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2506.15563v1","updated":"2025-06-18T15:39:02Z","published":"2025-06-18T15:39:02Z","title":"Control and Realism: Best of Both Worlds in Layout-to-Image without\n  Training","summary":"  Layout-to-Image generation aims to create complex scenes with precise control\nover the placement and arrangement of subjects. Existing works have\ndemonstrated that pre-trained Text-to-Image diffusion models can achieve this\ngoal without training on any specific data; however, they often face challenges\nwith imprecise localization and unrealistic artifacts. Focusing on these\ndrawbacks, we propose a novel training-free method, WinWinLay. At its core,\nWinWinLay presents two key strategies, Non-local Attention Energy Function and\nAdaptive Update, that collaboratively enhance control precision and realism. On\none hand, we theoretically demonstrate that the commonly used attention energy\nfunction introduces inherent spatial distribution biases, hindering objects\nfrom being uniformly aligned with layout instructions. To overcome this issue,\nnon-local attention prior is explored to redistribute attention scores,\nfacilitating objects to better conform to the specified spatial conditions. On\nthe other hand, we identify that the vanilla backpropagation update rule can\ncause deviations from the pre-trained domain, leading to out-of-distribution\nartifacts. We accordingly introduce a Langevin dynamics-based adaptive update\nscheme as a remedy that promotes in-domain updating while respecting layout\nconstraints. Extensive experiments demonstrate that WinWinLay excels in\ncontrolling element placement and achieving photorealistic visual fidelity,\noutperforming the current state-of-the-art methods.\n","authors":["Bonan Li","Yinhan Hu","Songhua Liu","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15563v1.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2506.15562v1","updated":"2025-06-18T15:36:37Z","published":"2025-06-18T15:36:37Z","title":"Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and\n  Efficient Attention","summary":"  Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.\n","authors":["Syed Haider Ali","Asrar Ahmad","Muhammad Ali","Asifullah Khan","Muhammad Shahban","Nadeem Shaukat"],"pdf_url":"https://arxiv.org/pdf/2506.15562v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15560v1","updated":"2025-06-18T15:35:16Z","published":"2025-06-18T15:35:16Z","title":"RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth\n  Estimation","summary":"  Dense metric depth estimation using millimeter-wave radar typically requires\ndense LiDAR supervision, generated via multi-frame projection and\ninterpolation, to guide the learning of accurate depth from sparse radar\nmeasurements and RGB images. However, this paradigm is both costly and\ndata-intensive. To address this, we propose RaCalNet, a novel framework that\neliminates the need for dense supervision by using sparse LiDAR to supervise\nthe learning of refined radar measurements, resulting in a supervision density\nof merely around 1% compared to dense-supervised methods. Unlike previous\napproaches that associate radar points with broad image regions and rely\nheavily on dense labels, RaCalNet first recalibrates and refines sparse radar\npoints to construct accurate depth priors. These priors then serve as reliable\nanchors to guide monocular depth prediction, enabling metric-scale estimation\nwithout resorting to dense supervision. This design improves structural\nconsistency and preserves fine details. Despite relying solely on sparse\nsupervision, RaCalNet surpasses state-of-the-art dense-supervised methods,\nproducing depth maps with clear object contours and fine-grained textures.\nExtensive experiments on the ZJU-4DRadarCam dataset and real-world deployment\nscenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,\nrespectively.\n","authors":["Xingrui Qin","Wentao Zhao","Chuan Cao","Yihe Niu","Houcheng Jiang","Jingchuan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15560v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.15549v1","updated":"2025-06-18T15:21:34Z","published":"2025-06-18T15:21:34Z","title":"CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse\n  Myocardial Scar Synthesis and Segmentation","summary":"  Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.\n","authors":["Farheen Ramzan","Yusuf Kiberu","Nikesh Jathanna","Shahnaz Jamil-Copley","Richard H. Clayton"," Chen"," Chen"],"pdf_url":"https://arxiv.org/pdf/2506.15549v1.pdf","comment":"14 Pages"},{"id":"http://arxiv.org/abs/2505.16839v3","updated":"2025-06-18T15:17:40Z","published":"2025-05-22T16:07:12Z","title":"LaViDa: A Large Diffusion Language Model for Multimodal Understanding","summary":"  Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.\n","authors":["Shufan Li","Konstantinos Kallidromitis","Hritik Bansal","Akash Gokul","Yusuke Kato","Kazuki Kozuka","Jason Kuen","Zhe Lin","Kai-Wei Chang","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2505.16839v3.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2504.00857v2","updated":"2025-06-18T15:15:38Z","published":"2025-04-01T14:47:14Z","title":"Exploring Personalized Federated Learning Architectures for Violence\n  Detection in Surveillance Videos","summary":"  The challenge of detecting violent incidents in urban surveillance systems is\ncompounded by the voluminous and diverse nature of video data. This paper\npresents a targeted approach using Personalized Federated Learning (PFL) to\naddress these issues, specifically employing the Federated Learning with\nPersonalization Layers method within the Flower framework. Our methodology\nadapts learning models to the unique data characteristics of each surveillance\nnode, effectively managing the heterogeneous and non-IID nature of surveillance\nvideo data. Through rigorous experiments conducted on balanced and imbalanced\ndatasets, our PFL models demonstrated enhanced accuracy and efficiency,\nachieving up to 99.3% accuracy. This study underscores the potential of PFL to\nsignificantly improve the scalability and effectiveness of surveillance\nsystems, offering a robust, privacy-preserving solution for violence detection\nin complex urban environments.\n","authors":["Mohammad Kassir","Siba Haidar","Antoun Yaacoub"],"pdf_url":"https://arxiv.org/pdf/2504.00857v2.pdf","comment":"7 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.13045v2","updated":"2025-06-18T15:06:34Z","published":"2025-06-16T02:27:25Z","title":"A Comprehensive Survey on Continual Learning in Generative Models","summary":"  The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.\n","authors":["Haiyang Guo","Fanhu Zeng","Fei Zhu","Jiayi Wang","Xukai Wang","Jingang Zhou","Hongbo Zhao","Wenzhuo Liu","Shijie Ma","Da-Han Wang","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13045v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.08221v2","updated":"2025-06-18T15:03:21Z","published":"2025-03-11T09:40:31Z","title":"EgoBlind: Towards Egocentric Visual Assistance for the Blind","summary":"  We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,392 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 5,311 questions directly posed or generated and verified by blind\nindividuals to reflect their in-situation needs for visual assistance under\nvarious scenarios. We provide each question with an average of 3 reference\nanswers to alleviate subjective evaluation. Using EgoBlind, we comprehensively\nevaluate 16 advanced MLLMs and find that all models struggle, with the best\nperformers achieving accuracy near 60\\%, far behind human performance of\n87.4\\%. To guide future advancements, we identify and summarize major\nlimitations of existing MLLMs in egocentric visual assistance for the blind and\nexplore heuristic solutions for improvement. With these efforts, we hope\nEgoBlind can serve as a valuable foundation for developing more effective AI\nassistants to enhance the independence of the blind individuals' lives. Data\nand evaluation code are available at https://github.com/doc-doc/EgoBlind.\n","authors":["Junbin Xiao","Nanxin Huang","Hao Qiu","Zhulin Tao","Xun Yang","Richang Hong","Meng Wang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2503.08221v2.pdf","comment":"We extend and resplit the dataset"},{"id":"http://arxiv.org/abs/2506.15524v1","updated":"2025-06-18T14:58:49Z","published":"2025-06-18T14:58:49Z","title":"NTIRE 2025 Image Shadow Removal Challenge Report","summary":"  This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A\ntotal of 306 participants have registered, with 17 teams successfully\nsubmitting their solutions during the final evaluation phase. Following the\nlast two editions, this challenge had two evaluation tracks: one focusing on\nreconstruction fidelity and the other on visual perception through a user\nstudy. Both tracks were evaluated with images from the WSRD+ dataset,\nsimulating interactions between self- and cast-shadows with a large number of\ndiverse objects, textures, and materials.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zhuyun Zhou","Cailian Chen","Zongwei Wu","Radu Timofte","Mingjia Li","Jin Hu","Hainuo Wang","Hengxing Liu","Jiarui Wang","Qiming Hu","Xiaojie Guo","Xin Lu","Jiarong Yang","Yuanfei Bao","Anya Hu","Zihao Fan","Kunyu Wang","Jie Xiao","Xi Wang","Xueyang Fu","Zheng-Jun Zha","Yu-Fan Lin","Chia-Ming Lee","Chih-Chung Hsu","Xingbo Wang","Dong Li","Yuxu Chen","Bin Chen","Yuanbo Zhou","Yuanbin Chen","Hongwei Wang","Jiannan Lin","Qinquan Gao","Tong Tong","Zhao Zhang","Yanyan Wei","Wei Dong","Han Zhou","Seyed Amirreza Mousavi","Jun Chen","Haobo Liang","Jiajie Jing","Junyu Li","Yan Yang","Seoyeon Lee","Chaewon Kim","Ziyu Feng","Shidi Chen","Bowen Luan","Zewen Chen","Vijayalaxmi Ashok Aralikatti","G Gyaneshwar Rao","Nikhil Akalwadi","Chaitra Desai","Ramesh Ashok Tabib","Uma Mudenagudi","Anas M. Ali","Bilel Benjdira","Wadii Boulila","Alexandru Brateanu","Cosmin Ancuti","Tanmay Chaturvedi","Manish Kumar","Anmol Srivastav","Daksh Trivedi","Shashwat Thakur","Kishor Upla","Zeyu Xiao","Zhuoyuan Li","Boda Zhou","Shashank Shekhar","Kele Xu","Qisheng Xu","Zijian Gao","Tianjiao Wan","Suiyi Zhao","Bo Wang","Yan Luo","Mingshen Wang","Yilin Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19805v2","updated":"2025-06-18T14:51:33Z","published":"2025-05-26T10:39:36Z","title":"Translation-Equivariance of Normalization Layers and Aliasing in\n  Convolutional Neural Networks","summary":"  The design of convolutional neural architectures that are exactly equivariant\nto continuous translations is an active field of research. It promises to\nbenefit scientific computing, notably by making existing imaging systems more\nphysically accurate. Most efforts focus on the design of downsampling/pooling\nlayers, upsampling layers and activation functions, but little attention is\ndedicated to normalization layers. In this work, we present a novel theoretical\nframework for understanding the equivariance of normalization layers to\ndiscrete shifts and continuous translations. We also determine necessary and\nsufficient conditions for normalization layers to be equivariant in terms of\nthe dimensions they operate on. Using real feature maps from ResNet-18 and\nImageNet, we test those theoretical results empirically and find that they are\nconsistent with our predictions.\n","authors":["Jérémy Scanvic","Quentin Barthélemy","Julián Tachella"],"pdf_url":"https://arxiv.org/pdf/2505.19805v2.pdf","comment":"Accepted at the Workshop on the Theory of AI for Scientific Computing\n  (COLT 2025)"},{"id":"http://arxiv.org/abs/2506.15499v1","updated":"2025-06-18T14:41:24Z","published":"2025-06-18T14:41:24Z","title":"Pixel-level Certified Explanations via Randomized Smoothing","summary":"  Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.\n","authors":["Alaa Anani","Tobias Lorenz","Mario Fritz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2506.15499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15489v1","updated":"2025-06-18T14:26:57Z","published":"2025-06-18T14:26:57Z","title":"Advanced cervical cancer classification: enhancing pap smear images with\n  hybrid PMD Filter-CLAHE","summary":"  Cervical cancer remains a significant health problem, especially in\ndeveloping countries. Early detection is critical for effective treatment.\nConvolutional neural networks (CNN) have shown promise in automated cervical\ncancer screening, but their performance depends on Pap smear image quality.\nThis study investigates the impact of various image preprocessing techniques on\nCNN performance for cervical cancer classification using the SIPaKMeD dataset.\nThree preprocessing techniques were evaluated: perona-malik diffusion (PMD)\nfilter for noise reduction, contrast-limited adaptive histogram equalization\n(CLAHE) for image contrast enhancement, and the proposed hybrid PMD\nfilter-CLAHE approach. The enhanced image datasets were evaluated on pretrained\nmodels, such as ResNet-34, ResNet-50, SqueezeNet-1.0, MobileNet-V2,\nEfficientNet-B0, EfficientNet-B1, DenseNet-121, and DenseNet-201. The results\nshow that hybrid preprocessing PMD filter-CLAHE can improve the Pap smear image\nquality and CNN architecture performance compared to the original images. The\nmaximum metric improvements are 13.62% for accuracy, 10.04% for precision,\n13.08% for recall, and 14.34% for F1-score. The proposed hybrid PMD\nfilter-CLAHE technique offers a new perspective in improving cervical cancer\nclassification performance using CNN architectures.\n","authors":["Ach Khozaimi","Isnani Darti","Syaiful Anam","Wuryansari Muharini Kusumawinahyu"],"pdf_url":"https://arxiv.org/pdf/2506.15489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10685v2","updated":"2025-06-18T14:20:09Z","published":"2025-06-12T13:30:01Z","title":"Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework","summary":"  With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs.\n","authors":["Xia Du","Xiaoyuan Liu","Jizhe Zhou","Zheng Lin","Chi-man Pun","Cong Wu","Tao Li","Zhe Chen","Wei Ni","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2506.10685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15483v1","updated":"2025-06-18T14:17:53Z","published":"2025-06-18T14:17:53Z","title":"GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis\n  for Unseen Objects","summary":"  While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.\n","authors":["Shujia Li","Haiyu Zhang","Xinyuan Chen","Yaohui Wang","Yutong Ban"],"pdf_url":"https://arxiv.org/pdf/2506.15483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15477v1","updated":"2025-06-18T14:09:34Z","published":"2025-06-18T14:09:34Z","title":"Multimodal Large Language Models for Medical Report Generation via\n  Customized Prompt Tuning","summary":"  Medical report generation from imaging data remains a challenging task in\nclinical practice. While large language models (LLMs) show great promise in\naddressing this challenge, their effective integration with medical imaging\ndata still deserves in-depth exploration. In this paper, we present MRG-LLM, a\nnovel multimodal large language model (MLLM) that combines a frozen LLM with a\nlearnable visual encoder and introduces a dynamic prompt customization\nmechanism. Our key innovation lies in generating instance-specific prompts\ntailored to individual medical images through conditional affine\ntransformations derived from visual features. We propose two implementations:\nprompt-wise and promptbook-wise customization, enabling precise and targeted\nreport generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets\ndemonstrate that MRG-LLM achieves state-of-the-art performance in medical\nreport generation. Our code will be made publicly available.\n","authors":["Chunlei Li","Jingyang Hou","Yilei Shi","Jingliang Hu","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2506.15477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09881v2","updated":"2025-06-18T13:58:29Z","published":"2025-06-11T15:54:47Z","title":"Leveraging Depth and Language for Open-Vocabulary Domain-Generalized\n  Semantic Segmentation","summary":"  Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.\n","authors":["Siyu Chen","Ting Han","Chengzheng Fu","Changshe Zhang","Chaolei Wang","Jinhe Su","Guorong Cai","Meiliu Wu"],"pdf_url":"https://arxiv.org/pdf/2506.09881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14696v2","updated":"2025-06-18T13:54:55Z","published":"2025-06-17T16:37:00Z","title":"YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object\n  Detection Framework","summary":"  Multispectral object detection, which integrates information from multiple\nbands, can enhance detection accuracy and environmental adaptability, holding\ngreat application potential across various fields. Although existing methods\nhave made progress in cross-modal interaction, low-light conditions, and model\nlightweight, there are still challenges like the lack of a unified single-stage\nframework, difficulty in balancing performance and fusion strategy, and\nunreasonable modality weight allocation. To address these, based on the YOLOv11\nframework, we present YOLOv11-RGBT, a new comprehensive multimodal object\ndetection framework. We designed six multispectral fusion modes and\nsuccessfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After\nreevaluating the importance of the two modalities, we proposed a P3 mid-fusion\nstrategy and multispectral controllable fine-tuning (MCF) strategy for\nmultispectral models. These improvements optimize feature fusion, reduce\nredundancy and mismatches, and boost overall model performance. Experiments\nshow our framework excels on three major open-source multispectral object\ndetection datasets, like LLVIP and FLIR. Particularly, the multispectral\ncontrollable fine-tuning strategy significantly enhanced model adaptability and\nrobustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP\nby 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and\nstrategies' effectiveness. The code is available at:\nhttps://github.com/wandahangFY/YOLOv11-RGBT.\n","authors":["Dahang Wan","Rongsheng Lu","Yang Fang","Xianli Lang","Shuangbao Shu","Jingjing Chen","Siyuan Shen","Ting Xu","Zecong Ye"],"pdf_url":"https://arxiv.org/pdf/2506.14696v2.pdf","comment":"29 pages, 8 figures . The errors in the first version have been\n  corrected, and no new version will be submitted in the near future. The next\n  version will include more experiments"},{"id":"http://arxiv.org/abs/2504.15134v3","updated":"2025-06-18T13:21:45Z","published":"2025-04-21T14:37:37Z","title":"Instance-Adaptive Keypoint Learning with Local-to-Global Geometric\n  Aggregation for Category-Level Object Pose Estimation","summary":"  Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this issue, we propose INKL-Pose, a novel category-level object pose\nestimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our method first predicts\nsemantically consistent and geometrically informative keypoints using an\nInstance-Adaptive Keypoint Detector, then refines them: (1) a Local Keypoint\nFeature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint\nFeature Aggregator using bidirectional Mamba for structural consistency. To\nenable bidirectional modeling in Mamba, we introduce a simple yet effective\nFeature Sequence Flipping strategy that preserves spatial coherence while\nconstructing backward feature sequence. Additionally, we design a surface loss\nand a separation loss to encourage uniform coverage and spatial diversity in\nkeypoint distribution. The resulting keypoints are mapped to a canonical space\nfor 6D pose and size regression. Extensive experiments on CAMERA25, REAL275,\nand HouseCat6D show that INKL-Pose achieves state-of-the-art performance with\n16.7M parameters and runs at 36 FPS on an NVIDIA RTX 4090D GPU.\n","authors":["Xiao Zhang","Lu Zou","Tao Lu","Yuan Yao","Zhangjin Huang","Guoping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15134v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23131v2","updated":"2025-06-18T13:17:37Z","published":"2025-03-29T15:50:08Z","title":"RefChartQA: Grounding Visual Answer on Chart Images through Instruction\n  Tuning","summary":"  Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps://github.com/moured/RefChartQA.\n","authors":["Alexander Vogel","Omar Moured","Yufan Chen","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2503.23131v2.pdf","comment":"Accepted by ICDAR 2025. All models and code will be publicly\n  available at https://github.com/moured/RefChartQA"},{"id":"http://arxiv.org/abs/2506.15442v1","updated":"2025-06-18T13:14:46Z","published":"2025-06-18T13:14:46Z","title":"Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material","summary":"  3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.\n","authors":["Team Hunyuan3D","Shuhui Yang","Mingxin Yang","Yifei Feng","Xin Huang","Sheng Zhang","Zebin He","Di Luo","Haolin Liu","Yunfei Zhao","Qingxiang Lin","Zeqiang Lai","Xianghui Yang","Huiwen Shi","Zibo Zhao","Bowen Zhang","Hongyu Yan","Lifu Wang","Sicong Liu","Jihong Zhang","Meng Chen","Liang Dong","Yiwen Jia","Yulin Cai","Jiaao Yu","Yixuan Tang","Dongyuan Guo","Junlin Yu","Hao Zhang","Zheng Ye","Peng He","Runzhou Wu","Shida Wei","Chao Zhang","Yonghao Tan","Yifu Sun","Lin Niu","Shirui Huang","Bojian Zheng","Shu Liu","Shilin Chen","Xiang Yuan","Xiaofeng Yang","Kai Liu","Jianchen Zhu","Peng Chen","Tian Liu","Di Wang","Yuhong Liu"," Linus","Jie Jiang","Jingwei Huang","Chunchao Guo"],"pdf_url":"https://arxiv.org/pdf/2506.15442v1.pdf","comment":"Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"},{"id":"http://arxiv.org/abs/2506.12787v2","updated":"2025-06-18T12:41:12Z","published":"2025-06-15T09:36:45Z","title":"Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting","summary":"  Modeling the wireless radiance field (WRF) is fundamental to modern\ncommunication systems, enabling key tasks such as localization, sensing, and\nchannel estimation. Traditional approaches, which rely on empirical formulas or\nphysical simulations, often suffer from limited accuracy or require strong\nscene priors. Recent neural radiance field (NeRF-based) methods improve\nreconstruction fidelity through differentiable volumetric rendering, but their\nreliance on computationally expensive multilayer perceptron (MLP) queries\nhinders real-time deployment. To overcome these challenges, we introduce\nGaussian splatting (GS) to the wireless domain, leveraging its efficiency in\nmodeling optical radiance fields to enable compact and accurate WRF\nreconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian\nsplatting framework that synthesizes WRF spectra at arbitrary positions under\nsingle-sided transceiver mobility. SwiftWRF employs CUDA-accelerated\nrasterization to render spectra at over 100000 fps and uses a lightweight MLP\nto model the deformation of 2D Gaussians, effectively capturing\nmobility-induced WRF variations. In addition to novel spectrum synthesis, the\nefficacy of SwiftWRF is further underscored in its applications in\nangle-of-arrival (AoA) and received signal strength indicator (RSSI)\nprediction. Experiments conducted on both real-world and synthetic indoor\nscenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster\nthan existing state-of-the-art methods, while significantly enhancing its\nsignal quality. The project page is https://evan-sudo.github.io/swiftwrf/.\n","authors":["Mufan Liu","Cixiao Zhang","Qi Yang","Yujie Cao","Yiling Xu","Yin Xu","Shu Sun","Mingzeng Dai","Yunfeng Guan"],"pdf_url":"https://arxiv.org/pdf/2506.12787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15576v2","updated":"2025-06-18T12:27:58Z","published":"2025-03-19T13:19:06Z","title":"A Bird Song Detector for improving bird identification through Deep\n  Learning: a case study from Doñana","summary":"  Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.\n","authors":["Alba Márquez-Rodríguez","Miguel Ángel Mohedano-Munoz","Manuel J. Marín-Jiménez","Eduardo Santamaría-García","Giulia Bastianelli","Pedro Jordano","Irene Mendoza"],"pdf_url":"https://arxiv.org/pdf/2503.15576v2.pdf","comment":"23 pages, 14 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector"},{"id":"http://arxiv.org/abs/2506.15404v1","updated":"2025-06-18T12:22:17Z","published":"2025-06-18T12:22:17Z","title":"NERO: Explainable Out-of-Distribution Detection with Neuron-level\n  Relevance","summary":"  Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.\n","authors":["Anju Chhetri","Jari Korhonen","Prashnna Gyawali","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2506.15404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15402v1","updated":"2025-06-18T12:20:34Z","published":"2025-06-18T12:20:34Z","title":"MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System","summary":"  Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.\n","authors":["Miaoxin Pan","Jinnan Li","Yaowen Zhang","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2506.15402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03097v2","updated":"2025-06-18T12:18:26Z","published":"2023-08-06T12:23:40Z","title":"Incorporating Pre-training Data Matters in Unsupervised Domain\n  Adaptation","summary":"  In deep learning, initializing models with pre-trained weights has become the\nde facto practice for various downstream tasks. Many unsupervised domain\nadaptation (UDA) methods typically adopt a backbone pre-trained on ImageNet,\nand focus on reducing the source-target domain discrepancy. However, the impact\nof pre-training on adaptation received little attention. In this study, we\ndelve into UDA from the novel perspective of pre-training. We first demonstrate\nthe impact of pre-training by analyzing the dynamic distribution discrepancies\nbetween pre-training data domain and the source/ target domain during\nadaptation. Then, we reveal that the target error also stems from the\npre-training in the following two factors: 1) empirically, target error arises\nfrom the gradually degenerative pre-trained knowledge during adaptation; 2)\ntheoretically, the error bound depends on difference between the gradient of\nloss function, \\ie, on the target domain and pre-training data domain. To\naddress these two issues, we redefine UDA as a three-domain problem, \\ie,\nsource domain, target domain, and pre-training data domain; then we propose a\nnovel framework, named TriDA. We maintain the pre-trained knowledge and improve\nthe error bound by incorporating pre-training data into adaptation for both\nvanilla UDA and source-free UDA scenarios. For efficiency, we introduce a\nselection strategy for pre-training data, and offer a solution with synthesized\nimages when pre-training data is unavailable during adaptation. Notably, TriDA\nis effective even with a small amount of pre-training or synthesized images,\nand seamlessly complements the two scenario UDA methods, demonstrating\nstate-of-the-art performance across multiple benchmarks. We hope our work\nprovides new insights for better understanding and application of domain\nadaptation.\n","authors":["Yinsong Xu","Aidong Men","Yang Liu","Xiahai Zhuang","Qingchao Chen"],"pdf_url":"https://arxiv.org/pdf/2308.03097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15395v1","updated":"2025-06-18T12:12:10Z","published":"2025-06-18T12:12:10Z","title":"A Real-time Endoscopic Image Denoising System","summary":"  Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.\n","authors":["Yu Xing","Shishi Huang","Meng Lv","Guo Chen","Huailiang Wang","Lingzhi Sui"],"pdf_url":"https://arxiv.org/pdf/2506.15395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15381v1","updated":"2025-06-18T11:51:40Z","published":"2025-06-18T11:51:40Z","title":"When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free\n  Image Synthesis with Alignment of Domain and Class","summary":"  Open-source pre-trained models hold great potential for diverse applications,\nbut their utility declines when their training data is unavailable. Data-Free\nImage Synthesis (DFIS) aims to generate images that approximate the learned\ndata distribution of a pre-trained model without accessing the original data.\nHowever, existing DFIS meth ods produce samples that deviate from the training\ndata distribution due to the lack of prior knowl edge about natural images. To\novercome this limitation, we propose DDIS, the first Diffusion-assisted\nData-free Image Synthesis method that leverages a text-to-image diffusion model\nas a powerful image prior, improving synthetic image quality. DDIS extracts\nknowledge about the learned distribution from the given model and uses it to\nguide the diffusion model, enabling the generation of images that accurately\nalign with the training data distribution. To achieve this, we introduce Domain\nAlignment Guidance (DAG) that aligns the synthetic data domain with the\ntraining data domain during the diffusion sampling process. Furthermore, we\noptimize a single Class Alignment Token (CAT) embedding to effectively capture\nclass-specific attributes in the training dataset. Experiments on PACS and Ima\ngeNet demonstrate that DDIS outperforms prior DFIS methods by generating\nsamples that better reflect the training data distribution, achieving SOTA\nperformance in data-free applications.\n","authors":["Yujin Kim","Hyunsoo Kim","Hyunwoo J. Kim","Suhyun Kim"],"pdf_url":"https://arxiv.org/pdf/2506.15381v1.pdf","comment":"Published at ICML 2025"},{"id":"http://arxiv.org/abs/2505.07449v5","updated":"2025-06-18T11:40:27Z","published":"2025-05-12T11:23:37Z","title":"Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video\n  Generation Model","summary":"  In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora.\n","authors":["Wei Li","Ming Hu","Guoan Wang","Lihao Liu","Kaijin Zhou","Junzhi Ning","Xin Guo","Zongyuan Ge","Lixu Gu","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2505.07449v5.pdf","comment":"Early accepted in MICCAI25"},{"id":"http://arxiv.org/abs/2503.18742v2","updated":"2025-06-18T11:38:27Z","published":"2025-03-24T14:50:28Z","title":"SFDLA: Source-Free Document Layout Analysis","summary":"  Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter.\n","authors":["Sebastian Tewes","Yufan Chen","Omar Moured","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2503.18742v2.pdf","comment":"Accepted by ICDAR 2025. The benchmark, models, and code will be\n  publicly available at https://github.com/s3setewe/sfdla-DLAdapter"},{"id":"http://arxiv.org/abs/2506.15369v1","updated":"2025-06-18T11:36:49Z","published":"2025-06-18T11:36:49Z","title":"Unsupervised Pelage Pattern Unwrapping for Animal Re-identification","summary":"  Existing individual re-identification methods often struggle with the\ndeformable nature of animal fur or skin patterns which undergo geometric\ndistortions due to body movement and posture changes. In this paper, we propose\na geometry-aware texture mapping approach that unwarps pelage patterns, the\nunique markings found on an animal's skin or fur, into a canonical UV space,\nenabling more robust feature matching. Our method uses surface normal\nestimation to guide the unwrapping process while preserving the geometric\nconsistency between the 3D surface and the 2D texture space. We focus on two\nchallenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards\n(Panthera pardus). Both species have distinctive yet highly deformable fur\npatterns. By integrating our pattern-preserving UV mapping with existing\nre-identification techniques, we demonstrate improved accuracy across diverse\nposes and viewing angles. Our framework does not require ground truth UV\nannotations and can be trained in a self-supervised manner. Experiments on seal\nand leopard datasets show up to a 5.4% improvement in re-identification\naccuracy.\n","authors":["Aleksandr Algasov","Ekaterina Nepovinnykh","Fedor Zolotarev","Tuomas Eerola","Heikki Kälviäinen","Pavel Zemčík","Charles V. Stewart"],"pdf_url":"https://arxiv.org/pdf/2506.15369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15368v1","updated":"2025-06-18T11:35:30Z","published":"2025-06-18T11:35:30Z","title":"Open-World Object Counting in Videos","summary":"  We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.\n","authors":["Niki Amini-Naieni","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2506.15368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15365v1","updated":"2025-06-18T11:34:08Z","published":"2025-06-18T11:34:08Z","title":"FedWSIDD: Federated Whole Slide Image Classification via Dataset\n  Distillation","summary":"  Federated learning (FL) has emerged as a promising approach for collaborative\nmedical image analysis, enabling multiple institutions to build robust\npredictive models while preserving sensitive patient data. In the context of\nWhole Slide Image (WSI) classification, FL faces significant challenges,\nincluding heterogeneous computational resources across participating medical\ninstitutes and privacy concerns. To address these challenges, we propose\nFedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn\nand transmit synthetic slides. On the server side, FedWSIDD aggregates\nsynthetic slides from participating centres and distributes them across all\ncentres. On the client side, we introduce a novel DD algorithm tailored to\nhistopathology datasets which incorporates stain normalisation into the\ndistillation process to generate a compact set of highly informative synthetic\nslides. These synthetic slides, rather than model parameters, are transmitted\nto the server. After communication, the received synthetic slides are combined\nwith original slides for local tasks. Extensive experiments on multiple WSI\nclassification tasks, including CAMELYON16 and CAMELYON17, demonstrate that\nFedWSIDD offers flexibility for heterogeneous local models, enhances local WSI\nclassification performance, and preserves patient privacy. This makes it a\nhighly effective solution for complex WSI classification tasks. The code is\navailable at FedWSIDD.\n","authors":["Haolong Jin","Shenglin Liu","Cong Cong","Qingmin Feng","Yongzhi Liu","Lina Huang","Yingzi Hu"],"pdf_url":"https://arxiv.org/pdf/2506.15365v1.pdf","comment":"MICCAI 2025"},{"id":"http://arxiv.org/abs/2408.05412v2","updated":"2025-06-18T10:45:08Z","published":"2024-08-10T02:46:11Z","title":"Style-Preserving Lip Sync via Audio-Aware Style Reference","summary":"  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n","authors":["Weizhi Zhong","Jichang Li","Yinqi Cai","Ming Li","Feng Gao","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2408.05412v2.pdf","comment":"submitted to IEEE Transactions on Multimedia(TMM)"},{"id":"http://arxiv.org/abs/2506.06761v2","updated":"2025-06-18T10:34:41Z","published":"2025-06-07T11:05:33Z","title":"The OCR Quest for Generalization: Learning to recognize low-resource\n  alphabets with model editing","summary":"  Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.\n","authors":["Adrià Molina Rodríguez","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2506.06761v2.pdf","comment":"Preprint (under review) For Journal"},{"id":"http://arxiv.org/abs/2504.04893v5","updated":"2025-06-18T10:02:15Z","published":"2025-04-07T10:01:38Z","title":"SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models","summary":"  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam.\n","authors":["Justus Westerhoff","Erblina Purelku","Jakob Hackstein","Jonas Loos","Leo Pinetzki","Lorenz Hufe"],"pdf_url":"https://arxiv.org/pdf/2504.04893v5.pdf","comment":"Accepted at CVPR 2025 Workshop EVAL-FoMo-2"},{"id":"http://arxiv.org/abs/2506.15318v1","updated":"2025-06-18T09:47:45Z","published":"2025-06-18T09:47:45Z","title":"OpenPath: Open-Set Active Learning for Pathology Image Classification\n  via Pre-trained Vision-Language Models","summary":"  Pathology image classification plays a crucial role in accurate medical\ndiagnosis and treatment planning. Training high-performance models for this\ntask typically requires large-scale annotated datasets, which are both\nexpensive and time-consuming to acquire. Active Learning (AL) offers a solution\nby iteratively selecting the most informative samples for annotation, thereby\nreducing the labeling effort. However, most AL methods are designed under the\nassumption of a closed-set scenario, where all the unannotated images belong to\ntarget classes. In real-world clinical environments, the unlabeled pool often\ncontains a substantial amount of Out-Of-Distribution (OOD) data, leading to low\nefficiency of annotation in traditional AL methods. Furthermore, most existing\nAL methods start with random selection in the first query round, leading to a\nsignificant waste of labeling costs in open-set scenarios. To address these\nchallenges, we propose OpenPath, a novel open-set active learning approach for\npathological image classification leveraging a pre-trained Vision-Language\nModel (VLM). In the first query, we propose task-specific prompts that combine\ntarget and relevant non-target class prompts to effectively select\nIn-Distribution (ID) and informative samples from the unlabeled pool. In\nsubsequent queries, Diverse Informative ID Sampling (DIS) that includes\nPrototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic\nSampling (EGSS) is proposed to ensure both purity and informativeness in a\nquery, avoiding the selection of OOD samples. Experiments on two public\npathology image datasets show that OpenPath significantly enhances the model's\nperformance due to its high purity of selected samples, and outperforms several\nstate-of-the-art open-set AL methods. The code is available at\n\\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..\n","authors":["Lanfeng Zhong","Xin Liao","Shichuan Zhang","Shaoting Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15318v1.pdf","comment":"MICCAI 2025 early accept"},{"id":"http://arxiv.org/abs/2506.14168v2","updated":"2025-06-18T09:44:09Z","published":"2025-06-17T04:08:18Z","title":"VideoMAR: Autoregressive Video Generatio with Continuous Tokens","summary":"  Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).\n","authors":["Hu Yu","Biao Gong","Hangjie Yuan","DanDan Zheng","Weilong Chai","Jingdong Chen","Kecheng Zheng","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.14168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15313v1","updated":"2025-06-18T09:42:30Z","published":"2025-06-18T09:42:30Z","title":"MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual\n  Learning","summary":"  In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.\n","authors":["Leonid Ivanov","Vasily Yuryev","Dmitry Yudin"],"pdf_url":"https://arxiv.org/pdf/2506.15313v1.pdf","comment":"Preprint. Submitted. 12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.15312v1","updated":"2025-06-18T09:41:30Z","published":"2025-06-18T09:41:30Z","title":"One-shot Face Sketch Synthesis in the Wild via Generative Diffusion\n  Prior and Instruction Tuning","summary":"  Face sketch synthesis is a technique aimed at converting face photos into\nsketches. Existing face sketch synthesis research mainly relies on training\nwith numerous photo-sketch sample pairs from existing datasets. However, these\nlarge-scale discriminative learning methods will have to face problems such as\ndata scarcity and high human labor costs. Once the training data becomes\nscarce, their generative performance significantly degrades. In this paper, we\npropose a one-shot face sketch synthesis method based on diffusion models. We\noptimize text instructions on a diffusion model using face photo-sketch image\npairs. Then, the instructions derived through gradient-based optimization are\nused for inference. To simulate real-world scenarios more accurately and\nevaluate method effectiveness more comprehensively, we introduce a new\nbenchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark\nconsists of 400 pairs of face photo-sketch images, including sketches with\ndifferent styles and photos with different backgrounds, ages, sexes,\nexpressions, illumination, etc. For a solid out-of-distribution evaluation, we\nselect only one pair of images for training at each time, with the rest used\nfor inference. Extensive experiments demonstrate that the proposed method can\nconvert various photos into realistic and highly consistent sketches in a\none-shot context. Compared to other methods, our approach offers greater\nconvenience and broader applicability. The dataset will be available at:\nhttps://github.com/HanWu3125/OS-Sketch\n","authors":["Han Wu","Junyao Li","Kangbo Zhao","Sen Zhang","Yukai Shi","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2506.15312v1.pdf","comment":"We propose a novel framework for face sketch synthesis, where merely\n  a single pair of samples suffices to enable in-the-wild face sketch synthesis"},{"id":"http://arxiv.org/abs/2506.15298v1","updated":"2025-06-18T09:29:51Z","published":"2025-06-18T09:29:51Z","title":"MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and\n  Visual Question Answering","summary":"  Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.\n","authors":["Xinqi Fan","Jingting Li","John See","Moi Hoon Yap","Wen-Huang Cheng","Xiaobai Li","Xiaopeng Hong","Su-Jing Wang","Adrian K. Davision"],"pdf_url":"https://arxiv.org/pdf/2506.15298v1.pdf","comment":"Micro-Expression Grand Challenge (MEGC) at ACM MM 2025"},{"id":"http://arxiv.org/abs/2506.13097v2","updated":"2025-06-18T09:23:19Z","published":"2025-06-16T05:04:12Z","title":"Pro-AD: Learning Comprehensive Prototypes with Prototype-based\n  Constraint for Multi-class Unsupervised Anomaly Detection","summary":"  Prototype-based reconstruction methods for unsupervised anomaly detection\nutilize a limited set of learnable prototypes which only aggregates\ninsufficient normal information, resulting in undesirable reconstruction.\nHowever, increasing the number of prototypes may lead to anomalies being well\nreconstructed through the attention mechanism, which we refer to as the \"Soft\nIdentity Mapping\" problem. In this paper, we propose Pro-AD to address these\nissues and fully utilize the prototypes to boost the performance of anomaly\ndetection. Specifically, we first introduce an expanded set of learnable\nprototypes to provide sufficient capacity for semantic information. Then we\nemploy a Dynamic Bidirectional Decoder which integrates the process of the\nnormal information aggregation and the target feature reconstruction via\nprototypes, with the aim of allowing the prototypes to aggregate more\ncomprehensive normal semantic information from different levels of the image\nfeatures and the target feature reconstruction to not only utilize its\ncontextual information but also dynamically leverage the learned comprehensive\nprototypes. Additionally, to prevent the anomalies from being well\nreconstructed using sufficient semantic information through the attention\nmechanism, Pro-AD introduces a Prototype-based Constraint that applied within\nthe target feature reconstruction process of the decoder, which further\nimproves the performance of our approach. Extensive experiments on multiple\nchallenging benchmarks demonstrate that our Pro-AD achieve state-of-the-art\nperformance, highlighting its superior robustness and practical effectiveness\nfor Multi-class Unsupervised Anomaly Detection task.\n","authors":["Ziqing Zhou","Bin-Bin Gao","Yurui Pan","Lidong Wang","Wenbing Zhu","Yong Liu","Jun Liu","Mingmin Chi","Dong Wu","Bo Peng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15290v1","updated":"2025-06-18T09:16:36Z","published":"2025-06-18T09:16:36Z","title":"Human Motion Capture from Loose and Sparse Inertial Sensors with\n  Garment-aware Diffusion Models","summary":"  Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.\n","authors":["Andela Ilic","Jiaxi Jiang","Paul Streli","Xintong Liu","Christian Holz"],"pdf_url":"https://arxiv.org/pdf/2506.15290v1.pdf","comment":"Accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2506.15285v1","updated":"2025-06-18T09:08:42Z","published":"2025-06-18T09:08:42Z","title":"AI-driven visual monitoring of industrial assembly tasks","summary":"  Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT\n","authors":["Mattia Nardon","Stefano Messelodi","Antonio Granata","Fabio Poiesi","Alberto Danese","Davide Boscaini"],"pdf_url":"https://arxiv.org/pdf/2506.15285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15279v1","updated":"2025-06-18T09:00:08Z","published":"2025-06-18T09:00:08Z","title":"BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via\n  Bezier Curve Refinement","summary":"  Laparoscopic liver surgery, while minimally invasive, poses significant\nchallenges in accurately identifying critical anatomical structures. Augmented\nreality (AR) systems, integrating MRI/CT with laparoscopic images based on\n2D-3D registration, offer a promising solution for enhancing surgical\nnavigation. A vital aspect of the registration progress is the precise\ndetection of curvilinear anatomical landmarks in laparoscopic images. In this\npaper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that\nsignificantly enhances landmark detection in laparoscopic liver surgery\nprimarily via the Bezier curve refinement strategy. The framework starts with a\nMulti-modal Feature Extraction (MFE) module designed to robustly capture\nsemantic features. Then we propose Adaptive Curve Proposal Initialization\n(ACPI) to generate pixel-aligned Bezier curves and confidence scores for\nreliable initial proposals. Additionally, we design the Hierarchical Curve\nRefinement (HCR) mechanism to enhance these proposals iteratively through a\nmulti-stage process, capturing fine-grained contextual details from multi-scale\npixel-level features for precise Bezier curve adjustment. Extensive evaluations\non the L3D and P2ILF datasets demonstrate that BCRNet outperforms\nstate-of-the-art methods, achieving significant performance improvements. Code\nwill be available.\n","authors":["Qian Li","Feng Liu","Shuojue Yang","Daiyun Shen","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2506.15279v1.pdf","comment":"Accepted at MICCAI 2025, 11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2506.15276v1","updated":"2025-06-18T08:57:12Z","published":"2025-06-18T08:57:12Z","title":"MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion","summary":"  Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.\n","authors":["Jun Zhu","Xinfeng Zhang","Lv Tang","JunHao Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.15276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09194v2","updated":"2025-06-18T08:49:21Z","published":"2024-08-17T13:12:04Z","title":"DRL-Based Resource Allocation for Motion Blur Resistant Federated\n  Self-Supervised Learning in IoV","summary":"  In the Internet of Vehicles (IoV), Federated Learning (FL) provides a\nprivacy-preserving solution by aggregating local models without sharing data.\nTraditional supervised learning requires image data with labels, but data\nlabeling involves significant manual effort. Federated Self-Supervised Learning\n(FSSL) utilizes Self-Supervised Learning (SSL) for local training in FL,\neliminating the need for labels while protecting privacy. Compared to other SSL\nmethods, Momentum Contrast (MoCo) reduces the demand for computing resources\nand storage space by creating a dictionary. However, using MoCo in FSSL\nrequires uploading the local dictionary from vehicles to Base Station (BS),\nwhich poses a risk of privacy leakage. Simplified Contrast (SimCo) addresses\nthe privacy leakage issue in MoCo-based FSSL by using dual temperature instead\nof a dictionary to control sample distribution. Additionally, considering the\nnegative impact of motion blur on model aggregation, and based on SimCo, we\npropose a motion blur-resistant FSSL method, referred to as BFSSL. Furthermore,\nwe address energy consumption and delay in the BFSSL process by proposing a\nDeep Reinforcement Learning (DRL)-based resource allocation scheme, called\nDRL-BFSSL. In this scheme, BS allocates the Central Processing Unit (CPU)\nfrequency and transmission power of vehicles to minimize energy consumption and\nlatency, while aggregating received models based on the motion blur level.\nSimulation results validate the effectiveness of our proposed aggregation and\nresource allocation methods.\n","authors":["Xueying Gu","Qiong Wu","Pingyi Fan","Qiang Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2408.09194v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at: https://github.com/qiongwu86/DRL-BFSSL"},{"id":"http://arxiv.org/abs/2506.15260v1","updated":"2025-06-18T08:37:55Z","published":"2025-06-18T08:37:55Z","title":"Domain Adaptation for Image Classification of Defects in Semiconductor\n  Manufacturing","summary":"  In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.\n","authors":["Adrian Poniatowski","Natalie Gentner","Manuel Barusco","Davide Dalle Pezze","Samuele Salti","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2506.15260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15258v1","updated":"2025-06-18T08:35:50Z","published":"2025-06-18T08:35:50Z","title":"Privacy-Preserving Chest X-ray Classification in Latent Space with\n  Homomorphically Encrypted Neural Inference","summary":"  Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images\n","authors":["Jonghun Kim","Gyeongdeok Jo","Shinyoung Ra","Hyunjin Park"],"pdf_url":"https://arxiv.org/pdf/2506.15258v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.19479v2","updated":"2025-06-18T08:32:27Z","published":"2024-11-29T05:34:21Z","title":"FLARE: Towards Universal Dataset Purification against Backdoor Attacks","summary":"  Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison datasets with adversary-specified triggers to implant hidden\nbackdoors, enabling malicious manipulation of model predictions. Dataset\npurification serves as a proactive defense by removing malicious training\nsamples to prevent backdoor injection at its source. We first reveal that the\ncurrent advanced purification methods rely on a latent assumption that the\nbackdoor connections between triggers and target labels in backdoor attacks are\nsimpler to learn than the benign features. We demonstrate that this assumption,\nhowever, does not always hold, especially in all-to-all (A2A) and untargeted\n(UT) attacks. As a result, purification methods that analyze the separation\nbetween the poisoned and benign samples in the input-output space or the final\nhidden layer space are less effective. We observe that this separability is not\nconfined to a single layer but varies across different hidden layers. Motivated\nby this understanding, we propose FLARE, a universal purification method to\ncounter various backdoor attacks. FLARE aggregates abnormal activations from\nall hidden layers to construct representations for clustering. To enhance\nseparation, FLARE develops an adaptive subspace selection algorithm to isolate\nthe optimal space for dividing an entire dataset into two clusters. FLARE\nassesses the stability of each cluster and identifies the cluster with higher\nstability as poisoned. Extensive evaluations on benchmark datasets demonstrate\nthe effectiveness of FLARE against 22 representative backdoor attacks,\nincluding all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and\nits robustness to adaptive attacks. Codes are available at\n\\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and\n\\href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.\n","authors":["Linshan Hou","Wei Luo","Zhongyun Hua","Songhua Chen","Leo Yu Zhang","Yiming Li"],"pdf_url":"https://arxiv.org/pdf/2411.19479v2.pdf","comment":"15 pages, This paper is accepted and will appear in TIFS (CCF-A)"},{"id":"http://arxiv.org/abs/2506.15244v1","updated":"2025-06-18T08:22:19Z","published":"2025-06-18T08:22:19Z","title":"Retrospective Memory for Camouflaged Object Detection","summary":"  Camouflaged object detection (COD) primarily focuses on learning subtle yet\ndiscriminative representations from complex scenes. Existing methods\npredominantly follow the parametric feedforward architecture based on static\nvisual representation modeling. However, they lack explicit mechanisms for\nacquiring historical context, limiting their adaptation and effectiveness in\nhandling challenging camouflage scenes. In this paper, we propose a\nrecall-augmented COD architecture, namely RetroMem, which dynamically modulates\ncamouflage pattern perception and inference by integrating relevant historical\nknowledge into the process. Specifically, RetroMem employs a two-stage training\nparadigm consisting of a learning stage and a recall stage to construct,\nupdate, and utilize memory representations effectively. During the learning\nstage, we design a dense multi-scale adapter (DMA) to improve the pretrained\nencoder's capability to capture rich multi-scale visual information with very\nfew trainable parameters, thereby providing foundational inferences. In the\nrecall stage, we propose a dynamic memory mechanism (DMM) and an inference\npattern reconstruction (IPR). These components fully leverage the latent\nrelationships between learned knowledge and current sample context to\nreconstruct the inference of camouflage patterns, thereby significantly\nimproving the model's understanding of camouflage scenes. Extensive experiments\non several widely used datasets demonstrate that our RetroMem significantly\noutperforms existing state-of-the-art methods.\n","authors":["Chenxi Zhang","Jiayun Wu","Qing Zhang","Yazhe Zhai","Youwei Pang"],"pdf_url":"https://arxiv.org/pdf/2506.15244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15242v1","updated":"2025-06-18T08:21:19Z","published":"2025-06-18T08:21:19Z","title":"RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate\n  Camera Pose Estimation under Complex Trajectories","summary":"  Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories.\n","authors":["Qingsong Yan","Qiang Wang","Kaiyong Zhao","Jie Chen","Bo Li","Xiaowen Chu","Fei Deng"],"pdf_url":"https://arxiv.org/pdf/2506.15242v1.pdf","comment":"IROS 2025"},{"id":"http://arxiv.org/abs/2405.12661v3","updated":"2025-06-18T08:17:51Z","published":"2024-05-21T10:18:45Z","title":"EmoEdit: Evoking Emotions through Image Manipulation","summary":"  Affective Image Manipulation (AIM) seeks to modify user-provided images to\nevoke specific emotional responses. This task is inherently complex due to its\ntwofold objective: significantly evoking the intended emotion, while preserving\nthe original image composition. Existing AIM methods primarily adjust color and\nstyle, often failing to elicit precise and profound emotional shifts. Drawing\non psychological insights, we introduce EmoEdit, which extends AIM by\nincorporating content modifications to enhance emotional impact. Specifically,\nwe first construct EmoEditSet, a large-scale AIM dataset comprising 40,120\npaired data through emotion attribution and data construction. To make existing\ngenerative models emotion-aware, we design the Emotion adapter and train it\nusing EmoEditSet. We further propose an instruction loss to capture the\nsemantic variations in data pairs. Our method is evaluated both qualitatively\nand quantitatively, demonstrating superior performance compared to existing\nstate-of-the-art techniques. Additionally, we showcase the portability of our\nEmotion adapter to other diffusion-based models, enhancing their emotion\nknowledge with diverse semantics.\n","authors":["Jingyuan Yang","Jiawei Feng","Weibin Luo","Dani Lischinski","Daniel Cohen-Or","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2405.12661v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15231v1","updated":"2025-06-18T08:14:28Z","published":"2025-06-18T08:14:28Z","title":"Convolutional Feature Enhancement and Attention Fusion BiFPN for Ship\n  Detection in SAR Images","summary":"  Synthetic Aperture Radar (SAR) enables submeter-resolution imaging and\nall-weather monitoring via active microwave and advanced signal processing.\nCurrently, SAR has found extensive applications in critical maritime domains\nsuch as ship detection. However, SAR ship detection faces several challenges,\nincluding significant scale variations among ships, the presence of small\noffshore vessels mixed with noise, and complex backgrounds for large nearshore\nships. To address these issues, this paper proposes a novel feature enhancement\nand fusion framework named C-AFBiFPN. C-AFBiFPN constructs a Convolutional\nFeature Enhancement (CFE) module following the backbone network, aiming to\nenrich feature representation and enhance the ability to capture and represent\nlocal details and contextual information. Furthermore, C-AFBiFPN innovatively\nintegrates BiFormer attention within the fusion strategy of BiFPN, creating the\nAFBiFPN network. AFBiFPN improves the global modeling capability of cross-scale\nfeature fusion and can adaptively focus on critical feature regions. The\nexperimental results on SAR Ship Detection Dataset (SSDD) indicate that the\nproposed approach substantially enhances detection accuracy for small targets,\nrobustness against occlusions, and adaptability to multi-scale features.\n","authors":["Liangjie Meng","Danxia Li","Jinrong He","Lili Ma","Zhixin Li"],"pdf_url":"https://arxiv.org/pdf/2506.15231v1.pdf","comment":"5 pages, 4 figures, 2 tables. Code available at\n  https://github.com/mlj666219/C-AFBiFPN/tree/master"},{"id":"http://arxiv.org/abs/2309.10815v4","updated":"2025-06-18T08:04:33Z","published":"2023-09-19T17:54:22Z","title":"PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes","summary":"  Training perception systems for self-driving cars requires substantial 2D\nannotations that are labor-intensive to manual label. While existing datasets\nprovide rich annotations on pre-recorded sequences, they fall short in labeling\nrarely encountered viewpoints, potentially hampering the generalization ability\nfor perception models. In this paper, we present PanopticNeRF-360, a novel\napproach that combines coarse 3D annotations with noisy 2D semantic cues to\ngenerate high-quality panoptic labels and images from any viewpoint. Our key\ninsight lies in exploiting the complementarity of 3D and 2D priors to mutually\nenhance geometry and semantics. Specifically, we propose to leverage coarse 3D\nbounding primitives and noisy 2D semantic and instance predictions to guide\ngeometry optimization, by encouraging predicted labels to match panoptic pseudo\nground truth. Simultaneously, the improved geometry assists in filtering 3D&2D\nannotation noise by fusing semantics in 3D space via a learned semantic field.\nTo further enhance appearance, we combine MLP and hash grids to yield hybrid\nscene features, striking a balance between high-frequency appearance and\ncontiguous semantics. Our experiments demonstrate PanopticNeRF-360's\nstate-of-the-art performance over label transfer methods on the challenging\nurban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables\nomnidirectional rendering of high-fidelity, multi-view and spatiotemporally\nconsistent appearance, semantic and instance labels. We make our code and data\navailable at https://github.com/fuxiao0719/PanopticNeRF\n","authors":["Xiao Fu","Shangzhan Zhang","Tianrun Chen","Yichong Lu","Xiaowei Zhou","Andreas Geiger","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2309.10815v4.pdf","comment":"TPAMI 2025. Project page:\n  http://fuxiao0719.github.io/projects/panopticnerf360/ Code:\n  https://github.com/fuxiao0719/PanopticNeRF/tree/panopticnerf360"},{"id":"http://arxiv.org/abs/2503.13956v2","updated":"2025-06-18T08:04:33Z","published":"2025-03-18T06:48:08Z","title":"Improving LLM Video Understanding with 16 Frames Per Second","summary":"  Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. We will release the source code, model checkpoints,\nand data at\n\\href{https://github.com/bytedance/F-16}{https://github.com/bytedance/F-16}.\n","authors":["Yixuan Li","Changli Tang","Jimin Zhuang","Yudong Yang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15220v1","updated":"2025-06-18T07:58:41Z","published":"2025-06-18T07:58:41Z","title":"video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15218v1","updated":"2025-06-18T07:55:06Z","published":"2025-06-18T07:55:06Z","title":"DM-FNet: Unified multimodal medical image fusion via diffusion\n  process-trained encoder-decoder","summary":"  Multimodal medical image fusion (MMIF) extracts the most meaningful\ninformation from multiple source images, enabling a more comprehensive and\naccurate diagnosis. Achieving high-quality fusion results requires a careful\nbalance of brightness, color, contrast, and detail; this ensures that the fused\nimages effectively display relevant anatomical structures and reflect the\nfunctional status of the tissues. However, existing MMIF methods have limited\ncapacity to capture detailed features during conventional training and suffer\nfrom insufficient cross-modal feature interaction, leading to suboptimal fused\nimage quality. To address these issues, this study proposes a two-stage\ndiffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In\nStage I, a diffusion process trains UNet for image reconstruction. UNet\ncaptures detailed information through progressive denoising and represents\nmultilevel data, providing a rich set of feature representations for the\nsubsequent fusion network. In Stage II, noisy images at various steps are input\ninto the fusion network to enhance the model's feature recognition capability.\nThree key fusion modules are also integrated to process medical images from\ndifferent modalities adaptively. Ultimately, the robust network structure and a\nhybrid loss function are integrated to harmonize the fused image's brightness,\ncolor, contrast, and detail, enhancing its quality and information density. The\nexperimental results across various medical image types demonstrate that the\nproposed method performs exceptionally well regarding objective evaluation\nmetrics. The fused image preserves appropriate brightness, a comprehensive\ndistribution of radioactive tracers, rich textures, and clear edges. The code\nis available at https://github.com/HeDan-11/DM-FNet.\n","authors":["Dan He","Weisheng Li","Guofen Wang","Yuping Huang","Shiqiang Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15218v1.pdf","comment":"This paper has been accepted by IEEE Transactions on Multimedia (TMM)\n  in March 2025"},{"id":"http://arxiv.org/abs/2506.13443v2","updated":"2025-06-18T07:33:50Z","published":"2025-06-16T12:57:29Z","title":"PRO: Projection Domain Synthesis for CT Imaging","summary":"  Synthesizing high quality CT projection data remains a significant challenge\ndue to the limited availability of annotated data and the complex nature of CT\nimaging. In this work, we present PRO, a projection domain synthesis foundation\nmodel for CT imaging. To the best of our knowledge, this is the first study\nthat performs CT synthesis in the projection domain. Unlike previous approaches\nthat operate in the image domain, PRO learns rich structural representations\nfrom raw projection data and leverages anatomical text prompts for controllable\nsynthesis. This projection domain strategy enables more faithful modeling of\nunderlying imaging physics and anatomical structures. Moreover, PRO functions\nas a foundation model, capable of generalizing across diverse downstream tasks\nby adjusting its generative behavior via prompt inputs. Experimental results\ndemonstrated that incorporating our synthesized data significantly improves\nperformance across multiple downstream tasks, including low-dose and\nsparse-view reconstruction. These findings underscore the versatility and\nscalability of PRO in data generation for various CT applications. These\nresults highlight the potential of projection domain synthesis as a powerful\ntool for data augmentation and robust CT imaging. Our source code is publicly\navailable at: https://github.com/yqx7150/PRO.\n","authors":["Kang Chen","Bin Huang","Xuebin Yang","Junyan Zhang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00473v5","updated":"2025-06-18T07:33:15Z","published":"2024-11-30T13:21:15Z","title":"Jailbreak Large Vision-Language Models Through Multi-Modal Linkage","summary":"  With the significant advancement of Large Vision-Language Models (VLMs),\nconcerns about their potential misuse and abuse have grown rapidly. Previous\nstudies have highlighted VLMs' vulnerability to jailbreak attacks, where\ncarefully crafted inputs can lead the model to produce content that violates\nethical and legal standards. However, existing methods struggle against\nstate-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content\nand lack of stealthy malicious guidance. In this work, we propose a novel\njailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing\ninspiration from cryptography, MML utilizes an encryption-decryption process\nacross text and image modalities to mitigate over-exposure of malicious\ninformation. To align the model's output with malicious intent covertly, MML\nemploys a technique called \"evil alignment\", framing the attack within a video\ngame production scenario. Comprehensive experiments demonstrate MML's\neffectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of\n97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our\ncode is available at https://github.com/wangyu-ovo/MML.\n","authors":["Yu Wang","Xiaofei Zhou","Yichen Wang","Geyuan Zhang","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2412.00473v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15201v1","updated":"2025-06-18T07:29:40Z","published":"2025-06-18T07:29:40Z","title":"Privacy-Shielded Image Compression: Defending Against Exploitation from\n  Vision-Language Pretrained Models","summary":"  The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.\n","authors":["Xuelin Shen","Jiayin Xu","Kangsheng Yin","Wenhan Yang"],"pdf_url":"https://arxiv.org/pdf/2506.15201v1.pdf","comment":"11 pages, 6 figures, publised to ICML 2025"},{"id":"http://arxiv.org/abs/2506.15200v1","updated":"2025-06-18T07:28:47Z","published":"2025-06-18T07:28:47Z","title":"Conquering the Retina: Bringing Visual in-Context Learning to OCT","summary":"  Recent advancements in medical image analysis have led to the development of\nhighly specialized models tailored to specific clinical tasks. These models\nhave demonstrated exceptional performance and remain a crucial research\ndirection. Yet, their applicability is limited to predefined tasks, requiring\nexpertise and extensive resources for development and adaptation. In contrast,\ngeneralist models offer a different form of utility: allowing medical\npractitioners to define tasks on the fly without the need for task-specific\nmodel development. In this work, we explore how to train generalist models for\nthe domain of retinal optical coherence tomography using visual in-context\nlearning (VICL), i.e., training models to generalize across tasks based on a\nfew examples provided at inference time. To facilitate rigorous assessment, we\npropose a broad evaluation protocol tailored to VICL in OCT. We extensively\nevaluate a state-of-the-art medical VICL approach on multiple retinal OCT\ndatasets, establishing a first baseline to highlight the potential and current\nlimitations of in-context learning for OCT. To foster further research and\npractical adoption, we openly release our code.\n","authors":["Alessio Negrini","Simon Reiß"],"pdf_url":"https://arxiv.org/pdf/2506.15200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14315v2","updated":"2025-06-18T07:15:43Z","published":"2025-06-17T08:50:05Z","title":"ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies","summary":"  Automatic creation of 3D scenes for immersive VR presence has been a\nsignificant research focus for decades. However, existing methods often rely on\neither high-poly mesh modeling with post-hoc simplification or massive 3D\nGaussians, resulting in a complex pipeline or limited visual realism. In this\npaper, we demonstrate that such exhaustive modeling is unnecessary for\nachieving compelling immersive experience. We introduce ImmerseGen, a novel\nagent-guided framework for compact and photorealistic world modeling.\nImmerseGen represents scenes as hierarchical compositions of lightweight\ngeometric proxies, i.e., simplified terrain and billboard meshes, and generates\nphotorealistic appearance by synthesizing RGBA textures onto these proxies.\nSpecifically, we propose terrain-conditioned texturing for user-centric base\nworld synthesis, and RGBA asset texturing for midground and foreground scenery.\nThis reformulation offers several advantages: (i) it simplifies modeling by\nenabling agents to guide generative models in producing coherent textures that\nintegrate seamlessly with the scene; (ii) it bypasses complex geometry creation\nand decimation by directly synthesizing photorealistic textures on proxies,\npreserving visual quality without degradation; (iii) it enables compact\nrepresentations suitable for real-time rendering on mobile VR headsets. To\nautomate scene creation from text prompts, we introduce VLM-based modeling\nagents enhanced with semantic grid-based analysis for improved spatial\nreasoning and accurate asset placement. ImmerseGen further enriches scenes with\ndynamic effects and ambient audio to support multisensory immersion.\nExperiments on scene generation and live VR showcases demonstrate that\nImmerseGen achieves superior photorealism, spatial coherence and rendering\nefficiency compared to prior methods. Project webpage:\nhttps://immersegen.github.io.\n","authors":["Jinyan Yuan","Bangbang Yang","Keke Wang","Panwang Pan","Lin Ma","Xuehai Zhang","Xiao Liu","Zhaopeng Cui","Yuewen Ma"],"pdf_url":"https://arxiv.org/pdf/2506.14315v2.pdf","comment":"Project webpage: https://immersegen.github.io"},{"id":"http://arxiv.org/abs/2506.15182v1","updated":"2025-06-18T06:55:38Z","published":"2025-06-18T06:55:38Z","title":"Classification of Multi-Parametric Body MRI Series Using Deep Learning","summary":"  Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.\n","authors":["Boah Kim","Tejas Sudharshan Mathai","Kimberly Helm","Peter A. Pinto","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2506.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15180v1","updated":"2025-06-18T06:52:10Z","published":"2025-06-18T06:52:10Z","title":"ReSeDis: A Dataset for Referring-based Object Search across Large-Scale\n  Image Collections","summary":"  Large-scale visual search engines are expected to solve a dual problem at\nonce: (i) locate every image that truly contains the object described by a\nsentence and (ii) identify the object's bounding box or exact pixels within\neach hit. Existing techniques address only one side of this challenge. Visual\ngrounding yields tight boxes and masks but rests on the unrealistic assumption\nthat the object is present in every test image, producing a flood of false\nalarms when applied to web-scale collections. Text-to-image retrieval excels at\nsifting through massive databases to rank relevant images, yet it stops at\nwhole-image matches and offers no fine-grained localization. We introduce\nReferring Search and Discovery (ReSeDis), the first task that unifies\ncorpus-level retrieval with pixel-level grounding. Given a free-form\ndescription, a ReSeDis model must decide whether the queried object appears in\neach image and, if so, where it is, returning bounding boxes or segmentation\nmasks. To enable rigorous study, we curate a benchmark in which every\ndescription maps uniquely to object instances scattered across a large, diverse\ncorpus, eliminating unintended matches. We further design a task-specific\nmetric that jointly scores retrieval recall and localization precision.\nFinally, we provide a straightforward zero-shot baseline using a frozen\nvision-language model, revealing significant headroom for future study. ReSeDis\noffers a realistic, end-to-end testbed for building the next generation of\nrobust and scalable multimodal search systems.\n","authors":["Ziling Huang","Yidan Zhang","Shin'ichi Satoh"],"pdf_url":"https://arxiv.org/pdf/2506.15180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04818v2","updated":"2025-06-18T06:44:06Z","published":"2025-04-07T08:17:54Z","title":"SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection\n  Enhancement","summary":"  Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods.\n","authors":["Zuying Xie","Changtao Miao","Ajian Liu","Jiabao Guo","Feng Li","Dan Guo","Yunfeng Diao"],"pdf_url":"https://arxiv.org/pdf/2504.04818v2.pdf","comment":"Accepted in ICME 2025 (Oral)"},{"id":"http://arxiv.org/abs/2506.15166v1","updated":"2025-06-18T06:27:08Z","published":"2025-06-18T06:27:08Z","title":"Echo-DND: A dual noise diffusion model for robust and precise left\n  ventricle segmentation in echocardiography","summary":"  Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND\n","authors":["Abdur Rahman","Keerthiveena Balraj","Manojkumar Ramteke","Anurag Singh Rathore"],"pdf_url":"https://arxiv.org/pdf/2506.15166v1.pdf","comment":"Version of record published in Discover Applied Sciences (Springer\n  Nature). The definitive article is available at\n  https://doi.org/10.1007/s42452-025-07055-5"},{"id":"http://arxiv.org/abs/2412.09441v2","updated":"2025-06-18T06:21:19Z","published":"2024-12-12T16:57:20Z","title":"MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental\n  Learning","summary":"  Class-Incremental Learning (CIL) requires models to continually acquire\nknowledge of new classes without forgetting old ones. Despite Pre-trained\nModels (PTMs) have shown excellent performance in CIL, catastrophic forgetting\nstill occurs as the model learns new concepts. Existing work seeks to utilize\nlightweight components to adjust the PTM, while the forgetting phenomenon still\ncomes from {\\em parameter and retrieval} levels. Specifically, iterative\nupdates of the model result in parameter drift, while mistakenly retrieving\nirrelevant modules leads to the mismatch during inference. To this end, we\npropose MOdel Surgery (MOS) to rescue the model from forgetting previous\nknowledge. By training task-specific adapters, we continually adjust the PTM to\ndownstream tasks. To mitigate parameter-level forgetting, we present an adapter\nmerging approach to learn task-specific adapters, which aims to bridge the gap\nbetween different components while reserve task-specific information. Besides,\nto address retrieval-level forgetting, we introduce a training-free\nself-refined adapter retrieval mechanism during inference, which leverages the\nmodel's inherent ability for better adapter retrieval. By jointly rectifying\nthe model with those steps, MOS can robustly resist catastrophic forgetting in\nthe learning process. Extensive experiments on seven benchmark datasets\nvalidate MOS's state-of-the-art performance. Code is available at:\nhttps://github.com/sun-hailong/AAAI25-MOS\n","authors":["Hai-Long Sun","Da-Wei Zhou","Hanbin Zhao","Le Gan","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2412.09441v2.pdf","comment":"Accepted to AAAI 2025. Code is available at:\n  https://github.com/sun-hailong/AAAI25-MOS"},{"id":"http://arxiv.org/abs/2506.15160v1","updated":"2025-06-18T06:08:17Z","published":"2025-06-18T06:08:17Z","title":"Enhancing point cloud analysis via neighbor aggregation correction based\n  on cross-stage structure correlation","summary":"  Point cloud analysis is the cornerstone of many downstream tasks, among which\naggregating local structures is the basis for understanding point cloud data.\nWhile numerous works aggregate neighbor using three-dimensional relative\ncoordinates, there are irrelevant point interference and feature hierarchy gap\nproblems due to the limitation of local coordinates. Although some works\naddress this limitation by refining spatial description though explicit\nmodeling of cross-stage structure, these enhancement methods based on direct\ngeometric structure encoding have problems of high computational overhead and\nnoise sensitivity. To overcome these problems, we propose the Point\nDistribution Set Abstraction module (PDSA) that utilizes the correlation in the\nhigh-dimensional space to correct the feature distribution during aggregation,\nwhich improves the computational efficiency and robustness. PDSA distinguishes\nthe point correlation based on a lightweight cross-stage structural descriptor,\nand enhances structural homogeneity by reducing the variance of the neighbor\nfeature matrix and increasing classes separability though long-distance\nmodeling. Additionally, we introducing a key point mechanism to optimize the\ncomputational overhead. The experimental result on semantic segmentation and\nclassification tasks based on different baselines verify the generalization of\nthe method we proposed, and achieve significant performance improvement with\nless parameter cost. The corresponding ablation and visualization results\ndemonstrate the effectiveness and rationality of our method. The code and\ntraining weight is available at: https://github.com/AGENT9717/PointDistribution\n","authors":["Jiaqi Shi","Jin Xiao","Xiaoguang Hu","Boyang Song","Hao Jiang","Tianyou Chen","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15160v1.pdf","comment":"17 papes, 7 figures"},{"id":"http://arxiv.org/abs/2506.15157v1","updated":"2025-06-18T06:02:06Z","published":"2025-06-18T06:02:06Z","title":"Robust Instant Policy: Leveraging Student's t-Regression Model for\n  Robust In-context Imitation Learning of Robot Manipulation","summary":"  Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.\n","authors":["Hanbit Oh","Andrea M. Salcedo-Vázquez","Ixchel G. Ramirez-Alpizar","Yukiyasu Domae"],"pdf_url":"https://arxiv.org/pdf/2506.15157v1.pdf","comment":"IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2025 accepted"},{"id":"http://arxiv.org/abs/2506.15153v1","updated":"2025-06-18T05:35:40Z","published":"2025-06-18T05:35:40Z","title":"SynPo: Boosting Training-Free Few-Shot Medical Segmentation via\n  High-Quality Negative Prompts","summary":"  The advent of Large Vision Models (LVMs) offers new opportunities for\nfew-shot medical image segmentation. However, existing training-free methods\nbased on LVMs fail to effectively utilize negative prompts, leading to poor\nperformance on low-contrast medical images. To address this issue, we propose\nSynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core\ninsight: improving the quality of negative prompts. To select point prompts in\na more reliable confidence map, we design a novel Confidence Map Synergy Module\nby combining the strengths of DINOv2 and SAM. Based on the confidence map, we\nselect the top-k pixels as the positive points set and choose the negative\npoints set using a Gaussian distribution, followed by independent K-means\nclustering for both sets. Then, these selected points are leveraged as\nhigh-quality prompts for SAM to get the segmentation results. Extensive\nexperiments demonstrate that SynPo achieves performance comparable to\nstate-of-the-art training-based few-shot methods.\n","authors":["Yufei Liu","Haoke Xiao","Jiaxing Chai","Yongcun Zhang","Rong Wang","Zijie Meng","Zhiming Luo"],"pdf_url":"https://arxiv.org/pdf/2506.15153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13214v2","updated":"2025-06-18T05:21:32Z","published":"2024-07-18T06:54:49Z","title":"A Curated and Re-annotated Peripheral Blood Cell Dataset Integrating\n  Four Public Resources","summary":"  We present TXL-PBC, a curated and re-annotated peripheral blood cell dataset\nconstructed by integrating four publicly available resources: Blood Cell Count\nand Detection (BCCD), Blood Cell Detection Dataset (BCDD), Peripheral Blood\nCells (PBC), and Raabin White Blood Cell (Raabin-WBC). Through rigorous sample\nselection, semi-automatic annotation using the YOLOv8n model, and comprehensive\nmanual review, we ensured high annotation accuracy and consistency. The final\ndataset contains 1,260 images and 18,143 bounding box annotations for three\nmajor blood cell types: white blood cells (WBC), red blood cells (RBC), and\nplatelets. We provide detailed visual analyses of the data distribution,\ndemonstrating the diversity and balance of the dataset. To further validate the\nquality and utility of TXL-PBC, we trained several mainstream object detection\nmodels, including YOLOv5s, YOLOv8s, YOLOv11s, SSD300, Faster R-CNN, and\nRetinaNet, and report their baseline performance. The TXL-PBC dataset is openly\navailable on Figshare and GitHub, offering a valuable resource for the\ndevelopment and benchmarking of blood cell detection models and related machine\nlearning research.\n","authors":["Lu Gan","Xi Li","Xichun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10563v2","updated":"2025-06-18T05:14:07Z","published":"2025-04-14T16:46:32Z","title":"Data Augmentation Through Random Style Replacement","summary":"  In this paper, we introduce a novel data augmentation technique that combines\nthe advantages of style augmentation and random erasing by selectively\nreplacing image subregions with style-transferred patches. Our approach first\napplies a random style transfer to training images, then randomly substitutes\nselected areas of these images with patches derived from the style-transferred\nversions. This method is able to seamlessly accommodate a wide range of\nexisting style transfer algorithms and can be readily integrated into diverse\ndata augmentation pipelines. By incorporating our strategy, the training\nprocess becomes more robust and less prone to overfitting. Comparative\nexperiments demonstrate that, relative to previous style augmentation methods,\nour technique achieves superior performance and faster convergence.\n","authors":["Qikai Yang","Cheng Ji","Huaiying Luo","Panfeng Li","Zhicheng Ding"],"pdf_url":"https://arxiv.org/pdf/2504.10563v2.pdf","comment":"Accepted by 2025 6th International Conference on Computer Vision,\n  Image and Deep Learning"},{"id":"http://arxiv.org/abs/2412.14018v2","updated":"2025-06-18T04:36:29Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video\n  Generation","summary":"  Surgical video generation can enhance medical education and research, but\nexisting methods lack fine-grained motion control and realism. We introduce\nSurgSora, a framework that generates high-fidelity, motion-controllable\nsurgical videos from a single input frame and user-specified motion cues.\nUnlike prior approaches that treat objects indiscriminately or rely on\nground-truth segmentation masks, SurgSora leverages self-predicted object\nfeatures and depth information to refine RGB appearance and optical flow for\nprecise video synthesis. It consists of three key modules: (1) the Dual\nSemantic Injector, which extracts object-specific RGB-D features and\nsegmentation cues to enhance spatial representations; (2) the Decoupled Flow\nMapper, which fuses multi-scale optical flow with semantic features for\nrealistic motion dynamics; and (3) the Trajectory Controller, which estimates\nsparse optical flow and enables user-guided object movement. By conditioning\nthese enriched features within the Stable Video Diffusion, SurgSora achieves\nstate-of-the-art visual authenticity and controllability in advancing surgical\nvideo synthesis, as demonstrated by extensive quantitative and qualitative\ncomparisons. Our human evaluation in collaboration with expert surgeons further\ndemonstrates the high realism of SurgSora-generated videos, highlighting the\npotential of our method for surgical training and education. Our project is\navailable at https://surgsora.github.io/surgsora.github.io.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06940v4","updated":"2025-06-18T04:35:42Z","published":"2024-10-09T14:34:53Z","title":"Representation Alignment for Generation: Training Diffusion Transformers\n  Is Easier Than You Think","summary":"  Recent studies have shown that the denoising process in (generative)\ndiffusion models can induce meaningful (discriminative) representations inside\nthe model, though the quality of these representations still lags behind those\nlearned through recent self-supervised learning methods. We argue that one main\nbottleneck in training large-scale diffusion models for generation lies in\neffectively learning these representations. Moreover, training can be made\neasier by incorporating high-quality external visual representations, rather\nthan relying solely on the diffusion models to learn them independently. We\nstudy this by introducing a straightforward regularization called\nREPresentation Alignment (REPA), which aligns the projections of noisy input\nhidden states in denoising networks with clean image representations obtained\nfrom external, pretrained visual encoders. The results are striking: our simple\nstrategy yields significant improvements in both training efficiency and\ngeneration quality when applied to popular diffusion and flow-based\ntransformers, such as DiTs and SiTs. For instance, our method can speed up SiT\ntraining by over 17.5$\\times$, matching the performance (without\nclassifier-free guidance) of a SiT-XL model trained for 7M steps in less than\n400K steps. In terms of final generation quality, our approach achieves\nstate-of-the-art results of FID=1.42 using classifier-free guidance with the\nguidance interval.\n","authors":["Sihyun Yu","Sangkyung Kwak","Huiwon Jang","Jongheon Jeong","Jonathan Huang","Jinwoo Shin","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2410.06940v4.pdf","comment":"ICLR 2025 (Oral). Project page: https://sihyun.me/REPA"},{"id":"http://arxiv.org/abs/2505.09630v2","updated":"2025-06-18T04:13:19Z","published":"2025-05-01T08:19:51Z","title":"Generative diffusion model surrogates for mechanistic agent-based\n  biological models","summary":"  Mechanistic, multicellular, agent-based models are commonly used to\ninvestigate tissue, organ, and organism-scale biology at single-cell\nresolution. The Cellular-Potts Model (CPM) is a powerful and popular framework\nfor developing and interrogating these models. CPMs become computationally\nexpensive at large space- and time- scales making application and investigation\nof developed models difficult. Surrogate models may allow for the accelerated\nevaluation of CPMs of complex biological systems. However, the stochastic\nnature of these models means each set of parameters may give rise to different\nmodel configurations, complicating surrogate model development. In this work,\nwe leverage denoising diffusion probabilistic models to train a generative AI\nsurrogate of a CPM used to investigate in vitro vasculogenesis. We describe the\nuse of an image classifier to learn the characteristics that define unique\nareas of a 2-dimensional parameter space. We then apply this classifier to aid\nin surrogate model selection and verification. Our CPM model surrogate\ngenerates model configurations 20,000 timesteps ahead of a reference\nconfiguration and demonstrates approximately a 22x reduction in computational\ntime as compared to native code execution. Our work represents a step towards\nthe implementation of DDPMs to develop digital twins of stochastic biological\nsystems.\n","authors":["Tien Comlekoglu","J. Quetzalcoatl Toledo-Marín","Douglas W. DeSimone","Shayn M. Peirce","Geoffrey Fox","James A. Glazier"],"pdf_url":"https://arxiv.org/pdf/2505.09630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14719v3","updated":"2025-06-18T03:58:23Z","published":"2025-05-19T14:01:03Z","title":"MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention\n  Fusion","summary":"  The combination of Spiking Neural Networks (SNNs) with Vision Transformer\narchitectures has garnered significant attention due to their potential for\nenergy-efficient and high-performance computing paradigms. However, a\nsubstantial performance gap still exists between SNN-based and ANN-based\ntransformer architectures. While existing methods propose spiking\nself-attention mechanisms that are successfully combined with SNNs, the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting features from different image scales. In this paper, we address this\nissue and propose MSVIT. This novel spike-driven Transformer architecture\nfirstly uses multi-scale spiking attention (MSSA) to enhance the capabilities\nof spiking attention blocks. We validate our approach across various main\ndatasets. The experimental results show that MSVIT outperforms existing\nSNN-based models, positioning itself as a state-of-the-art solution among\nSNN-transformer architectures. The codes are available at\nhttps://github.com/Nanhu-AI-Lab/MSViT.\n","authors":["Wei Hua","Chenlin Zhou","Jibin Wu","Yansong Chua","Yangyang Shu"],"pdf_url":"https://arxiv.org/pdf/2505.14719v3.pdf","comment":"11pages, 2figures, accepted by IJCAI'25 (34th International Joint\n  Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2506.12542v2","updated":"2025-06-18T03:37:46Z","published":"2025-06-14T15:31:54Z","title":"PLD: A Choice-Theoretic List-Wise Knowledge Distillation","summary":"  Knowledge distillation is a model compression technique in which a compact\n\"student\" network is trained to replicate the predictive behavior of a larger\n\"teacher\" network. In logit-based knowledge distillation it has become the de\nfacto approach to augment cross-entropy with a distillation term. Typically\nthis term is either a KL divergence-matching marginal probabilities or a\ncorrelation-based loss capturing intra- and inter-class relationships but in\nevery case it sits as an add-on to cross-entropy with its own weight that must\nbe carefully tuned. In this paper we adopt a choice-theoretic perspective and\nrecast knowledge distillation under the Plackett-Luce model by interpreting\nteacher logits as \"worth\" scores. We introduce Plackett-Luce Distillation\n(PLD), a weighted list-wise ranking loss in which the teacher model transfers\nknowledge of its full ranking of classes, weighting each ranked choice by its\nown confidence. PLD directly optimizes a single teacher-optimal ranking of the\ntrue label first, followed by the remaining classes in descending teacher\nconfidence, yielding a convex, translation-invariant surrogate that subsumes\nweighted cross-entropy. Empirically on standard image classification\nbenchmarks, PLD improves Top-1 accuracy by an average of +0.42% over DIST\n(arXiv:2205.10536) and +1.04% over KD (arXiv:1503.02531) in homogeneous\nsettings and by +0.48% and +1.09% over DIST and KD, respectively, in\nheterogeneous settings.\n","authors":["Ejafa Bassam","Dawei Zhu","Kaigui Bian"],"pdf_url":"https://arxiv.org/pdf/2506.12542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13155v2","updated":"2025-06-18T03:26:43Z","published":"2022-06-27T09:58:34Z","title":"Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich\n  Document Understanding","summary":"  Multi-modal document pre-trained models have proven to be very effective in a\nvariety of visually-rich document understanding (VrDU) tasks. Though existing\ndocument pre-trained models have achieved excellent performance on standard\nbenchmarks for VrDU, the way they model and exploit the interactions between\nvision and language on documents has hindered them from better generalization\nability and higher accuracy. In this work, we investigate the problem of\nvision-language joint representation learning for VrDU mainly from the\nperspective of supervisory signals. Specifically, a pre-training paradigm\ncalled Bi-VLDoc is proposed, in which a bidirectional vision-language\nsupervision strategy and a vision-language hybrid-attention mechanism are\ndevised to fully explore and utilize the interactions between these two\nmodalities, to learn stronger cross-modal document representations with richer\nsemantics. Benefiting from the learned informative cross-modal document\nrepresentations, Bi-VLDoc significantly advances the state-of-the-art\nperformance on three widely-used document understanding benchmarks, including\nForm Understanding (from 85.14% to 93.44%), Receipt Information Extraction\n(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).\nOn Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance\ncompared to previous single model methods.\n","authors":["Chuwei Luo","Guozhi Tang","Qi Zheng","Cong Yao","Lianwen Jin","Chenliang Li","Yang Xue","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2206.13155v2.pdf","comment":"IJDAR 2025"},{"id":"http://arxiv.org/abs/2412.04664v3","updated":"2025-06-18T03:07:32Z","published":"2024-12-05T23:19:51Z","title":"Multiclass Post-Earthquake Building Assessment Integrating\n  High-Resolution Optical and SAR Satellite Imagery, Ground Motion, and Soil\n  Data with Transformers","summary":"  Timely and accurate assessments of building damage are crucial for effective\nresponse and recovery in the aftermath of earthquakes. Conventional preliminary\ndamage assessments (PDA) often rely on manual door-to-door inspections, which\nare not only time-consuming but also pose significant safety risks. To safely\nexpedite the PDA process, researchers have studied the applicability of\nsatellite imagery processed with heuristic and machine learning approaches.\nThese approaches output binary or, more recently, multiclass damage states at\nthe scale of a block or a single building. However, the current performance of\nsuch approaches limits practical applicability. To address this limitation, we\nintroduce a metadata-enriched, transformer based framework that combines\nhigh-resolution post-earthquake satellite imagery with building-specific\nmetadata relevant to the seismic performance of the structure. Our model\nachieves state-of-the-art performance in multiclass post-earthquake damage\nidentification for buildings from the Turkey-Syria earthquake on February 6,\n2023. Specifically, we demonstrate that incorporating metadata, such as seismic\nintensity indicators, soil properties, and SAR damage proxy maps not only\nenhances the model's accuracy and ability to distinguish between damage\nclasses, but also improves its generalizability across various regions.\nFurthermore, we conducted a detailed, class-wise analysis of feature importance\nto understand the model's decision-making across different levels of building\ndamage. This analysis reveals how individual metadata features uniquely\ncontribute to predictions for each damage class. By leveraging both satellite\nimagery and metadata, our proposed framework enables faster and more accurate\ndamage assessments for precise, multiclass, building-level evaluations that can\nimprove disaster response and accelerate recovery efforts for affected\ncommunities.\n","authors":["Deepank Singh","Vedhus Hoskere","Pietro Milillo"],"pdf_url":"https://arxiv.org/pdf/2412.04664v3.pdf","comment":"28 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2506.15084v1","updated":"2025-06-18T02:49:09Z","published":"2025-06-18T02:49:09Z","title":"An Empirical Study of Bugs in Data Visualization Libraries","summary":"  Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.\n","authors":["Weiqi Lu","Yongqiang Tian","Xiaohan Zhong","Haoyang Ma","Zhenyang Xu","Shing-Chi Cheung","Chengnian Sun"],"pdf_url":"https://arxiv.org/pdf/2506.15084v1.pdf","comment":"Proc. ACM Softw. Eng. 2, FSE"},{"id":"http://arxiv.org/abs/2506.13589v2","updated":"2025-06-18T02:46:20Z","published":"2025-06-16T15:18:15Z","title":"AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long\n  Video Understanding","summary":"  Multimodal Large Language Models (MLLMs) struggle with long videos due to\nfixed context windows and weak long-term dependency modeling. Existing\nRetrieval-Augmented Generation (RAG) methods for videos use static retrieval\nstrategies, leading to inefficiencies for simple queries and information loss\nfor complex tasks. To address this, we propose AdaVideoRAG, a novel framework\nthat dynamically adapts retrieval granularity based on query complexity using a\nlightweight intent classifier. Our framework employs an Omni-Knowledge Indexing\nmodule to build hierarchical databases from text (captions, ASR, OCR), visual\nfeatures, and semantic graphs, enabling optimal resource allocation across\ntasks. We also introduce the HiVU benchmark for comprehensive evaluation.\nExperiments demonstrate improved efficiency and accuracy for long-video\nunderstanding, with seamless integration into existing MLLMs. AdaVideoRAG\nestablishes a new paradigm for adaptive retrieval in video analysis. Codes will\nbe open-sourced at https://github.com/xzc-zju/AdaVideoRAG.\n","authors":["Zhucun Xue","Jiangning Zhang","Xurong Xie","Yuxuan Cai","Yong Liu","Xiangtai Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2506.13589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15078v1","updated":"2025-06-18T02:43:40Z","published":"2025-06-18T02:43:40Z","title":"Enhancing Vector Quantization with Distributional Matching: A\n  Theoretical and Empirical Study","summary":"  The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.\n","authors":["Xianghong Fang","Litao Guo","Hengchao Chen","Yuxuan Zhang"," XiaofanXia","Dingjie Song","Yexin Liu","Hao Wang","Harry Yang","Yuan Yuan","Qiang Sun"],"pdf_url":"https://arxiv.org/pdf/2506.15078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01557v3","updated":"2025-06-18T02:28:35Z","published":"2025-01-02T22:24:13Z","title":"Click-Calib: A Robust Extrinsic Calibration Method for Surround-View\n  Systems","summary":"  Surround-View System (SVS) is an essential component in Advanced Driver\nAssistance System (ADAS) and requires precise calibrations. However,\nconventional offline extrinsic calibration methods are cumbersome and\ntime-consuming as they rely heavily on physical patterns. Additionally, these\nmethods primarily focus on short-range areas surrounding the vehicle, resulting\nin lower calibration quality in more distant zones. To address these\nlimitations, we propose Click-Calib, a pattern-free approach for offline SVS\nextrinsic calibration. Without requiring any special setup, the user only needs\nto click a few keypoints on the ground in natural scenes. Unlike other offline\ncalibration approaches, Click-Calib optimizes camera poses over a wide range by\nminimizing reprojection distance errors of keypoints, thereby achieving\naccurate calibrations at both short and long distances. Furthermore,\nClick-Calib supports both single-frame and multiple-frame modes, with the\nlatter offering even better results. Evaluations on our in-house dataset and\nthe public WoodScape dataset demonstrate its superior accuracy and robustness\ncompared to baseline methods. Code is available at\nhttps://github.com/lwangvaleo/click_calib.\n","authors":["Lihao Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01557v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02803v2","updated":"2025-06-18T01:50:39Z","published":"2025-06-03T12:33:47Z","title":"SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via\n  Visual Global Thinking","summary":"  Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.\n","authors":["Sifan Li","Yujun Cai","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2506.02803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08555v2","updated":"2025-06-18T00:44:46Z","published":"2025-06-10T08:23:14Z","title":"Towards Cross-Subject EMG Pattern Recognition via Dual-Branch\n  Adversarial Feature Disentanglement","summary":"  Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems.\n","authors":["Xinyue Niu","Akira Furui"],"pdf_url":"https://arxiv.org/pdf/2506.08555v2.pdf","comment":"6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025. New\n  version corrects numerical errors in Table 1. Conclusions are unaffected"},{"id":"http://arxiv.org/abs/2506.15033v1","updated":"2025-06-18T00:24:29Z","published":"2025-06-18T00:24:29Z","title":"Break Stylistic Sophon: Are We Really Meant to Confine the Imagination\n  in Style Transfer?","summary":"  In this pioneering study, we introduce StyleWallfacer, a groundbreaking\nunified training and inference framework, which not only addresses various\nissues encountered in the style transfer process of traditional methods but\nalso unifies the framework for different tasks. This framework is designed to\nrevolutionize the field by enabling artist level style transfer and text driven\nstylization. First, we propose a semantic-based style injection method that\nuses BLIP to generate text descriptions strictly aligned with the semantics of\nthe style image in CLIP space. By leveraging a large language model to remove\nstyle-related descriptions from these descriptions, we create a semantic gap.\nThis gap is then used to fine-tune the model, enabling efficient and drift-free\ninjection of style knowledge. Second, we propose a data augmentation strategy\nbased on human feedback, incorporating high-quality samples generated early in\nthe fine-tuning process into the training set to facilitate progressive\nlearning and significantly reduce its overfitting. Finally, we design a\ntraining-free triple diffusion process using the fine-tuned model, which\nmanipulates the features of self-attention layers in a manner similar to the\ncross-attention mechanism. Specifically, in the generation process, the key and\nvalue of the content-related process are replaced with those of the\nstyle-related process to inject style while maintaining text control over the\nmodel. We also introduce query preservation to mitigate disruptions to the\noriginal content. Under such a design, we have achieved high-quality\nimage-driven style transfer and text-driven stylization, delivering\nartist-level style transfer results while preserving the original image\ncontent. Moreover, we achieve image color editing during the style transfer\nprocess for the first time.\n","authors":["Gary Song Yan","Yusen Zhang","Jinyu Zhao","Hao Zhang","Zhangping Yang","Guanye Xiong","Yanfei Liu","Tao Zhang","Yujie He","Siyuan Tian","Yao Gou","Min Li"],"pdf_url":"https://arxiv.org/pdf/2506.15033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15029v1","updated":"2025-06-18T00:11:06Z","published":"2025-06-18T00:11:06Z","title":"An accurate and revised version of optical character recognition-based\n  speech synthesis using LabVIEW","summary":"  Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).\n","authors":["Prateek Mehta","Anasuya Patil"],"pdf_url":"https://arxiv.org/pdf/2506.15029v1.pdf","comment":"9 pages, 9 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2505.16065v2","updated":"2025-06-18T17:04:04Z","published":"2025-05-21T22:33:40Z","title":"Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated\n  Synthetic Data Augmentation","summary":"  Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.\n","authors":["Ruijie Xi","He Ba","Hao Yuan","Rishu Agrawal","Yuxin Tian","Ruoyan Long","Arul Prakash"],"pdf_url":"https://arxiv.org/pdf/2505.16065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15576v1","updated":"2025-06-18T15:53:47Z","published":"2025-06-18T15:53:47Z","title":"DiscRec: Disentangled Semantic-Collaborative Modeling for Generative\n  Recommendation","summary":"  Generative recommendation is emerging as a powerful paradigm that directly\ngenerates item predictions, moving beyond traditional matching-based\napproaches. However, current methods face two key challenges: token-item\nmisalignment, where uniform token-level modeling ignores item-level granularity\nthat is critical for collaborative signal learning, and semantic-collaborative\nsignal entanglement, where collaborative and semantic signals exhibit distinct\ndistributions yet are fused in a unified embedding space, leading to\nconflicting optimization objectives that limit the recommendation performance.\n  To address these issues, we propose DiscRec, a novel framework that enables\nDisentangled Semantic-Collaborative signal modeling with flexible fusion for\ngenerative Recommendation.First, DiscRec introduces item-level position\nembeddings, assigned based on indices within each semantic ID, enabling\nexplicit modeling of item structure in input token sequences.Second, DiscRec\nemploys a dual-branch module to disentangle the two signals at the embedding\nlayer: a semantic branch encodes semantic signals using original token\nembeddings, while a collaborative branch applies localized attention restricted\nto tokens within the same item to effectively capture collaborative signals. A\ngating mechanism subsequently fuses both branches while preserving the model's\nability to model sequential dependencies. Extensive experiments on four\nreal-world datasets demonstrate that DiscRec effectively decouples these\nsignals and consistently outperforms state-of-the-art baselines. Our codes are\navailable on https://github.com/Ten-Mao/DiscRec.\n","authors":["Chang Liu","Yimeng Bai","Xiaoyan Zhao","Yang Zhang","Fuli Feng","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2506.15576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.14651v3","updated":"2025-06-18T14:49:00Z","published":"2022-05-29T13:14:53Z","title":"Contributions to Representation Learning with Graph Autoencoders and\n  Applications to Music Recommendation","summary":"  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as\ntwo powerful groups of unsupervised node embedding methods, with various\napplications to graph-based machine learning problems such as link prediction\nand community detection. Nonetheless, at the beginning of this Ph.D. project,\nGAE and VGAE models were also suffering from key limitations, preventing them\nfrom being adopted in the industry. In this thesis, we present several\ncontributions to improve these models, with the general aim of facilitating\ntheir use to address industrial-level problems involving graph representations.\nFirstly, we propose two strategies to overcome the scalability issues of\nprevious GAE and VGAE models, permitting to effectively train these models on\nlarge graphs with millions of nodes and edges. These strategies leverage graph\ndegeneracy and stochastic subgraph decoding techniques, respectively. Besides,\nwe introduce Gravity-Inspired GAE and VGAE, providing the first extensions of\nthese models for directed graphs, that are ubiquitous in industrial\napplications. We also consider extensions of GAE and VGAE models for dynamic\ngraphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily\ncomplex, and we propose to simplify them by leveraging linear encoders. Lastly,\nwe introduce Modularity-Aware GAE and VGAE to improve community detection on\ngraphs, while jointly preserving good performances on link prediction. In the\nlast part of this thesis, we evaluate our methods on several graphs extracted\nfrom the music streaming service Deezer. We put the emphasis on graph-based\nmusic recommendation problems. In particular, we show that our methods can\nimprove the detection of communities of similar musical items to recommend to\nusers, that they can effectively rank similar artists in a cold start setting,\nand that they permit modeling the music genre perception across cultures.\n","authors":["Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2205.14651v3.pdf","comment":"Ph.D. thesis defended at \\'Ecole Polytechnique (IPP) in March 2022.\n  As mentioned in this thesis, several chapters present results also published\n  in scientific articles written with co-authors"},{"id":"http://arxiv.org/abs/2409.20302v3","updated":"2025-06-18T13:36:39Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"  Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose yet another approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurement for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in the OV tested originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some false mappings detected\nby OV systems are not actually untrue.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2409.20302v3.pdf","comment":"15 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2506.15284v1","updated":"2025-06-18T09:05:32Z","published":"2025-06-18T09:05:32Z","title":"Multi-Interest Recommendation: A Survey","summary":"  Existing recommendation methods often struggle to model users' multifaceted\npreferences due to the diversity and volatility of user behavior, as well as\nthe inherent uncertainty and ambiguity of item attributes in practical\nscenarios. Multi-interest recommendation addresses this challenge by extracting\nmultiple interest representations from users' historical interactions, enabling\nfine-grained preference modeling and more accurate recommendations. It has\ndrawn broad interest in recommendation research. However, current\nrecommendation surveys have either specialized in frontier recommendation\nmethods or delved into specific tasks and downstream applications. In this\nwork, we systematically review the progress, solutions, challenges, and future\ndirections of multi-interest recommendation by answering the following three\nquestions: (1) Why is multi-interest modeling significantly important for\nrecommendation? (2) What aspects are focused on by multi-interest modeling in\nrecommendation? and (3) How can multi-interest modeling be applied, along with\nthe technical details of the representative modules? We hope that this survey\nestablishes a fundamental framework and delivers a preliminary overview for\nresearchers interested in this field and committed to further exploration. The\nimplementation of multi-interest recommendation summarized in this survey is\nmaintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.\n","authors":["Zihao Li","Qiang Chen","Lixin Zou","Aixin Sun","Chenliang Li"],"pdf_url":"https://arxiv.org/pdf/2506.15284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15267v1","updated":"2025-06-18T08:42:01Z","published":"2025-06-18T08:42:01Z","title":"Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative\n  Next-User Modeling","summary":"  The item cold-start problem is critical for online recommendation systems, as\nthe success of this phase determines whether high-quality new items can\ntransition to popular ones, receive essential feedback to inspire creators, and\nthus lead to the long-term retention of creators. However, modern\nrecommendation systems still struggle to address item cold-start challenges due\nto the heavy reliance on item and historical interactions, which are\nnon-trivial for cold-start items lacking sufficient exposure and feedback.\nLookalike algorithms provide a promising solution by extending feedback for new\nitems based on lookalike users. Traditional lookalike algorithms face such\nlimitations: (1) failing to effectively model the lookalike users and further\nimprove recommendations with the existing rule- or model-based methods; and (2)\nstruggling to utilize the interaction signals and incorporate diverse features\nin modern recommendation systems.\n  Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel\nframework for enhancing cold-start recommendations via generative next-user\nmodeling. Specifically, we employ a transformer-based model to capture the\nunidirectional relationships among recently interacted users and utilize these\nsequences to generate the next potential user who is most likely to interact\nwith the item. The additional item features are also integrated as prefix\nprompt embeddings to assist the next-user generation. The effectiveness of\nNext-User Retrieval is evaluated through both offline experiments and online\nA/B tests. Our method achieves significant improvements with increases of\n0.0142% in daily active users and +0.1144% in publications in Douyin,\nshowcasing its practical applicability and scalability.\n","authors":["Yu-Ting Lan","Yang Huo","Yi Shen","Xiao Yang","Zuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2506.15267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01375v2","updated":"2025-06-18T06:33:31Z","published":"2025-06-02T07:04:16Z","title":"Generative Next POI Recommendation with Semantic ID","summary":"  Point-of-interest (POI) recommendation systems aim to predict the next\ndestinations of user based on their preferences and historical check-ins.\nExisting generative POI recommendation methods usually employ random numeric\nIDs for POIs, limiting the ability to model semantic relationships between\nsimilar locations. In this paper, we propose Generative Next POI Recommendation\nwith Semantic ID (GNPR-SID), an LLM-based POI recommendation model with a novel\nsemantic POI ID (SID) representation method that enhances the semantic\nunderstanding of POI modeling. There are two key components in our GNPR-SID:\n(1) a Semantic ID Construction module that generates semantically rich POI IDs\nbased on semantic and collaborative features, and (2) a Generative POI\nRecommendation module that fine-tunes LLMs to predict the next POI using these\nsemantic IDs. By incorporating user interaction patterns and POI semantic\nfeatures into the semantic ID generation, our method improves the\nrecommendation accuracy and generalization of the model. To construct\nsemantically related SIDs, we propose a POI quantization method based on\nresidual quantized variational autoencoder, which maps POIs into a discrete\nsemantic space. We also propose a diversity loss to ensure that SIDs are\nuniformly distributed across the semantic space. Extensive experiments on three\nbenchmark datasets demonstrate that GNPR-SID substantially outperforms\nstate-of-the-art methods, achieving up to 16% improvement in recommendation\naccuracy.\n","authors":["Dongsheng Wang","Yuxi Huang","Shen Gao","Yifan Wang","Chengrui Huang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2506.01375v2.pdf","comment":"11 pages, 4 figures, the paper has been accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2506.15120v1","updated":"2025-06-18T03:39:13Z","published":"2025-06-18T03:39:13Z","title":"Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rényi Divergence-Based Solution","summary":"  Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.\n","authors":["Shengjia Zhang","Jiawei Chen","Changdong Li","Sheng Zhou","Qihao Shi","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15120v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2407.06060v3","updated":"2025-06-18T02:04:36Z","published":"2024-07-08T16:01:04Z","title":"MERGE -- A Bimodal Audio-Lyrics Dataset for Static Music Emotion\n  Recognition","summary":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a lack of public,\nsizable and quality-controlled bimodal databases has hampered the development\nand improvement of bimodal audio-lyrics systems. This article proposes three\nnew audio, lyrics, and bimodal MER research datasets, collectively referred to\nas MERGE, which were created using a semi-automatic approach. To\ncomprehensively assess the proposed datasets and establish a baseline for\nbenchmarking, we conducted several experiments for each modality, using feature\nengineering, machine learning, and deep learning methodologies. Additionally,\nwe propose and validate fixed train-validation-test splits. The obtained\nresults confirm the viability of the proposed datasets, achieving the best\noverall result of 81.74\\% F1-score for bimodal classification.\n","authors":["Pedro Lima Louro","Hugo Redinho","Ricardo Santos","Ricardo Malheiro","Renato Panda","Rui Pedro Paiva"],"pdf_url":"https://arxiv.org/pdf/2407.06060v3.pdf","comment":"18 pages, 2 figures, 8 tables, submitted to IEEE Transactions on\n  Affective Computing"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2506.15684v1","updated":"2025-06-18T17:59:59Z","published":"2025-06-18T17:59:59Z","title":"Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards","summary":"  Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.\n","authors":["Qingming Liu","Zhen Liu","Dinghuai Zhang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2506.15684v1.pdf","comment":"Technical Report (21 pages, 21 figures)"},{"id":"http://arxiv.org/abs/2506.15680v1","updated":"2025-06-18T17:59:38Z","published":"2025-06-18T17:59:38Z","title":"Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos","summary":"  Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .\n","authors":["Kaifeng Zhang","Baoyu Li","Kris Hauser","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2506.15680v1.pdf","comment":"Project page: https://kywind.github.io/pgnd"},{"id":"http://arxiv.org/abs/2506.15679v1","updated":"2025-06-18T17:59:35Z","published":"2025-06-18T17:59:35Z","title":"Dense SAE Latents Are Features, Not Bugs","summary":"  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n","authors":["Xiaoqing Sun","Alessandro Stolfo","Joshua Engels","Ben Wu","Senthooran Rajamanoharan","Mrinmaya Sachan","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2506.15679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15665v1","updated":"2025-06-18T17:42:45Z","published":"2025-06-18T17:42:45Z","title":"A Data-Integrated Framework for Learning Fractional-Order Nonlinear\n  Dynamical Systems","summary":"  This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.\n","authors":["Bahram Yaghooti","Chengyu Li","Bruno Sinopoli"],"pdf_url":"https://arxiv.org/pdf/2506.15665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15660v1","updated":"2025-06-18T17:39:03Z","published":"2025-06-18T17:39:03Z","title":"On the Upper Bounds for the Matrix Spectral Norm","summary":"  We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.\n","authors":["Alexey Naumov","Maxim Rakhuba","Denis Ryapolov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2506.15660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15654v1","updated":"2025-06-18T17:31:26Z","published":"2025-06-18T17:31:26Z","title":"CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy\n  Optimization","summary":"  Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.\n","authors":["Ranting Hu"],"pdf_url":"https://arxiv.org/pdf/2506.15654v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2506.15651v1","updated":"2025-06-18T17:29:19Z","published":"2025-06-18T17:29:19Z","title":"AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning","summary":"  Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.\n","authors":["Tevin Wang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.15651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15649v1","updated":"2025-06-18T17:23:36Z","published":"2025-06-18T17:23:36Z","title":"Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning","summary":"  Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.\n","authors":["Ankan Deria","Adinath Madhavrao Dukre","Feilong Tang","Sara Atito","Sudipta Roy","Muhammad Awais","Muhammad Haris Khan","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2506.15649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15648v1","updated":"2025-06-18T17:18:23Z","published":"2025-06-18T17:18:23Z","title":"deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses","summary":"  Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.\n","authors":["Georgios Androutsopoulos","Antonio Bianchi"],"pdf_url":"https://arxiv.org/pdf/2506.15648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15643v1","updated":"2025-06-18T17:13:53Z","published":"2025-06-18T17:13:53Z","title":"Revisiting Randomization in Greedy Model Search","summary":"  Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.\n","authors":["Xin Chen","Jason M. Klusowski","Yan Shuo Tan","Chang Yu"],"pdf_url":"https://arxiv.org/pdf/2506.15643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14036v2","updated":"2025-06-18T17:08:53Z","published":"2025-06-16T22:20:44Z","title":"Robust Physics-Informed Neural Network Approach for Estimating\n  Heterogeneous Elastic Properties from Noisy Displacement Data","summary":"  Accurately estimating spatially heterogeneous elasticity parameters,\nparticularly Young's modulus and Poisson's ratio, from noisy displacement\nmeasurements remains significantly challenging in inverse elasticity problems.\nExisting inverse estimation techniques are often limited by instability,\npronounced sensitivity to measurement noise, and difficulty in recovering\nabsolute-scale Young's modulus. This work presents a novel Inverse Elasticity\nPhysics-Informed Neural Network (IE-PINN) specifically designed to robustly\nreconstruct heterogeneous distributions of elasticity parameters from noisy\ndisplacement data based on linear elasticity physics. IE-PINN integrates three\ndistinct neural network architectures dedicated to separately modeling\ndisplacement fields, strain fields, and elasticity distributions, thereby\nsignificantly enhancing stability and accuracy against measurement noise.\nAdditionally, a two-phase estimation strategy is introduced: the first phase\nrecovers relative spatial distributions of Young's modulus and Poisson's ratio,\nand the second phase calibrates the absolute scale of Young's modulus using\nimposed loading boundary conditions. Additional methodological innovations,\nincluding positional encoding, sine activation functions, and a sequential\npretraining protocol, further enhance the model's performance and robustness.\nExtensive numerical experiments demonstrate that IE-PINN effectively overcomes\ncritical limitations encountered by existing methods, delivering accurate\nabsolute-scale elasticity estimations even under severe noise conditions. This\nadvancement holds substantial potential for clinical imaging diagnostics and\nmechanical characterization, where measurements typically encounter substantial\nnoise.\n","authors":["Tatthapong Srikitrungruang","Matthew Lemon","Sina Aghaee Dabaghan Fard","Jaesung Lee","Yuxiao Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.14036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21794v2","updated":"2025-06-18T17:03:35Z","published":"2024-07-31T17:59:58Z","title":"Generalized Out-of-Distribution Detection and Beyond in Vision Language\n  Model Era: A Survey","summary":"  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of these fields in\nthe VLM era. Our framework reveals that, with some field inactivity and\nintegration, the demanding challenges have become OOD detection and AD. Then,\nwe highlight the significant shift in the definition, problem settings, and\nbenchmarks; we thus feature a comprehensive review of the methodology for OOD\ndetection and related tasks to clarify their relationship to OOD detection.\nFinally, we explore the advancements in the emerging Large Vision Language\nModel (LVLM) era, such as GPT-4V. We conclude with open challenges and future\ndirections. The resource is available at\nhttps://github.com/AtsuMiyai/Awesome-OOD-VLM.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Yueqian Lin","Qing Yu","Go Irie","Shafiq Joty","Yixuan Li","Hai Li","Ziwei Liu","Toshihiko Yamasaki","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2407.21794v2.pdf","comment":"Accepted at TMLR2025. Survey paper. We welcome questions, issues, and\n  paper requests via https://github.com/AtsuMiyai/Awesome-OOD-VLM"},{"id":"http://arxiv.org/abs/2506.15626v1","updated":"2025-06-18T16:56:44Z","published":"2025-06-18T16:56:44Z","title":"Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction","summary":"  $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.\n","authors":["Vincent Roca","Marc Tommasi","Paul Andrey","Aurélien Bellet","Markus D. Schirmer","Hilde Henon","Laurent Puy","Julien Ramon","Grégory Kuchcinski","Martin Bretzner","Renaud Lopes"],"pdf_url":"https://arxiv.org/pdf/2506.15626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15620v1","updated":"2025-06-18T16:51:26Z","published":"2025-06-18T16:51:26Z","title":"GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification","summary":"  Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.\n","authors":["Modar Sulaiman","Kallol Roy"],"pdf_url":"https://arxiv.org/pdf/2506.15620v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.15617v1","updated":"2025-06-18T16:50:34Z","published":"2025-06-18T16:50:34Z","title":"The Compositional Architecture of Regret in Large Language Models","summary":"  Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.\n","authors":["Xiangxiang Cui","Shu Yang","Tianjin Huang","Wanyu Lin","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15617v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.04840v2","updated":"2025-06-18T16:50:32Z","published":"2025-02-07T11:18:04Z","title":"Coherent Local Explanations for Mathematical Optimization","summary":"  The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.\n","authors":["Daan Otto","Jannis Kurtz","S. Ilker Birbil"],"pdf_url":"https://arxiv.org/pdf/2502.04840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09033v2","updated":"2025-06-18T16:49:26Z","published":"2025-06-10T17:56:45Z","title":"Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning","summary":"  The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To facilitate learning, we employ a lightweight\nrule-based reward comprising format rewards, final outcome rewards, and a novel\ncost reward for optimizing the balance between performance and cost, opening a\npathway toward enhancing performance-cost trade-offs via RL. Router-R1 also\nconditions only on simple model descriptors such as pricing, latency, and\nexample performance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.\n","authors":["Haozhen Zhang","Tao Feng","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.09033v2.pdf","comment":"Code is available at https://github.com/ulab-uiuc/Router-R1. Models\n  and Datasets are available at\n  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03"},{"id":"http://arxiv.org/abs/2409.07448v4","updated":"2025-06-18T16:35:21Z","published":"2024-09-11T17:52:37Z","title":"A Novel Perturb-ability Score to Mitigate Evasion Adversarial Attacks on\n  Flow-Based ML-NIDS","summary":"  As network security threats evolve, safeguarding flow-based Machine Learning\n(ML)-based Network Intrusion Detection Systems (NIDS) from evasion adversarial\nattacks is crucial. This paper introduces the notion of feature perturb-ability\nand presents a novel Perturb-ability Score (PS), which quantifies how\nsusceptible NIDS features are to manipulation in the problem-space by an\nattacker. PS thereby identifies features structurally resistant to evasion\nattacks in flow-based ML-NIDS due to the semantics of network traffic fields,\nas these features are constrained by domain-specific limitations and\ncorrelations. Consequently, attempts to manipulate such features would likely\neither compromise the attack's malicious functionality, render the traffic\ninvalid for processing, or potentially both outcomes simultaneously.\n  We introduce and demonstrate the effectiveness of our PS-enabled defenses,\nPS-guided feature selection and PS-guided feature masking, in enhancing\nflow-based NIDS resilience. Experimental results across various ML-based NIDS\nmodels and public datasets show that discarding or masking highly manipulatable\nfeatures (high-PS features) can maintain solid detection performance while\nsignificantly reducing vulnerability to evasion adversarial attacks. Our\nfindings confirm that PS effectively identifies flow-based NIDS features\nsusceptible to problem-space perturbations. This novel approach leverages\nproblem-space NIDS domain constraints as lightweight universal defense\nmechanisms against evasion adversarial attacks targeting flow-based ML-NIDS.\n","authors":["Mohamed elShehaby","Ashraf Matrawy"],"pdf_url":"https://arxiv.org/pdf/2409.07448v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15606v1","updated":"2025-06-18T16:30:02Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabrel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17244v4","updated":"2025-06-18T16:28:29Z","published":"2025-02-24T15:21:02Z","title":"A dataset of high-resolution plantar pressures for gait analysis across\n  varying footwear and walking speeds","summary":"  Gait refers to the patterns of limb movement generated during walking, which\nare unique to each individual due to both physical and behavioral traits.\nWalking patterns have been widely studied in biometrics, biomechanics, sports,\nand rehabilitation. While traditional methods rely on video and motion capture,\nadvances in plantar pressure sensing technology now offer deeper insights into\ngait. However, underfoot pressures during walking remain underexplored due to\nthe lack of large, publicly accessible datasets. To address this, we introduce\nthe UNB StepUP-P150 dataset: a footStep database for gait analysis and\nrecognition using Underfoot Pressure, including data from 150 individuals. This\ndataset comprises high-resolution plantar pressure data (4 sensors per\ncm-squared) collected using a 1.2m by 3.6m pressure-sensing walkway. It\ncontains over 200,000 footsteps from participants walking with various speeds\n(preferred, slow-to-stop, fast, and slow) and footwear conditions (barefoot,\nstandard shoes, and two personal shoes), supporting advancements in biometric\ngait recognition and presenting new research opportunities in biomechanics and\ndeep learning. UNB StepUP-P150 establishes a new benchmark for plantar\npressure-based gait analysis and recognition.\n","authors":["Robyn Larracy","Angkoon Phinyomark","Ala Salehi","Eve MacDonald","Saeed Kazemi","Shikder Shafiul Bashar","Aaron Tabor","Erik Scheme"],"pdf_url":"https://arxiv.org/pdf/2502.17244v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01481v3","updated":"2025-06-18T16:21:42Z","published":"2025-05-02T15:58:38Z","title":"VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding","summary":"  Synthetic video generation has gained significant attention for its realism\nand broad applications, but remains prone to violations of common sense and\nphysical laws. This highlights the need for reliable abnormality detectors that\nunderstand such principles and are robust to hallucinations. To address this,\nwe introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from\nsynthetic videos generated by models like Veo2, Sora, and Kling, paired with\nexpert-crafted counterintuitive QA to evaluate the critical thinking abilities\nof Multi-modal Large Language Models (MLLMs) on abnormalities that are\nperceptually obvious to humans but often hallucinated due to language priors.\nVideoHallu evaluates MLLMs' abnormality detection abilities with examples\nacross alignment, consistency, commonsense, and physics. We benchmark SOTA\nMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and\nVideoChat-R1. We observe that these models perform well on many real-world\nbenchmarks like MVBench and MovieChat, but still struggle with basic\nphysics-based and commonsense reasoning in synthetic videos. We further show\nthat post-training with Group Relative Policy Optimization (GRPO), using\ncurriculum learning on datasets combining video QA with counterintuitive\ncommonsense and physics reasoning over real and synthetic videos, improves\nMLLMs' abnormality detection and critical thinking, demonstrating the value of\ntargeted training for improving their understanding of commonsense and physical\nlaws. Our code is available at https://github.com/zli12321/VideoHallu.git.\n","authors":["Zongxia Li","Xiyang Wu","Guangyao Shi","Yubin Qin","Hongyang Du","Tianyi Zhou","Dinesh Manocha","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2505.01481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12529v2","updated":"2025-06-18T16:11:36Z","published":"2025-02-18T04:32:52Z","title":"Alternating Regret for Online Convex Optimization","summary":"  Motivated by alternating learning dynamics in two-player games, a recent work\nby Cevher et al.(2024) shows that $o(\\sqrt{T})$ alternating regret is possible\nfor any $T$-round adversarial Online Linear Optimization (OLO) problem, and\nleft as an open question whether the same is true for general Online Convex\nOptimization (OCO). We answer this question in the affirmative by showing that\nthe continuous Hedge algorithm achieves\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}T^{\\frac{1}{3}})$ alternating regret for\nany adversarial $d$-dimensional OCO problems. We show that this implies an\nalternating learning dynamic that finds a Nash equilibrium for any\nconvex-concave zero-sum games or a coarse correlated equilibrium for any convex\ntwo-player general-sum games at a rate of\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}/T^{\\frac{2}{3}})$. To further improve the\ntime complexity and/or the dimension dependence, we propose another simple\nalgorithm, Follow-the-Regularized-Leader with a regularizer whose convex\nconjugate is 3rd-order smooth, for OCO with smooth and self-concordant loss\nfunctions (such as linear or quadratic losses). We instantiate our algorithm\nwith different regularizers and show that, for example, when the decision set\nis the $\\ell_2$ ball, our algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{\\frac{2}{5}})$ alternating regret with no dimension\ndependence (and a better $\\tilde{\\mathcal{O}}(T^{\\frac{1}{3}})$ bound for\nquadratic losses). We complement our results by showing some algorithm-specific\nalternating regret lower bounds, including a somewhat surprising\n$\\Omega(\\sqrt{T})$ lower bound for a Regret Matching variant that is widely\nused in alternating learning dynamics.\n","authors":["Soumita Hait","Ping Li","Haipeng Luo","Mengxiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15594v1","updated":"2025-06-18T16:09:18Z","published":"2025-06-18T16:09:18Z","title":"WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts","summary":"  Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.\n","authors":["Negar Foroutan","Angelika Romanou","Matin Ansaripour","Julian Martin Eisenschlos","Karl Aberer","Rémi Lebret"],"pdf_url":"https://arxiv.org/pdf/2506.15594v1.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2506.15588v1","updated":"2025-06-18T16:05:09Z","published":"2025-06-18T16:05:09Z","title":"Memory-Efficient Differentially Private Training with Gradient Random\n  Projection","summary":"  Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.\n","authors":["Alex Mulrooney","Devansh Gupta","James Flemings","Huanyu Zhang","Murali Annavaram","Meisam Razaviyayn","Xinwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13678v2","updated":"2025-06-18T16:04:08Z","published":"2025-06-16T16:32:51Z","title":"A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction","summary":"  Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.\n","authors":["Yi Wang","Zhenghong Wang","Fan Zhang","Chengling Tang","Chaogui Kang","Di Zhu","Zhongfu Ma","Sijie Ruan","Weiyu Zhang","Yu Zheng","Philip S. Yu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13678v2.pdf","comment":"18 pages, 13 figures, under review"},{"id":"http://arxiv.org/abs/2506.01324v2","updated":"2025-06-18T15:49:32Z","published":"2025-06-02T05:10:40Z","title":"Near-Optimal Clustering in Mixture of Markov Chains","summary":"  We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.\n","authors":["Junghyun Lee","Yassir Jedra","Alexandre Proutière","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2506.01324v2.pdf","comment":"36 pages. Minor corrections in v2"},{"id":"http://arxiv.org/abs/2506.15571v1","updated":"2025-06-18T15:48:30Z","published":"2025-06-18T15:48:30Z","title":"MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh\n  Smoothing","summary":"  Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.\n","authors":["Le Vu Anh","Nguyen Viet Anh","Mehmet Dik","Tu Nguyen Thi Ngoc"],"pdf_url":"https://arxiv.org/pdf/2506.15571v1.pdf","comment":"9 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.15567v1","updated":"2025-06-18T15:43:10Z","published":"2025-06-18T15:43:10Z","title":"Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents","summary":"  Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.\n","authors":["Aline Dobrovsky","Konstantin Schekotihin","Christian Burmer"],"pdf_url":"https://arxiv.org/pdf/2506.15567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15566v1","updated":"2025-06-18T15:43:08Z","published":"2025-06-18T15:43:08Z","title":"Task-Agnostic Experts Composition for Continual Learning","summary":"  Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.\n","authors":["Luigi Quarantiello","Andrea Cossu","Vincenzo Lomonaco"],"pdf_url":"https://arxiv.org/pdf/2506.15566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03074v3","updated":"2025-06-18T15:42:12Z","published":"2025-06-03T16:52:24Z","title":"GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for\n  Generalized Low-Rank Trace Regression","summary":"  We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).\n","authors":["Junghyun Lee","Kyoungseok Jang","Kwang-Sung Jun","Milan Vojnović","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2506.03074v3.pdf","comment":"53 pages, 2 figures, 3 tables; Accepted as a Spotlight Poster to the\n  42nd International Conference on Machine Learning (ICML 2025). Minor\n  correction to the arXiv title in v2 ;). Added ToC in v3"},{"id":"http://arxiv.org/abs/2505.12992v3","updated":"2025-06-18T15:41:14Z","published":"2025-05-19T11:30:41Z","title":"Fractured Chain-of-Thought Reasoning","summary":"  Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning. Code is available at\nhttps://github.com/BaohaoLiao/frac-cot.\n","authors":["Baohao Liao","Hanze Dong","Yuhui Xu","Doyen Sahoo","Christof Monz","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.12992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15559v1","updated":"2025-06-18T15:34:41Z","published":"2025-06-18T15:34:41Z","title":"Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates","summary":"  Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.\n","authors":["Danish Gufran","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2506.15559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13023v4","updated":"2025-06-18T15:33:09Z","published":"2024-06-18T19:30:46Z","title":"$k$-Submodular Interdiction Problems under Distributional\n  Risk-Receptiveness and Robustness: Application to Machine Learning","summary":"  We study submodular optimization in adversarial context, applicable to\nmachine learning problems such as feature selection using data susceptible to\nuncertainties and attacks. We focus on Stackelberg games between an attacker\n(or interdictor) and a defender where the attacker aims to minimize the\ndefender's objective of maximizing a $k$-submodular function. We allow\nuncertainties arising from the success of attacks and inherent data noise, and\naddress challenges due to incomplete knowledge of the probability distribution\nof random parameters. Specifically, we introduce Distributionally Robust\n$k$-Submodular Interdiction Problem (DRO $k$-SIP) and Distributionally\nRisk-Receptive $k$-Submodular Interdiction Problem (DRR $k$-SIP) along with\nfinitely convergent exact algorithms for solving them. When solving the DRO\n$k$-SIP, the attacker optimizes their expected payoff with respect to the\nworst-case probability distribution within the ambiguity set, and thereby have\nrobust attack strategies despite distributional ambiguity. In contrast, the DRR\n$k$-SIP identifies attacker strategies with the best-case probability\ndistribution, and identifies critical vulnerabilities for the defender. The\noptimal values derived from both DRO $k$-SIP and DRR $k$-SIP offer a confidence\ninterval-like range for the expected value of the defender's objective\nfunction, capturing distributional ambiguity. We conduct computational\nexperiments on instances of feature selection and sensor placement problems,\nusing Wisconsin breast cancer data and synthetic data, respectively.\n","authors":["Seonghun Park","Manish Bansal"],"pdf_url":"https://arxiv.org/pdf/2406.13023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15554v1","updated":"2025-06-18T15:27:40Z","published":"2025-06-18T15:27:40Z","title":"DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones","summary":"  Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.\n","authors":["Akhil Singampalli","Danish Gufran","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2506.15554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15544v1","updated":"2025-06-18T15:17:21Z","published":"2025-06-18T15:17:21Z","title":"Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning","summary":"  Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.\n","authors":["Roger Creus Castanyer","Johan Obando-Ceron","Lu Li","Pierre-Luc Bacon","Glen Berseth","Aaron Courville","Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2506.15544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15543v1","updated":"2025-06-18T15:17:03Z","published":"2025-06-18T15:17:03Z","title":"Learning Algorithms in the Limit","summary":"  This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.\n","authors":["Hristo Papazov","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2506.15543v1.pdf","comment":"Accepted at COLT 2025. This version matches the proceedings version"},{"id":"http://arxiv.org/abs/2506.15538v1","updated":"2025-06-18T15:13:07Z","published":"2025-06-18T15:13:07Z","title":"Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework","summary":"  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).\n","authors":["Laura Kopf","Nils Feldhus","Kirill Bykov","Philine Lou Bommer","Anna Hedström","Marina M. -C. Höhne","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2506.15538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15535v1","updated":"2025-06-18T15:10:38Z","published":"2025-06-18T15:10:38Z","title":"A Simplified Analysis of SGD for Linear Regression with Weight Averaging","summary":"  Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.\n","authors":["Alexandru Meterez","Depen Morwani","Costin-Andrei Oncescu","Jingfeng Wu","Cengiz Pehlevan","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2506.15535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13045v2","updated":"2025-06-18T15:06:34Z","published":"2025-06-16T02:27:25Z","title":"A Comprehensive Survey on Continual Learning in Generative Models","summary":"  The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.\n","authors":["Haiyang Guo","Fanhu Zeng","Fei Zhu","Jiayi Wang","Xukai Wang","Jingang Zhou","Hongbo Zhao","Wenzhuo Liu","Shijie Ma","Da-Han Wang","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13045v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2506.15530v1","updated":"2025-06-18T15:01:25Z","published":"2025-06-18T15:01:25Z","title":"Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music\n  Diffusion Models","summary":"  Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.\n","authors":["Teysir Baoueb","Xiaoyu Bie","Xi Wang","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2506.15530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11759v5","updated":"2025-06-18T14:58:26Z","published":"2024-10-15T16:28:55Z","title":"LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery","summary":"  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing\nsample-efficient ANM methods often rely on restrictive assumptions on the data\ngenerating process, limiting their applicability to real-world settings. We\npropose local search in additive noise models, LoSAM, a topological ordering\nmethod for learning a unique DAG in ANMs with mixed causal mechanisms and\ngeneral noise distributions. We introduce new causal substructures and criteria\nfor identifying roots and leaves, enabling efficient top-down learning. We\nprove asymptotic consistency and polynomial runtime, ensuring scalability and\nsample efficiency. We test LoSAM on synthetic and real-world data,\ndemonstrating state-of-the-art performance across all mixed mechanism settings.\n","authors":["Sujai Hiremath","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2410.11759v5.pdf","comment":"To appear at the Forty-First Annual Conference on Uncertainty in\n  Artificial Intelligence (UAI 2025)"},{"id":"http://arxiv.org/abs/2407.15621v3","updated":"2025-06-18T14:52:47Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Online Retrieval-augmented Generation for Radiology Question\n  Answering","summary":"  Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval-augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from Radiopaedia in real-time. Accuracy\nwas investigated. Statistical analyses were performed using bootstrapping. The\nresults were further compared with human performance. RadioRAG improved\ndiagnostic accuracy across most LLMs, with relative accuracy increases ranging\nup to 54% for different LLMs. It matched or exceeded non-RAG models and the\nhuman radiologist in question answering across radiologic subspecialties,\nparticularly in breast imaging and emergency radiology. However, the degree of\nimprovement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1\nsaw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,\nhighlighting variability in RadioRAG's effectiveness. LLMs benefit when\nprovided access to domain-specific data beyond their training data. RadioRAG\nshows potential to improve LLM accuracy and factuality in radiology question\nanswering by integrating real-time domain-specific data.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Lisa Adams","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v3.pdf","comment":"Published in Radiology: Artificial Intelligence"},{"id":"http://arxiv.org/abs/2205.14651v3","updated":"2025-06-18T14:49:00Z","published":"2022-05-29T13:14:53Z","title":"Contributions to Representation Learning with Graph Autoencoders and\n  Applications to Music Recommendation","summary":"  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as\ntwo powerful groups of unsupervised node embedding methods, with various\napplications to graph-based machine learning problems such as link prediction\nand community detection. Nonetheless, at the beginning of this Ph.D. project,\nGAE and VGAE models were also suffering from key limitations, preventing them\nfrom being adopted in the industry. In this thesis, we present several\ncontributions to improve these models, with the general aim of facilitating\ntheir use to address industrial-level problems involving graph representations.\nFirstly, we propose two strategies to overcome the scalability issues of\nprevious GAE and VGAE models, permitting to effectively train these models on\nlarge graphs with millions of nodes and edges. These strategies leverage graph\ndegeneracy and stochastic subgraph decoding techniques, respectively. Besides,\nwe introduce Gravity-Inspired GAE and VGAE, providing the first extensions of\nthese models for directed graphs, that are ubiquitous in industrial\napplications. We also consider extensions of GAE and VGAE models for dynamic\ngraphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily\ncomplex, and we propose to simplify them by leveraging linear encoders. Lastly,\nwe introduce Modularity-Aware GAE and VGAE to improve community detection on\ngraphs, while jointly preserving good performances on link prediction. In the\nlast part of this thesis, we evaluate our methods on several graphs extracted\nfrom the music streaming service Deezer. We put the emphasis on graph-based\nmusic recommendation problems. In particular, we show that our methods can\nimprove the detection of communities of similar musical items to recommend to\nusers, that they can effectively rank similar artists in a cold start setting,\nand that they permit modeling the music genre perception across cultures.\n","authors":["Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2205.14651v3.pdf","comment":"Ph.D. thesis defended at \\'Ecole Polytechnique (IPP) in March 2022.\n  As mentioned in this thesis, several chapters present results also published\n  in scientific articles written with co-authors"},{"id":"http://arxiv.org/abs/2506.15513v1","updated":"2025-06-18T14:48:19Z","published":"2025-06-18T14:48:19Z","title":"RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation","summary":"  Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.\n","authors":["Le Vu Anh","Nguyen Viet Anh","Mehmet Dik","Luong Van Nghia"],"pdf_url":"https://arxiv.org/pdf/2506.15513v1.pdf","comment":"11 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.05929v6","updated":"2025-06-18T14:45:27Z","published":"2024-09-09T10:40:50Z","title":"M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the\n  Joint-Embedding Predictive Architecture","summary":"  Current multimodal learning strategies primarily optimize in the original\ntoken space. Such a framework is easy to incorporate with the backbone of\npretrained language model, but might result in modality collapse. To alleviate\nsuch issues, we leverage the Joint-Embedding Predictive Architecture (JEPA) on\nthe multimodal tasks, which converts the input embedding into the output\nembedding space by a predictor and then conducts the cross-modal alignment on\nthe latent space. We implement this predictor by a Multi-Gate Mixture of\nExperts (MMoE) and name the framework as M3-JEPA, accordingly. The gating\nfunction disentangles the modality-specific and shared information and derives\ninformation-theoretic optimality. The framework is implemented with both\ncontrastive and regularization loss, and solved by alternative gradient descent\n(AGD) between different multimodal tasks. By thoroughly designed experiments,\nwe show that M3-JEPA can obtain state-of-the-art performance on different\nmodalities and tasks, generalize to unseen datasets and domains, and is\ncomputationally efficient in both training and inference. Our observation\nsuggests that M3-JEPA might become a new basis to self-supervised learning in\nthe open world.\n","authors":["Hongyang Lei","Xiaolong Cheng","Qi Qin","Dan Wang","Kun Fan","Huazhen Huang","Qingqing Gu","Yetao Wu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.05929v6.pdf","comment":"16 pages, 5 figures. ICML 2025"},{"id":"http://arxiv.org/abs/2506.15507v1","updated":"2025-06-18T14:45:06Z","published":"2025-06-18T14:45:06Z","title":"Over-squashing in Spatiotemporal Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.\n","authors":["Ivan Marisca","Jacob Bamberger","Cesare Alippi","Michael M. Bronstein"],"pdf_url":"https://arxiv.org/pdf/2506.15507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13912v2","updated":"2025-06-18T14:44:41Z","published":"2025-03-18T05:16:36Z","title":"KANITE: Kolmogorov-Arnold Networks for ITE estimation","summary":"  We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)\nfor Individual Treatment Effect (ITE) estimation under multiple treatments\nsetting in causal inference. By utilizing KAN's unique abilities to learn\nunivariate activation functions as opposed to learning linear weights by\nMulti-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE\nframework comprises two key architectures: 1.Integral Probability Metric (IPM)\narchitecture: This employs an IPM loss in a specialized manner to effectively\nalign towards ITE estimation across multiple treatments. 2. Entropy Balancing\n(EB) architecture: This uses weights for samples that are learned by optimizing\nentropy subject to balancing the covariates across treatment groups. Extensive\nevaluations on benchmark datasets demonstrate that KANITE outperforms\nstate-of-the-art algorithms in both $\\epsilon_{\\text{PEHE}}$ and\n$\\epsilon_{\\text{ATE}}$ metrics. Our experiments highlight the advantages of\nKANITE in achieving improved causal estimates, emphasizing the potential of\nKANs to advance causal inference methodologies across diverse application\nareas.\n","authors":["Eshan Mehendale","Abhinav Thorat","Ravi Kolla","Niranjan Pedanekar"],"pdf_url":"https://arxiv.org/pdf/2503.13912v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.15506v1","updated":"2025-06-18T14:43:26Z","published":"2025-06-18T14:43:26Z","title":"Insights on Adversarial Attacks for Tabular Machine Learning via a\n  Systematic Literature Review","summary":"  Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.\n","authors":["Salijona Dyrmishi","Mohamed Djilani","Thibault Simonetto","Salah Ghamizi","Maxime Cordy"],"pdf_url":"https://arxiv.org/pdf/2506.15506v1.pdf","comment":"This paper is currently under review at ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2506.15505v1","updated":"2025-06-18T14:43:04Z","published":"2025-06-18T14:43:04Z","title":"Time-dependent density estimation using binary classifiers","summary":"  We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.\n","authors":["Agnimitra Dasgupta","Javier Murgoitio-Esandi","Ali Fardisi","Assad A Oberai"],"pdf_url":"https://arxiv.org/pdf/2506.15505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15504v1","updated":"2025-06-18T14:42:34Z","published":"2025-06-18T14:42:34Z","title":"Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge","summary":"  Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.\n","authors":["Li Zheng","Sihang Wang","Hao Fei","Zuquan Peng","Fei Li","Jianming Fu","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2506.15504v1.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2410.17161v3","updated":"2025-06-18T14:42:07Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v3.pdf","comment":"ICML 2025 Poster Paper, Camera Ready Version"},{"id":"http://arxiv.org/abs/2506.15499v1","updated":"2025-06-18T14:41:24Z","published":"2025-06-18T14:41:24Z","title":"Pixel-level Certified Explanations via Randomized Smoothing","summary":"  Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.\n","authors":["Alaa Anani","Tobias Lorenz","Mario Fritz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2506.15499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03727v2","updated":"2025-06-18T14:38:47Z","published":"2025-01-07T12:16:26Z","title":"Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives","summary":"  Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Given that language impairments manifest\nearly in NCD progression, visual-stimulated narrative (VSN)-based analysis\noffers a promising avenue for NCD detection. Current VSN-based NCD detection\nmethods primarily focus on linguistic microstructures (e.g., pauses, lexical\ndiversity), which are potentially linked to bottom-up (stimulus-driven)\ncognitive processing. While these features illuminate basic language abilities,\nthe higher-order linguistic macrostructures (e.g., thematic or logical\ndevelopment), which may reflect top-down (concept-driven) cognitive abilities,\nremain underexplored. These patterns are crucial for NCD detection yet\nchallenging to quantify due to their abstract and complex nature. To bridge\nthis gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic\nTopic Model (DTM) to track topic evolution over time, and (2) Text-Image\nTemporal Alignment Network (TITAN) to measure cross-modal consistency between\nspeech and visual stimuli. Experimental results validated the efficiency of\nproposed approaches in NCD detection, with TITAN achieving superior performance\nboth on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 =\n0.8889). The feature contribution analysis revealed that macrostructural\nfeatures (e.g., topic variability, topic change rate, and topic consistency)\nconstituted the most significant contributors in the model's decision pathways,\noutperforming investigated microstructural features. These findings underscore\nthe critical role of macrostructural patterns in understanding cognitive\nimpairment mechanisms in NCDs.\n","authors":["Jinchao Li","Yuejiao Wang","Junan Li","Jiawen Kang","Bo Zheng","Simon Wong","Brian Mak","Helene Fung","Jean Woo","Man-Wai Mak","Timothy Kwok","Vincent Mok","Xianmin Gong","Xixin Wu","Xunying Liu","Patrick Wong","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2501.03727v2.pdf","comment":"13 pages, 7 figures, submitted to JSTSP"},{"id":"http://arxiv.org/abs/2506.15498v1","updated":"2025-06-18T14:37:59Z","published":"2025-06-18T14:37:59Z","title":"SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling","summary":"  Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.\n","authors":["Md Imbesat Hassan Rizvi","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2506.15498v1.pdf","comment":"8 pages main content, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.15492v1","updated":"2025-06-18T14:30:04Z","published":"2025-06-18T14:30:04Z","title":"LIT-LVM: Structured Regularization for Interaction Terms in Linear\n  Predictors using Latent Variable Models","summary":"  Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.\n","authors":["Mohammadreza Nemati","Zhipeng Huang","Kevin S. Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13061v2","updated":"2025-06-18T14:18:09Z","published":"2025-06-16T03:09:25Z","title":"Fast Convergence for High-Order ODE Solvers in Diffusion Probabilistic\n  Models","summary":"  Diffusion probabilistic models generate samples by learning to reverse a\nnoise-injection process that transforms data into noise. Reformulating this\nreverse process as a deterministic probability flow ordinary differential\nequation (ODE) enables efficient sampling using high-order solvers, often\nrequiring only $\\mathcal{O}(10)$ steps. Since the score function is typically\napproximated by a neural network, analyzing the interaction between its\nregularity, approximation error, and numerical integration error is key to\nunderstanding the overall sampling accuracy. In this work, we continue our\nanalysis of the convergence properties of the deterministic sampling methods\nderived from probability flow ODEs [25], focusing on $p$-th order (exponential)\nRunge-Kutta schemes for any integer $p \\geq 1$. Under the assumption that the\nfirst and second derivatives of the approximate score function are bounded, we\ndevelop $p$-th order (exponential) Runge-Kutta schemes and demonstrate that the\ntotal variation distance between the target distribution and the generated data\ndistribution can be bounded above by \\begin{align*}\n  O\\bigl(d^{\\frac{7}{4}}\\varepsilon_{\\text{score}}^{\\frac{1}{2}}\n+d(dH_{\\max})^p\\bigr), \\end{align*} where $\\varepsilon^2_{\\text{score}}$\ndenotes the $L^2$ error in the score function approximation, $d$ is the data\ndimension and $H_{\\max}$ represents the maximum step size used in the solver.\nWe numerically verify the regularity assumption on benchmark datasets,\nconfirming that the first and second derivatives of the approximate score\nfunction remain bounded in practice. Our theoretical guarantees hold for\ngeneral forward processes with arbitrary variance schedules.\n","authors":["Daniel Zhengyu Huang","Jiaoyang Huang","Zhengjiang Lin"],"pdf_url":"https://arxiv.org/pdf/2506.13061v2.pdf","comment":"64 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.01953v2","updated":"2025-06-18T14:15:09Z","published":"2025-02-04T03:02:24Z","title":"Local minima of the empirical risk in high dimension: General theorems\n  and convex examples","summary":"  We consider a general model for high-dimensional empirical risk minimization\nwhereby the data $\\mathbf{x}_i$ are $d$-dimensional isotropic Gaussian vectors,\nthe model is parametrized by $\\mathbf{\\Theta}\\in\\mathbb{R}^{d\\times k}$, and\nthe loss depends on the data via the projection\n$\\mathbf{\\Theta}^\\mathsf{T}\\mathbf{x}_i$. This setting covers as special cases\nclassical statistics methods (e.g. multinomial regression and other generalized\nlinear models), but also two-layer fully connected neural networks with $k$\nhidden neurons. We use the Kac-Rice formula from Gaussian process theory to\nderive a bound on the expected number of local minima of this empirical risk,\nunder the proportional asymptotics in which $n,d\\to\\infty$, with $n\\asymp d$.\nVia Markov's inequality, this bound allows to determine the positions of these\nminimizers (with exponential deviation bounds) and hence derive sharp\nasymptotics on the estimation and prediction error. In this paper, we apply our\ncharacterization to convex losses, where high-dimensional asymptotics were not\n(in general) rigorously established for $k\\ge 2$. We show that our approach is\ntight and allows to prove previously conjectured results. In addition, we\ncharacterize the spectrum of the Hessian at the minimizer. A companion paper\napplies our general result to non-convex examples.\n","authors":["Kiana Asgari","Andrea Montanari","Basil Saeed"],"pdf_url":"https://arxiv.org/pdf/2502.01953v2.pdf","comment":"101 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.11511v2","updated":"2025-06-18T14:10:39Z","published":"2025-04-15T10:45:55Z","title":"Position Paper: Rethinking Privacy in RL for Sequential Decision-making\n  in the Age of LLMs","summary":"  The rise of reinforcement learning (RL) in critical real-world applications\ndemands a fundamental rethinking of privacy in AI systems. Traditional privacy\nframeworks, designed to protect isolated data points, fall short for sequential\ndecision-making systems where sensitive information emerges from temporal\npatterns, behavioral strategies, and collaborative dynamics. Modern RL\nparadigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in\nlarge language models (LLMs), exacerbate these challenges by introducing\ncomplex, interactive, and context-dependent learning environments that\ntraditional methods do not address. In this position paper, we argue for a new\nprivacy paradigm built on four core principles: multi-scale protection,\nbehavioral pattern protection, collaborative privacy preservation, and\ncontext-aware adaptation. These principles expose inherent tensions between\nprivacy, utility, and interpretability that must be navigated as RL systems\nbecome more pervasive in high-stakes domains like healthcare, autonomous\nvehicles, and decision support systems powered by LLMs. To tackle these\nchallenges, we call for the development of new theoretical frameworks,\npractical mechanisms, and rigorous evaluation methodologies that collectively\nenable effective privacy protection in sequential decision-making systems.\n","authors":["Flint Xiaofeng Fan","Cheston Tan","Roger Wattenhofer","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2504.11511v2.pdf","comment":"IJCNN 2025 Position Paper Track"},{"id":"http://arxiv.org/abs/2506.15479v1","updated":"2025-06-18T14:10:38Z","published":"2025-06-18T14:10:38Z","title":"Creating User-steerable Projections with Interactive Semantic Mapping","summary":"  Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.\n","authors":["Artur André Oliveira","Mateus Espadoto","Roberto Hirata Jr.","Roberto M. Cesar Jr.","Alex C. Telea"],"pdf_url":"https://arxiv.org/pdf/2506.15479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10244v2","updated":"2025-06-18T14:06:53Z","published":"2025-06-11T23:57:26Z","title":"A new type of federated clustering: A non-model-sharing approach","summary":"  In recent years, the growing need to leverage sensitive data across\ninstitutions has led to increased attention on federated learning (FL), a\ndecentralized machine learning paradigm that enables model training without\nsharing raw data. However, existing FL-based clustering methods, known as\nfederated clustering, typically assume simple data partitioning scenarios such\nas horizontal or vertical splits, and cannot handle more complex distributed\nstructures. This study proposes data collaboration clustering (DC-Clustering),\na novel federated clustering method that supports clustering over complex data\npartitioning scenarios where horizontal and vertical splits coexist. In\nDC-Clustering, each institution shares only intermediate representations\ninstead of raw data, ensuring privacy preservation while enabling collaborative\nclustering. The method allows flexible selection between k-means and spectral\nclustering, and achieves final results with a single round of communication\nwith the central server. We conducted extensive experiments using synthetic and\nopen benchmark datasets. The results show that our method achieves clustering\nperformance comparable to centralized clustering where all data are pooled.\nDC-Clustering addresses an important gap in current FL research by enabling\neffective knowledge discovery from distributed heterogeneous data. Its\npractical properties -- privacy preservation, communication efficiency, and\nflexibility -- make it a promising tool for privacy-sensitive domains such as\nhealthcare and finance.\n","authors":["Yuji Kawamata","Kaoru Kamijo","Masateru Kihira","Akihiro Toyoda","Tomoru Nakayama","Akira Imakura","Tetsuya Sakurai","Yukihiko Okada"],"pdf_url":"https://arxiv.org/pdf/2506.10244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11982v2","updated":"2025-06-18T14:06:33Z","published":"2025-06-13T17:39:41Z","title":"Interpretable representation learning of quantum data enabled by\n  probabilistic variational autoencoders","summary":"  Interpretable machine learning is rapidly becoming a crucial tool for\nscientific discovery. Among existing approaches, variational autoencoders\n(VAEs) have shown promise in extracting the hidden physical features of some\ninput data, with no supervision nor prior knowledge of the system at study.\nYet, the ability of VAEs to create meaningful, interpretable representations\nrelies on their accurate approximation of the underlying probability\ndistribution of their input. When dealing with quantum data, VAEs must hence\naccount for its intrinsic randomness and complex correlations. While VAEs have\nbeen previously applied to quantum data, they have often neglected its\nprobabilistic nature, hindering the extraction of meaningful physical\ndescriptors. Here, we demonstrate that two key modifications enable VAEs to\nlearn physically meaningful latent representations: a decoder capable of\nfaithfully reproduce quantum states and a probabilistic loss tailored to this\ntask. Using benchmark quantum spin models, we identify regimes where standard\nmethods fail while the representations learned by our approach remain\nmeaningful and interpretable. Applied to experimental data from Rydberg atom\narrays, the model autonomously uncovers the phase structure without access to\nprior labels, Hamiltonian details, or knowledge of relevant order parameters,\nhighlighting its potential as an unsupervised and interpretable tool for the\nstudy of quantum systems.\n","authors":["Paulin de Schoulepnikoff","Gorka Muñoz-Gil","Hendrik Poulsen Nautrup","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2506.11982v2.pdf","comment":"Main text 10 pages, total document 16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.13244v3","updated":"2025-06-18T14:04:08Z","published":"2025-06-16T08:42:31Z","title":"No-Regret Learning Under Adversarial Resource Constraints: A Spending\n  Plan Is All You Need!","summary":"  We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan.\n","authors":["Francesco Emanuele Stradi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti","Christian Kroer"],"pdf_url":"https://arxiv.org/pdf/2506.13244v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15751v4","updated":"2025-06-18T14:02:28Z","published":"2025-01-27T03:35:25Z","title":"Adversarially Robust Bloom Filters: Privacy, Reductions, and Open\n  Problems","summary":"  A Bloom filter is a space-efficient probabilistic data structure that\nrepresents a set $S$ of elements from a larger universe $U$. This efficiency\ncomes with a trade-off, namely, it allows for a small chance of false\npositives. When you query the Bloom filter about an element x, the filter will\nrespond 'Yes' if $x \\in S$. If $x \\notin S$, it may still respond 'Yes' with\nprobability at most $\\varepsilon$. We investigate the adversarial robustness\nand privacy of Bloom filters, addressing open problems across three prominent\nframeworks: the game-based model of Naor-Oved-Yogev (NOY), the simulator-based\nmodel of Filic et. al., and learning-augmented variants. We prove the first\nformal connection between the Filic and NOY models, showing that Filic\ncorrectness implies AB-test resilience. We resolve a longstanding open question\nby proving that PRF-backed Bloom filters fail the NOY model's stronger BP-test.\nFinally, we introduce the first private Bloom filters with differential privacy\nguarantees, including constructions applicable to learned Bloom filters. Our\ntaxonomy organizes the space of robustness and privacy guarantees, clarifying\nrelationships between models and constructions.\n","authors":["Hayder Tirmazi"],"pdf_url":"https://arxiv.org/pdf/2501.15751v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15468v1","updated":"2025-06-18T13:58:45Z","published":"2025-06-18T13:58:45Z","title":"Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI","summary":"  We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.\n","authors":["Ryota Okumura","Tadahiro Taniguchi","Akira Taniguchi","Yoshinobu Hagiwara"],"pdf_url":"https://arxiv.org/pdf/2506.15468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15464v1","updated":"2025-06-18T13:55:30Z","published":"2025-06-18T13:55:30Z","title":"Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic\n  Graphs","summary":"  Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.\n","authors":["Le Vu Anh","Mehmet Dik","Nguyen Viet Anh"],"pdf_url":"https://arxiv.org/pdf/2506.15464v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15461v1","updated":"2025-06-18T13:48:33Z","published":"2025-06-18T13:48:33Z","title":"All is Not Lost: LLM Recovery without Checkpoints","summary":"  Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.\n","authors":["Nikolay Blagoev","Oğuzhan Ersoy","Lydia Yiyu Chen"],"pdf_url":"https://arxiv.org/pdf/2506.15461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09957v2","updated":"2025-06-18T13:42:20Z","published":"2024-09-16T03:05:11Z","title":"Deep Graph Anomaly Detection: A Survey and New Perspectives","summary":"  Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.\n","authors":["Hezhe Qiao","Hanghang Tong","Bo An","Irwin King","Charu Aggarwal","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2409.09957v2.pdf","comment":"Accepted by TKDE"},{"id":"http://arxiv.org/abs/2506.15452v1","updated":"2025-06-18T13:25:48Z","published":"2025-06-18T13:25:48Z","title":"Warping and Matching Subsequences Between Time Series","summary":"  Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.\n","authors":["Simiao Lin","Wannes Meert","Pieter Robberechts","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2506.15452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15448v1","updated":"2025-06-18T13:19:22Z","published":"2025-06-18T13:19:22Z","title":"Semi-supervised Graph Anomaly Detection via Robust Homophily Learning","summary":"  Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.\n","authors":["Guoguo Ai","Hezhe Qiao","Hui Yan","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2506.15448v1.pdf","comment":"18 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.15446v1","updated":"2025-06-18T13:18:36Z","published":"2025-06-18T13:18:36Z","title":"Zero-Shot Reinforcement Learning Under Partial Observability","summary":"  Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.\n","authors":["Scott Jeen","Tom Bewley","Jonathan M. Cullen"],"pdf_url":"https://arxiv.org/pdf/2506.15446v1.pdf","comment":"Reinforcement Learning Conference 2025"},{"id":"http://arxiv.org/abs/2503.01630v2","updated":"2025-06-18T13:02:48Z","published":"2025-03-03T15:05:48Z","title":"Machine Learners Should Acknowledge the Legal Implications of Large\n  Language Models as Personal Data","summary":"  Does GPT know you? The answer depends on your level of public recognition;\nhowever, if your information was available on a website, the answer could be\nyes. Most Large Language Models (LLMs) memorize training data to some extent.\nThus, even when an LLM memorizes only a small amount of personal data, it\ntypically falls within the scope of data protection laws. If a person is\nidentified or identifiable, the implications are far-reaching. The LLM is\nsubject to EU General Data Protection Regulation requirements even after the\ntraining phase is concluded. To back our arguments: (1.) We reiterate that LLMs\noutput training data at inference time, be it verbatim or in generalized form.\n(2.) We show that some LLMs can thus be considered personal data on their own.\nThis triggers a cascade of data protection implications such as data subject\nrights, including rights to access, rectification, or erasure. These rights\nextend to the information embedded within the AI model. (3.) This paper argues\nthat machine learning researchers must acknowledge the legal implications of\nLLMs as personal data throughout the full ML development lifecycle, from data\ncollection and curation to model provision on e.g., GitHub or Hugging Face.\n(4.) We propose different ways for the ML research community to deal with these\nlegal implications. Our paper serves as a starting point for improving the\nalignment between data protection law and the technical capabilities of LLMs.\nOur findings underscore the need for more interaction between the legal domain\nand the ML community.\n","authors":["Henrik Nolte","Michèle Finck","Kristof Meding"],"pdf_url":"https://arxiv.org/pdf/2503.01630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14095v2","updated":"2025-06-18T12:59:54Z","published":"2025-06-17T01:19:28Z","title":"Transformers Learn Faster with Semantic Focus","summary":"  Various forms of sparse attention have been explored to mitigate the\nquadratic computational and memory cost of the attention mechanism in\ntransformers. We study sparse transformers not through a lens of efficiency but\nrather in terms of learnability and generalization. Empirically studying a\nrange of attention mechanisms, we find that input-dependent sparse attention\nmodels appear to converge faster and generalize better than standard attention\nmodels, while input-agnostic sparse attention models show no such benefits -- a\nphenomenon that is robust across architectural and optimization hyperparameter\nchoices. This can be interpreted as demonstrating that concentrating a model's\n\"semantic focus\" with respect to the tokens currently being considered (in the\nform of input-dependent sparse attention) accelerates learning. We develop a\ntheoretical characterization of the conditions that explain this behavior. We\nestablish a connection between the stability of the standard softmax and the\nloss function's Lipschitz properties, then show how sparsity affects the\nstability of the softmax and the subsequent convergence and generalization\nguarantees resulting from the attention mechanism. This allows us to\ntheoretically establish that input-agnostic sparse attention does not provide\nany benefits. We also characterize conditions when semantic focus\n(input-dependent sparse attention) can provide improved guarantees, and we\nvalidate that these conditions are in fact met in our empirical evaluations.\n","authors":["Parikshit Ram","Kenneth L. Clarkson","Tim Klinger","Shashanka Ubaru","Alexander G. Gray"],"pdf_url":"https://arxiv.org/pdf/2506.14095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15421v1","updated":"2025-06-18T12:46:39Z","published":"2025-06-18T12:46:39Z","title":"Reward Models in Deep Reinforcement Learning: A Survey","summary":"  In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.\n","authors":["Rui Yu","Shenghua Wan","Yucen Wang","Chen-Xiao Gao","Le Gan","Zongzhang Zhang","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2506.15421v1.pdf","comment":"IJCAI 2025 Survey Track (To Appear)"},{"id":"http://arxiv.org/abs/2409.17287v2","updated":"2025-06-18T12:37:28Z","published":"2024-09-20T17:30:19Z","title":"Blockchain-Enabled Variational Information Bottleneck for Data\n  Extraction Based on Mutual Information in Internet of Vehicles","summary":"  The Internet of Vehicles (IoV) network can address the issue of limited\ncomputing resources and data processing capabilities of individual vehicles,\nbut it also brings the risk of privacy leakage to vehicle users. Applying\nblockchain technology can establish secure data links within the IoV, solving\nthe problems of insufficient computing resources for each vehicle and the\nsecurity of data transmission over the network. However, with the development\nof the IoV, the amount of data interaction between multiple vehicles and\nbetween vehicles and base stations, roadside units, etc., is continuously\nincreasing. There is a need to further reduce the interaction volume, and\nintelligent data compression is key to solving this problem. The VIB technique\nfacilitates the training of encoding and decoding models, substantially\ndiminishing the volume of data that needs to be transmitted. This paper\nintroduces an innovative approach that integrates blockchain with VIB, referred\nto as BVIB, designed to lighten computational workloads and reinforce the\nsecurity of the network. We first construct a new network framework by\nseparating the encoding and decoding networks to address the computational\nburden issue, and then propose a new algorithm to enhance the security of IoV\nnetworks. We also discuss the impact of the data extraction rate on system\nlatency to determine the most suitable data extraction rate. An experimental\nframework combining Python and C++ has been established to substantiate the\nefficacy of our BVIB approach. Comprehensive simulation studies indicate that\nthe BVIB consistently excels in comparison to alternative foundational\nmethodologies.\n","authors":["Cui Zhang","Wenjun Zhang","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2409.17287v2.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/BVIB-for-Data-Extraction-Based-on\n  Mutual-Information-in-the-IoV"},{"id":"http://arxiv.org/abs/2411.18043v2","updated":"2025-06-18T12:35:32Z","published":"2024-11-27T04:25:13Z","title":"Heterogeneous Relationships of Subjects and Shapelets for\n  Semi-supervised Multivariate Series Classification","summary":"  Multivariate time series (MTS) classification is widely applied in fields\nsuch as industry, healthcare, and finance, aiming to extract key features from\ncomplex time series data for accurate decision-making and prediction. However,\nexisting methods for MTS often struggle due to the challenges of effectively\nmodeling high-dimensional data and the lack of labeled data, resulting in poor\nclassification performance. To address this issue, we propose a heterogeneous\nrelationships of subjects and shapelets method for semi-supervised MTS\nclassification. This method offers a novel perspective by integrating various\ntypes of additional information while capturing the relationships between them.\nSpecifically, we first utilize a contrast temporal self-attention module to\nobtain sparse MTS representations, and then model the similarities between\nthese representations using soft dynamic time warping to construct a similarity\ngraph. Secondly, we learn the shapelets for different subject types,\nincorporating both the subject features and their shapelets as additional\ninformation to further refine the similarity graph, ultimately generating a\nheterogeneous graph. Finally, we use a dual level graph attention network to\nget prediction. Through this method, we successfully transform dataset into a\nheterogeneous graph, integrating multiple additional information and achieving\nprecise semi-supervised node classification. Experiments on the Human Activity\nRecognition, sleep stage classification and University of East Anglia datasets\ndemonstrate that our method outperforms current state-of-the-art methods in MTS\nclassification tasks, validating its superiority.\n","authors":["Mingsen Du","Meng Chen","Yongjian Li","Cun Ji","Shoushui Wei"],"pdf_url":"https://arxiv.org/pdf/2411.18043v2.pdf","comment":"We would like to request the withdrawal of our manuscript due to\n  logical errors in the paper"},{"id":"http://arxiv.org/abs/2411.12222v2","updated":"2025-06-18T12:34:55Z","published":"2024-11-19T04:32:41Z","title":"Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time\n  Series Node Classification","summary":"  Multivariate time series (MTS) data is generated through multiple sensors\nacross various domains such as engineering application, health monitoring, and\nthe internet of things, characterized by its temporal changes and high\ndimensional characteristics. Over the past few years, many studies have\nexplored the long-range dependencies and similarities in MTS. However,\nlong-range dependencies are difficult to model due to their temporal changes\nand high dimensionality makes it difficult to obtain similarities effectively\nand efficiently. Thus, to address these issues, we propose contrast\nsimilarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).\nFirstly, to obtain the dynamic similarity of each sample, we initially use\ntemporal contrast learning module to acquire MTS representations. And then we\nconstruct a similarity matrix between MTS representations using Fast Dynamic\nTime Warping (FastDTW). Secondly, we apply the DPMamba to consider the\nbidirectional nature of MTS, allowing us to better capture long-range and\nshort-range dependencies within the data. Finally, we utilize the\nKolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the\ninformation interaction in the matrix and MTS node classification task. By\ncomprehensively considering the long-range dependencies and dynamic similarity\nfeatures, we achieved precise MTS node classification. We conducted experiments\non multiple University of East Anglia (UEA) MTS datasets, which encompass\ndiverse application scenarios. Our results demonstrate the superiority of our\nmethod through both supervised and semi-supervised experiments on the MTS\nclassification task.\n","authors":["Mingsen Du","Meng Chen","Yongjian Li","Xiuxin Zhang","Jiahui Gao","Cun Ji","Shoushui Wei"],"pdf_url":"https://arxiv.org/pdf/2411.12222v2.pdf","comment":"We would like to request the withdrawal of our manuscript due to\n  logical errors in the paper"},{"id":"http://arxiv.org/abs/2410.16500v2","updated":"2025-06-18T12:34:17Z","published":"2024-10-21T20:45:13Z","title":"Implementation and Assessment of Machine Learning Models for Forecasting\n  Suspected Opioid Overdoses in Emergency Medical Services Data","summary":"  We present efforts in the fields of machine learning and time series\nforecasting to accurately predict counts of future suspected opioid overdoses\nrecorded by Emergency Medical Services (EMS) in the state of Kentucky.\nForecasts help government agencies properly prepare and distribute resources\nrelated to opioid overdoses. Our approach uses county and district level\naggregations of suspected opioid overdose encounters and forecasts future\ncounts for different time intervals. Models with different levels of complexity\nwere evaluated to minimize forecasting error. A variety of additional\ncovariates relevant to opioid overdoses and public health were tested to\ndetermine their impact on model performance. Our evaluation shows that useful\npredictions can be generated with limited error for different types of regions,\nand high performance can be achieved using commonly available covariates and\nrelatively simple forecasting models.\n","authors":["Aaron D. Mullen","Daniel R. Harris","Peter Rock","Katherine Thompson","Svetla Slavova","Jeffery Talbert","V. K. Cody Bumgardner"],"pdf_url":"https://arxiv.org/pdf/2410.16500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08462v2","updated":"2025-06-18T12:32:53Z","published":"2024-07-11T12:58:47Z","title":"Distributed Deep Reinforcement Learning Based Gradient Quantization for\n  Federated Learning Enabled Vehicle Edge Computing","summary":"  Federated Learning (FL) can protect the privacy of the vehicles in vehicle\nedge computing (VEC) to a certain extent through sharing the gradients of\nvehicles' local models instead of local data. The gradients of vehicles' local\nmodels are usually large for the vehicular artificial intelligence (AI)\napplications, thus transmitting such large gradients would cause large\nper-round latency. Gradient quantization has been proposed as one effective\napproach to reduce the per-round latency in FL enabled VEC through compressing\ngradients and reducing the number of bits, i.e., the quantization level, to\ntransmit gradients. The selection of quantization level and thresholds\ndetermines the quantization error, which further affects the model accuracy and\ntraining time. To do so, the total training time and quantization error (QE)\nbecome two key metrics for the FL enabled VEC. It is critical to jointly\noptimize the total training time and QE for the FL enabled VEC. However, the\ntime-varying channel condition causes more challenges to solve this problem. In\nthis paper, we propose a distributed deep reinforcement learning (DRL)-based\nquantization level allocation scheme to optimize the long-term reward in terms\nof the total training time and QE. Extensive simulations identify the optimal\nweighted factors between the total training time and QE, and demonstrate the\nfeasibility and effectiveness of the proposed scheme.\n","authors":["Cui Zhang","Wenjun Zhang","Qiong Wu","Pingyi Fan","Qiang Fan","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.08462v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at:\n  https://github.com/qiongwu86/Distributed-Deep-Reinforcement-Learning-Based-Gradient\n  Quantization-for-Federated-Learning-Enabled-Vehicle-Edge-Computing"},{"id":"http://arxiv.org/abs/2503.15576v2","updated":"2025-06-18T12:27:58Z","published":"2025-03-19T13:19:06Z","title":"A Bird Song Detector for improving bird identification through Deep\n  Learning: a case study from Doñana","summary":"  Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.\n","authors":["Alba Márquez-Rodríguez","Miguel Ángel Mohedano-Munoz","Manuel J. Marín-Jiménez","Eduardo Santamaría-García","Giulia Bastianelli","Pedro Jordano","Irene Mendoza"],"pdf_url":"https://arxiv.org/pdf/2503.15576v2.pdf","comment":"23 pages, 14 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector"},{"id":"http://arxiv.org/abs/2506.15408v1","updated":"2025-06-18T12:25:37Z","published":"2025-06-18T12:25:37Z","title":"Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI","summary":"  Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.\n","authors":["David Dembinsky","Adriano Lucieri","Stanislav Frolov","Hiba Najjar","Ko Watanabe","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2506.15408v1.pdf","comment":"Submitted to TMLR, under review"},{"id":"http://arxiv.org/abs/2502.03029v3","updated":"2025-06-18T12:24:26Z","published":"2025-02-05T09:31:27Z","title":"On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation","summary":"  The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention.\n","authors":["Nghiem T. Diep","Huy Nguyen","Chau Nguyen","Minh Le","Duy M. H. Nguyen","Daniel Sonntag","Mathias Niepert","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2502.03029v3.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15404v1","updated":"2025-06-18T12:22:17Z","published":"2025-06-18T12:22:17Z","title":"NERO: Explainable Out-of-Distribution Detection with Neuron-level\n  Relevance","summary":"  Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.\n","authors":["Anju Chhetri","Jari Korhonen","Prashnna Gyawali","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2506.15404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14670v3","updated":"2025-06-18T12:18:24Z","published":"2023-11-24T18:59:04Z","title":"Differentiable and accelerated spherical harmonic and Wigner transforms","summary":"  Many areas of science and engineering encounter data defined on spherical\nmanifolds. Modelling and analysis of spherical data often necessitates\nspherical harmonic transforms, at high degrees, and increasingly requires\nefficient computation of gradients for machine learning or other differentiable\nprogramming tasks. We develop novel algorithmic structures for accelerated and\ndifferentiable computation of generalised Fourier transforms on the sphere\n$\\mathbb{S}^2$ and rotation group $\\text{SO}(3)$, i.e. spherical harmonic and\nWigner transforms, respectively. We present a recursive algorithm for the\ncalculation of Wigner $d$-functions that is both stable to high harmonic\ndegrees and extremely parallelisable. By tightly coupling this with separable\nspherical transforms, we obtain algorithms that exhibit an extremely\nparallelisable structure that is well-suited for the high throughput computing\nof modern hardware accelerators (e.g. GPUs). We also develop a hybrid automatic\nand manual differentiation approach so that gradients can be computed\nefficiently. Our algorithms are implemented within the JAX differentiable\nprogramming framework in the S2FFT software code. Numerous samplings of the\nsphere are supported, including equiangular and HEALPix sampling. Computational\nerrors are at the order of machine precision for spherical samplings that admit\na sampling theorem. When benchmarked against alternative C codes we observe up\nto a 400-fold acceleration. Furthermore, when distributing over multiple GPUs\nwe achieve very close to optimal linear scaling with increasing number of GPUs\ndue to the highly parallelised and balanced nature of our algorithms. Provided\naccess to sufficiently many GPUs our transforms thus exhibit an unprecedented\neffective linear time complexity.\n","authors":["Matthew A. Price","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2311.14670v3.pdf","comment":"31 pages, 7 figures, accepted by Journal of Computational Physics,\n  code available at https://github.com/astro-informatics/s2fft"},{"id":"http://arxiv.org/abs/2407.13123v2","updated":"2025-06-18T12:15:24Z","published":"2024-07-18T03:18:59Z","title":"Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint\n  Phase-shift Optimization and Multi-User Power Allocation","summary":"  Vehicular edge computing (VEC) is an emerging technology with significant\npotential in the field of internet of vehicles (IoV), enabling vehicles to\nperform intensive computational tasks locally or offload them to nearby edge\ndevices. However, the quality of communication links may be severely\ndeteriorated due to obstacles such as buildings, impeding the offloading\nprocess. To address this challenge, we introduce the use of Reconfigurable\nIntelligent Surfaces (RIS), which provide alternative communication pathways to\nassist vehicular communication. By dynamically adjusting the phase-shift of the\nRIS, the performance of VEC systems can be substantially improved. In this\nwork, we consider a RIS-assisted VEC system, and design an optimal scheme for\nlocal execution power, offloading power, and RIS phase-shift, where random task\narrivals and channel variations are taken into account. To address the scheme,\nwe propose an innovative deep reinforcement learning (DRL) framework that\ncombines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing\nRIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm for optimizing the power allocation of vehicle user\n(VU). Simulation results show that our proposed scheme outperforms the\ntraditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient\n(TD3) and some typical stochastic schemes.\n","authors":["Kangwei Qi","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.13123v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at\n  https://github.com/qiongwu86/DDPG-RIS-MADDPG-POWER. arXiv admin note: text\n  overlap with arXiv:2406.11318"},{"id":"http://arxiv.org/abs/2506.15397v1","updated":"2025-06-18T12:15:13Z","published":"2025-06-18T12:15:13Z","title":"Learn to Vaccinate: Combining Structure Learning and Effective\n  Vaccination for Epidemic and Outbreak Control","summary":"  The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.\n","authors":["Sepehr Elahi","Paula Mürmann","Patrick Thiran"],"pdf_url":"https://arxiv.org/pdf/2506.15397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11310v2","updated":"2025-06-18T12:07:56Z","published":"2024-07-16T01:51:32Z","title":"Digital Twin Vehicular Edge Computing Network: Task Offloading and\n  Resource Allocation","summary":"  With the increasing demand for multiple applications on internet of vehicles.\nIt requires vehicles to carry out multiple computing tasks in real time.\nHowever, due to the insufficient computing capability of vehicles themselves,\noffloading tasks to vehicular edge computing (VEC) servers and allocating\ncomputing resources to tasks becomes a challenge. In this paper, a multi task\ndigital twin (DT) VEC network is established. By using DT to develop offloading\nstrategies and resource allocation strategies for multiple tasks of each\nvehicle in a single slot, an optimization problem is constructed. To solve it,\nwe propose a multi-agent reinforcement learning method on the task offloading\nand resource allocation. Numerous experiments demonstrate that our method is\neffective compared to other benchmark algorithms.\n","authors":["Yu Xie","Qiong Wu","Pingyi Fan"],"pdf_url":"https://arxiv.org/pdf/2407.11310v2.pdf","comment":"This paper has been accepted by ICICSP 2024. The source code has been\n  released\n  at:https://github.com/qiongwu86/Digital-Twin-Vehicular-Edge-Computing-Network_Task-Offloading-and-Resource-Allocation"},{"id":"http://arxiv.org/abs/2506.15387v1","updated":"2025-06-18T12:03:34Z","published":"2025-06-18T12:03:34Z","title":"Multi-Timescale Gradient Sliding for Distributed Optimization","summary":"  We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).\n","authors":["Junhui Zhang","Patrick Jaillet"],"pdf_url":"https://arxiv.org/pdf/2506.15387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03767v2","updated":"2025-06-18T12:02:14Z","published":"2025-03-03T11:21:48Z","title":"A Survey on Semantic Communications in Internet of Vehicles","summary":"  Internet of Vehicles (IoV), as the core of intelligent transportation system,\nenables comprehensive interconnection between vehicles and their surroundings\nthrough multiple communication modes, which is significant for autonomous\ndriving and intelligent traffic management. However, with the emergence of new\napplications, traditional communication technologies face the problems of\nscarce spectrum resources and high latency. Semantic communication, which\nfocuses on extracting, transmitting, and recovering some useful semantic\ninformation from messages, can reduce redundant data transmission, improve\nspectrum utilization, and provide innovative solutions to communication\nchallenges in the IoV. This paper systematically reviews state of art of\nsemantic communications in the IoV, elaborates the technical background of IoV\nand semantic communications, and deeply discusses key technologies of semantic\ncommunications in IoV, including semantic information extraction, semantic\ncommunication architecture, resource allocation and management, and so on.\nThrough specific case studies, it demonstrates that semantic communications can\nbe effectively employed in the scenarios of traffic environment perception and\nunderstanding, intelligent driving decision support, IoV service optimization,\nand intelligent traffic management. Additionally, it analyzes the current\nchallenges and future research directions. This survey reveals that semantic\ncommunications has broad application prospects in IoV, but it is necessary to\nsolve the real existing problems by combining advanced technologies to promote\nits wide application in IoV and contributing to the development of intelligent\ntransportation system.\n","authors":["Sha Ye","Qiong Wu","Pingyi Fan","Qiang Fan"],"pdf_url":"https://arxiv.org/pdf/2503.03767v2.pdf","comment":"This paper has been accepted to Entropy"},{"id":"http://arxiv.org/abs/2506.15385v1","updated":"2025-06-18T11:59:15Z","published":"2025-06-18T11:59:15Z","title":"Provable Maximum Entropy Manifold Exploration via Diffusion Models","summary":"  Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.\n","authors":["Riccardo De Santi","Marin Vlastelica","Ya-Ping Hsieh","Zebang Shen","Niao He","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2506.15385v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.15383v1","updated":"2025-06-18T11:53:13Z","published":"2025-06-18T11:53:13Z","title":"Global Ground Metric Learning with Applications to scRNA data","summary":"  Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.\n","authors":["Damin Kühn","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2506.15383v1.pdf","comment":"This method is provided as a Python package on PyPI, see\n  https://github.com/DaminK/ggml-ot"},{"id":"http://arxiv.org/abs/2506.04265v2","updated":"2025-06-18T11:50:18Z","published":"2025-06-03T08:04:43Z","title":"CORA: Coalitional Rational Advantage Decomposition for Multi-Agent\n  Policy Gradients","summary":"  This work focuses on the credit assignment problem in cooperative multi-agent\nreinforcement learning (MARL). Sharing the global advantage among agents often\nleads to suboptimal policy updates as it fails to account for the distinct\ncontributions of agents. Although numerous methods consider global or\nindividual contributions for credit assignment, a detailed analysis at the\ncoalition level remains lacking in many approaches. This work analyzes the\nover-updating problem during multi-agent policy updates from a coalition-level\nperspective. To address this issue, we propose a credit assignment method\ncalled Coalitional Rational Advantage Decomposition (CORA). CORA evaluates\ncoalitional advantages via marginal contributions from all possible coalitions\nand decomposes advantages using the core solution from cooperative game theory,\nensuring coalitional rationality. To reduce computational overhead, CORA\nemploys random coalition sampling. Experiments on matrix games, differential\ngames, and multi-agent collaboration benchmarks demonstrate that CORA\noutperforms strong baselines, particularly in tasks with multiple local optima.\nThese findings highlight the importance of coalition-aware credit assignment\nfor improving MARL performance.\n","authors":["Mengda Ji","Genjiu Xu","Liying Wang"],"pdf_url":"https://arxiv.org/pdf/2506.04265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15378v1","updated":"2025-06-18T11:47:59Z","published":"2025-06-18T11:47:59Z","title":"Sampling 3D Molecular Conformers with Diffusion Transformers","summary":"  Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.\n","authors":["J. Thorben Frank","Winfried Ripken","Gregor Lied","Klaus-Robert Müller","Oliver T. Unke","Stefan Chmiela"],"pdf_url":"https://arxiv.org/pdf/2506.15378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03196v2","updated":"2025-06-18T11:36:11Z","published":"2025-06-01T14:29:25Z","title":"Graph Neural Networks for Jamming Source Localization","summary":"  Graph-based learning provides a powerful framework for modeling complex\nrelational structures; however, its application within the domain of wireless\nsecurity remains significantly underexplored. In this work, we introduce the\nfirst application of graph-based learning for jamming source localization,\naddressing the imminent threat of jamming attacks in wireless networks. Unlike\ngeometric optimization techniques that struggle under environmental\nuncertainties and dense interference, we reformulate the localization as an\ninductive graph regression task. Our approach integrates structured node\nrepresentations that encode local and global signal aggregation, ensuring\nspatial coherence and adaptive signal fusion. To enhance robustness, we\nincorporate an attention-based \\ac{GNN} that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex \\ac{RF} environments with various sampling\ndensities, network topologies, jammer characteristics, and signal propagation\nconditions, conducting comprehensive ablation studies on graph construction,\nfeature selection, and pooling strategies. Results demonstrate that our novel\ngraph-based learning framework significantly outperforms established\nlocalization baselines, particularly in challenging scenarios with sparse and\nobfuscated signal information. Our code is available at\nhttps://github.com/tiiuae/gnn-jamming-source-localization.\n","authors":["Dania Herzalla","Willian T. Lunardi","Martin Andreoni"],"pdf_url":"https://arxiv.org/pdf/2506.03196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15366v1","updated":"2025-06-18T11:34:15Z","published":"2025-06-18T11:34:15Z","title":"Performative Validity of Recourse Explanations","summary":"  When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.\n","authors":["Gunnar König","Hidde Fokkema","Timo Freiesleben","Celestine Mendler-Dünner","Ulrike on Luxburg"],"pdf_url":"https://arxiv.org/pdf/2506.15366v1.pdf","comment":"34 pages, 3 figures, 1 table, Preprint"},{"id":"http://arxiv.org/abs/2503.06253v2","updated":"2025-06-18T11:04:21Z","published":"2025-03-08T15:28:26Z","title":"MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM\n  Red Teaming","summary":"  With LLM usage rapidly increasing, their vulnerability to jailbreaks that\ncreate harmful outputs are a major security risk. As new jailbreaking\nstrategies emerge and models are changed by fine-tuning, continuous testing for\nsecurity vulnerabilities is necessary. Existing Red Teaming methods fall short\nin cost efficiency, attack success rate, attack diversity, or extensibility as\nnew attack types emerge. We address these challenges with Modular And Diverse\nMalicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses\nautomatic assignment of attack strategies into relevant attack clusters,\nchooses the most relevant clusters for a malicious goal, and then combines\nstrategies from the selected clusters to achieve diverse novel attacks with\nhigh attack success rates. MAD-MAX further merges promising attacks together at\neach iteration of Red Teaming to boost performance and introduces a similarity\nfilter to prune out similar attacks for increased cost efficiency. The MAD-MAX\napproach is designed to be easily extensible with newly discovered attack\nstrategies and outperforms the prominent Red Teaming method Tree of Attacks\nwith Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and\nqueries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals\nin our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX\ndoes so with only 10.9 average queries to the target LLM compared to TAP with\n23.3.\n  WARNING: This paper contains contents which are offensive in nature.\n","authors":["Stefan Schoepf","Muhammad Zaid Hameed","Ambrish Rawat","Kieran Fraser","Giulio Zizzo","Giandomenico Cornacchia","Mark Purcell"],"pdf_url":"https://arxiv.org/pdf/2503.06253v2.pdf","comment":"Data in Generative Models Workshop: The Bad, the Ugly, and the Greats\n  (DIG-BUGS) at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15349v1","updated":"2025-06-18T11:03:39Z","published":"2025-06-18T11:03:39Z","title":"Enhancing One-run Privacy Auditing with Quantile Regression-Based\n  Membership Inference","summary":"  Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.\n","authors":["Terrance Liu","Matteo Boglioni","Yiwei Fu","Shengyuan Hu","Pratiksha Thaker","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07179v2","updated":"2025-06-18T11:00:36Z","published":"2023-06-12T15:21:02Z","title":"Benchmarking Neural Network Training Algorithms","summary":"  Training algorithms, broadly construed, are an essential part of every deep\nlearning pipeline. Training algorithm improvements that speed up training\nacross a wide variety of workloads (e.g., better update rules, tuning\nprotocols, learning rate schedules, or data selection schemes) could save time,\nsave computational resources, and lead to better, more accurate, models.\nUnfortunately, as a community, we are currently unable to reliably identify\ntraining algorithm improvements, or even determine the state-of-the-art\ntraining algorithm. In this work, using concrete experiments, we argue that\nreal progress in speeding up training requires new benchmarks that resolve\nthree basic challenges faced by empirical comparisons of training algorithms:\n(1) how to decide when training is complete and precisely measure training\ntime, (2) how to handle the sensitivity of measurements to exact workload\ndetails, and (3) how to fairly compare algorithms that require hyperparameter\ntuning. In order to address these challenges, we introduce a new, competitive,\ntime-to-result benchmark using multiple workloads running on fixed hardware,\nthe AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of\nworkload variants that make it possible to detect benchmark submissions that\nare more robust to workload changes than current widely-used methods. Finally,\nwe evaluate baseline submissions constructed using various optimizers that\nrepresent current practice, as well as other optimizers that have recently\nreceived attention in the literature. These baseline results collectively\ndemonstrate the feasibility of our benchmark, show that non-trivial gaps\nbetween methods exist, and set a provisional state-of-the-art for future\nbenchmark submissions to try and surpass.\n","authors":["George E. Dahl","Frank Schneider","Zachary Nado","Naman Agarwal","Chandramouli Shama Sastry","Philipp Hennig","Sourabh Medapati","Runa Eschenhagen","Priya Kasimbeg","Daniel Suo","Juhan Bae","Justin Gilmer","Abel L. Peirson","Bilal Khan","Rohan Anil","Mike Rabbat","Shankar Krishnan","Daniel Snider","Ehsan Amid","Kongtao Chen","Chris J. Maddison","Rakshith Vasudev","Michal Badura","Ankush Garg","Peter Mattson"],"pdf_url":"https://arxiv.org/pdf/2306.07179v2.pdf","comment":"102 pages, 8 figures, 41 tables"},{"id":"http://arxiv.org/abs/2506.15346v1","updated":"2025-06-18T10:55:26Z","published":"2025-06-18T10:55:26Z","title":"Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges","summary":"  Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.\n","authors":["A. S. Stankevich","I. B. Petrov"],"pdf_url":"https://arxiv.org/pdf/2506.15346v1.pdf","comment":"Submitted to \"Computational Mathematics And Mathematical Physics\",\n  ISSN 1555-6662, issue 8, August 2025"},{"id":"http://arxiv.org/abs/2505.17830v2","updated":"2025-06-18T10:55:09Z","published":"2025-05-23T12:43:55Z","title":"Imagine Beyond! Distributionally Robust Auto-Encoding for State Space\n  Coverage in Online Reinforcement Learning","summary":"  Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.\n","authors":["Nicolas Castanet","Olivier Sigaud","Sylvain Lamprier"],"pdf_url":"https://arxiv.org/pdf/2505.17830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22963v2","updated":"2025-06-18T10:53:35Z","published":"2025-05-29T01:05:02Z","title":"Agile Orchestration at Will: An Entire Smart Service-Based Security\n  Architecture Towards 6G","summary":"  The upcoming 6G will fundamentally reshape mobile networks beyond\ncommunications, unlocking a multitude of applications that were once considered\nunimaginable. Meanwhile, security and resilience are especially highlighted in\nthe 6G design principles. However, safeguarding 6G networks will be quite\nchallenging due to various known and unknown threats from highly heterogeneous\nnetworks and diversified security requirements of distinct use cases, calling\nfor a comprehensive re-design of security architecture. This motivates us to\npropose ES3A (Entire Smart Service-based Security Architecture), a novel\nsecurity architecture for 6G networks. Specifically, we first discuss six\nhigh-level principles of our ES3A that include hierarchy, flexibility,\nscalability, resilience, endogeny, and trust and privacy. With these goals in\nmind, we then introduce three guidelines from a deployment perspective,\nenvisioning our ES3A that offers service-based security, end-to-end protection,\nand smart security automation for 6G networks. Our architecture consists of\nthree layers and three domains. It relies on a two-stage orchestration\nmechanism to tailor smart security strategies for customized protection in\nhigh-dynamic 6G networks, thereby addressing the aforementioned challenges.\nFinally, we prototype the proposed ES3A on a real-world radio system based on\nSoftware-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.\nWe also provide a case to show the superiority of our architecture.\n","authors":["Zhuoran Duan","Guoshun Nan","Rushan Li","Zijun Wang","Lihua Xiong","Chaoying Yuan","Guorong Liu","Hui Xu","Qimei Cui","Xiaofeng Tao","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2505.22963v2.pdf","comment":"Accepted by IEEE Wireless Communications Magazine"},{"id":"http://arxiv.org/abs/2403.05754v7","updated":"2025-06-18T10:42:23Z","published":"2024-03-09T01:34:26Z","title":"Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition","summary":"  In this paper, we propose two hybrid quantum-inspired neural networks with\nadaptive residual and dense connections respectively for pattern recognition.\nWe explain the frameworks of the symmetrical circuit models in the\nquantum-inspired layers in our hybrid models. We also illustrate the potential\nsuperiority of our hybrid models to prevent gradient explosion owing to the\nsine and cosine functions in the quantum-inspired layers. Groups of numerical\nexperiments on generalization power showcase that our hybrid models are\ncomparable to the pure classical models with different noisy datasets utilized.\nFurthermore, the comparison between our hybrid models and a state-of-the-art\nhybrid quantum-classical convolutional network demonstrates 3%-4% higher\naccuracy of our hybrid densely-connected model than the hybrid\nquantum-classical network. Additionally, compared with other two hybrid\nquantum-inspired residual networks, our hybrid models showcase a little higher\naccuracy on image datasets with asymmetrical noises. Simultaneously, in terms\nof groups of robustness experiments, the outcomes demonstrate that our two\nhybrid models outperform pure classical models notably in resistance to\nadversarial parameter attacks with various asymmetrical noises. They also\nindicate the slight superiority of our densely-connected hybrid model over the\nhybrid quantum-classical network to both symmetrical and asymmetrical attacks.\nMeanwhile, the accuracy of our two hybrid models is a little bit higher than\nthat of the two hybrid quantum-inspired residual networks. In addition, an\nablation study indicate that the recognition accuracy of our two hybrid models\nis 2%-3% higher than that of the traditional quantum-inspired neural network\nwithout residual or dense connection. Eventually, we discuss the application\nscenarios of our hybrid models by analyzing their computational complexity.\n","authors":["Andi Chen","Hua-Lei Yin","Zeng-Bing Chen","Shengjun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05754v7.pdf","comment":"21 pages of main paper with two links of a 20-page supplementary\n  material and the program codes below the acknowledgement in the main paper"},{"id":"http://arxiv.org/abs/2405.00074v2","updated":"2025-06-18T10:36:17Z","published":"2024-04-30T07:24:41Z","title":"PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating\n  Pre-trained Neural Networks","summary":"  We present PAODING, a toolkit to debloat pretrained neural network models\nthrough the lens of data-free pruning. To preserve the model fidelity, PAODING\nadopts an iterative process, which dynamically measures the effect of deleting\na neuron to identify candidates that have the least impact to the output layer.\nOur evaluation shows that PAODING can significantly reduce the model size,\ngeneralize on different datasets and models, and meanwhile preserve the model\nfidelity in terms of test accuracy and adversarial robustness. PAODING is\npublicly available on PyPI via https://pypi.org/project/paoding-dl.\n","authors":["Mark Huasong Meng","Hao Guan","Liuhuo Wan","Sin Gee Teo","Guangdong Bai","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2405.00074v2.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2506.06761v2","updated":"2025-06-18T10:34:41Z","published":"2025-06-07T11:05:33Z","title":"The OCR Quest for Generalization: Learning to recognize low-resource\n  alphabets with model editing","summary":"  Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.\n","authors":["Adrià Molina Rodríguez","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2506.06761v2.pdf","comment":"Preprint (under review) For Journal"},{"id":"http://arxiv.org/abs/2506.15337v1","updated":"2025-06-18T10:32:26Z","published":"2025-06-18T10:32:26Z","title":"Knowledge Distillation Framework for Accelerating High-Accuracy Neural\n  Network-Based Molecular Dynamics Simulations","summary":"  Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.\n","authors":["Naoki Matsumura","Yuta Yoshimoto","Yuto Iwasaki","Meguru Yamazaki","Yasufumi Sakai"],"pdf_url":"https://arxiv.org/pdf/2506.15337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.00783v3","updated":"2025-06-18T10:26:20Z","published":"2022-04-02T07:09:17Z","title":"Supervised Robustness-preserving Data-free Neural Network Pruning","summary":"  When deploying pre-trained neural network models in real-world applications,\nmodel consumers often encounter resource-constraint platforms such as mobile\nand smart devices. They typically use the pruning technique to reduce the size\nand complexity of the model, generating a lighter one with less resource\nconsumption. Nonetheless, most existing pruning methods are proposed with the\npremise that the model after being pruned has a chance to be fine-tuned or even\nretrained based on the original training data. This may be unrealistic in\npractice, as the data controllers are often reluctant to provide their model\nconsumers with the original data. In this work, we study the neural network\npruning in the data-free context, aiming to yield lightweight models that are\nnot only accurate in prediction but also robust against undesired inputs in\nopen-world deployments. Considering the absence of the fine-tuning and\nretraining that can fix the mis-pruned units, we replace the traditional\naggressive one-shot strategy with a conservative one that treats the pruning as\na progressive process. We propose a pruning method based on stochastic\noptimization that uses robustness-related metrics to guide the pruning process.\nOur method is implemented as a Python program and evaluated with a series of\nexperiments on diverse neural network models. The experimental results show\nthat it significantly outperforms existing one-shot data-free pruning\napproaches in terms of robustness preservation and accuracy.\n","authors":["Mark Huasong Meng","Guangdong Bai","Sin Gee Teo","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2204.00783v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2501.10885v3","updated":"2025-06-18T10:13:51Z","published":"2025-01-18T21:44:38Z","title":"CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention","summary":"  Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.\n","authors":["Alexandru Dimofte","Glenn Anta Bucagu","Thorir Mar Ingolfsson","Xiaying Wang","Andrea Cossettini","Luca Benini","Yawei Li"],"pdf_url":"https://arxiv.org/pdf/2501.10885v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15330v1","updated":"2025-06-18T10:10:02Z","published":"2025-06-18T10:10:02Z","title":"Universal Laboratory Model: prognosis of abnormal clinical outcomes\n  based on routine tests","summary":"  Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.\n","authors":["Pavel Karpov","Ilya Petrenkov","Ruslan Raiman"],"pdf_url":"https://arxiv.org/pdf/2506.15330v1.pdf","comment":"7 pages, 2 figues"},{"id":"http://arxiv.org/abs/2506.12708v2","updated":"2025-06-18T10:04:59Z","published":"2025-06-15T03:41:34Z","title":"Serving Large Language Models on Huawei CloudMatrix384","summary":"  The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.\n","authors":["Pengfei Zuo","Huimin Lin","Junbo Deng","Nan Zou","Xingkun Yang","Yingyu Diao","Weifeng Gao","Ke Xu","Zhangyu Chen","Shirui Lu","Zhao Qiu","Peiyang Li","Xianyu Chang","Zhengzhong Yu","Fangzheng Miao","Jia Zheng","Ying Li","Yuan Feng","Bei Wang","Zaijian Zong","Mosong Zhou","Wenli Zhou","Houjiang Chen","Xingyu Liao","Yipeng Li","Wenxiao Zhang","Ping Zhu","Yinggang Wang","Chuanjie Xiao","Depeng Liang","Dong Cao","Juncheng Liu","Yongqiang Yang","Xiaolong Bai","Yi Li","Huaguo Xie","Huatao Wu","Zhibin Yu","Lv Chen","Hu Liu","Yujun Ding","Haipei Zhu","Jing Xia","Yi Xiong","Zhou Yu","Heng Liao"],"pdf_url":"https://arxiv.org/pdf/2506.12708v2.pdf","comment":"59 pages, 24 figures"},{"id":"http://arxiv.org/abs/2506.15329v1","updated":"2025-06-18T10:01:17Z","published":"2025-06-18T10:01:17Z","title":"When and How Unlabeled Data Provably Improve In-Context Learning","summary":"  Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.\n","authors":["Yingcong Li","Xiangyu Chang","Muti Kara","Xiaofeng Liu","Amit Roy-Chowdhury","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2506.15329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15315v1","updated":"2025-06-18T09:44:13Z","published":"2025-06-18T09:44:13Z","title":"Proximal Operators of Sorted Nonconvex Penalties","summary":"  This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.\n","authors":["Anne Gagneux","Mathurin Massias","Emmanuel Soubies"],"pdf_url":"https://arxiv.org/pdf/2506.15315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15309v1","updated":"2025-06-18T09:39:51Z","published":"2025-06-18T09:39:51Z","title":"Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation","summary":"  Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.\n","authors":["Júlia Vilalta-Mor","Alexis Molina","Laura Ortega Varga","Isaac Filella-Merce","Victor Guallar"],"pdf_url":"https://arxiv.org/pdf/2506.15309v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.15307v1","updated":"2025-06-18T09:36:57Z","published":"2025-06-18T09:36:57Z","title":"SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language\n  Models Using Forward-Only Passes","summary":"  Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.\n","authors":["Jinglong Luo","Zhuo Zhang","Yehong Zhang","Shiyu Liu","Ye Dong","Xun Zhou","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15305v1","updated":"2025-06-18T09:35:50Z","published":"2025-06-18T09:35:50Z","title":"Conditional Generative Modeling for Enhanced Credit Risk Management in\n  Supply Chain Finance","summary":"  The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.\n","authors":["Qingkai Zhang","L. Jeff Hong","Houmin Yan"],"pdf_url":"https://arxiv.org/pdf/2506.15305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15304v1","updated":"2025-06-18T09:35:33Z","published":"2025-06-18T09:35:33Z","title":"ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification","summary":"  Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.\n","authors":["Negar Foroutan","Jakhongir Saydaliev","Ye Eun Kim","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2506.15304v1.pdf","comment":"Submitted to EMNLP"},{"id":"http://arxiv.org/abs/2506.12749v2","updated":"2025-06-18T09:25:11Z","published":"2025-06-15T07:13:52Z","title":"Free Privacy Protection for Wireless Federated Learning: Enjoy It or\n  Suffer from It?","summary":"  Inherent communication noises have the potential to preserve privacy for\nwireless federated learning (WFL) but have been overlooked in digital\ncommunication systems predominantly using floating-point number standards,\ne.g., IEEE 754, for data storage and transmission. This is due to the\npotentially catastrophic consequences of bit errors in floating-point numbers,\ne.g., on the sign or exponent bits. This paper presents a novel channel-native\nbit-flipping differential privacy (DP) mechanism tailored for WFL, where\ntransmit bits are randomly flipped and communication noises are leveraged, to\ncollectively preserve the privacy of WFL in digital communication systems. The\nkey idea is to interpret the bit perturbation at the transmitter and bit errors\ncaused by communication noises as a bit-flipping DP process. This is achieved\nby designing a new floating-point-to-fixed-point conversion method that only\ntransmits the bits in the fraction part of model parameters, hence eliminating\nthe need for transmitting the sign and exponent bits and preventing the\ncatastrophic consequence of bit errors. We analyze a new metric to measure the\nbit-level distance of the model parameters and prove that the proposed\nmechanism satisfies (\\lambda,\\epsilon)-R\\'enyi DP and does not violate the WFL\nconvergence. Experiments validate privacy and convergence analysis of the\nproposed mechanism and demonstrate its superiority to the state-of-the-art\nGaussian mechanisms that are channel-agnostic and add Gaussian noise for\nprivacy protection.\n","authors":["Weicai Li","Tiejun Lv","Xiyu Zhao","Xin Yuan","Wei Ni"],"pdf_url":"https://arxiv.org/pdf/2506.12749v2.pdf","comment":"16 pages, 8 figures, accepted by IEEE Transactions on Information\n  Forensics and Security"},{"id":"http://arxiv.org/abs/2407.13538v3","updated":"2025-06-18T09:24:12Z","published":"2024-07-18T14:10:50Z","title":"EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion\n  Models","summary":"  High-resolution time series data are crucial for the operation and planning\nof energy systems such as electrical power systems and heating systems. Such\ndata often cannot be shared due to privacy concerns, necessitating the use of\nsynthetic data. However, high-resolution time series data is difficult to model\ndue to its inherent high dimensionality and complex temporal dependencies.\nLeveraging the recent development of generative AI, especially diffusion\nmodels, we propose EnergyDiff, a universal data generation framework for energy\ntime series data. EnergyDiff builds on state-of-the-art denoising diffusion\nprobabilistic models, utilizing a proposed denoising network dedicated to\nhigh-resolution time series data and introducing a novel Marginal Calibration\ntechnique. Our extensive experimental results demonstrate that EnergyDiff\nachieves significant improvement in capturing the temporal dependencies and\nmarginal distributions compared to baselines, particularly at the 1-minute\nresolution. EnergyDiff's universality is validated across diverse energy\ndomains (e.g., electricity demand, heat pump, PV, multiple time resolutions (1\nminute, 15 minutes, 30 minutes and 1 hour), and at both customer and\ntransformer levels.\n","authors":["Nan Lin","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2407.13538v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.15289v1","updated":"2025-06-18T09:15:18Z","published":"2025-06-18T09:15:18Z","title":"DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for\n  Optimizing Large-Scale EV Charging Infrastructure","summary":"  The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.\n","authors":["Chuan Li","Shunyu Zhao","Vincent Gauthier","Hassine Moungla"],"pdf_url":"https://arxiv.org/pdf/2506.15289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11093v2","updated":"2025-06-18T09:01:09Z","published":"2024-04-17T06:17:08Z","title":"Simulating Non-Markovian Open Quantum Dynamics with Neural Quantum\n  States","summary":"  Reducing computational scaling for simulating non-Markovian dissipative\ndynamics using artificial neural networks is both a major focus and formidable\nchallenge in open quantum systems. To enable neural quantum states (NQSs), we\nencode environmental memory in dissipatons (quasiparticles with characteristic\nlifetimes), yielding the dissipaton-embedded quantum master equation (DQME).\nThe resulting NQS-DQME framework achieves compact representation of many-body\ncorrelations and non-Markovian memory. Benchmarking against numerically exact\nhierarchical equations of motion confirms NQS-DQME maintains comparable\naccuracy while enhancing scalability and interpretability. This methodology\nopens new paths to explore non-Markovian open quantum dynamics in previously\nintractable systems.\n","authors":["Long Cao","Liwei Ge","Daochi Zhang","Xiang Li","Yao Wang","Rui-Xue Xu","YiJing Yan","Xiao Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.11093v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03943v3","updated":"2025-06-18T08:58:45Z","published":"2024-10-04T22:00:13Z","title":"Oscillatory State-Space Models","summary":"  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba and LRU by nearly 2x on a sequence modeling task with\nsequences of length 50k.\n","authors":["T. Konstantin Rusch","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.03943v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2409.06525v3","updated":"2025-06-18T08:58:10Z","published":"2024-09-10T14:02:34Z","title":"MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based\n  Likelihood Estimation","summary":"  We introduce MENSA, a novel deep learning model for multi-event survival\nanalysis, which predicts the time until an instance experiences multiple\ndistinct events based on its features. MENSA learns a shared representation of\nthe input features while capturing the complex dependence structures between\nevents. In practice, it optimizes the sum of the traditional negative\nlog-likelihood across events and a novel trajectory-based likelihood, which\nencourages the model to learn the temporal order in which events occur.\nExperiments on real-world clinical datasets demonstrate that MENSA improves\nrisk and time-to-event prediction compared to state-of-the-art models across\nsingle-event, competing-risk, and multi-event settings. Moreover, MENSA\nachieves this with fewer parameters and lower computational cost (FLOPs) than\nseveral deep learning baselines, particularly in high-dimensional feature\nspaces (more than 100 features).\n","authors":["Christian Marius Lillelund","Ali Hossein Gharari Foomani","Weijie Sun","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2409.06525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05185v3","updated":"2025-06-18T08:55:23Z","published":"2024-04-08T04:22:55Z","title":"Convergence analysis of controlled particle systems arising in deep\n  learning: from finite to infinite sample size","summary":"  This paper deals with a class of neural SDEs and studies the limiting\nbehavior of the associated sampled optimal control problems as the sample size\ngrows to infinity. The neural SDEs with $N$ samples can be linked to the\n$N$-particle systems with centralized control. We analyze the\nHamilton-Jacobi-Bellman equation corresponding to the $N$-particle system and\nestablish regularity results which are uniform in $N$. The uniform regularity\nestimates are obtained by the stochastic maximum principle and the analysis of\na backward stochastic Riccati equation. Using these uniform regularity results,\nwe show the convergence of the minima of the objective functionals and optimal\nparameters of the neural SDEs as the sample size $N$ tends to infinity. The\nlimiting objects can be identified with suitable functions defined on the\nWasserstein space of Borel probability measures. Furthermore, quantitative\nconvergence rates are also obtained.\n","authors":["Huafu Liao","Alpár R. Mészáros","Chenchen Mou","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.05185v3.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2408.09194v2","updated":"2025-06-18T08:49:21Z","published":"2024-08-17T13:12:04Z","title":"DRL-Based Resource Allocation for Motion Blur Resistant Federated\n  Self-Supervised Learning in IoV","summary":"  In the Internet of Vehicles (IoV), Federated Learning (FL) provides a\nprivacy-preserving solution by aggregating local models without sharing data.\nTraditional supervised learning requires image data with labels, but data\nlabeling involves significant manual effort. Federated Self-Supervised Learning\n(FSSL) utilizes Self-Supervised Learning (SSL) for local training in FL,\neliminating the need for labels while protecting privacy. Compared to other SSL\nmethods, Momentum Contrast (MoCo) reduces the demand for computing resources\nand storage space by creating a dictionary. However, using MoCo in FSSL\nrequires uploading the local dictionary from vehicles to Base Station (BS),\nwhich poses a risk of privacy leakage. Simplified Contrast (SimCo) addresses\nthe privacy leakage issue in MoCo-based FSSL by using dual temperature instead\nof a dictionary to control sample distribution. Additionally, considering the\nnegative impact of motion blur on model aggregation, and based on SimCo, we\npropose a motion blur-resistant FSSL method, referred to as BFSSL. Furthermore,\nwe address energy consumption and delay in the BFSSL process by proposing a\nDeep Reinforcement Learning (DRL)-based resource allocation scheme, called\nDRL-BFSSL. In this scheme, BS allocates the Central Processing Unit (CPU)\nfrequency and transmission power of vehicles to minimize energy consumption and\nlatency, while aggregating received models based on the motion blur level.\nSimulation results validate the effectiveness of our proposed aggregation and\nresource allocation methods.\n","authors":["Xueying Gu","Qiong Wu","Pingyi Fan","Qiang Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2408.09194v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at: https://github.com/qiongwu86/DRL-BFSSL"},{"id":"http://arxiv.org/abs/2506.15271v1","updated":"2025-06-18T08:46:59Z","published":"2025-06-18T08:46:59Z","title":"Unlocking Post-hoc Dataset Inference with Synthetic Data","summary":"  The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.\n","authors":["Bihe Zhao","Pratyush Maini","Franziska Boenisch","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2506.15271v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15264v1","updated":"2025-06-18T08:40:49Z","published":"2025-06-18T08:40:49Z","title":"Centroid Approximation for Byzantine-Tolerant Federated Learning","summary":"  Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.\n","authors":["Mélanie Cambus","Darya Melnyk","Tijana Milentijević","Stefan Schmid"],"pdf_url":"https://arxiv.org/pdf/2506.15264v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.13104v2","updated":"2025-06-18T08:40:30Z","published":"2024-11-20T07:59:35Z","title":"DRL-Based Optimization for AoI and Energy Consumption in C-V2X Enabled\n  IoV","summary":"  To address communication latency issues, the Third Generation Partnership\nProject (3GPP) has defined Cellular-Vehicle to Everything (C-V2X) technology,\nwhich includes Vehicle-to-Vehicle (V2V) communication for direct\nvehicle-to-vehicle communication. However, this method requires vehicles to\nautonomously select communication resources based on the Semi-Persistent\nScheduling (SPS) protocol, which may lead to collisions due to different\nvehicles sharing the same communication resources, thereby affecting\ncommunication effectiveness. Non-Orthogonal Multiple Access (NOMA) is\nconsidered a potential solution for handling large-scale vehicle communication,\nas it can enhance the Signal-to-Interference-plus-Noise Ratio (SINR) by\nemploying Successive Interference Cancellation (SIC), thereby reducing the\nnegative impact of communication collisions. When evaluating vehicle\ncommunication performance, traditional metrics such as reliability and\ntransmission delay present certain contradictions. Introducing the new metric\nAge of Information (AoI) provides a more comprehensive evaluation of\ncommunication system. Additionally, to ensure service quality, user terminals\nneed to possess high computational capabilities, which may lead to increased\nenergy consumption, necessitating a trade-off between communication energy\nconsumption and effectiveness. Given the complexity and dynamics of\ncommunication systems, Deep Reinforcement Learning (DRL) serves as an\nintelligent learning method capable of learning optimal strategies in dynamic\nenvironments. Therefore, this paper analyzes the effects of multi-priority\nqueues and NOMA on AoI in the C-V2X vehicular communication system and proposes\nan energy consumption and AoI optimization method based on DRL. Finally,\nthrough comparative simulations with baseline methods, the proposed approach\ndemonstrates its advances in terms of energy consumption and AoI.\n","authors":["Zheng Zhang","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2411.13104v2.pdf","comment":"This paper has been accepted by IEEE Transactions on Green\n  Communications and Networking. The source code has been released at:\n  https://github.com/qiongwu86/DRL-Based-Optimization-for-Information-of-Age-and-Energy-Consumption-in-C-V2X-Enabled-IoV"},{"id":"http://arxiv.org/abs/2506.15263v1","updated":"2025-06-18T08:40:22Z","published":"2025-06-18T08:40:22Z","title":"Minimizing Structural Vibrations via Guided Flow Matching Design\n  Optimization","summary":"  Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.\n","authors":["Jan van Delden","Julius Schultz","Sebastian Rothe","Christian Libner","Sabine C. Langer","Timo Lüddecke"],"pdf_url":"https://arxiv.org/pdf/2506.15263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04524v2","updated":"2025-06-18T08:39:46Z","published":"2025-04-06T15:48:26Z","title":"Trust Region Preference Approximation: A simple and stable reinforcement\n  learning algorithm for LLM reasoning","summary":"  Recently, Large Language Models (LLMs) have rapidly evolved, approaching\nArtificial General Intelligence (AGI) while benefiting from large-scale\nreinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent\nreward-based optimization algorithms, such as Proximal Policy Optimization\n(PPO) and Group Relative Policy Optimization (GRPO) have achieved significant\nperformance on reasoning tasks, whereas preference-based optimization\nalgorithms such as Direct Preference Optimization (DPO) significantly improve\nthe performance of LLMs on human alignment. However, despite the strong\nperformance of reward-based optimization methods in alignment tasks , they\nremain vulnerable to reward hacking. Furthermore, preference-based algorithms\n(such as Online DPO) haven't yet matched the performance of reward-based\noptimization algorithms (like PPO) on reasoning tasks, making their exploration\nin this specific area still a worthwhile pursuit. Motivated by these\nchallenges, we propose the Trust Region Preference Approximation (TRPA)\nalgorithm, which integrates rule-based optimization with preference-based\noptimization for reasoning tasks. As a preference-based algorithm, TRPA\nnaturally eliminates the reward hacking issue. TRPA constructs preference\nlevels using predefined rules, forms corresponding preference pairs, and\nleverages a novel optimization algorithm for RL training with a theoretical\nmonotonic improvement guarantee. Experimental results demonstrate that TRPA not\nonly achieves competitive performance on reasoning tasks but also exhibits\nrobust stability. The code of this paper are released and updating on\nhttps://github.com/XueruiSu/Trust-Region-Preference-Approximation.git.\n","authors":["Xuerui Su","Shufang Xie","Guoqing Liu","Yingce Xia","Renqian Luo","Peiran Jin","Zhiming Ma","Yue Wang","Zun Wang","Yuting Liu"],"pdf_url":"https://arxiv.org/pdf/2504.04524v2.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2506.14665v2","updated":"2025-06-18T08:39:15Z","published":"2025-06-17T15:56:56Z","title":"Accurate and scalable exchange-correlation with deep learning","summary":"  Density Functional Theory (DFT) is the most widely used electronic structure\nmethod for predicting the properties of molecules and materials. Although DFT\nis, in principle, an exact reformulation of the Schr\\\"odinger equation,\npractical applications rely on approximations to the unknown\nexchange-correlation (XC) functional. Most existing XC functionals are\nconstructed using a limited set of increasingly complex, hand-crafted features\nthat improve accuracy at the expense of computational efficiency. Yet, no\ncurrent approximation achieves the accuracy and generality for predictive\nmodeling of laboratory experiments at chemical accuracy -- typically defined as\nerrors below 1 kcal/mol. In this work, we present Skala, a modern deep\nlearning-based XC functional that bypasses expensive hand-designed features by\nlearning representations directly from data. Skala achieves chemical accuracy\nfor atomization energies of small molecules while retaining the computational\nefficiency typical of semi-local DFT. This performance is enabled by training\non an unprecedented volume of high-accuracy reference data generated using\ncomputationally intensive wavefunction-based methods. Notably, Skala\nsystematically improves with additional training data covering diverse\nchemistry. By incorporating a modest amount of additional high-accuracy data\ntailored to chemistry beyond atomization energies, Skala achieves accuracy\ncompetitive with the best-performing hybrid functionals across general main\ngroup chemistry, at the cost of semi-local DFT. As the training dataset\ncontinues to expand, Skala is poised to further enhance the predictive power of\nfirst-principles simulations.\n","authors":["Giulia Luise","Chin-Wei Huang","Thijs Vogels","Derk P. Kooi","Sebastian Ehlert","Stephanie Lanius","Klaas J. H. Giesbertz","Amir Karton","Deniz Gunceler","Megan Stanley","Wessel P. Bruinsma","Lin Huang","Xinran Wei","José Garrido Torres","Abylay Katbashev","Bálint Máté","Sékou-Oumar Kaba","Roberto Sordillo","Yingrong Chen","David B. Williams-Young","Christopher M. Bishop","Jan Hermann","Rianne van den Berg","Paola Gori-Giorgi"],"pdf_url":"https://arxiv.org/pdf/2506.14665v2.pdf","comment":"Main: 13 pages plus references, 11 figures and tables. Supplementary\n  information: 19 pages, 12 figures and tables. v2 update: fix rendering of\n  figure 1 and part of figure 5 in Safari PDF viewer"},{"id":"http://arxiv.org/abs/2411.19479v2","updated":"2025-06-18T08:32:27Z","published":"2024-11-29T05:34:21Z","title":"FLARE: Towards Universal Dataset Purification against Backdoor Attacks","summary":"  Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison datasets with adversary-specified triggers to implant hidden\nbackdoors, enabling malicious manipulation of model predictions. Dataset\npurification serves as a proactive defense by removing malicious training\nsamples to prevent backdoor injection at its source. We first reveal that the\ncurrent advanced purification methods rely on a latent assumption that the\nbackdoor connections between triggers and target labels in backdoor attacks are\nsimpler to learn than the benign features. We demonstrate that this assumption,\nhowever, does not always hold, especially in all-to-all (A2A) and untargeted\n(UT) attacks. As a result, purification methods that analyze the separation\nbetween the poisoned and benign samples in the input-output space or the final\nhidden layer space are less effective. We observe that this separability is not\nconfined to a single layer but varies across different hidden layers. Motivated\nby this understanding, we propose FLARE, a universal purification method to\ncounter various backdoor attacks. FLARE aggregates abnormal activations from\nall hidden layers to construct representations for clustering. To enhance\nseparation, FLARE develops an adaptive subspace selection algorithm to isolate\nthe optimal space for dividing an entire dataset into two clusters. FLARE\nassesses the stability of each cluster and identifies the cluster with higher\nstability as poisoned. Extensive evaluations on benchmark datasets demonstrate\nthe effectiveness of FLARE against 22 representative backdoor attacks,\nincluding all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and\nits robustness to adaptive attacks. Codes are available at\n\\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and\n\\href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.\n","authors":["Linshan Hou","Wei Luo","Zhongyun Hua","Songhua Chen","Leo Yu Zhang","Yiming Li"],"pdf_url":"https://arxiv.org/pdf/2411.19479v2.pdf","comment":"15 pages, This paper is accepted and will appear in TIFS (CCF-A)"},{"id":"http://arxiv.org/abs/2506.15251v1","updated":"2025-06-18T08:28:53Z","published":"2025-06-18T08:28:53Z","title":"Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model","summary":"  Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.\n","authors":["Yee Hin Chong","Peng Qu"],"pdf_url":"https://arxiv.org/pdf/2506.15251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15249v1","updated":"2025-06-18T08:26:30Z","published":"2025-06-18T08:26:30Z","title":"Context-Aware Deep Lagrangian Networks for Model Predictive Control","summary":"  Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.\n","authors":["Lucas Schulze","Jan Peters","Oleg Arenz"],"pdf_url":"https://arxiv.org/pdf/2506.15249v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2504.08200v2","updated":"2025-06-18T08:25:00Z","published":"2025-04-11T02:05:51Z","title":"Influential Bandits: Pulling an Arm May Change the Environment","summary":"  While classical formulations of multi-armed bandit problems assume that each\narm's reward is independent and stationary, real-world applications often\ninvolve non-stationary environments and interdependencies between arms. In\nparticular, selecting one arm may influence the future rewards of other arms, a\nscenario not adequately captured by existing models such as rotting bandits or\nrestless bandits. To address this limitation, we propose the influential bandit\nproblem, which models inter-arm interactions through an unknown, symmetric,\npositive semi-definite interaction matrix that governs the dynamics of arm\nlosses. We formally define this problem and establish two regret lower bounds,\nincluding a superlinear $\\Omega(T^2 / \\log^2 T)$ bound for the standard LCB\nalgorithm (loss minimization version of UCB) and an algorithm-independent\n$\\Omega(T)$ bound, which highlight the inherent difficulty of the setting. We\nthen introduce a new algorithm based on a lower confidence bound (LCB)\nestimator tailored to the structure of the loss dynamics. Under mild\nassumptions, our algorithm achieves a regret of $O(KT \\log T)$, which is nearly\noptimal in terms of its dependence on the time horizon. The algorithm is simple\nto implement and computationally efficient. Empirical evaluations on both\nsynthetic and real-world datasets demonstrate the presence of inter-arm\ninfluence and confirm the superior performance of our method compared to\nconventional bandit algorithms.\n","authors":["Ryoma Sato","Shinji Ito"],"pdf_url":"https://arxiv.org/pdf/2504.08200v2.pdf","comment":"TMLR"},{"id":"http://arxiv.org/abs/2506.13584v2","updated":"2025-06-18T08:20:25Z","published":"2025-06-16T15:07:44Z","title":"From Data-Driven to Purpose-Driven Artificial Intelligence: Systems\n  Thinking for Data-Analytic Automation of Patient Care","summary":"  In this work, we reflect on the data-driven modeling paradigm that is gaining\nground in AI-driven automation of patient care. We argue that the repurposing\nof existing real-world patient datasets for machine learning may not always\nrepresent an optimal approach to model development as it could lead to\nundesirable outcomes in patient care. We reflect on the history of data\nanalysis to explain how the data-driven paradigm rose to popularity, and we\nenvision ways in which systems thinking and clinical domain theory could\ncomplement the existing model development approaches in reaching human-centric\noutcomes. We call for a purpose-driven machine learning paradigm that is\ngrounded in clinical theory and the sociotechnical realities of real-world\noperational contexts. We argue that understanding the utility of existing\npatient datasets requires looking in two directions: upstream towards the data\ngeneration, and downstream towards the automation objectives. This\npurpose-driven perspective to AI system development opens up new methodological\nopportunities and holds promise for AI automation of patient care.\n","authors":["Daniel Anadria","Roel Dobbe","Anastasia Giachanou","Ruurd Kuiper","Richard Bartels","Wouter van Amsterdam","Íñigo Martínez de Rituerto de Troya","Carmen Zürcher","Daniel Oberski"],"pdf_url":"https://arxiv.org/pdf/2506.13584v2.pdf","comment":"The work is under review at ACM Health"},{"id":"http://arxiv.org/abs/2504.05716v3","updated":"2025-06-18T08:17:16Z","published":"2025-04-08T06:34:15Z","title":"Single-Agent vs. Multi-Agent LLM Strategies for Automated Student\n  Reflection Assessment","summary":"  We explore the use of Large Language Models (LLMs) for automated assessment\nof open-text student reflections and prediction of academic performance.\nTraditional methods for evaluating reflections are time-consuming and may not\nscale effectively in educational settings. In this work, we employ LLMs to\ntransform student reflections into quantitative scores using two assessment\nstrategies (single-agent and multi-agent) and two prompting techniques\n(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278\nreflections from 377 students over three academic terms, demonstrate that the\nsingle-agent with few-shot strategy achieves the highest match rate with human\nevaluations. Furthermore, models utilizing LLM-assessed reflection scores\noutperform baselines in both at-risk student identification and grade\nprediction tasks. These findings suggest that LLMs can effectively automate\nreflection assessment, reduce educators' workload, and enable timely support\nfor students who may need additional assistance. Our work emphasizes the\npotential of integrating advanced generative AI technologies into educational\npractices to enhance student engagement and academic success.\n","authors":["Gen Li","Li Chen","Cheng Tang","Valdemar Švábenský","Daisuke Deguchi","Takayoshi Yamashita","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2504.05716v3.pdf","comment":"Published in Proceedings of the 29th Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2025), see\n  https://doi.org/10.1007/978-981-96-8186-0_24"},{"id":"http://arxiv.org/abs/2408.05249v2","updated":"2025-06-18T07:45:09Z","published":"2024-08-08T14:36:16Z","title":"Advancing oncology with federated learning: transcending boundaries in\n  breast, lung, and prostate cancer. A systematic review","summary":"  Federated Learning (FL) has emerged as a promising solution to address the\nlimitations of centralised machine learning (ML) in oncology, particularly in\novercoming privacy concerns and harnessing the power of diverse, multi-center\ndata. This systematic review synthesises current knowledge on the\nstate-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer.\nDistinct from previous surveys, our comprehensive review critically evaluates\nthe real-world implementation and impact of FL on cancer care, demonstrating\nits effectiveness in enhancing ML generalisability, performance and data\nprivacy in clinical settings and data. We evaluated state-of-the-art advances\nin FL, demonstrating its growing adoption amid tightening data privacy\nregulations. FL outperformed centralised ML in 15 out of the 25 studies\nreviewed, spanning diverse ML models and clinical applications, and\nfacilitating integration of multi-modal information for precision medicine.\nDespite the current challenges identified in reproducibility, standardisation\nand methodology across studies, the demonstrable benefits of FL in harnessing\nreal-world data and addressing clinical needs highlight its significant\npotential for advancing cancer research. We propose that future research should\nfocus on addressing these limitations and investigating further advanced FL\nmethods, to fully harness data diversity and realise the transformative power\nof cutting-edge FL in cancer care.\n","authors":["Anshu Ankolekar","Sebastian Boie","Maryam Abdollahyan","Emanuela Gadaleta","Seyed Alireza Hasheminasab","Guang Yang","Charles Beauville","Nikolaos Dikaios","George Anthony Kastis","Michael Bussmann","Sara Khalid","Hagen Kruger","Philippe Lambin","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2408.05249v2.pdf","comment":"5 Figures, 3 Tables, 1 Supplementary Table"},{"id":"http://arxiv.org/abs/2111.07243v3","updated":"2025-06-18T07:35:46Z","published":"2021-11-14T05:18:31Z","title":"Simulating Diffusion Bridges with Score Matching","summary":"  We consider the problem of simulating diffusion bridges, which are diffusion\nprocesses that are conditioned to initialize and terminate at two given states.\nThe simulation of diffusion bridges has applications in diverse scientific\nfields and plays a crucial role in the statistical inference of\ndiscretely-observed diffusions. This is known to be a challenging problem that\nhas received much attention in the last two decades. This article contributes\nto this rich body of literature by presenting a new avenue to obtain diffusion\nbridge approximations. Our approach is based on a backward time representation\nof a diffusion bridge, which may be simulated if one can time-reverse the\nunconditioned diffusion. We introduce a variational formulation to learn this\ntime-reversal with function approximation and rely on a score matching method\nto circumvent intractability. Another iteration of our proposed methodology\napproximates the Doob's $h$-transform defining the forward time representation\nof a diffusion bridge. We discuss algorithmic considerations and extensions,\nand present numerical results on an Ornstein--Uhlenbeck process, a model from\nfinancial econometrics for interest rates, and a model from genetics for cell\ndifferentiation and development to illustrate the effectiveness of our\napproach.\n","authors":["Jeremy Heng","Valentin De Bortoli","Arnaud Doucet","James Thornton"],"pdf_url":"https://arxiv.org/pdf/2111.07243v3.pdf","comment":"Revised"},{"id":"http://arxiv.org/abs/2506.15199v1","updated":"2025-06-18T07:25:09Z","published":"2025-06-18T07:25:09Z","title":"Interpretability and Generalization Bounds for Learning Spatial Physics","summary":"  While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.\n","authors":["Alejandro Francisco Queiruga","Theo Gutman-Solo","Shuai Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.15199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20606v3","updated":"2025-06-18T07:21:56Z","published":"2025-02-28T00:10:52Z","title":"Map Space Belief Prediction for Manipulation-Enhanced Mapping","summary":"  Searching for objects in cluttered environments requires selecting efficient\nviewpoints and manipulation actions to remove occlusions and reduce uncertainty\nin object locations, shapes, and categories. In this work, we address the\nproblem of manipulation-enhanced semantic mapping, where a robot has to\nefficiently identify all objects in a cluttered shelf. Although Partially\nObservable Markov Decision Processes~(POMDPs) are standard for decision-making\nunder uncertainty, representing unstructured interactive worlds remains\nchallenging in this formalism. To tackle this, we define a POMDP whose belief\nis summarized by a metric-semantic grid map and propose a novel framework that\nuses neural networks to perform map-space belief updates to reason efficiently\nand simultaneously about object geometries, locations, categories, occlusions,\nand manipulation physics. Further, to enable accurate information gain\nanalysis, the learned belief updates should maintain calibrated estimates of\nuncertainty. Therefore, we propose Calibrated Neural-Accelerated Belief Updates\n(CNABUs) to learn a belief propagation model that generalizes to novel\nscenarios and provides confidence-calibrated predictions for unknown areas. Our\nexperiments show that our novel POMDP planner improves map completeness and\naccuracy over existing methods in challenging simulations and successfully\ntransfers to real-world cluttered shelves in zero-shot fashion.\n","authors":["Joao Marcos Correia Marques","Nils Dengler","Tobias Zaenker","Jesper Mucke","Shenlong Wang","Maren Bennewitz","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2502.20606v3.pdf","comment":"14 pages, 10 figures; Published at RSS 2025 - this version contains a\n  small fix to figure 6 which was missing a plot in the original submission"},{"id":"http://arxiv.org/abs/2303.17992v3","updated":"2025-06-18T07:19:17Z","published":"2023-03-31T12:09:36Z","title":"A Second-Order Majorant Algorithm for Nonnegative Matrix Factorization","summary":"  Nonnegative Matrix Factorization (NMF) is a fundamental tool in unsupervised\nlearning, widely used for tasks such as dimensionality reduction, feature\nextraction, representation learning, and topic modeling. Many algorithms have\nbeen developed for NMF, including the well-known Multiplicative Updates (MU)\nalgorithm, which belongs to a broader class of majorization-minimization\ntechniques. In this work, we introduce a general second-order optimization\nframework for NMF under both quadratic and $\\beta$-divergence loss functions.\nThis approach, called Second-Order Majorant (SOM), constructs a local quadratic\nmajorization of the loss function by majorizing its Hessian matrix. It includes\nMU as a special case, while enabling faster variants. In particular, we propose\nmSOM, a new algorithm within this class that leverages a tighter local\napproximation to accelerate convergence. We provide a convergence analysis,\nshowing linear convergence for individual factor updates and global convergence\nto a stationary point for the alternating version, AmSOM algorithm. Numerical\nexperiments on both synthetic and real data sets demonstrate that mSOM\nconsistently outperforms state-of-the-art algorithms across multiple loss\nfunctions.\n","authors":["Mai-Quyen Pham","Jérémy Cohen","Thierry Chonavel"],"pdf_url":"https://arxiv.org/pdf/2303.17992v3.pdf","comment":"Updated version in JMLR style. This version matches the manuscript\n  currently under review at JMLR and includes substantial improvements over the\n  original arXiv version"},{"id":"http://arxiv.org/abs/2506.15190v1","updated":"2025-06-18T07:11:48Z","published":"2025-06-18T07:11:48Z","title":"Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal\n  Behaviors","summary":"  Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.\n","authors":["Jiyi Wang","Jingyang Ke","Bo Dai","Anqi Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15190v1.pdf","comment":"9 pages and 4 figures for the main text"},{"id":"http://arxiv.org/abs/2506.15182v1","updated":"2025-06-18T06:55:38Z","published":"2025-06-18T06:55:38Z","title":"Classification of Multi-Parametric Body MRI Series Using Deep Learning","summary":"  Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.\n","authors":["Boah Kim","Tejas Sudharshan Mathai","Kimberly Helm","Peter A. Pinto","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2506.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15181v1","updated":"2025-06-18T06:53:52Z","published":"2025-06-18T06:53:52Z","title":"ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed\n  Machine Learning","summary":"  Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.\n","authors":["Bing Liu","Chengcheng Zhao","Li Chai","Peng Cheng","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00712v8","updated":"2025-06-18T06:53:36Z","published":"2023-01-02T15:09:12Z","title":"On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness\n  Results and Improved Analysis","summary":"  Bilevel optimization reveals the inner structure of otherwise oblique\noptimization problems, such as hyperparameter tuning, neural architecture\nsearch, and meta-learning. A common goal in bilevel optimization is to minimize\na hyper-objective that implicitly depends on the solution set of the\nlower-level function. Although this hyper-objective approach is widely used,\nits theoretical properties have not been thoroughly investigated in cases where\nthe lower-level functions lack strong convexity. In this work, we first provide\nhardness results to show that the goal of finding stationary points of the\nhyper-objective for nonconvex-convex bilevel optimization can be intractable\nfor zero-respecting algorithms. Then we study a class of tractable\nnonconvex-nonconvex bilevel problems when the lower-level function satisfies\nthe Polyak-{\\L}ojasiewicz (PL) condition. We show a simple first-order\nalgorithm can achieve better complexity bounds of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ and\n$\\tilde{\\mathcal{O}}(\\epsilon^{-6})$ in the deterministic, partially\nstochastic, and fully stochastic setting respectively. The complexities in the\nfirst two cases are optimal up to logarithmic factors.\n","authors":["Lesi Chen","Jing Xu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00712v8.pdf","comment":"Published in COLT 2024. This arXiv version refines Assumption 4.1\n  (d); adds discussions on related works in Appendix A; and corrects the kappa\n  dependency in the upper bounds"},{"id":"http://arxiv.org/abs/2502.02834v3","updated":"2025-06-18T06:52:53Z","published":"2025-02-05T02:31:50Z","title":"Task-Aware Virtual Training: Enhancing Generalization in\n  Meta-Reinforcement Learning for Out-of-Distribution Tasks","summary":"  Meta reinforcement learning aims to develop policies that generalize to\nunseen tasks sampled from a task distribution. While context-based meta-RL\nmethods improve task representation using task latents, they often struggle\nwith out-of-distribution (OOD) tasks. To address this, we propose Task-Aware\nVirtual Training (TAVT), a novel algorithm that accurately captures task\ncharacteristics for both training and OOD scenarios using metric-based\nrepresentation learning. Our method successfully preserves task characteristics\nin virtual tasks and employs a state regularization technique to mitigate\noverestimation errors in state-varying environments. Numerical results\ndemonstrate that TAVT significantly enhances generalization to OOD tasks across\nvarious MuJoCo and MetaWorld environments. Our code is available at\nhttps://github.com/JM-Kim-94/tavt.git.\n","authors":["Jeongmo Kim","Yisak Park","Minung Kim","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02834v3.pdf","comment":"9 pages main paper, 20 pages appendices with reference. Accepted to\n  ICML 2025"},{"id":"http://arxiv.org/abs/2506.14673v2","updated":"2025-06-18T06:49:11Z","published":"2025-06-17T16:07:36Z","title":"Uniform Mean Estimation for Heavy-Tailed Distributions via\n  Median-of-Means","summary":"  The Median of Means (MoM) is a mean estimator that has gained popularity in\nthe context of heavy-tailed data. In this work, we analyze its performance in\nthe task of simultaneously estimating the mean of each function in a class\n$\\mathcal{F}$ when the data distribution possesses only the first $p$ moments\nfor $p \\in (1,2]$. We prove a new sample complexity bound using a novel\nsymmetrization technique that may be of independent interest. Additionally, we\npresent applications of our result to $k$-means clustering with unbounded\ninputs and linear regression with general losses, improving upon existing\nworks.\n","authors":["Mikael Møller Høgsgaard","Andrea Paudice"],"pdf_url":"https://arxiv.org/pdf/2506.14673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02844v3","updated":"2025-06-18T06:49:09Z","published":"2025-02-05T02:59:23Z","title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement\n  Learning","summary":"  Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystemwide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL. Our code is available at\nhttps://github.com/sunwoolee0504/WALL.\n","authors":["Sunwoo Lee","Jaebak Hwang","Yonghyeon Jo","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02844v3.pdf","comment":"9 pages main, 23 pages appendix with reference. Accepeted by ICML\n  2025"},{"id":"http://arxiv.org/abs/2506.15176v1","updated":"2025-06-18T06:43:55Z","published":"2025-06-18T06:43:55Z","title":"In-Context Learning for Gradient-Free Receiver Adaptation: Principles,\n  Applications, and Theory","summary":"  In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.\n","authors":["Matteo Zecchin","Tomer Raviv","Dileep Kalathil","Krishna Narayanan","Nir Shlezinger","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2506.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07765v2","updated":"2025-06-18T06:35:16Z","published":"2025-05-12T17:12:53Z","title":"Solving Nonlinear PDEs with Sparse Radial Basis Function Networks","summary":"  We propose a novel framework for solving nonlinear PDEs using sparse radial\nbasis function (RBF) networks. Sparsity-promoting regularization is employed to\nprevent over-parameterization and reduce redundant features. This work is\nmotivated by longstanding challenges in traditional RBF collocation methods,\nalong with the limitations of physics-informed neural networks (PINNs) and\nGaussian process (GP) approaches, aiming to blend their respective strengths in\na unified framework. The theoretical foundation of our approach lies in the\nfunction space of Reproducing Kernel Banach Spaces (RKBS) induced by\none-hidden-layer neural networks of possibly infinite width. We prove a\nrepresenter theorem showing that the sparse optimization problem in the RKBS\nadmits a finite solution and establishes error bounds that offer a foundation\nfor generalizing classical numerical analysis. The algorithmic framework is\nbased on a three-phase algorithm to maintain computational efficiency through\nadaptive feature selection, second-order optimization, and pruning of inactive\nneurons. Numerical experiments demonstrate the effectiveness of our method and\nhighlight cases where it offers notable advantages over GP approaches. This\nwork opens new directions for adaptive PDE solvers grounded in rigorous\nanalysis with efficient, learning-inspired implementation.\n","authors":["Zihan Shao","Konstantin Pieper","Xiaochuan Tian"],"pdf_url":"https://arxiv.org/pdf/2505.07765v2.pdf","comment":"51 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.00396v2","updated":"2025-06-18T06:30:20Z","published":"2024-10-01T04:39:04Z","title":"Dynamic neuron approach to deep neural networks: Decoupling neurons for\n  renormalization group analysis","summary":"  Deep neural network architectures often consist of repetitive structural\nelements. We introduce an approach that reveals these patterns and can be\nbroadly applied to the study of deep learning. Similarly to how a power strip\nhelps untangle and organize complex cable connections, this approach treats\nneurons as additional degrees of freedom in interactions, simplifying the\nstructure and enhancing the intuitive understanding of interactions within deep\nneural networks. Furthermore, it reveals the translational symmetry of deep\nneural networks, which simplifies the application of the renormalization group\ntransformation-a method that effectively analyzes the scaling behavior of the\nsystem. By utilizing translational symmetry and renormalization group\ntransformations, we can analyze critical phenomena. This approach may open new\navenues for studying deep neural networks using statistical physics.\n","authors":["Donghee Lee","Hye-Sung Lee","Jaeok Yi"],"pdf_url":"https://arxiv.org/pdf/2410.00396v2.pdf","comment":"Version matching the publication"},{"id":"http://arxiv.org/abs/2411.11171v5","updated":"2025-06-18T06:29:57Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Transparent, Compact and Competitive German-Only Language\n  Models from Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v5.pdf","comment":"camera ready @ACL25;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"},{"id":"http://arxiv.org/abs/2506.15366v1","updated":"2025-06-18T11:34:15Z","published":"2025-06-18T11:34:15Z","title":"Performative Validity of Recourse Explanations","summary":"  When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.\n","authors":["Gunnar König","Hidde Fokkema","Timo Freiesleben","Celestine Mendler-Dünner","Ulrike Von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2506.15366v1.pdf","comment":"34 pages, 3 figures, 1 table, Preprint"}],"Multimedia":[{"id":"http://arxiv.org/abs/2506.15677v1","updated":"2025-06-18T17:58:17Z","published":"2025-06-18T17:58:17Z","title":"Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence","summary":"  AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.\n","authors":["Yining Hong","Rui Sun","Bingxuan Li","Xingcheng Yao","Maxine Wu","Alexander Chien","Da Yin","Ying Nian Wu","Zhecan James Wang","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2506.15677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12573v2","updated":"2025-06-18T17:03:04Z","published":"2025-06-14T16:58:42Z","title":"Video-Guided Text-to-Music Generation Using Public Domain Movie\n  Collections","summary":"  Despite recent advancements in music generation systems, their application in\nfilm production remains limited, as they struggle to capture the nuances of\nreal-world filmmaking, where filmmakers consider multiple factors-such as\nvisual content, dialogue, and emotional tone-when selecting or composing music\nfor a scene. This limitation primarily stems from the absence of comprehensive\ndatasets that integrate these elements. To address this gap, we introduce Open\nScreen Sound Library (OSSL), a dataset consisting of movie clips from public\ndomain films, totaling approximately 36.5 hours, paired with high-quality\nsoundtracks and human-annotated mood information. To demonstrate the\neffectiveness of our dataset in improving the performance of pre-trained models\non film music generation tasks, we introduce a new video adapter that enhances\nan autoregressive transformer-based text-to-music model by adding video-based\nconditioning. Our experimental results demonstrate that our proposed approach\neffectively enhances MusicGen-Medium in terms of both objective measures of\ndistributional and paired fidelity, and subjective compatibility in mood and\ngenre. The dataset and code are available at\nhttps://havenpersona.github.io/ossl-v1.\n","authors":["Haven Kim","Zachary Novack","Weihan Xu","Julian McAuley","Hao-Wen Dong"],"pdf_url":"https://arxiv.org/pdf/2506.12573v2.pdf","comment":"ISMIR 2025 regular paper. Dataset, code, and demo available at\n  https://havenpersona.github.io/ossl-v1"},{"id":"http://arxiv.org/abs/2503.08221v2","updated":"2025-06-18T15:03:21Z","published":"2025-03-11T09:40:31Z","title":"EgoBlind: Towards Egocentric Visual Assistance for the Blind","summary":"  We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,392 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 5,311 questions directly posed or generated and verified by blind\nindividuals to reflect their in-situation needs for visual assistance under\nvarious scenarios. We provide each question with an average of 3 reference\nanswers to alleviate subjective evaluation. Using EgoBlind, we comprehensively\nevaluate 16 advanced MLLMs and find that all models struggle, with the best\nperformers achieving accuracy near 60\\%, far behind human performance of\n87.4\\%. To guide future advancements, we identify and summarize major\nlimitations of existing MLLMs in egocentric visual assistance for the blind and\nexplore heuristic solutions for improvement. With these efforts, we hope\nEgoBlind can serve as a valuable foundation for developing more effective AI\nassistants to enhance the independence of the blind individuals' lives. Data\nand evaluation code are available at https://github.com/doc-doc/EgoBlind.\n","authors":["Junbin Xiao","Nanxin Huang","Hao Qiu","Zhulin Tao","Xun Yang","Richang Hong","Meng Wang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2503.08221v2.pdf","comment":"We extend and resplit the dataset"},{"id":"http://arxiv.org/abs/2408.05412v2","updated":"2025-06-18T10:45:08Z","published":"2024-08-10T02:46:11Z","title":"Style-Preserving Lip Sync via Audio-Aware Style Reference","summary":"  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n","authors":["Weizhi Zhong","Jichang Li","Yinqi Cai","Ming Li","Feng Gao","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2408.05412v2.pdf","comment":"submitted to IEEE Transactions on Multimedia(TMM)"},{"id":"http://arxiv.org/abs/2506.15298v1","updated":"2025-06-18T09:29:51Z","published":"2025-06-18T09:29:51Z","title":"MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and\n  Visual Question Answering","summary":"  Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.\n","authors":["Xinqi Fan","Jingting Li","John See","Moi Hoon Yap","Wen-Huang Cheng","Xiaobai Li","Xiaopeng Hong","Su-Jing Wang","Adrian K. Davision"],"pdf_url":"https://arxiv.org/pdf/2506.15298v1.pdf","comment":"Micro-Expression Grand Challenge (MEGC) at ACM MM 2025"},{"id":"http://arxiv.org/abs/2506.15276v1","updated":"2025-06-18T08:57:12Z","published":"2025-06-18T08:57:12Z","title":"MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion","summary":"  Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.\n","authors":["Jun Zhu","Xinfeng Zhang","Lv Tang","JunHao Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.15276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15228v1","updated":"2025-06-18T08:11:27Z","published":"2025-06-18T08:11:27Z","title":"ABC: Adaptive BayesNet Structure Learning for Computational Scalable\n  Multi-task Image Compression","summary":"  Neural Image Compression (NIC) has revolutionized image compression with its\nsuperior rate-distortion performance and multi-task capabilities, supporting\nboth human visual perception and machine vision tasks. However, its widespread\nadoption is hindered by substantial computational demands. While existing\napproaches attempt to address this challenge through module-specific\noptimizations or pre-defined complexity levels, they lack comprehensive control\nover computational complexity. We present ABC (Adaptive BayesNet structure\nlearning for computational scalable multi-task image Compression), a novel,\ncomprehensive framework that achieves computational scalability across all NIC\ncomponents through Bayesian network (BayesNet) structure learning. ABC\nintroduces three key innovations: (i) a heterogeneous bipartite BayesNet\n(inter-node structure) for managing neural backbone computations; (ii) a\nhomogeneous multipartite BayesNet (intra-node structure) for optimizing\nautoregressive unit processing; and (iii) an adaptive control module that\ndynamically adjusts the BayesNet structure based on device capabilities, input\ndata complexity, and downstream task requirements. Experiments demonstrate that\nABC enables full computational scalability with better complexity adaptivity\nand broader complexity control span, while maintaining competitive compression\nperformance. Furthermore, the framework's versatility allows integration with\nvarious NIC architectures that employ BayesNet representations, making it a\nrobust solution for ensuring computational scalability in NIC applications.\nCode is available in https://github.com/worldlife123/cbench_BaSIC.\n","authors":["Yufeng Zhang","Wenrui Dai","Hang Yu","Shizhan Liu","Junhui Hou","Jianguo Li","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2506.15228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15154v1","updated":"2025-06-18T05:51:36Z","published":"2025-06-18T05:51:36Z","title":"SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning","summary":"  Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.\n","authors":["Anuradha Chopra","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2506.15154v1.pdf","comment":"14 pages, 2 figures, Accepted to AIMC 2025"},{"id":"http://arxiv.org/abs/2412.14018v2","updated":"2025-06-18T04:36:29Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video\n  Generation","summary":"  Surgical video generation can enhance medical education and research, but\nexisting methods lack fine-grained motion control and realism. We introduce\nSurgSora, a framework that generates high-fidelity, motion-controllable\nsurgical videos from a single input frame and user-specified motion cues.\nUnlike prior approaches that treat objects indiscriminately or rely on\nground-truth segmentation masks, SurgSora leverages self-predicted object\nfeatures and depth information to refine RGB appearance and optical flow for\nprecise video synthesis. It consists of three key modules: (1) the Dual\nSemantic Injector, which extracts object-specific RGB-D features and\nsegmentation cues to enhance spatial representations; (2) the Decoupled Flow\nMapper, which fuses multi-scale optical flow with semantic features for\nrealistic motion dynamics; and (3) the Trajectory Controller, which estimates\nsparse optical flow and enables user-guided object movement. By conditioning\nthese enriched features within the Stable Video Diffusion, SurgSora achieves\nstate-of-the-art visual authenticity and controllability in advancing surgical\nvideo synthesis, as demonstrated by extensive quantitative and qualitative\ncomparisons. Our human evaluation in collaboration with expert surgeons further\ndemonstrates the high realism of SurgSora-generated videos, highlighting the\npotential of our method for surgical training and education. Our project is\navailable at https://surgsora.github.io/surgsora.github.io.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13155v2","updated":"2025-06-18T03:26:43Z","published":"2022-06-27T09:58:34Z","title":"Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich\n  Document Understanding","summary":"  Multi-modal document pre-trained models have proven to be very effective in a\nvariety of visually-rich document understanding (VrDU) tasks. Though existing\ndocument pre-trained models have achieved excellent performance on standard\nbenchmarks for VrDU, the way they model and exploit the interactions between\nvision and language on documents has hindered them from better generalization\nability and higher accuracy. In this work, we investigate the problem of\nvision-language joint representation learning for VrDU mainly from the\nperspective of supervisory signals. Specifically, a pre-training paradigm\ncalled Bi-VLDoc is proposed, in which a bidirectional vision-language\nsupervision strategy and a vision-language hybrid-attention mechanism are\ndevised to fully explore and utilize the interactions between these two\nmodalities, to learn stronger cross-modal document representations with richer\nsemantics. Benefiting from the learned informative cross-modal document\nrepresentations, Bi-VLDoc significantly advances the state-of-the-art\nperformance on three widely-used document understanding benchmarks, including\nForm Understanding (from 85.14% to 93.44%), Receipt Information Extraction\n(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).\nOn Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance\ncompared to previous single model methods.\n","authors":["Chuwei Luo","Guozhi Tang","Qi Zheng","Cong Yao","Lianwen Jin","Chenliang Li","Yang Xue","Luo Si"],"pdf_url":"https://arxiv.org/pdf/2206.13155v2.pdf","comment":"IJDAR 2025"},{"id":"http://arxiv.org/abs/2407.06060v3","updated":"2025-06-18T02:04:36Z","published":"2024-07-08T16:01:04Z","title":"MERGE -- A Bimodal Audio-Lyrics Dataset for Static Music Emotion\n  Recognition","summary":"  The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a lack of public,\nsizable and quality-controlled bimodal databases has hampered the development\nand improvement of bimodal audio-lyrics systems. This article proposes three\nnew audio, lyrics, and bimodal MER research datasets, collectively referred to\nas MERGE, which were created using a semi-automatic approach. To\ncomprehensively assess the proposed datasets and establish a baseline for\nbenchmarking, we conducted several experiments for each modality, using feature\nengineering, machine learning, and deep learning methodologies. Additionally,\nwe propose and validate fixed train-validation-test splits. The obtained\nresults confirm the viability of the proposed datasets, achieving the best\noverall result of 81.74\\% F1-score for bimodal classification.\n","authors":["Pedro Lima Louro","Hugo Redinho","Ricardo Santos","Ricardo Malheiro","Renato Panda","Rui Pedro Paiva"],"pdf_url":"https://arxiv.org/pdf/2407.06060v3.pdf","comment":"18 pages, 2 figures, 8 tables, submitted to IEEE Transactions on\n  Affective Computing"}]}}